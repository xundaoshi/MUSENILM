{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision pandas scikit-learn matplotlib tables pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa07b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "import math\n",
    "from collections import defaultdict, OrderedDict\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, matthews_corrcoef\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85b96e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.HDFStore('../ukdale.h5') #需要按照自己文件位置修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d105b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_meter(store=None, building=1, meter=1, period='1min', cutoff=1000.):\n",
    "    key = '/building{}/elec/meter{}'.format(building,meter)\n",
    "    m = store[key]\n",
    "    v = m.values.flatten()\n",
    "    t = m.index\n",
    "    s = pd.Series(v, index=t).clip(0.,cutoff)\n",
    "    s[s<10.] = 0.\n",
    "    return s.resample('1s').ffill(limit=300).fillna(0.).resample(period).mean().tz_convert('UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3241ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series(datastore, house, label, cutoff):\n",
    "    filename = './house_%1d_labels.dat' %house   #需要按照自己文件位置修改\n",
    "    print(filename)\n",
    "    labels = pd.read_csv(filename, delimiter=' ', header=None, index_col=0).to_dict()[1]\n",
    "    \n",
    "    for i in labels:\n",
    "        if labels[i] == label:\n",
    "            print(i, labels[i])\n",
    "            s = resample_meter(store, house, i, '1min', cutoff)\n",
    "            #s = resample_meter(store, house, i, '6s', cutoff)\n",
    "    \n",
    "    s.index.name = 'datetime'\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9652a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../house_1_labels.dat\n",
      "1 aggregate\n",
      "../house_1_labels.dat\n",
      "10 kettle\n",
      "../house_1_labels.dat\n",
      "12 fridge\n",
      "../house_1_labels.dat\n",
      "5 washing_machine\n",
      "../house_1_labels.dat\n",
      "13 microwave\n",
      "../house_1_labels.dat\n",
      "6 dishwasher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_19192\\3829807372.py:17: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_1_train = ds_1[pd.datetime(2013,4,12):pd.datetime(2014,12,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\3829807372.py:17: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_1_train = ds_1[pd.datetime(2013,4,12):pd.datetime(2014,12,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\3829807372.py:18: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_1_valid = ds_1[pd.datetime(2014,12,15):]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\3829807372.py:18: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_1_valid = ds_1[pd.datetime(2014,12,15):]\n"
     ]
    }
   ],
   "source": [
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_1 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_1.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_1_train = ds_1[pd.datetime(2013,4,12):pd.datetime(2014,12,15)]\n",
    "ds_1_valid = ds_1[pd.datetime(2014,12,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d357d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../house_2_labels.dat\n",
      "1 aggregate\n",
      "../house_2_labels.dat\n",
      "8 kettle\n",
      "../house_2_labels.dat\n",
      "14 fridge\n",
      "../house_2_labels.dat\n",
      "12 washing_machine\n",
      "../house_2_labels.dat\n",
      "15 microwave\n",
      "../house_2_labels.dat\n",
      "13 dish_washer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_19192\\845194013.py:19: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_2_train = ds_2[pd.Timestamp(2013,5,22):pd.Timestamp(2013,10,3,6,16)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\845194013.py:20: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_2_valid = ds_2[pd.Timestamp(2013,10,3,6,16):]\n"
     ]
    }
   ],
   "source": [
    "house = 2\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dish_washer', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_2 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_2.fillna(method='pad', inplace=True)\n",
    "\n",
    "#ds_2_train = ds_2[pd.datetime(2013,5,22):pd.datetime(2013,10,3,6,16)]\n",
    "#ds_2_valid = ds_2[pd.datetime(2013,10,3,6,16):]\n",
    "ds_2_train = ds_2[pd.Timestamp(2013,5,22):pd.Timestamp(2013,10,3,6,16)]\n",
    "ds_2_valid = ds_2[pd.Timestamp(2013,10,3,6,16):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "768b3462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../house_3_labels.dat\n",
      "1 aggregate\n",
      "../house_3_labels.dat\n",
      "2 kettle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_19192\\2982668517.py:17: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_3_train = ds_3[pd.datetime(2013,2,27):pd.datetime(2013,4,1,6,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\2982668517.py:17: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_3_train = ds_3[pd.datetime(2013,2,27):pd.datetime(2013,4,1,6,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\2982668517.py:18: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_3_valid = ds_3[pd.datetime(2013,4,1,6,15):]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\2982668517.py:18: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_3_valid = ds_3[pd.datetime(2013,4,1,6,15):]\n"
     ]
    }
   ],
   "source": [
    "house = 3\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = 0.*m\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_3 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_3.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_3_train = ds_3[pd.datetime(2013,2,27):pd.datetime(2013,4,1,6,15)]\n",
    "ds_3_valid = ds_3[pd.datetime(2013,4,1,6,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd73461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../house_4_labels.dat\n",
      "1 aggregate\n",
      "../house_4_labels.dat\n",
      "3 kettle_radio\n",
      "../house_4_labels.dat\n",
      "5 freezer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_19192\\1070642271.py:17: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_4_train = ds_4[pd.datetime(2013,3,9):pd.datetime(2013,9,24,6,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\1070642271.py:17: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_4_train = ds_4[pd.datetime(2013,3,9):pd.datetime(2013,9,24,6,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\1070642271.py:18: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_4_valid = ds_4[pd.datetime(2013,9,24,6,15):]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\1070642271.py:18: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_4_valid = ds_4[pd.datetime(2013,9,24,6,15):]\n"
     ]
    }
   ],
   "source": [
    "house = 4\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle_radio', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_4 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_4.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_4_train = ds_4[pd.datetime(2013,3,9):pd.datetime(2013,9,24,6,15)]\n",
    "ds_4_valid = ds_4[pd.datetime(2013,9,24,6,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30e7ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../house_5_labels.dat\n",
      "1 aggregate\n",
      "../house_5_labels.dat\n",
      "18 kettle\n",
      "../house_5_labels.dat\n",
      "19 fridge_freezer\n",
      "../house_5_labels.dat\n",
      "24 washer_dryer\n",
      "../house_5_labels.dat\n",
      "23 microwave\n",
      "../house_5_labels.dat\n",
      "22 dishwasher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_19192\\2498256263.py:17: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_5_train = ds_5[pd.datetime(2014,6,29):pd.datetime(2014,9,1)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\2498256263.py:17: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_5_train = ds_5[pd.datetime(2014,6,29):pd.datetime(2014,9,1)]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\2498256263.py:18: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_5_valid = ds_5[pd.datetime(2014,9,1):]\n",
      "C:\\usertemp\\xundao\\ipykernel_19192\\2498256263.py:18: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_5_valid = ds_5[pd.datetime(2014,9,1):]\n"
     ]
    }
   ],
   "source": [
    "house = 5\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge_freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washer_dryer', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_5 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_5.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_5_train = ds_5[pd.datetime(2014,6,29):pd.datetime(2014,9,1)]\n",
    "ds_5_valid = ds_5[pd.datetime(2014,9,1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7588e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1_train.reset_index().to_feather('./UKDALE_1_train.feather')\n",
    "ds_2_train.reset_index().to_feather('./UKDALE_2_train.feather')\n",
    "ds_3_train.reset_index().to_feather('./UKDALE_3_train.feather')\n",
    "ds_4_train.reset_index().to_feather('./UKDALE_4_train.feather')\n",
    "ds_5_train.reset_index().to_feather('./UKDALE_5_train.feather')\n",
    "\n",
    "ds_1_valid.reset_index().to_feather('./UKDALE_1_valid.feather')\n",
    "ds_2_valid.reset_index().to_feather('./UKDALE_2_valid.feather')\n",
    "ds_3_valid.reset_index().to_feather('./UKDALE_3_valid.feather')\n",
    "ds_4_valid.reset_index().to_feather('./UKDALE_4_valid.feather')\n",
    "ds_5_valid.reset_index().to_feather('./UKDALE_5_valid.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03dd4d",
   "metadata": {},
   "source": [
    "# Read the feather dataframe resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40daeeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status(app, threshold, min_off, min_on):\n",
    "    condition = app > threshold\n",
    "    # Find the indicies of changes in \"condition\"\n",
    "    d = np.diff(condition)\n",
    "    idx, = d.nonzero() \n",
    "\n",
    "    # We need to start things after the change in \"condition\". Therefore, \n",
    "    # we'll shift the index by 1 to the right.\n",
    "    idx += 1\n",
    "\n",
    "    if condition[0]:\n",
    "        # If the start of condition is True prepend a 0\n",
    "        idx = np.r_[0, idx]\n",
    "\n",
    "    if condition[-1]:\n",
    "        # If the end of condition is True, append the length of the array\n",
    "        idx = np.r_[idx, condition.size] # Edit\n",
    "\n",
    "    # Reshape the result into two columns\n",
    "    idx.shape = (-1,2)\n",
    "    on_events = idx[:,0].copy()\n",
    "    off_events = idx[:,1].copy()\n",
    "    assert len(on_events) == len(off_events)\n",
    "\n",
    "    if len(on_events) > 0:\n",
    "        off_duration = on_events[1:] - off_events[:-1]\n",
    "        off_duration = np.insert(off_duration, 0, 1000.)\n",
    "        on_events = on_events[off_duration > min_off]\n",
    "        off_events = off_events[np.roll(off_duration, -1) > min_off]\n",
    "        assert len(on_events) == len(off_events)\n",
    "\n",
    "        on_duration = off_events - on_events\n",
    "        on_events = on_events[on_duration > min_on]\n",
    "        off_events = off_events[on_duration > min_on]\n",
    "\n",
    "    s = app.copy()\n",
    "    #s.iloc[:] = 0.\n",
    "    s[:] = 0.\n",
    "\n",
    "    for on, off in zip(on_events, off_events):\n",
    "        #s.iloc[on:off] = 1.\n",
    "        s[on:off] = 1.\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432a478",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为get_status的函数，用于从一个电器的用电量数据中获取该电器的状态。该函数接受四个参数：app表示电器的用电量数据，threshold表示电器的开启阈值，min_off表示电器关闭持续时间的最小值，min_on表示电器开启持续时间的最小值。\n",
    "\n",
    "该函数首先根据阈值将电器的用电量数据转换为布尔型数组condition。然后，它使用numpy库中的diff函数获取condition中相邻元素的差值，并使用nonzero方法获取差值不为0的元素的索引。接着，它将索引向右移动一位，并在开头和结尾处分别添加0和数组长度。最后，它将索引按每两个元素为一组进行划分，并计算出每个组对应的电器开启和关闭时间点，并将其存储在on_events和off_events数组中。\n",
    "\n",
    "接下来，该函数使用on_events和off_events数组计算出电器关闭持续时间和开启持续时间，并筛选出持续时间大于最小值的时间段。然后，它将每个时间段内的用电量数据标记为1，其余部分标记为0，并将标记后的用电量数据存储在s数组中。\n",
    "\n",
    "最后，该函数返回标记后的用电量数据s。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13fb3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Power(data.Dataset):\n",
    "    def __init__(self, meter=None, appliance=None, status=None, \n",
    "                 length=256, border=680, max_power=1., train=False):\n",
    "        self.length = length\n",
    "        self.border = border\n",
    "        self.max_power = max_power\n",
    "        self.train = train\n",
    "\n",
    "        self.meter = meter.copy()/self.max_power\n",
    "        self.appliance = appliance.copy()/self.max_power\n",
    "        self.status = status.copy()\n",
    "\n",
    "        self.epochs = (len(self.meter) - 2*self.border) // self.length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        i = index * self.length + self.border\n",
    "        if self.train:\n",
    "            i = np.random.randint(self.border, len(self.meter) - self.length - self.border)\n",
    "\n",
    "        x = self.meter.iloc[i-self.border:i+self.length+self.border].values.astype('float32')\n",
    "        y = self.appliance.iloc[i:i+self.length].values.astype('float32')\n",
    "        s = self.status.iloc[i:i+self.length].values.astype('float32')\n",
    "        x -= x.mean()\n",
    "        \n",
    "        return x, y, s\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f127deb",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为`Power`的数据集类，用于从电器的用电量数据中生成训练和测试数据。该类的初始化函数接受五个参数：`meter`表示电表的用电量数据，`appliance`表示电器的用电量数据，`status`表示电器的状态数据，`length`表示每个样本的长度，`border`表示每个样本的边界长度，`max_power`表示用电量数据的最大值，`train`表示是否为训练数据。\n",
    "\n",
    "该类实现了两个方法：`__getitem__`和`__len__`。其中，`__getitem__`方法用于获取指定索引的样本数据，它首先计算出样本的起始位置`i`，如果是训练数据，则随机生成一个起始位置。然后，它从电表和电器的用电量数据中获取指定位置和长度的数据，并将其存储在`x`和`y`数组中。同时，它还从电器的状态数据中获取指定位置和长度的状态数据，并将其存储在`s`数组中。最后，它将`x`数组减去平均值，并返回三个数组作为样本数据。\n",
    "\n",
    "x：表示电表数据（meter data）。它是一个时间序列，包含了电表的读数。在代码中，x被定义为一个Numpy数组，包含了从(i-self.border)到(i+self.length+self.border)的电表数据。\n",
    "y：表示电器数据（appliance data）。它也是一个时间序列，包含了特定电器的能耗数据。在代码中，y被定义为一个Numpy数组，包含了从i到(i+self.length)的电器数据。\n",
    "s：表示状态数据（status data）。它也是一个时间序列，包含了与电器状态相关的数据。在代码中，s被定义为一个Numpy数组，包含了从i到(i+self.length)的状态数据。\n",
    "\n",
    "`__len__`方法用于返回整个数据集中样本的数量，它计算出整个数据集中可以生成的样本数，并返回该数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f1ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入MUSEAttention模块\n",
    "class Depth_Pointwise_Conv1d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k):\n",
    "        super().__init__()\n",
    "        if (k == 1):\n",
    "            self.depth_conv = nn.Identity()\n",
    "        else:\n",
    "            self.depth_conv = nn.Conv1d(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=in_ch,\n",
    "                kernel_size=k,\n",
    "                groups=in_ch,\n",
    "                padding=k // 2\n",
    "            )\n",
    "        self.pointwise_conv = nn.Conv1d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=1,\n",
    "            groups=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pointwise_conv(self.depth_conv(x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MUSEAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_k, d_v, h, dropout=.1):\n",
    "\n",
    "        super(MUSEAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv1 = Depth_Pointwise_Conv1d(h * d_v, d_model, 1)\n",
    "        self.conv3 = Depth_Pointwise_Conv1d(h * d_v, d_model, 3)\n",
    "        self.conv5 = Depth_Pointwise_Conv1d(h * d_v, d_model, 5)\n",
    "        self.dy_paras = nn.Parameter(torch.ones(3))\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n",
    "\n",
    "        # Self Attention\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "        if attention_weights is not None:\n",
    "            att = att * attention_weights\n",
    "        if attention_mask is not None:\n",
    "            att = att.masked_fill(attention_mask, -np.inf)\n",
    "        att = torch.softmax(att, -1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "\n",
    "        v2 = v.permute(0, 1, 3, 2).contiguous().view(b_s, -1, nk)  # bs,dim,n\n",
    "        self.dy_paras = nn.Parameter(self.softmax(self.dy_paras))\n",
    "        out2 = self.dy_paras[0] * self.conv1(v2) + self.dy_paras[1] * self.conv3(v2) + self.dy_paras[2] * self.conv5(v2)\n",
    "        out2 = out2.permute(0, 2, 1)  # bs.n.dim\n",
    "\n",
    "        out = out + out2\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858ee98",
   "metadata": {},
   "source": [
    "### MUSENILM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b41c0d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549902\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride,\n",
    "                              bias=False)\n",
    "        # self.conv=LSKblock(out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.bn(F.relu(self.conv(x)))\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        # self.upsample = nn.Upsample( scale_factor=kernel_size, mode='linear', align_corners=True)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        # return self.upsample(x)\n",
    "        # return self.drop(self.upsample(x))\n",
    "        return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='linear', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='nearest', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='bicubic', align_corners=True))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PTPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(PTPNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool1_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1 ** k, features * 2 ** k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool2_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2 ** k, features * 4 ** k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool3_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4 ** k, features * 10 ** k, kernel_size=3, padding=0)\n",
    "        self.MUSEA = MUSEAttention(d_model=30, d_k=30, d_v=30, h=8)\n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=10)\n",
    "        self.tpool3 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=15)\n",
    "        self.tpool4 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=30)\n",
    "        self.tpool5 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=25)\n",
    "        self.tpool6 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=15)\n",
    "        self.tpool7 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=5)\n",
    "\n",
    "        self.decoder = Decoder(2 * features * 12 ** k, features * 1 ** k, kernel_size=p ** 3, stride=p ** 3)\n",
    "\n",
    "        self.activation = nn.Conv1d(features * 1 ** k, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print('x:',x.shape)\n",
    "        enc1 = self.encoder1(x)\n",
    "        #print('enc1:',enc1.shape)\n",
    "        enc2 = self.encoder2(self.pool1(enc1)+self.pool1_1(enc1))\n",
    "       # print('enc2:',enc2.shape)\n",
    "        enc3 = self.encoder3(self.pool2(enc2)+self.pool2_1(enc2))\n",
    "       # print('enc3:',enc3.shape)\n",
    "        enc4 = self.encoder4(self.pool3(enc3)+self.pool3_1(enc3))\n",
    "        #print('enc4:',enc4.shape)\n",
    "        mu1 = self.MUSEA(enc4, enc4, enc4)\n",
    "        \n",
    "        tp1 = self.tpool1(mu1)\n",
    "       # print('tp1:',tp1.shape)\n",
    "        tp2 = self.tpool2(mu1)\n",
    "        #print('tp2:',tp2.shape)\n",
    "        tp3 = self.tpool3(mu1)\n",
    "       # print('tp3:',tp3.shape)\n",
    "        tp4 = self.tpool4(mu1)\n",
    "        #print('tp4:',tp4.shape)\n",
    "        tp5 = self.tpool6(mu1)\n",
    "        #print('tp5:',tp5.shape)\n",
    "        tp6 = self.tpool6(mu1)\n",
    "        #print('tp6:',tp6.shape)\n",
    "        tp7 = self.tpool7(mu1)\n",
    "        #print('tp7:',tp7.shape)\n",
    "        \n",
    "        dec = self.decoder(torch.cat([mu1, tp1, tp2, tp3, tp4, tp5, tp6, tp7], dim=1))\n",
    "        #print('dec:',dec.shape)\n",
    "\n",
    "        act = self.activation(dec)\n",
    "        #print('act:',act.shape)\n",
    "        return act\n",
    "\n",
    "\n",
    "x = torch.randn(32, 1, 270)\n",
    "x = x.to('cuda')\n",
    "model = PTPNet(1,3,32).cuda()\n",
    "#print(model)\n",
    "#print(model(x).shape)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1a1d7",
   "metadata": {},
   "source": [
    "### UNMUSENILM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "979e0eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496259\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride,\n",
    "                              bias=False)\n",
    "        # self.conv=LSKblock(out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.bn(F.relu(self.conv(x)))\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        # self.upsample = nn.Upsample( scale_factor=kernel_size, mode='linear', align_corners=True)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        # return self.upsample(x)\n",
    "        # return self.drop(self.upsample(x))\n",
    "        return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='linear', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='nearest', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='bicubic', align_corners=True))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PTPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(PTPNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool1_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1 ** k, features * 2 ** k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool2_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2 ** k, features * 4 ** k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool3_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4 ** k, features * 10 ** k, kernel_size=3, padding=0)\n",
    "        #self.MUSEA = MUSEAttention(d_model=30, d_k=30, d_v=30, h=8)\n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=10)\n",
    "        self.tpool3 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=15)\n",
    "        self.tpool4 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=30)\n",
    "        self.tpool5 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=25)\n",
    "        self.tpool6 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=15)\n",
    "        self.tpool7 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=5)\n",
    "\n",
    "        self.decoder = Decoder(2 * features * 12 ** k, features * 1 ** k, kernel_size=p ** 3, stride=p ** 3)\n",
    "\n",
    "        self.activation = nn.Conv1d(features * 1 ** k, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print('x:',x.shape)\n",
    "        enc1 = self.encoder1(x)\n",
    "        #print('enc1:',enc1.shape)\n",
    "        enc2 = self.encoder2(self.pool1(enc1)+self.pool1_1(enc1))\n",
    "       # print('enc2:',enc2.shape)\n",
    "        enc3 = self.encoder3(self.pool2(enc2)+self.pool2_1(enc2))\n",
    "       # print('enc3:',enc3.shape)\n",
    "        enc4 = self.encoder4(self.pool3(enc3)+self.pool3_1(enc3))\n",
    "       # print('enc4:',enc4.shape)\n",
    "        #mu1 = self.MUSEA(enc4, enc4, enc4)\n",
    "        \n",
    "        tp1 = self.tpool1(enc4)\n",
    "       # print('tp1:',tp1.shape)\n",
    "        tp2 = self.tpool2(enc4)\n",
    "        #print('tp2:',tp2.shape)\n",
    "        tp3 = self.tpool3(enc4)\n",
    "       # print('tp3:',tp3.shape)\n",
    "        tp4 = self.tpool4(enc4)\n",
    "        #print('tp4:',tp4.shape)\n",
    "        tp5 = self.tpool6(enc4)\n",
    "        #print('tp5:',tp5.shape)\n",
    "        tp6 = self.tpool6(enc4)\n",
    "        #print('tp6:',tp6.shape)\n",
    "        tp7 = self.tpool7(enc4)\n",
    "        #print('tp7:',tp7.shape)\n",
    "        \n",
    "        dec = self.decoder(torch.cat([enc4, tp1, tp2, tp3, tp4, tp5, tp6, tp7], dim=1))\n",
    "        #print('dec:',dec.shape)\n",
    "\n",
    "        act = self.activation(dec)\n",
    "        #print('act:',act.shape)\n",
    "        return act\n",
    "\n",
    "\n",
    "x = torch.randn(32, 1, 270)\n",
    "x = x.to('cuda')\n",
    "model = PTPNet(1,3,32).cuda()\n",
    "#print(model)\n",
    "#print(model(x).shape)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e61711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization_1121.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'F:/ruanjian/Graphviz/bin/'\n",
    "\n",
    "dot = make_dot(model(x), params=dict(model.named_parameters()))\n",
    "\n",
    "# 保存为图片到当前文件夹\n",
    "dot.render('model_visualization_1121', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8bf0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"2654pt\" height=\"2536pt\"\n",
       " viewBox=\"0.00 0.00 2654.00 2536.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 2532)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-2532 2650,-2532 2650,4 -4,4\"/>\n",
       "<!-- 1759433427280 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1759433427280</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"1824,-32 1730,-32 1730,0 1824,0 1824,-32\"/>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-6.5\" font-family=\"monospace\" font-size=\"10.00\"> (32, 3, 480)</text>\n",
       "</g>\n",
       "<!-- 1759433524176 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1759433524176</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1845,-88 1709,-88 1709,-68 1845,-68 1845,-88\"/>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-74.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524176&#45;&gt;1759433427280 -->\n",
       "<g id=\"edge189\" class=\"edge\">\n",
       "<title>1759433524176&#45;&gt;1759433427280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1777,-67.62C1777,-61.1 1777,-52.05 1777,-43.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1780.5,-43.65 1777,-33.65 1773.5,-43.65 1780.5,-43.65\"/>\n",
       "</g>\n",
       "<!-- 1759433524224 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1759433524224</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1694,-144 1600,-144 1600,-124 1694,-124 1694,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"1647\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524224&#45;&gt;1759433524176 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1759433524224&#45;&gt;1759433524176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1669.64,-123.59C1690.12,-115.09 1720.52,-102.46 1743.75,-92.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1744.89,-96.13 1752.78,-89.06 1742.2,-89.66 1744.89,-96.13\"/>\n",
       "</g>\n",
       "<!-- 1759433524320 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1759433524320</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1700,-206 1564,-206 1564,-186 1700,-186 1700,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"1632\" y=\"-192.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524320&#45;&gt;1759433524224 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1759433524320&#45;&gt;1759433524224</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1634.34,-185.62C1636.38,-177.48 1639.4,-165.39 1641.98,-155.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1645.32,-156.13 1644.35,-145.58 1638.53,-154.43 1645.32,-156.13\"/>\n",
       "</g>\n",
       "<!-- 1759433524464 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1759433524464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1620,-268 1532,-268 1532,-248 1620,-248 1620,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-254.5\" font-family=\"monospace\" font-size=\"10.00\">CatBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524464&#45;&gt;1759433524320 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1759433524464&#45;&gt;1759433524320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1584.75,-247.62C1593.02,-238.77 1605.62,-225.26 1615.74,-214.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1618.2,-216.92 1622.46,-207.22 1613.08,-212.14 1618.2,-216.92\"/>\n",
       "</g>\n",
       "<!-- 1759433524608 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1759433524608</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1485,-808 1397,-808 1397,-788 1485,-788 1485,-808\"/>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-794.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1485.27,-792.45C1582.88,-781.2 1808,-747.99 1808,-681 1808,-681 1808,-681 1808,-381 1808,-301.57 1700.16,-273.74 1631.53,-264.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1632.33,-260.65 1621.96,-262.82 1631.42,-267.59 1632.33,-260.65\"/>\n",
       "</g>\n",
       "<!-- 1759433526912 -->\n",
       "<g id=\"node74\" class=\"node\">\n",
       "<title>1759433526912</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2025,-752 1901,-752 1901,-732 2025,-732 2025,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"1963\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433526912 -->\n",
       "<g id=\"edge76\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433526912</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1485.06,-792.44C1574.67,-783.17 1779.02,-762.03 1889.25,-750.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1889.5,-754.12 1899.09,-749.61 1888.78,-747.16 1889.5,-754.12\"/>\n",
       "</g>\n",
       "<!-- 1759433539696 -->\n",
       "<g id=\"node90\" class=\"node\">\n",
       "<title>1759433539696</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2335,-752 2211,-752 2211,-732 2335,-732 2335,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"2273\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433539696 -->\n",
       "<g id=\"edge93\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433539696</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1485.44,-794.12C1621.07,-785.31 2030.72,-758.73 2199.3,-747.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2199.51,-751.28 2209.27,-747.14 2199.06,-744.29 2199.51,-751.28\"/>\n",
       "</g>\n",
       "<!-- 1759433540320 -->\n",
       "<g id=\"node106\" class=\"node\">\n",
       "<title>1759433540320</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"280,-752 156,-752 156,-732 280,-732 280,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"218\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433540320 -->\n",
       "<g id=\"edge110\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433540320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1396.75,-795.05C1214.4,-786.99 520.06,-756.34 291.46,-746.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.94,-742.76 281.8,-745.82 291.63,-749.75 291.94,-742.76\"/>\n",
       "</g>\n",
       "<!-- 1759433540656 -->\n",
       "<g id=\"node122\" class=\"node\">\n",
       "<title>1759433540656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"830,-752 706,-752 706,-732 830,-732 830,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"768\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433540656 -->\n",
       "<g id=\"edge127\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433540656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1396.58,-793.44C1283.16,-784.33 981.91,-760.16 841.7,-748.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"842.02,-745.43 831.78,-748.12 841.46,-752.41 842.02,-745.43\"/>\n",
       "</g>\n",
       "<!-- 1759433541040 -->\n",
       "<g id=\"node138\" class=\"node\">\n",
       "<title>1759433541040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1176,-752 1052,-752 1052,-732 1176,-732 1176,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"1114\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433541040 -->\n",
       "<g id=\"edge144\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433541040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1396.56,-789.66C1341.46,-780.56 1246.95,-764.95 1182.53,-754.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1183.2,-750.88 1172.77,-752.7 1182.06,-757.79 1183.2,-750.88\"/>\n",
       "</g>\n",
       "<!-- 1759433541376 -->\n",
       "<g id=\"node154\" class=\"node\">\n",
       "<title>1759433541376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1420,-752 1296,-752 1296,-732 1420,-732 1420,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"1358\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433541376 -->\n",
       "<g id=\"edge161\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433541376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1426.54,-787.59C1414.24,-779.59 1396.32,-767.93 1381.89,-758.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1384.13,-755.82 1373.84,-753.3 1380.31,-761.69 1384.13,-755.82\"/>\n",
       "</g>\n",
       "<!-- 1759433541712 -->\n",
       "<g id=\"node162\" class=\"node\">\n",
       "<title>1759433541712</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1587,-752 1463,-752 1463,-732 1587,-732 1587,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"1525\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433541712 -->\n",
       "<g id=\"edge174\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433541712</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1455.63,-787.59C1468.08,-779.59 1486.22,-767.93 1500.82,-758.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1502.46,-761.65 1508.98,-753.3 1498.68,-755.76 1502.46,-761.65\"/>\n",
       "</g>\n",
       "<!-- 1759433525040 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1759433525040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1521,-864 1361,-864 1361,-844 1521,-844 1521,-864\"/>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-850.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525040&#45;&gt;1759433524608 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1759433525040&#45;&gt;1759433524608</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1441,-843.59C1441,-837.01 1441,-827.96 1441,-819.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1444.5,-819.81 1441,-809.81 1437.5,-819.81 1444.5,-819.81\"/>\n",
       "</g>\n",
       "<!-- 1759433525184 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>1759433525184</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1356,-920 1262,-920 1262,-900 1356,-900 1356,-920\"/>\n",
       "<text text-anchor=\"middle\" x=\"1309\" y=\"-906.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525184&#45;&gt;1759433525040 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1759433525184&#45;&gt;1759433525040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1331.99,-899.59C1352.78,-891.09 1383.65,-878.46 1407.24,-868.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1408.5,-872.08 1416.43,-865.05 1405.85,-865.6 1408.5,-872.08\"/>\n",
       "</g>\n",
       "<!-- 1759433525232 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>1759433525232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1361,-982 1225,-982 1225,-962 1361,-962 1361,-982\"/>\n",
       "<text text-anchor=\"middle\" x=\"1293\" y=\"-968.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525232&#45;&gt;1759433525184 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1759433525232&#45;&gt;1759433525184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1295.5,-961.62C1297.67,-953.48 1300.9,-941.39 1303.65,-931.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1306.99,-932.14 1306.18,-921.58 1300.22,-930.34 1306.99,-932.14\"/>\n",
       "</g>\n",
       "<!-- 1759433525328 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>1759433525328</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1281,-1044 1193,-1044 1193,-1024 1281,-1024 1281,-1044\"/>\n",
       "<text text-anchor=\"middle\" x=\"1237\" y=\"-1030.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525328&#45;&gt;1759433525232 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>1759433525328&#45;&gt;1759433525232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1245.75,-1023.62C1254.02,-1014.77 1266.62,-1001.26 1276.74,-990.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1279.2,-992.92 1283.46,-983.22 1274.08,-988.14 1279.2,-992.92\"/>\n",
       "</g>\n",
       "<!-- 1759433525424 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>1759433525424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1293,-1106 1181,-1106 1181,-1086 1293,-1086 1293,-1106\"/>\n",
       "<text text-anchor=\"middle\" x=\"1237\" y=\"-1092.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433525424&#45;&gt;1759433525328 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1759433525424&#45;&gt;1759433525328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1237,-1085.62C1237,-1077.56 1237,-1065.65 1237,-1055.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1240.5,-1055.63 1237,-1045.63 1233.5,-1055.63 1240.5,-1055.63\"/>\n",
       "</g>\n",
       "<!-- 1759433525568 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>1759433525568</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1344,-1168 1154,-1168 1154,-1148 1344,-1148 1344,-1168\"/>\n",
       "<text text-anchor=\"middle\" x=\"1249\" y=\"-1154.5\" font-family=\"monospace\" font-size=\"10.00\">MaxPool2DWithIndicesBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525568&#45;&gt;1759433525424 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>1759433525568&#45;&gt;1759433525424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1247.12,-1147.62C1245.5,-1139.48 1243.08,-1127.39 1241.01,-1117.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1244.51,-1116.72 1239.12,-1107.6 1237.65,-1118.09 1244.51,-1116.72\"/>\n",
       "</g>\n",
       "<!-- 1759433525664 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>1759433525664</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1302,-1224 1178,-1224 1178,-1204 1302,-1204 1302,-1224\"/>\n",
       "<text text-anchor=\"middle\" x=\"1240\" y=\"-1210.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525664&#45;&gt;1759433525568 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>1759433525664&#45;&gt;1759433525568</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1241.57,-1203.59C1242.68,-1196.93 1244.21,-1187.75 1245.59,-1179.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1249.01,-1180.23 1247.2,-1169.79 1242.11,-1179.08 1249.01,-1180.23\"/>\n",
       "</g>\n",
       "<!-- 1759433525712 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>1759433525712</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1205,-1280 1117,-1280 1117,-1260 1205,-1260 1205,-1280\"/>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1266.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525712&#45;&gt;1759433525664 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>1759433525712&#45;&gt;1759433525664</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1174.76,-1259.59C1186.36,-1251.67 1203.2,-1240.16 1216.86,-1230.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1218.6,-1233.87 1224.88,-1225.33 1214.65,-1228.09 1218.6,-1233.87\"/>\n",
       "</g>\n",
       "<!-- 1759433525904 -->\n",
       "<g id=\"node60\" class=\"node\">\n",
       "<title>1759433525904</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1152,-1224 1028,-1224 1028,-1204 1152,-1204 1152,-1224\"/>\n",
       "<text text-anchor=\"middle\" x=\"1090\" y=\"-1210.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525712&#45;&gt;1759433525904 -->\n",
       "<g id=\"edge61\" class=\"edge\">\n",
       "<title>1759433525712&#45;&gt;1759433525904</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1148.63,-1259.59C1138.35,-1251.77 1123.48,-1240.46 1111.29,-1231.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1113.51,-1228.48 1103.43,-1225.21 1109.27,-1234.05 1113.51,-1228.48\"/>\n",
       "</g>\n",
       "<!-- 1759433525808 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>1759433525808</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1241,-1336 1081,-1336 1081,-1316 1241,-1316 1241,-1336\"/>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1322.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525808&#45;&gt;1759433525712 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>1759433525808&#45;&gt;1759433525712</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1161,-1315.59C1161,-1309.01 1161,-1299.96 1161,-1291.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1164.5,-1291.81 1161,-1281.81 1157.5,-1291.81 1164.5,-1291.81\"/>\n",
       "</g>\n",
       "<!-- 1759433525952 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>1759433525952</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1077,-1392 983,-1392 983,-1372 1077,-1372 1077,-1392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1030\" y=\"-1378.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525952&#45;&gt;1759433525808 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>1759433525952&#45;&gt;1759433525808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1052.82,-1371.59C1073.45,-1363.09 1104.08,-1350.46 1127.49,-1340.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1128.69,-1344.1 1136.6,-1337.06 1126.02,-1337.63 1128.69,-1344.1\"/>\n",
       "</g>\n",
       "<!-- 1759433526048 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>1759433526048</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1081,-1454 945,-1454 945,-1434 1081,-1434 1081,-1454\"/>\n",
       "<text text-anchor=\"middle\" x=\"1013\" y=\"-1440.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526048&#45;&gt;1759433525952 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>1759433526048&#45;&gt;1759433525952</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1015.66,-1433.62C1017.97,-1425.48 1021.39,-1413.39 1024.31,-1403.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1027.65,-1404.14 1027.01,-1393.57 1020.91,-1402.24 1027.65,-1404.14\"/>\n",
       "</g>\n",
       "<!-- 1759433526144 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>1759433526144</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1001,-1516 913,-1516 913,-1496 1001,-1496 1001,-1516\"/>\n",
       "<text text-anchor=\"middle\" x=\"957\" y=\"-1502.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526144&#45;&gt;1759433526048 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>1759433526144&#45;&gt;1759433526048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M965.75,-1495.62C974.02,-1486.77 986.62,-1473.26 996.74,-1462.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"999.2,-1464.92 1003.46,-1455.22 994.08,-1460.14 999.2,-1464.92\"/>\n",
       "</g>\n",
       "<!-- 1759433526240 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>1759433526240</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1013,-1578 901,-1578 901,-1558 1013,-1558 1013,-1578\"/>\n",
       "<text text-anchor=\"middle\" x=\"957\" y=\"-1564.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526240&#45;&gt;1759433526144 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>1759433526240&#45;&gt;1759433526144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M957,-1557.62C957,-1549.56 957,-1537.65 957,-1527.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"960.5,-1527.63 957,-1517.63 953.5,-1527.63 960.5,-1527.63\"/>\n",
       "</g>\n",
       "<!-- 1759433526432 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>1759433526432</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1064,-1640 874,-1640 874,-1620 1064,-1620 1064,-1640\"/>\n",
       "<text text-anchor=\"middle\" x=\"969\" y=\"-1626.5\" font-family=\"monospace\" font-size=\"10.00\">MaxPool2DWithIndicesBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526432&#45;&gt;1759433526240 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>1759433526432&#45;&gt;1759433526240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M967.12,-1619.62C965.5,-1611.48 963.08,-1599.39 961.01,-1589.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"964.51,-1588.72 959.12,-1579.6 957.65,-1590.09 964.51,-1588.72\"/>\n",
       "</g>\n",
       "<!-- 1759433526528 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>1759433526528</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1022,-1696 898,-1696 898,-1676 1022,-1676 1022,-1696\"/>\n",
       "<text text-anchor=\"middle\" x=\"960\" y=\"-1682.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526528&#45;&gt;1759433526432 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>1759433526528&#45;&gt;1759433526432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M961.57,-1675.59C962.68,-1668.93 964.21,-1659.75 965.59,-1651.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"969.01,-1652.23 967.2,-1641.79 962.11,-1651.08 969.01,-1652.23\"/>\n",
       "</g>\n",
       "<!-- 1759433526624 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>1759433526624</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"925,-1752 837,-1752 837,-1732 925,-1732 925,-1752\"/>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1738.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526624&#45;&gt;1759433526528 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>1759433526624&#45;&gt;1759433526528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M894.76,-1731.59C906.36,-1723.67 923.2,-1712.16 936.86,-1702.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"938.6,-1705.87 944.88,-1697.33 934.65,-1700.09 938.6,-1705.87\"/>\n",
       "</g>\n",
       "<!-- 1759433526816 -->\n",
       "<g id=\"node51\" class=\"node\">\n",
       "<title>1759433526816</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"872,-1696 748,-1696 748,-1676 872,-1676 872,-1696\"/>\n",
       "<text text-anchor=\"middle\" x=\"810\" y=\"-1682.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526624&#45;&gt;1759433526816 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>1759433526624&#45;&gt;1759433526816</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M868.63,-1731.59C858.35,-1723.77 843.48,-1712.46 831.29,-1703.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"833.51,-1700.48 823.43,-1697.21 829.27,-1706.05 833.51,-1700.48\"/>\n",
       "</g>\n",
       "<!-- 1759433526720 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>1759433526720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"961,-1808 801,-1808 801,-1788 961,-1788 961,-1808\"/>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1794.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526720&#45;&gt;1759433526624 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>1759433526720&#45;&gt;1759433526624</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M881,-1787.59C881,-1781.01 881,-1771.96 881,-1763.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"884.5,-1763.81 881,-1753.81 877.5,-1763.81 884.5,-1763.81\"/>\n",
       "</g>\n",
       "<!-- 1759433526864 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>1759433526864</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"797,-1864 703,-1864 703,-1844 797,-1844 797,-1864\"/>\n",
       "<text text-anchor=\"middle\" x=\"750\" y=\"-1850.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526864&#45;&gt;1759433526720 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>1759433526864&#45;&gt;1759433526720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M772.82,-1843.59C793.45,-1835.09 824.08,-1822.46 847.49,-1812.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"848.69,-1816.1 856.6,-1809.06 846.02,-1809.63 848.69,-1816.1\"/>\n",
       "</g>\n",
       "<!-- 1759433526960 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>1759433526960</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"801,-1926 665,-1926 665,-1906 801,-1906 801,-1926\"/>\n",
       "<text text-anchor=\"middle\" x=\"733\" y=\"-1912.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526960&#45;&gt;1759433526864 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>1759433526960&#45;&gt;1759433526864</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M735.66,-1905.62C737.97,-1897.48 741.39,-1885.39 744.31,-1875.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"747.65,-1876.14 747.01,-1865.57 740.91,-1874.24 747.65,-1876.14\"/>\n",
       "</g>\n",
       "<!-- 1759433527056 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>1759433527056</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"721,-1988 633,-1988 633,-1968 721,-1968 721,-1988\"/>\n",
       "<text text-anchor=\"middle\" x=\"677\" y=\"-1974.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433527056&#45;&gt;1759433526960 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>1759433527056&#45;&gt;1759433526960</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M685.75,-1967.62C694.02,-1958.77 706.62,-1945.26 716.74,-1934.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"719.2,-1936.92 723.46,-1927.22 714.08,-1932.14 719.2,-1936.92\"/>\n",
       "</g>\n",
       "<!-- 1759433527104 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>1759433527104</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"733,-2050 621,-2050 621,-2030 733,-2030 733,-2050\"/>\n",
       "<text text-anchor=\"middle\" x=\"677\" y=\"-2036.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433527104&#45;&gt;1759433527056 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>1759433527104&#45;&gt;1759433527056</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M677,-2029.62C677,-2021.56 677,-2009.65 677,-1999.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"680.5,-1999.63 677,-1989.63 673.5,-1999.63 680.5,-1999.63\"/>\n",
       "</g>\n",
       "<!-- 1759433526336 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>1759433526336</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"784,-2112 594,-2112 594,-2092 784,-2092 784,-2112\"/>\n",
       "<text text-anchor=\"middle\" x=\"689\" y=\"-2098.5\" font-family=\"monospace\" font-size=\"10.00\">MaxPool2DWithIndicesBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526336&#45;&gt;1759433527104 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>1759433526336&#45;&gt;1759433527104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M687.12,-2091.62C685.5,-2083.48 683.08,-2071.39 681.01,-2061.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"684.51,-2060.72 679.12,-2051.6 677.65,-2062.09 684.51,-2060.72\"/>\n",
       "</g>\n",
       "<!-- 1759433526768 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>1759433526768</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"742,-2168 618,-2168 618,-2148 742,-2148 742,-2168\"/>\n",
       "<text text-anchor=\"middle\" x=\"680\" y=\"-2154.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526768&#45;&gt;1759433526336 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>1759433526768&#45;&gt;1759433526336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M681.57,-2147.59C682.68,-2140.93 684.21,-2131.75 685.59,-2123.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"689.01,-2124.23 687.2,-2113.79 682.11,-2123.08 689.01,-2124.23\"/>\n",
       "</g>\n",
       "<!-- 1759433527200 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>1759433527200</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"645,-2224 557,-2224 557,-2204 645,-2204 645,-2224\"/>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2210.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433527200&#45;&gt;1759433526768 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>1759433527200&#45;&gt;1759433526768</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M614.76,-2203.59C626.36,-2195.67 643.2,-2184.16 656.86,-2174.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"658.6,-2177.87 664.88,-2169.33 654.65,-2172.09 658.6,-2177.87\"/>\n",
       "</g>\n",
       "<!-- 1759433539840 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>1759433539840</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"592,-2168 468,-2168 468,-2148 592,-2148 592,-2168\"/>\n",
       "<text text-anchor=\"middle\" x=\"530\" y=\"-2154.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433527200&#45;&gt;1759433539840 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>1759433527200&#45;&gt;1759433539840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M588.63,-2203.59C578.35,-2195.77 563.48,-2184.46 551.29,-2175.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"553.51,-2172.48 543.43,-2169.21 549.27,-2178.05 553.51,-2172.48\"/>\n",
       "</g>\n",
       "<!-- 1759433539792 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>1759433539792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"681,-2280 521,-2280 521,-2260 681,-2260 681,-2280\"/>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2266.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539792&#45;&gt;1759433527200 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>1759433539792&#45;&gt;1759433527200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M601,-2259.59C601,-2253.01 601,-2243.96 601,-2235.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"604.5,-2235.81 601,-2225.81 597.5,-2235.81 604.5,-2235.81\"/>\n",
       "</g>\n",
       "<!-- 1759433539888 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>1759433539888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"517,-2336 423,-2336 423,-2316 517,-2316 517,-2336\"/>\n",
       "<text text-anchor=\"middle\" x=\"470\" y=\"-2322.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539888&#45;&gt;1759433539792 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>1759433539888&#45;&gt;1759433539792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M492.82,-2315.59C513.45,-2307.09 544.08,-2294.46 567.49,-2284.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"568.69,-2288.1 576.6,-2281.06 566.02,-2281.63 568.69,-2288.1\"/>\n",
       "</g>\n",
       "<!-- 1759433539984 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>1759433539984</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"521,-2398 385,-2398 385,-2378 521,-2378 521,-2398\"/>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-2384.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539984&#45;&gt;1759433539888 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>1759433539984&#45;&gt;1759433539888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M455.66,-2377.62C457.97,-2369.48 461.39,-2357.39 464.31,-2347.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"467.65,-2348.14 467.01,-2337.57 460.91,-2346.24 467.65,-2348.14\"/>\n",
       "</g>\n",
       "<!-- 1759433464800 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>1759433464800</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"503,-2460 403,-2460 403,-2440 503,-2440 503,-2460\"/>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-2446.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464800&#45;&gt;1759433539984 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>1759433464800&#45;&gt;1759433539984</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M453,-2439.62C453,-2431.56 453,-2419.65 453,-2409.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456.5,-2409.63 453,-2399.63 449.5,-2409.63 456.5,-2409.63\"/>\n",
       "</g>\n",
       "<!-- 1759421711920 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>1759421711920</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"521,-2528 385,-2528 385,-2496 521,-2496 521,-2528\"/>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-2514.5\" font-family=\"monospace\" font-size=\"10.00\">encoder1.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-2502.5\" font-family=\"monospace\" font-size=\"10.00\"> (32, 1, 3)</text>\n",
       "</g>\n",
       "<!-- 1759421711920&#45;&gt;1759433464800 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>1759421711920&#45;&gt;1759433464800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M453,-2495.55C453,-2488.34 453,-2479.66 453,-2471.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456.5,-2471.92 453,-2461.92 449.5,-2471.92 456.5,-2471.92\"/>\n",
       "</g>\n",
       "<!-- 1759433464464 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>1759433464464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"651,-2336 551,-2336 551,-2316 651,-2316 651,-2336\"/>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2322.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464464&#45;&gt;1759433539792 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>1759433464464&#45;&gt;1759433539792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M601,-2315.59C601,-2309.01 601,-2299.96 601,-2291.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"604.5,-2291.81 601,-2281.81 597.5,-2291.81 604.5,-2291.81\"/>\n",
       "</g>\n",
       "<!-- 1759421712000 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>1759421712000</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"663,-2404 539,-2404 539,-2372 663,-2372 663,-2404\"/>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2390.5\" font-family=\"monospace\" font-size=\"10.00\">encoder1.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2378.5\" font-family=\"monospace\" font-size=\"10.00\"> (32)</text>\n",
       "</g>\n",
       "<!-- 1759421712000&#45;&gt;1759433464464 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>1759421712000&#45;&gt;1759433464464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M601,-2371.55C601,-2364.34 601,-2355.66 601,-2347.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"604.5,-2347.92 601,-2337.92 597.5,-2347.92 604.5,-2347.92\"/>\n",
       "</g>\n",
       "<!-- 1759433464032 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>1759433464032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"778,-2336 678,-2336 678,-2316 778,-2316 778,-2336\"/>\n",
       "<text text-anchor=\"middle\" x=\"728\" y=\"-2322.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464032&#45;&gt;1759433539792 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>1759433464032&#45;&gt;1759433539792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M705.88,-2315.59C685.97,-2307.13 656.45,-2294.58 633.79,-2284.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"635.26,-2281.76 624.69,-2281.07 632.52,-2288.21 635.26,-2281.76\"/>\n",
       "</g>\n",
       "<!-- 1759421712080 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>1759421712080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"793,-2404 681,-2404 681,-2372 793,-2372 793,-2404\"/>\n",
       "<text text-anchor=\"middle\" x=\"737\" y=\"-2390.5\" font-family=\"monospace\" font-size=\"10.00\">encoder1.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"737\" y=\"-2378.5\" font-family=\"monospace\" font-size=\"10.00\"> (32)</text>\n",
       "</g>\n",
       "<!-- 1759421712080&#45;&gt;1759433464032 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>1759421712080&#45;&gt;1759433464032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M734.68,-2371.55C733.59,-2364.26 732.27,-2355.45 731.09,-2347.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"734.58,-2347.27 729.64,-2337.9 727.66,-2348.31 734.58,-2347.27\"/>\n",
       "</g>\n",
       "<!-- 1759433527152 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>1759433527152</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"592,-2050 480,-2050 480,-2030 592,-2030 592,-2050\"/>\n",
       "<text text-anchor=\"middle\" x=\"536\" y=\"-2036.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433527152&#45;&gt;1759433527056 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>1759433527152&#45;&gt;1759433527056</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M558.03,-2029.62C581.31,-2019.72 618.26,-2003.99 644.81,-1992.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"645.9,-1996.04 653.73,-1988.9 643.16,-1989.6 645.9,-1996.04\"/>\n",
       "</g>\n",
       "<!-- 1759433524800 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>1759433524800</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"576,-2112 452,-2112 452,-2092 576,-2092 576,-2112\"/>\n",
       "<text text-anchor=\"middle\" x=\"514\" y=\"-2098.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524800&#45;&gt;1759433527152 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>1759433524800&#45;&gt;1759433527152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M517.44,-2091.62C520.46,-2083.39 524.95,-2071.13 528.76,-2060.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"531.98,-2062.13 532.14,-2051.53 525.41,-2059.72 531.98,-2062.13\"/>\n",
       "</g>\n",
       "<!-- 1759433539840&#45;&gt;1759433524800 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>1759433539840&#45;&gt;1759433524800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M527.21,-2147.59C525.22,-2140.86 522.45,-2131.53 519.97,-2123.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"523.38,-2122.34 517.19,-2113.75 516.67,-2124.33 523.38,-2122.34\"/>\n",
       "</g>\n",
       "<!-- 1759433463792 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>1759433463792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"854,-1988 754,-1988 754,-1968 854,-1968 854,-1988\"/>\n",
       "<text text-anchor=\"middle\" x=\"804\" y=\"-1974.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463792&#45;&gt;1759433526960 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>1759433463792&#45;&gt;1759433526960</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M792.91,-1967.62C782.11,-1958.5 765.48,-1944.45 752.47,-1933.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"754.84,-1930.87 744.94,-1927.09 750.32,-1936.22 754.84,-1930.87\"/>\n",
       "</g>\n",
       "<!-- 1759421712480 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>1759421712480</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"887,-2056 751,-2056 751,-2024 887,-2024 887,-2056\"/>\n",
       "<text text-anchor=\"middle\" x=\"819\" y=\"-2042.5\" font-family=\"monospace\" font-size=\"10.00\">encoder2.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"819\" y=\"-2030.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 32, 3)</text>\n",
       "</g>\n",
       "<!-- 1759421712480&#45;&gt;1759433463792 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>1759421712480&#45;&gt;1759433463792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M815.14,-2023.55C813.29,-2016.17 811.06,-2007.24 809.08,-1999.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"812.54,-1998.73 806.72,-1989.88 805.75,-2000.43 812.54,-1998.73\"/>\n",
       "</g>\n",
       "<!-- 1759433463504 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>1759433463504</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"931,-1864 831,-1864 831,-1844 931,-1844 931,-1864\"/>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1850.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463504&#45;&gt;1759433526720 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>1759433463504&#45;&gt;1759433526720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M881,-1843.59C881,-1837.01 881,-1827.96 881,-1819.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"884.5,-1819.81 881,-1809.81 877.5,-1819.81 884.5,-1819.81\"/>\n",
       "</g>\n",
       "<!-- 1759421712560 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>1759421712560</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"943,-1932 819,-1932 819,-1900 943,-1900 943,-1932\"/>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1918.5\" font-family=\"monospace\" font-size=\"10.00\">encoder2.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1906.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421712560&#45;&gt;1759433463504 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>1759421712560&#45;&gt;1759433463504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M881,-1899.55C881,-1892.34 881,-1883.66 881,-1875.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"884.5,-1875.92 881,-1865.92 877.5,-1875.92 884.5,-1875.92\"/>\n",
       "</g>\n",
       "<!-- 1759433463072 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>1759433463072</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1058,-1864 958,-1864 958,-1844 1058,-1844 1058,-1864\"/>\n",
       "<text text-anchor=\"middle\" x=\"1008\" y=\"-1850.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463072&#45;&gt;1759433526720 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>1759433463072&#45;&gt;1759433526720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M985.88,-1843.59C965.97,-1835.13 936.45,-1822.58 913.79,-1812.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"915.26,-1809.76 904.69,-1809.07 912.52,-1816.21 915.26,-1809.76\"/>\n",
       "</g>\n",
       "<!-- 1759421712640 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>1759421712640</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1073,-1932 961,-1932 961,-1900 1073,-1900 1073,-1932\"/>\n",
       "<text text-anchor=\"middle\" x=\"1017\" y=\"-1918.5\" font-family=\"monospace\" font-size=\"10.00\">encoder2.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1017\" y=\"-1906.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421712640&#45;&gt;1759433463072 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>1759421712640&#45;&gt;1759433463072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1014.68,-1899.55C1013.59,-1892.26 1012.27,-1883.45 1011.09,-1875.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1014.58,-1875.27 1009.64,-1865.9 1007.66,-1876.31 1014.58,-1875.27\"/>\n",
       "</g>\n",
       "<!-- 1759433526192 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>1759433526192</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"872,-1578 760,-1578 760,-1558 872,-1558 872,-1578\"/>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-1564.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526192&#45;&gt;1759433526144 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>1759433526192&#45;&gt;1759433526144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M838.03,-1557.62C861.31,-1547.72 898.26,-1531.99 924.81,-1520.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"925.9,-1524.04 933.73,-1516.9 923.16,-1517.6 925.9,-1524.04\"/>\n",
       "</g>\n",
       "<!-- 1759433526576 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>1759433526576</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"856,-1640 732,-1640 732,-1620 856,-1620 856,-1640\"/>\n",
       "<text text-anchor=\"middle\" x=\"794\" y=\"-1626.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526576&#45;&gt;1759433526192 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>1759433526576&#45;&gt;1759433526192</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M797.44,-1619.62C800.46,-1611.39 804.95,-1599.13 808.76,-1588.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"811.98,-1590.13 812.14,-1579.53 805.41,-1587.72 811.98,-1590.13\"/>\n",
       "</g>\n",
       "<!-- 1759433526816&#45;&gt;1759433526576 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>1759433526816&#45;&gt;1759433526576</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M807.21,-1675.59C805.22,-1668.86 802.45,-1659.53 799.97,-1651.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"803.38,-1650.34 797.19,-1641.75 796.67,-1652.33 803.38,-1650.34\"/>\n",
       "</g>\n",
       "<!-- 1759433462832 -->\n",
       "<g id=\"node52\" class=\"node\">\n",
       "<title>1759433462832</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1134,-1516 1034,-1516 1034,-1496 1134,-1496 1134,-1516\"/>\n",
       "<text text-anchor=\"middle\" x=\"1084\" y=\"-1502.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462832&#45;&gt;1759433526048 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>1759433462832&#45;&gt;1759433526048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1072.91,-1495.62C1062.11,-1486.5 1045.48,-1472.45 1032.47,-1461.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1034.84,-1458.87 1024.94,-1455.09 1030.32,-1464.22 1034.84,-1458.87\"/>\n",
       "</g>\n",
       "<!-- 1759421712960 -->\n",
       "<g id=\"node53\" class=\"node\">\n",
       "<title>1759421712960</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1167,-1584 1031,-1584 1031,-1552 1167,-1552 1167,-1584\"/>\n",
       "<text text-anchor=\"middle\" x=\"1099\" y=\"-1570.5\" font-family=\"monospace\" font-size=\"10.00\">encoder3.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1099\" y=\"-1558.5\" font-family=\"monospace\" font-size=\"10.00\"> (128, 64, 3)</text>\n",
       "</g>\n",
       "<!-- 1759421712960&#45;&gt;1759433462832 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>1759421712960&#45;&gt;1759433462832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1095.14,-1551.55C1093.29,-1544.17 1091.06,-1535.24 1089.08,-1527.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1092.54,-1526.73 1086.72,-1517.88 1085.75,-1528.43 1092.54,-1526.73\"/>\n",
       "</g>\n",
       "<!-- 1759433462544 -->\n",
       "<g id=\"node54\" class=\"node\">\n",
       "<title>1759433462544</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1211,-1392 1111,-1392 1111,-1372 1211,-1372 1211,-1392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1378.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462544&#45;&gt;1759433525808 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>1759433462544&#45;&gt;1759433525808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1161,-1371.59C1161,-1365.01 1161,-1355.96 1161,-1347.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1164.5,-1347.81 1161,-1337.81 1157.5,-1347.81 1164.5,-1347.81\"/>\n",
       "</g>\n",
       "<!-- 1759421713040 -->\n",
       "<g id=\"node55\" class=\"node\">\n",
       "<title>1759421713040</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1223,-1460 1099,-1460 1099,-1428 1223,-1428 1223,-1460\"/>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1446.5\" font-family=\"monospace\" font-size=\"10.00\">encoder3.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1434.5\" font-family=\"monospace\" font-size=\"10.00\"> (128)</text>\n",
       "</g>\n",
       "<!-- 1759421713040&#45;&gt;1759433462544 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>1759421713040&#45;&gt;1759433462544</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1161,-1427.55C1161,-1420.34 1161,-1411.66 1161,-1403.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1164.5,-1403.92 1161,-1393.92 1157.5,-1403.92 1164.5,-1403.92\"/>\n",
       "</g>\n",
       "<!-- 1759433462160 -->\n",
       "<g id=\"node56\" class=\"node\">\n",
       "<title>1759433462160</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1338,-1392 1238,-1392 1238,-1372 1338,-1372 1338,-1392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1288\" y=\"-1378.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462160&#45;&gt;1759433525808 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>1759433462160&#45;&gt;1759433525808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1265.88,-1371.59C1245.97,-1363.13 1216.45,-1350.58 1193.79,-1340.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1195.26,-1337.76 1184.69,-1337.07 1192.52,-1344.21 1195.26,-1337.76\"/>\n",
       "</g>\n",
       "<!-- 1759421713120 -->\n",
       "<g id=\"node57\" class=\"node\">\n",
       "<title>1759421713120</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1353,-1460 1241,-1460 1241,-1428 1353,-1428 1353,-1460\"/>\n",
       "<text text-anchor=\"middle\" x=\"1297\" y=\"-1446.5\" font-family=\"monospace\" font-size=\"10.00\">encoder3.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1297\" y=\"-1434.5\" font-family=\"monospace\" font-size=\"10.00\"> (128)</text>\n",
       "</g>\n",
       "<!-- 1759421713120&#45;&gt;1759433462160 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>1759421713120&#45;&gt;1759433462160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1294.68,-1427.55C1293.59,-1420.26 1292.27,-1411.45 1291.09,-1403.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1294.58,-1403.27 1289.64,-1393.9 1287.66,-1404.31 1294.58,-1403.27\"/>\n",
       "</g>\n",
       "<!-- 1759433525376 -->\n",
       "<g id=\"node58\" class=\"node\">\n",
       "<title>1759433525376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1152,-1106 1040,-1106 1040,-1086 1152,-1086 1152,-1106\"/>\n",
       "<text text-anchor=\"middle\" x=\"1096\" y=\"-1092.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433525376&#45;&gt;1759433525328 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>1759433525376&#45;&gt;1759433525328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1118.03,-1085.62C1141.31,-1075.72 1178.26,-1059.99 1204.81,-1048.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1205.9,-1052.04 1213.73,-1044.9 1203.16,-1045.6 1205.9,-1052.04\"/>\n",
       "</g>\n",
       "<!-- 1759433524896 -->\n",
       "<g id=\"node59\" class=\"node\">\n",
       "<title>1759433524896</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1136,-1168 1012,-1168 1012,-1148 1136,-1148 1136,-1168\"/>\n",
       "<text text-anchor=\"middle\" x=\"1074\" y=\"-1154.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524896&#45;&gt;1759433525376 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>1759433524896&#45;&gt;1759433525376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1077.44,-1147.62C1080.46,-1139.39 1084.95,-1127.13 1088.76,-1116.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1091.98,-1118.13 1092.14,-1107.53 1085.41,-1115.72 1091.98,-1118.13\"/>\n",
       "</g>\n",
       "<!-- 1759433525904&#45;&gt;1759433524896 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>1759433525904&#45;&gt;1759433524896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1087.21,-1203.59C1085.22,-1196.86 1082.45,-1187.53 1079.97,-1179.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1083.38,-1178.34 1077.19,-1169.75 1076.67,-1180.33 1083.38,-1178.34\"/>\n",
       "</g>\n",
       "<!-- 1759433461872 -->\n",
       "<g id=\"node61\" class=\"node\">\n",
       "<title>1759433461872</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1414,-1044 1314,-1044 1314,-1024 1414,-1024 1414,-1044\"/>\n",
       "<text text-anchor=\"middle\" x=\"1364\" y=\"-1030.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433461872&#45;&gt;1759433525232 -->\n",
       "<g id=\"edge62\" class=\"edge\">\n",
       "<title>1759433461872&#45;&gt;1759433525232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1352.91,-1023.62C1342.11,-1014.5 1325.48,-1000.45 1312.47,-989.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1314.84,-986.87 1304.94,-983.09 1310.32,-992.22 1314.84,-986.87\"/>\n",
       "</g>\n",
       "<!-- 1759421713440 -->\n",
       "<g id=\"node62\" class=\"node\">\n",
       "<title>1759421713440</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1447,-1112 1311,-1112 1311,-1080 1447,-1080 1447,-1112\"/>\n",
       "<text text-anchor=\"middle\" x=\"1379\" y=\"-1098.5\" font-family=\"monospace\" font-size=\"10.00\">encoder4.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1379\" y=\"-1086.5\" font-family=\"monospace\" font-size=\"10.00\"> (320, 128, 3)</text>\n",
       "</g>\n",
       "<!-- 1759421713440&#45;&gt;1759433461872 -->\n",
       "<g id=\"edge63\" class=\"edge\">\n",
       "<title>1759421713440&#45;&gt;1759433461872</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1375.14,-1079.55C1373.29,-1072.17 1371.06,-1063.24 1369.08,-1055.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1372.54,-1054.73 1366.72,-1045.88 1365.75,-1056.43 1372.54,-1054.73\"/>\n",
       "</g>\n",
       "<!-- 1759425755984 -->\n",
       "<g id=\"node63\" class=\"node\">\n",
       "<title>1759425755984</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1491,-920 1391,-920 1391,-900 1491,-900 1491,-920\"/>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-906.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759425755984&#45;&gt;1759433525040 -->\n",
       "<g id=\"edge64\" class=\"edge\">\n",
       "<title>1759425755984&#45;&gt;1759433525040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1441,-899.59C1441,-893.01 1441,-883.96 1441,-875.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1444.5,-875.81 1441,-865.81 1437.5,-875.81 1444.5,-875.81\"/>\n",
       "</g>\n",
       "<!-- 1759421713520 -->\n",
       "<g id=\"node64\" class=\"node\">\n",
       "<title>1759421713520</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1503,-988 1379,-988 1379,-956 1503,-956 1503,-988\"/>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-974.5\" font-family=\"monospace\" font-size=\"10.00\">encoder4.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-962.5\" font-family=\"monospace\" font-size=\"10.00\"> (320)</text>\n",
       "</g>\n",
       "<!-- 1759421713520&#45;&gt;1759425755984 -->\n",
       "<g id=\"edge65\" class=\"edge\">\n",
       "<title>1759421713520&#45;&gt;1759425755984</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1441,-955.55C1441,-948.34 1441,-939.66 1441,-931.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1444.5,-931.92 1441,-921.92 1437.5,-931.92 1444.5,-931.92\"/>\n",
       "</g>\n",
       "<!-- 1759425753440 -->\n",
       "<g id=\"node65\" class=\"node\">\n",
       "<title>1759425753440</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1618,-920 1518,-920 1518,-900 1618,-900 1618,-920\"/>\n",
       "<text text-anchor=\"middle\" x=\"1568\" y=\"-906.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759425753440&#45;&gt;1759433525040 -->\n",
       "<g id=\"edge66\" class=\"edge\">\n",
       "<title>1759425753440&#45;&gt;1759433525040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1545.88,-899.59C1525.97,-891.13 1496.45,-878.58 1473.79,-868.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1475.26,-865.76 1464.69,-865.07 1472.52,-872.21 1475.26,-865.76\"/>\n",
       "</g>\n",
       "<!-- 1759421713600 -->\n",
       "<g id=\"node66\" class=\"node\">\n",
       "<title>1759421713600</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1633,-988 1521,-988 1521,-956 1633,-956 1633,-988\"/>\n",
       "<text text-anchor=\"middle\" x=\"1577\" y=\"-974.5\" font-family=\"monospace\" font-size=\"10.00\">encoder4.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1577\" y=\"-962.5\" font-family=\"monospace\" font-size=\"10.00\"> (320)</text>\n",
       "</g>\n",
       "<!-- 1759421713600&#45;&gt;1759425753440 -->\n",
       "<g id=\"edge67\" class=\"edge\">\n",
       "<title>1759421713600&#45;&gt;1759425753440</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1574.68,-955.55C1573.59,-948.26 1572.27,-939.45 1571.09,-931.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1574.58,-931.27 1569.64,-921.9 1567.66,-932.31 1574.58,-931.27\"/>\n",
       "</g>\n",
       "<!-- 1759433524512 -->\n",
       "<g id=\"node67\" class=\"node\">\n",
       "<title>1759433524512</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1924,-330 1836,-330 1836,-310 1924,-310 1924,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1880\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524512&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge68\" class=\"edge\">\n",
       "<title>1759433524512&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1845.13,-309.55C1837.53,-307.61 1829.52,-305.66 1822,-304 1756.9,-289.65 1681.4,-276.32 1631.39,-267.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1632.12,-264.53 1621.68,-266.34 1630.97,-271.44 1632.12,-264.53\"/>\n",
       "</g>\n",
       "<!-- 1759433525136 -->\n",
       "<g id=\"node68\" class=\"node\">\n",
       "<title>1759433525136</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2025,-392 1859,-392 1859,-372 2025,-372 2025,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1942\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433525136&#45;&gt;1759433524512 -->\n",
       "<g id=\"edge69\" class=\"edge\">\n",
       "<title>1759433525136&#45;&gt;1759433524512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1932.31,-371.62C1923.07,-362.68 1908.92,-348.99 1897.67,-338.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1900.12,-335.6 1890.5,-331.17 1895.26,-340.63 1900.12,-335.6\"/>\n",
       "</g>\n",
       "<!-- 1759433525760 -->\n",
       "<g id=\"node69\" class=\"node\">\n",
       "<title>1759433525760</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2047,-448 1887,-448 1887,-428 2047,-428 2047,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"1967\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525760&#45;&gt;1759433525136 -->\n",
       "<g id=\"edge70\" class=\"edge\">\n",
       "<title>1759433525760&#45;&gt;1759433525136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1962.65,-427.59C1959.45,-420.7 1955.01,-411.1 1951.06,-402.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1954.32,-401.28 1946.94,-393.67 1947.97,-404.22 1954.32,-401.28\"/>\n",
       "</g>\n",
       "<!-- 1759433524944 -->\n",
       "<g id=\"node70\" class=\"node\">\n",
       "<title>1759433524944</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2060,-504 1966,-504 1966,-484 2060,-484 2060,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2013\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524944&#45;&gt;1759433525760 -->\n",
       "<g id=\"edge71\" class=\"edge\">\n",
       "<title>1759433524944&#45;&gt;1759433525760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2004.99,-483.59C1998.72,-476.24 1989.83,-465.8 1982.22,-456.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1985.05,-454.79 1975.9,-449.45 1979.72,-459.33 1985.05,-454.79\"/>\n",
       "</g>\n",
       "<!-- 1759433526000 -->\n",
       "<g id=\"node71\" class=\"node\">\n",
       "<title>1759433526000</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2091,-566 1955,-566 1955,-546 2091,-546 2091,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"2023\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526000&#45;&gt;1759433524944 -->\n",
       "<g id=\"edge72\" class=\"edge\">\n",
       "<title>1759433526000&#45;&gt;1759433524944</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2021.44,-545.62C2020.09,-537.56 2018.11,-525.65 2016.4,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2019.86,-514.89 2014.77,-505.61 2012.96,-516.05 2019.86,-514.89\"/>\n",
       "</g>\n",
       "<!-- 1759433526672 -->\n",
       "<g id=\"node72\" class=\"node\">\n",
       "<title>1759433526672</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2081,-628 1969,-628 1969,-608 2081,-608 2081,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2025\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526672&#45;&gt;1759433526000 -->\n",
       "<g id=\"edge73\" class=\"edge\">\n",
       "<title>1759433526672&#45;&gt;1759433526000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2024.69,-607.62C2024.42,-599.56 2024.02,-587.65 2023.68,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2027.19,-577.5 2023.35,-567.63 2020.19,-577.74 2027.19,-577.5\"/>\n",
       "</g>\n",
       "<!-- 1759433525472 -->\n",
       "<g id=\"node73\" class=\"node\">\n",
       "<title>1759433525472</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2090,-690 1966,-690 1966,-670 2090,-670 2090,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"2028\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525472&#45;&gt;1759433526672 -->\n",
       "<g id=\"edge74\" class=\"edge\">\n",
       "<title>1759433525472&#45;&gt;1759433526672</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2027.53,-669.62C2027.13,-661.56 2026.53,-649.65 2026.02,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2029.53,-639.44 2025.53,-629.62 2022.54,-639.79 2029.53,-639.44\"/>\n",
       "</g>\n",
       "<!-- 1759433526912&#45;&gt;1759433525472 -->\n",
       "<g id=\"edge75\" class=\"edge\">\n",
       "<title>1759433526912&#45;&gt;1759433525472</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1973.16,-731.62C1982.94,-722.59 1997.97,-708.72 2009.82,-697.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2012.04,-700.49 2017.02,-691.14 2007.29,-695.35 2012.04,-700.49\"/>\n",
       "</g>\n",
       "<!-- 1759433463216 -->\n",
       "<g id=\"node75\" class=\"node\">\n",
       "<title>1759433463216</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2210,-628 2110,-628 2110,-608 2210,-608 2210,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2160\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463216&#45;&gt;1759433526000 -->\n",
       "<g id=\"edge77\" class=\"edge\">\n",
       "<title>1759433463216&#45;&gt;1759433526000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2138.59,-607.62C2116.07,-597.76 2080.39,-582.13 2054.62,-570.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2056.21,-567.72 2045.64,-566.92 2053.4,-574.13 2056.21,-567.72\"/>\n",
       "</g>\n",
       "<!-- 1759421713920 -->\n",
       "<g id=\"node76\" class=\"node\">\n",
       "<title>1759421713920</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2232,-696 2108,-696 2108,-664 2232,-664 2232,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"2170\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool1.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2170\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421713920&#45;&gt;1759433463216 -->\n",
       "<g id=\"edge78\" class=\"edge\">\n",
       "<title>1759421713920&#45;&gt;1759433463216</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2167.42,-663.55C2166.21,-656.26 2164.74,-647.45 2163.43,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2166.91,-639.19 2161.82,-629.9 2160.01,-640.34 2166.91,-639.19\"/>\n",
       "</g>\n",
       "<!-- 1759433462736 -->\n",
       "<g id=\"node77\" class=\"node\">\n",
       "<title>1759433462736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1947,-628 1847,-628 1847,-608 1947,-608 1947,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462736&#45;&gt;1759433526000 -->\n",
       "<g id=\"edge79\" class=\"edge\">\n",
       "<title>1759433462736&#45;&gt;1759433526000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1916.69,-607.62C1937.21,-597.85 1969.63,-582.41 1993.28,-571.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1994.56,-574.42 2002.08,-566.96 1991.55,-568.1 1994.56,-574.42\"/>\n",
       "</g>\n",
       "<!-- 1759421714000 -->\n",
       "<g id=\"node78\" class=\"node\">\n",
       "<title>1759421714000</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1948,-696 1836,-696 1836,-664 1948,-664 1948,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1892\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool1.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1892\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421714000&#45;&gt;1759433462736 -->\n",
       "<g id=\"edge80\" class=\"edge\">\n",
       "<title>1759421714000&#45;&gt;1759433462736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1893.29,-663.55C1893.9,-656.26 1894.63,-647.45 1895.28,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1898.75,-640.17 1896.09,-629.92 1891.77,-639.59 1898.75,-640.17\"/>\n",
       "</g>\n",
       "<!-- 1759433462256 -->\n",
       "<g id=\"node79\" class=\"node\">\n",
       "<title>1759433462256</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2197,-504 2097,-504 2097,-484 2197,-484 2197,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2147\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462256&#45;&gt;1759433525760 -->\n",
       "<g id=\"edge81\" class=\"edge\">\n",
       "<title>1759433462256&#45;&gt;1759433525760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2115.65,-483.59C2086.27,-474.78 2042.14,-461.54 2009.59,-451.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2010.63,-448.44 2000.05,-448.91 2008.62,-455.14 2010.63,-448.44\"/>\n",
       "</g>\n",
       "<!-- 1759421714080 -->\n",
       "<g id=\"node80\" class=\"node\">\n",
       "<title>1759421714080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2221,-572 2109,-572 2109,-540 2221,-540 2221,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"2165\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool1.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2165\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421714080&#45;&gt;1759433462256 -->\n",
       "<g id=\"edge82\" class=\"edge\">\n",
       "<title>1759421714080&#45;&gt;1759433462256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2160.36,-539.55C2158.15,-532.17 2155.47,-523.24 2153.1,-515.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2156.48,-514.43 2150.26,-505.86 2149.78,-516.44 2156.48,-514.43\"/>\n",
       "</g>\n",
       "<!-- 1759433461824 -->\n",
       "<g id=\"node81\" class=\"node\">\n",
       "<title>1759433461824</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1943,-504 1843,-504 1843,-484 1943,-484 1943,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1893\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433461824&#45;&gt;1759433525760 -->\n",
       "<g id=\"edge83\" class=\"edge\">\n",
       "<title>1759433461824&#45;&gt;1759433525760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1905.89,-483.59C1916.71,-475.7 1932.41,-464.24 1945.19,-454.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1947.02,-457.91 1953.04,-449.19 1942.89,-452.26 1947.02,-457.91\"/>\n",
       "</g>\n",
       "<!-- 1759421714160 -->\n",
       "<g id=\"node82\" class=\"node\">\n",
       "<title>1759421714160</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1937,-572 1837,-572 1837,-540 1937,-540 1937,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1887\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool1.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1887\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421714160&#45;&gt;1759433461824 -->\n",
       "<g id=\"edge84\" class=\"edge\">\n",
       "<title>1759421714160&#45;&gt;1759433461824</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1888.55,-539.55C1889.27,-532.26 1890.15,-523.45 1890.94,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1894.4,-516.21 1891.91,-505.91 1887.43,-515.52 1894.4,-516.21\"/>\n",
       "</g>\n",
       "<!-- 1759433524032 -->\n",
       "<g id=\"node83\" class=\"node\">\n",
       "<title>1759433524032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2030,-330 1942,-330 1942,-310 2030,-310 2030,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1986\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524032&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge85\" class=\"edge\">\n",
       "<title>1759433524032&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1955.64,-309.56C1948.28,-307.51 1940.4,-305.51 1933,-304 1827.46,-282.54 1702.16,-269.58 1631.58,-263.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1632.09,-259.93 1621.82,-262.56 1631.49,-266.9 1632.09,-259.93\"/>\n",
       "</g>\n",
       "<!-- 1759433525616 -->\n",
       "<g id=\"node84\" class=\"node\">\n",
       "<title>1759433525616</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2326,-392 2160,-392 2160,-372 2326,-372 2326,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"2243\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433525616&#45;&gt;1759433524032 -->\n",
       "<g id=\"edge86\" class=\"edge\">\n",
       "<title>1759433525616&#45;&gt;1759433524032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2196.64,-371.58C2155.17,-362.97 2092.81,-349.54 2039,-336 2035.72,-335.18 2032.34,-334.29 2028.94,-333.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2030.18,-330.09 2019.61,-330.83 2028.33,-336.84 2030.18,-330.09\"/>\n",
       "</g>\n",
       "<!-- 1759433525520 -->\n",
       "<g id=\"node85\" class=\"node\">\n",
       "<title>1759433525520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2442,-448 2282,-448 2282,-428 2442,-428 2442,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"2362\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525520&#45;&gt;1759433525616 -->\n",
       "<g id=\"edge87\" class=\"edge\">\n",
       "<title>1759433525520&#45;&gt;1759433525616</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2341.27,-427.59C2322.79,-419.21 2295.46,-406.8 2274.31,-397.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2275.82,-394.05 2265.27,-393.1 2272.93,-400.42 2275.82,-394.05\"/>\n",
       "</g>\n",
       "<!-- 1759433527008 -->\n",
       "<g id=\"node86\" class=\"node\">\n",
       "<title>1759433527008</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2469,-504 2375,-504 2375,-484 2469,-484 2469,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2422\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433527008&#45;&gt;1759433525520 -->\n",
       "<g id=\"edge88\" class=\"edge\">\n",
       "<title>1759433527008&#45;&gt;1759433525520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2411.55,-483.59C2403.03,-475.93 2390.79,-464.91 2380.61,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2383.23,-453.4 2373.46,-449.31 2378.55,-458.6 2383.23,-453.4\"/>\n",
       "</g>\n",
       "<!-- 1759433526384 -->\n",
       "<g id=\"node87\" class=\"node\">\n",
       "<title>1759433526384</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2496,-566 2360,-566 2360,-546 2496,-546 2496,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"2428\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526384&#45;&gt;1759433527008 -->\n",
       "<g id=\"edge89\" class=\"edge\">\n",
       "<title>1759433526384&#45;&gt;1759433527008</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2427.06,-545.62C2426.26,-537.56 2425.07,-525.65 2424.04,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2427.54,-515.22 2423.06,-505.62 2420.57,-515.92 2427.54,-515.22\"/>\n",
       "</g>\n",
       "<!-- 1759433526288 -->\n",
       "<g id=\"node88\" class=\"node\">\n",
       "<title>1759433526288</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2495,-628 2383,-628 2383,-608 2495,-608 2495,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2439\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526288&#45;&gt;1759433526384 -->\n",
       "<g id=\"edge90\" class=\"edge\">\n",
       "<title>1759433526288&#45;&gt;1759433526384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2437.28,-607.62C2435.8,-599.56 2433.62,-587.65 2431.74,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2435.19,-576.81 2429.94,-567.6 2428.3,-578.07 2435.19,-576.81\"/>\n",
       "</g>\n",
       "<!-- 1759433525088 -->\n",
       "<g id=\"node89\" class=\"node\">\n",
       "<title>1759433525088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2504,-690 2380,-690 2380,-670 2504,-670 2504,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"2442\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525088&#45;&gt;1759433526288 -->\n",
       "<g id=\"edge91\" class=\"edge\">\n",
       "<title>1759433525088&#45;&gt;1759433526288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2441.53,-669.62C2441.13,-661.56 2440.53,-649.65 2440.02,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2443.53,-639.44 2439.53,-629.62 2436.54,-639.79 2443.53,-639.44\"/>\n",
       "</g>\n",
       "<!-- 1759433539696&#45;&gt;1759433525088 -->\n",
       "<g id=\"edge92\" class=\"edge\">\n",
       "<title>1759433539696&#45;&gt;1759433525088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2299.41,-731.62C2327.72,-721.57 2372.91,-705.53 2404.82,-694.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2405.82,-697.56 2414.07,-690.91 2403.48,-690.96 2405.82,-697.56\"/>\n",
       "</g>\n",
       "<!-- 1759433464848 -->\n",
       "<g id=\"node91\" class=\"node\">\n",
       "<title>1759433464848</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2623,-628 2523,-628 2523,-608 2623,-608 2623,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2573\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464848&#45;&gt;1759433526384 -->\n",
       "<g id=\"edge94\" class=\"edge\">\n",
       "<title>1759433464848&#45;&gt;1759433526384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2550.34,-607.62C2526.4,-597.72 2488.4,-581.99 2461.11,-570.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2462.47,-567.48 2451.9,-566.89 2459.8,-573.95 2462.47,-567.48\"/>\n",
       "</g>\n",
       "<!-- 1759421853760 -->\n",
       "<g id=\"node92\" class=\"node\">\n",
       "<title>1759421853760</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2646,-696 2522,-696 2522,-664 2646,-664 2646,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"2584\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool2.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2584\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421853760&#45;&gt;1759433464848 -->\n",
       "<g id=\"edge95\" class=\"edge\">\n",
       "<title>1759421853760&#45;&gt;1759433464848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2581.17,-663.55C2579.83,-656.26 2578.22,-647.45 2576.78,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2580.24,-639.1 2575,-629.9 2573.36,-640.36 2580.24,-639.1\"/>\n",
       "</g>\n",
       "<!-- 1759433464656 -->\n",
       "<g id=\"node93\" class=\"node\">\n",
       "<title>1759433464656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2362,-628 2262,-628 2262,-608 2362,-608 2362,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2312\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464656&#45;&gt;1759433526384 -->\n",
       "<g id=\"edge96\" class=\"edge\">\n",
       "<title>1759433464656&#45;&gt;1759433526384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2330.12,-607.62C2348.85,-597.94 2378.33,-582.69 2400.05,-571.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2401.38,-574.71 2408.66,-567 2398.17,-568.49 2401.38,-574.71\"/>\n",
       "</g>\n",
       "<!-- 1759421853840 -->\n",
       "<g id=\"node94\" class=\"node\">\n",
       "<title>1759421853840</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2362,-696 2250,-696 2250,-664 2362,-664 2362,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"2306\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool2.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"2306\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421853840&#45;&gt;1759433464656 -->\n",
       "<g id=\"edge97\" class=\"edge\">\n",
       "<title>1759421853840&#45;&gt;1759433464656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2307.55,-663.55C2308.27,-656.26 2309.15,-647.45 2309.94,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2313.4,-640.21 2310.91,-629.91 2306.43,-639.52 2313.4,-640.21\"/>\n",
       "</g>\n",
       "<!-- 1759433463696 -->\n",
       "<g id=\"node95\" class=\"node\">\n",
       "<title>1759433463696</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2603,-504 2503,-504 2503,-484 2603,-484 2603,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2553\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463696&#45;&gt;1759433525520 -->\n",
       "<g id=\"edge98\" class=\"edge\">\n",
       "<title>1759433463696&#45;&gt;1759433525520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2519.73,-483.59C2488.29,-474.71 2440.91,-461.31 2406.29,-451.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2407.56,-448.24 2396.99,-448.89 2405.66,-454.98 2407.56,-448.24\"/>\n",
       "</g>\n",
       "<!-- 1759421854000 -->\n",
       "<g id=\"node96\" class=\"node\">\n",
       "<title>1759421854000</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2626,-572 2514,-572 2514,-540 2626,-540 2626,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"2570\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool2.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2570\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854000&#45;&gt;1759433463696 -->\n",
       "<g id=\"edge99\" class=\"edge\">\n",
       "<title>1759421854000&#45;&gt;1759433463696</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2565.62,-539.55C2563.53,-532.17 2561,-523.24 2558.76,-515.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2562.17,-514.53 2556.08,-505.86 2555.44,-516.44 2562.17,-514.53\"/>\n",
       "</g>\n",
       "<!-- 1759433462448 -->\n",
       "<g id=\"node97\" class=\"node\">\n",
       "<title>1759433462448</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2352,-504 2252,-504 2252,-484 2352,-484 2352,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2302\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462448&#45;&gt;1759433525520 -->\n",
       "<g id=\"edge100\" class=\"edge\">\n",
       "<title>1759433462448&#45;&gt;1759433525520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2312.45,-483.59C2320.97,-475.93 2333.21,-464.91 2343.39,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2345.45,-458.6 2350.54,-449.31 2340.77,-453.4 2345.45,-458.6\"/>\n",
       "</g>\n",
       "<!-- 1759421854080 -->\n",
       "<g id=\"node98\" class=\"node\">\n",
       "<title>1759421854080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2342,-572 2242,-572 2242,-540 2342,-540 2342,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"2292\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool2.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"2292\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854080&#45;&gt;1759433462448 -->\n",
       "<g id=\"edge101\" class=\"edge\">\n",
       "<title>1759421854080&#45;&gt;1759433462448</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2294.58,-539.55C2295.79,-532.26 2297.26,-523.45 2298.57,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2301.99,-516.34 2300.18,-505.9 2295.09,-515.19 2301.99,-516.34\"/>\n",
       "</g>\n",
       "<!-- 1759433524368 -->\n",
       "<g id=\"node99\" class=\"node\">\n",
       "<title>1759433524368</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"424,-330 336,-330 336,-310 424,-310 424,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"380\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524368&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge102\" class=\"edge\">\n",
       "<title>1759433524368&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M424.34,-316.78C608.89,-307.52 1315.9,-272.05 1520.33,-261.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1520.34,-265.3 1530.15,-261.3 1519.99,-258.31 1520.34,-265.3\"/>\n",
       "</g>\n",
       "<!-- 1759433526480 -->\n",
       "<g id=\"node100\" class=\"node\">\n",
       "<title>1759433526480</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"411,-392 245,-392 245,-372 411,-372 411,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"328\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526480&#45;&gt;1759433524368 -->\n",
       "<g id=\"edge103\" class=\"edge\">\n",
       "<title>1759433526480&#45;&gt;1759433524368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.12,-371.62C343.72,-362.86 355.27,-349.53 364.62,-338.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"367.21,-341.11 371.11,-331.26 361.92,-336.52 367.21,-341.11\"/>\n",
       "</g>\n",
       "<!-- 1759433524992 -->\n",
       "<g id=\"node101\" class=\"node\">\n",
       "<title>1759433524992</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"362,-448 202,-448 202,-428 362,-428 362,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"282\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524992&#45;&gt;1759433526480 -->\n",
       "<g id=\"edge104\" class=\"edge\">\n",
       "<title>1759433524992&#45;&gt;1759433526480</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.01,-427.59C296.28,-420.24 305.17,-409.8 312.78,-400.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"315.28,-403.33 319.1,-393.45 309.95,-398.79 315.28,-403.33\"/>\n",
       "</g>\n",
       "<!-- 1759433525280 -->\n",
       "<g id=\"node102\" class=\"node\">\n",
       "<title>1759433525280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"268,-504 174,-504 174,-484 268,-484 268,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"221\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525280&#45;&gt;1759433524992 -->\n",
       "<g id=\"edge105\" class=\"edge\">\n",
       "<title>1759433525280&#45;&gt;1759433524992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M231.63,-483.59C240.28,-475.93 252.73,-464.91 263.08,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"265.2,-458.55 270.36,-449.3 260.56,-453.31 265.2,-458.55\"/>\n",
       "</g>\n",
       "<!-- 1759433540128 -->\n",
       "<g id=\"node103\" class=\"node\">\n",
       "<title>1759433540128</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"280,-566 144,-566 144,-546 280,-546 280,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540128&#45;&gt;1759433525280 -->\n",
       "<g id=\"edge106\" class=\"edge\">\n",
       "<title>1759433540128&#45;&gt;1759433525280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.41,-545.62C214.62,-537.56 216.4,-525.65 217.94,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"221.39,-516.02 219.41,-505.61 214.46,-514.98 221.39,-516.02\"/>\n",
       "</g>\n",
       "<!-- 1759433540032 -->\n",
       "<g id=\"node104\" class=\"node\">\n",
       "<title>1759433540032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"253,-628 141,-628 141,-608 253,-608 253,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540032&#45;&gt;1759433540128 -->\n",
       "<g id=\"edge107\" class=\"edge\">\n",
       "<title>1759433540032&#45;&gt;1759433540128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M199.34,-607.62C201.38,-599.48 204.4,-587.39 206.98,-577.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"210.32,-578.13 209.35,-567.58 203.53,-576.43 210.32,-578.13\"/>\n",
       "</g>\n",
       "<!-- 1759433540224 -->\n",
       "<g id=\"node105\" class=\"node\">\n",
       "<title>1759433540224</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"254,-690 130,-690 130,-670 254,-670 254,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"192\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540224&#45;&gt;1759433540032 -->\n",
       "<g id=\"edge108\" class=\"edge\">\n",
       "<title>1759433540224&#45;&gt;1759433540032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.78,-669.62C193.45,-661.56 194.45,-649.65 195.3,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.77,-639.88 196.11,-629.62 191.8,-639.3 198.77,-639.88\"/>\n",
       "</g>\n",
       "<!-- 1759433540320&#45;&gt;1759433540224 -->\n",
       "<g id=\"edge109\" class=\"edge\">\n",
       "<title>1759433540320&#45;&gt;1759433540224</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.94,-731.62C210.33,-723.3 204.94,-710.86 200.41,-700.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.74,-699.29 196.55,-691.5 197.32,-702.07 203.74,-699.29\"/>\n",
       "</g>\n",
       "<!-- 1759433465376 -->\n",
       "<g id=\"node107\" class=\"node\">\n",
       "<title>1759433465376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"375,-628 275,-628 275,-608 375,-608 375,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"325\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433465376&#45;&gt;1759433540128 -->\n",
       "<g id=\"edge111\" class=\"edge\">\n",
       "<title>1759433465376&#45;&gt;1759433540128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M307.34,-607.62C289.18,-597.98 260.64,-582.83 239.52,-571.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"241.34,-568.62 230.87,-567.02 238.06,-574.8 241.34,-568.62\"/>\n",
       "</g>\n",
       "<!-- 1759421854400 -->\n",
       "<g id=\"node108\" class=\"node\">\n",
       "<title>1759421854400</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"396,-696 272,-696 272,-664 396,-664 396,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"334\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool3.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"334\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421854400&#45;&gt;1759433465376 -->\n",
       "<g id=\"edge112\" class=\"edge\">\n",
       "<title>1759421854400&#45;&gt;1759433465376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M331.68,-663.55C330.59,-656.26 329.27,-647.45 328.09,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.58,-639.27 326.64,-629.9 324.66,-640.31 331.58,-639.27\"/>\n",
       "</g>\n",
       "<!-- 1759433465232 -->\n",
       "<g id=\"node109\" class=\"node\">\n",
       "<title>1759433465232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"114,-628 14,-628 14,-608 114,-608 114,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"64\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433465232&#45;&gt;1759433540128 -->\n",
       "<g id=\"edge113\" class=\"edge\">\n",
       "<title>1759433465232&#45;&gt;1759433540128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M87.12,-607.62C111.67,-597.67 150.69,-581.85 178.58,-570.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179.68,-573.88 187.63,-566.88 177.05,-567.39 179.68,-573.88\"/>\n",
       "</g>\n",
       "<!-- 1759421854480 -->\n",
       "<g id=\"node110\" class=\"node\">\n",
       "<title>1759421854480</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"112,-696 0,-696 0,-664 112,-664 112,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool3.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854480&#45;&gt;1759433465232 -->\n",
       "<g id=\"edge114\" class=\"edge\">\n",
       "<title>1759421854480&#45;&gt;1759433465232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58.06,-663.55C59.03,-656.26 60.21,-647.45 61.25,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.69,-640.28 62.55,-629.91 57.75,-639.36 64.69,-640.28\"/>\n",
       "</g>\n",
       "<!-- 1759433465040 -->\n",
       "<g id=\"node111\" class=\"node\">\n",
       "<title>1759433465040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"392,-504 292,-504 292,-484 392,-484 392,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"342\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433465040&#45;&gt;1759433524992 -->\n",
       "<g id=\"edge115\" class=\"edge\">\n",
       "<title>1759433465040&#45;&gt;1759433524992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M331.55,-483.59C323.03,-475.93 310.79,-464.91 300.61,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.23,-453.4 293.46,-449.31 298.55,-458.6 303.23,-453.4\"/>\n",
       "</g>\n",
       "<!-- 1759421854560 -->\n",
       "<g id=\"node112\" class=\"node\">\n",
       "<title>1759421854560</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"410,-572 298,-572 298,-540 410,-540 410,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"354\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool3.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"354\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854560&#45;&gt;1759433465040 -->\n",
       "<g id=\"edge116\" class=\"edge\">\n",
       "<title>1759421854560&#45;&gt;1759433465040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M350.91,-539.55C349.45,-532.26 347.69,-523.45 346.12,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.57,-515.01 344.18,-505.89 342.71,-516.38 349.57,-515.01\"/>\n",
       "</g>\n",
       "<!-- 1759433462688 -->\n",
       "<g id=\"node113\" class=\"node\">\n",
       "<title>1759433462688</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"150,-504 50,-504 50,-484 150,-484 150,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"100\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462688&#45;&gt;1759433524992 -->\n",
       "<g id=\"edge117\" class=\"edge\">\n",
       "<title>1759433462688&#45;&gt;1759433524992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M131.7,-483.59C161.53,-474.74 206.42,-461.43 239.37,-451.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.01,-455.11 248.6,-448.91 238.02,-448.4 240.01,-455.11\"/>\n",
       "</g>\n",
       "<!-- 1759421854640 -->\n",
       "<g id=\"node114\" class=\"node\">\n",
       "<title>1759421854640</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"126,-572 26,-572 26,-540 126,-540 126,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"76\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool3.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"76\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854640&#45;&gt;1759433462688 -->\n",
       "<g id=\"edge118\" class=\"edge\">\n",
       "<title>1759421854640&#45;&gt;1759433462688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82.18,-539.55C85.17,-532.08 88.79,-523.03 91.98,-515.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.21,-516.4 95.67,-505.81 88.71,-513.8 95.21,-516.4\"/>\n",
       "</g>\n",
       "<!-- 1759433524560 -->\n",
       "<g id=\"node115\" class=\"node\">\n",
       "<title>1759433524560</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"942,-330 854,-330 854,-310 942,-310 942,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"898\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524560&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge119\" class=\"edge\">\n",
       "<title>1759433524560&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M942.49,-315.06C1061.88,-304.5 1389.18,-275.53 1520.45,-263.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1520.63,-267.41 1530.28,-263.05 1520.02,-260.44 1520.63,-267.41\"/>\n",
       "</g>\n",
       "<!-- 1759433527248 -->\n",
       "<g id=\"node116\" class=\"node\">\n",
       "<title>1759433527248</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"874,-392 708,-392 708,-372 874,-372 874,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433527248&#45;&gt;1759433524560 -->\n",
       "<g id=\"edge120\" class=\"edge\">\n",
       "<title>1759433527248&#45;&gt;1759433524560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M807.72,-371.62C824.76,-362.07 851.45,-347.11 871.39,-335.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"873.07,-338.99 880.08,-331.05 869.64,-332.89 873.07,-338.99\"/>\n",
       "</g>\n",
       "<!-- 1759433539744 -->\n",
       "<g id=\"node117\" class=\"node\">\n",
       "<title>1759433539744</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"766,-448 606,-448 606,-428 766,-428 766,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"686\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539744&#45;&gt;1759433527248 -->\n",
       "<g id=\"edge121\" class=\"edge\">\n",
       "<title>1759433539744&#45;&gt;1759433527248</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M704.29,-427.59C720.3,-419.36 743.83,-407.26 762.34,-397.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"763.93,-400.86 771.22,-393.17 760.73,-394.63 763.93,-400.86\"/>\n",
       "</g>\n",
       "<!-- 1759433540272 -->\n",
       "<g id=\"node118\" class=\"node\">\n",
       "<title>1759433540272</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"673,-504 579,-504 579,-484 673,-484 673,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"626\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540272&#45;&gt;1759433539744 -->\n",
       "<g id=\"edge122\" class=\"edge\">\n",
       "<title>1759433540272&#45;&gt;1759433539744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M636.45,-483.59C644.97,-475.93 657.21,-464.91 667.39,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"669.45,-458.6 674.54,-449.31 664.77,-453.4 669.45,-458.6\"/>\n",
       "</g>\n",
       "<!-- 1759433540368 -->\n",
       "<g id=\"node119\" class=\"node\">\n",
       "<title>1759433540368</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"685,-566 549,-566 549,-546 685,-546 685,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"617\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540368&#45;&gt;1759433540272 -->\n",
       "<g id=\"edge123\" class=\"edge\">\n",
       "<title>1759433540368&#45;&gt;1759433540272</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M618.41,-545.62C619.62,-537.56 621.4,-525.65 622.94,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"626.39,-516.02 624.41,-505.61 619.46,-514.98 626.39,-516.02\"/>\n",
       "</g>\n",
       "<!-- 1759433540464 -->\n",
       "<g id=\"node120\" class=\"node\">\n",
       "<title>1759433540464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"668,-628 556,-628 556,-608 668,-608 668,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"612\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540464&#45;&gt;1759433540368 -->\n",
       "<g id=\"edge124\" class=\"edge\">\n",
       "<title>1759433540464&#45;&gt;1759433540368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M612.78,-607.62C613.45,-599.56 614.45,-587.65 615.3,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"618.77,-577.88 616.11,-567.62 611.8,-577.3 618.77,-577.88\"/>\n",
       "</g>\n",
       "<!-- 1759433540560 -->\n",
       "<g id=\"node121\" class=\"node\">\n",
       "<title>1759433540560</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"668,-690 544,-690 544,-670 668,-670 668,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"606\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540560&#45;&gt;1759433540464 -->\n",
       "<g id=\"edge125\" class=\"edge\">\n",
       "<title>1759433540560&#45;&gt;1759433540464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M606.94,-669.62C607.74,-661.56 608.93,-649.65 609.96,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"613.43,-639.92 610.94,-629.62 606.46,-639.22 613.43,-639.92\"/>\n",
       "</g>\n",
       "<!-- 1759433540656&#45;&gt;1759433540560 -->\n",
       "<g id=\"edge126\" class=\"edge\">\n",
       "<title>1759433540656&#45;&gt;1759433540560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M742.69,-731.62C715.66,-721.62 672.6,-705.67 642.04,-694.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"643.41,-691.12 632.82,-690.93 640.98,-697.69 643.41,-691.12\"/>\n",
       "</g>\n",
       "<!-- 1759433494640 -->\n",
       "<g id=\"node123\" class=\"node\">\n",
       "<title>1759433494640</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"792,-628 692,-628 692,-608 792,-608 792,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"742\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494640&#45;&gt;1759433540368 -->\n",
       "<g id=\"edge128\" class=\"edge\">\n",
       "<title>1759433494640&#45;&gt;1759433540368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M722.47,-607.62C702.1,-597.85 669.94,-582.41 646.49,-571.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"648.29,-568.14 637.76,-566.96 645.26,-574.45 648.29,-568.14\"/>\n",
       "</g>\n",
       "<!-- 1759421854960 -->\n",
       "<g id=\"node124\" class=\"node\">\n",
       "<title>1759421854960</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"810,-696 686,-696 686,-664 810,-664 810,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"748\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool4.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"748\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421854960&#45;&gt;1759433494640 -->\n",
       "<g id=\"edge129\" class=\"edge\">\n",
       "<title>1759421854960&#45;&gt;1759433494640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M746.45,-663.55C745.73,-656.26 744.85,-647.45 744.06,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"747.57,-639.52 743.09,-629.91 740.6,-640.21 747.57,-639.52\"/>\n",
       "</g>\n",
       "<!-- 1759433494592 -->\n",
       "<g id=\"node125\" class=\"node\">\n",
       "<title>1759433494592</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"529,-628 429,-628 429,-608 529,-608 529,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"479\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494592&#45;&gt;1759433540368 -->\n",
       "<g id=\"edge130\" class=\"edge\">\n",
       "<title>1759433494592&#45;&gt;1759433540368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M500.56,-607.62C523.25,-597.76 559.19,-582.13 585.15,-570.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"586.43,-574.11 594.2,-566.91 583.63,-567.69 586.43,-574.11\"/>\n",
       "</g>\n",
       "<!-- 1759421855040 -->\n",
       "<g id=\"node126\" class=\"node\">\n",
       "<title>1759421855040</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"526,-696 414,-696 414,-664 526,-664 526,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"470\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool4.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"470\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421855040&#45;&gt;1759433494592 -->\n",
       "<g id=\"edge131\" class=\"edge\">\n",
       "<title>1759421855040&#45;&gt;1759433494592</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M472.32,-663.55C473.41,-656.26 474.73,-647.45 475.91,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"479.34,-640.31 477.36,-629.9 472.42,-639.27 479.34,-640.31\"/>\n",
       "</g>\n",
       "<!-- 1759433465568 -->\n",
       "<g id=\"node127\" class=\"node\">\n",
       "<title>1759433465568</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"797,-504 697,-504 697,-484 797,-484 797,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"747\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433465568&#45;&gt;1759433539744 -->\n",
       "<g id=\"edge132\" class=\"edge\">\n",
       "<title>1759433465568&#45;&gt;1759433539744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M736.37,-483.59C727.72,-475.93 715.27,-464.91 704.92,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"707.44,-453.31 697.64,-449.3 702.8,-458.55 707.44,-453.31\"/>\n",
       "</g>\n",
       "<!-- 1759421855120 -->\n",
       "<g id=\"node128\" class=\"node\">\n",
       "<title>1759421855120</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"815,-572 703,-572 703,-540 815,-540 815,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"759\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool4.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"759\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421855120&#45;&gt;1759433465568 -->\n",
       "<g id=\"edge133\" class=\"edge\">\n",
       "<title>1759421855120&#45;&gt;1759433465568</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M755.91,-539.55C754.45,-532.26 752.69,-523.45 751.12,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"754.57,-515.01 749.18,-505.89 747.71,-516.38 754.57,-515.01\"/>\n",
       "</g>\n",
       "<!-- 1759433462112 -->\n",
       "<g id=\"node129\" class=\"node\">\n",
       "<title>1759433462112</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"537,-504 437,-504 437,-484 537,-484 537,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"487\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462112&#45;&gt;1759433539744 -->\n",
       "<g id=\"edge134\" class=\"edge\">\n",
       "<title>1759433462112&#45;&gt;1759433539744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M521.66,-483.59C554.56,-474.67 604.21,-461.19 640.33,-451.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"640.87,-454.87 649.61,-448.88 639.04,-448.12 640.87,-454.87\"/>\n",
       "</g>\n",
       "<!-- 1759421855200 -->\n",
       "<g id=\"node130\" class=\"node\">\n",
       "<title>1759421855200</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"531,-572 431,-572 431,-540 531,-540 531,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"481\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool4.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"481\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421855200&#45;&gt;1759433462112 -->\n",
       "<g id=\"edge135\" class=\"edge\">\n",
       "<title>1759421855200&#45;&gt;1759433462112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M482.55,-539.55C483.27,-532.26 484.15,-523.45 484.94,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"488.4,-516.21 485.91,-505.91 481.43,-515.52 488.4,-516.21\"/>\n",
       "</g>\n",
       "<!-- 1759433524656 -->\n",
       "<g id=\"node131\" class=\"node\">\n",
       "<title>1759433524656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1254,-330 1166,-330 1166,-310 1254,-310 1254,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1210\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524656&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge136\" class=\"edge\">\n",
       "<title>1759433524656&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1254.15,-311.76C1320.92,-300.82 1447.53,-280.06 1520.47,-268.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1520.9,-271.58 1530.2,-266.51 1519.77,-264.67 1520.9,-271.58\"/>\n",
       "</g>\n",
       "<!-- 1759433526096 -->\n",
       "<g id=\"node132\" class=\"node\">\n",
       "<title>1759433526096</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1197,-392 1031,-392 1031,-372 1197,-372 1197,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1114\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526096&#45;&gt;1759433524656 -->\n",
       "<g id=\"edge137\" class=\"edge\">\n",
       "<title>1759433526096&#45;&gt;1759433524656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1129,-371.62C1144.08,-362.2 1167.57,-347.52 1185.38,-336.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1187.2,-339.38 1193.82,-331.11 1183.49,-333.44 1187.2,-339.38\"/>\n",
       "</g>\n",
       "<!-- 1759433540416 -->\n",
       "<g id=\"node133\" class=\"node\">\n",
       "<title>1759433540416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1162,-448 1002,-448 1002,-428 1162,-428 1162,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"1082\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540416&#45;&gt;1759433526096 -->\n",
       "<g id=\"edge138\" class=\"edge\">\n",
       "<title>1759433540416&#45;&gt;1759433526096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1087.57,-427.59C1091.75,-420.55 1097.6,-410.67 1102.74,-402\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1105.63,-403.99 1107.72,-393.6 1099.61,-400.42 1105.63,-403.99\"/>\n",
       "</g>\n",
       "<!-- 1759433540752 -->\n",
       "<g id=\"node134\" class=\"node\">\n",
       "<title>1759433540752</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"956,-504 862,-504 862,-484 956,-484 956,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"909\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540752&#45;&gt;1759433540416 -->\n",
       "<g id=\"edge139\" class=\"edge\">\n",
       "<title>1759433540752&#45;&gt;1759433540416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M939.13,-483.59C967.24,-474.82 1009.41,-461.66 1040.66,-451.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1041.68,-455.25 1050.19,-448.93 1039.6,-448.57 1041.68,-455.25\"/>\n",
       "</g>\n",
       "<!-- 1759433539936 -->\n",
       "<g id=\"node135\" class=\"node\">\n",
       "<title>1759433539936</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"970,-566 834,-566 834,-546 970,-546 970,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"902\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539936&#45;&gt;1759433540752 -->\n",
       "<g id=\"edge140\" class=\"edge\">\n",
       "<title>1759433539936&#45;&gt;1759433540752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M903.09,-545.62C904.03,-537.56 905.42,-525.65 906.62,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"910.08,-515.95 907.76,-505.62 903.13,-515.14 910.08,-515.95\"/>\n",
       "</g>\n",
       "<!-- 1759433540848 -->\n",
       "<g id=\"node136\" class=\"node\">\n",
       "<title>1759433540848</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"952,-628 840,-628 840,-608 952,-608 952,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"896\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540848&#45;&gt;1759433539936 -->\n",
       "<g id=\"edge141\" class=\"edge\">\n",
       "<title>1759433540848&#45;&gt;1759433539936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M896.94,-607.62C897.74,-599.56 898.93,-587.65 899.96,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"903.43,-577.92 900.94,-567.62 896.46,-577.22 903.43,-577.92\"/>\n",
       "</g>\n",
       "<!-- 1759433540944 -->\n",
       "<g id=\"node137\" class=\"node\">\n",
       "<title>1759433540944</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"952,-690 828,-690 828,-670 952,-670 952,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"890\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540944&#45;&gt;1759433540848 -->\n",
       "<g id=\"edge142\" class=\"edge\">\n",
       "<title>1759433540944&#45;&gt;1759433540848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M890.94,-669.62C891.74,-661.56 892.93,-649.65 893.96,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"897.43,-639.92 894.94,-629.62 890.46,-639.22 897.43,-639.92\"/>\n",
       "</g>\n",
       "<!-- 1759433541040&#45;&gt;1759433540944 -->\n",
       "<g id=\"edge143\" class=\"edge\">\n",
       "<title>1759433541040&#45;&gt;1759433540944</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1079,-731.62C1040.48,-721.31 978.42,-704.68 935.99,-693.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"937.15,-690.01 926.59,-690.8 935.34,-696.77 937.15,-690.01\"/>\n",
       "</g>\n",
       "<!-- 1759433495168 -->\n",
       "<g id=\"node139\" class=\"node\">\n",
       "<title>1759433495168</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1212,-628 1112,-628 1112,-608 1212,-608 1212,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433495168&#45;&gt;1759433539936 -->\n",
       "<g id=\"edge145\" class=\"edge\">\n",
       "<title>1759433495168&#45;&gt;1759433539936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1121.09,-607.56C1075.98,-597.15 1003.3,-580.38 954.14,-569.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"955.11,-565.66 944.58,-566.83 953.53,-572.48 955.11,-565.66\"/>\n",
       "</g>\n",
       "<!-- 1759433541088 -->\n",
       "<g id=\"node151\" class=\"node\">\n",
       "<title>1759433541088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1372,-566 1236,-566 1236,-546 1372,-546 1372,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"1304\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433495168&#45;&gt;1759433541088 -->\n",
       "<g id=\"edge162\" class=\"edge\">\n",
       "<title>1759433495168&#45;&gt;1759433541088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1184.19,-607.62C1207.63,-597.72 1244.85,-581.99 1271.58,-570.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1272.72,-574.01 1280.57,-566.9 1270,-567.57 1272.72,-574.01\"/>\n",
       "</g>\n",
       "<!-- 1759421856080 -->\n",
       "<g id=\"node140\" class=\"node\">\n",
       "<title>1759421856080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1224,-696 1100,-696 1100,-664 1224,-664 1224,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool6.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421856080&#45;&gt;1759433495168 -->\n",
       "<g id=\"edge146\" class=\"edge\">\n",
       "<title>1759421856080&#45;&gt;1759433495168</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1162,-663.55C1162,-656.34 1162,-647.66 1162,-639.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1165.5,-639.92 1162,-629.92 1158.5,-639.92 1165.5,-639.92\"/>\n",
       "</g>\n",
       "<!-- 1759433495024 -->\n",
       "<g id=\"node141\" class=\"node\">\n",
       "<title>1759433495024</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1076,-628 976,-628 976,-608 1076,-608 1076,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1026\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433495024&#45;&gt;1759433539936 -->\n",
       "<g id=\"edge147\" class=\"edge\">\n",
       "<title>1759433495024&#45;&gt;1759433539936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1006.62,-607.62C986.51,-597.89 954.81,-582.55 931.57,-571.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"933.13,-568.17 922.6,-566.97 930.08,-574.47 933.13,-568.17\"/>\n",
       "</g>\n",
       "<!-- 1759433495024&#45;&gt;1759433541088 -->\n",
       "<g id=\"edge163\" class=\"edge\">\n",
       "<title>1759433495024&#45;&gt;1759433541088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1069.74,-607.56C1118.18,-597.11 1196.34,-580.24 1248.92,-568.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1249.54,-572.34 1258.57,-566.8 1248.06,-565.49 1249.54,-572.34\"/>\n",
       "</g>\n",
       "<!-- 1759421856160 -->\n",
       "<g id=\"node142\" class=\"node\">\n",
       "<title>1759421856160</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1082,-696 970,-696 970,-664 1082,-664 1082,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1026\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool6.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1026\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856160&#45;&gt;1759433495024 -->\n",
       "<g id=\"edge148\" class=\"edge\">\n",
       "<title>1759421856160&#45;&gt;1759433495024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1026,-663.55C1026,-656.34 1026,-647.66 1026,-639.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1029.5,-639.92 1026,-629.92 1022.5,-639.92 1029.5,-639.92\"/>\n",
       "</g>\n",
       "<!-- 1759433494832 -->\n",
       "<g id=\"node143\" class=\"node\">\n",
       "<title>1759433494832</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1219,-504 1119,-504 1119,-484 1219,-484 1219,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1169\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494832&#45;&gt;1759433540416 -->\n",
       "<g id=\"edge149\" class=\"edge\">\n",
       "<title>1759433494832&#45;&gt;1759433540416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1153.85,-483.59C1140.95,-475.59 1122.17,-463.93 1107.04,-454.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1108.9,-451.58 1098.56,-449.28 1105.21,-457.53 1108.9,-451.58\"/>\n",
       "</g>\n",
       "<!-- 1759433540800 -->\n",
       "<g id=\"node149\" class=\"node\">\n",
       "<title>1759433540800</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1355,-448 1195,-448 1195,-428 1355,-428 1355,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"1275\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433494832&#45;&gt;1759433540800 -->\n",
       "<g id=\"edge164\" class=\"edge\">\n",
       "<title>1759433494832&#45;&gt;1759433540800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1187.46,-483.59C1203.7,-475.32 1227.61,-463.14 1246.33,-453.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1247.72,-456.82 1255.04,-449.17 1244.54,-450.59 1247.72,-456.82\"/>\n",
       "</g>\n",
       "<!-- 1759421856240 -->\n",
       "<g id=\"node144\" class=\"node\">\n",
       "<title>1759421856240</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1218,-572 1106,-572 1106,-540 1218,-540 1218,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool6.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856240&#45;&gt;1759433494832 -->\n",
       "<g id=\"edge150\" class=\"edge\">\n",
       "<title>1759421856240&#45;&gt;1759433494832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1163.8,-539.55C1164.65,-532.26 1165.68,-523.45 1166.6,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1170.04,-516.25 1167.73,-505.91 1163.09,-515.44 1170.04,-516.25\"/>\n",
       "</g>\n",
       "<!-- 1759433494736 -->\n",
       "<g id=\"node145\" class=\"node\">\n",
       "<title>1759433494736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1101,-504 1001,-504 1001,-484 1101,-484 1101,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1051\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494736&#45;&gt;1759433540416 -->\n",
       "<g id=\"edge151\" class=\"edge\">\n",
       "<title>1759433494736&#45;&gt;1759433540416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1056.4,-483.59C1060.44,-476.55 1066.12,-466.67 1071.09,-458\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1073.97,-460.03 1075.91,-449.61 1067.89,-456.54 1073.97,-460.03\"/>\n",
       "</g>\n",
       "<!-- 1759433494736&#45;&gt;1759433540800 -->\n",
       "<g id=\"edge165\" class=\"edge\">\n",
       "<title>1759433494736&#45;&gt;1759433540800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1090.27,-483.53C1127.66,-474.52 1184.1,-460.91 1224.75,-451.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1225.4,-454.56 1234.3,-448.81 1223.76,-447.75 1225.4,-454.56\"/>\n",
       "</g>\n",
       "<!-- 1759421856320 -->\n",
       "<g id=\"node146\" class=\"node\">\n",
       "<title>1759421856320</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1088,-572 988,-572 988,-540 1088,-540 1088,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1038\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool6.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1038\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856320&#45;&gt;1759433494736 -->\n",
       "<g id=\"edge152\" class=\"edge\">\n",
       "<title>1759421856320&#45;&gt;1759433494736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1041.35,-539.55C1042.93,-532.26 1044.84,-523.45 1046.54,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1049.94,-516.4 1048.64,-505.89 1043.1,-514.92 1049.94,-516.4\"/>\n",
       "</g>\n",
       "<!-- 1759433524704 -->\n",
       "<g id=\"node147\" class=\"node\">\n",
       "<title>1759433524704</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1463,-330 1375,-330 1375,-310 1463,-310 1463,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1419\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524704&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge153\" class=\"edge\">\n",
       "<title>1759433524704&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1443.53,-309.62C1469.61,-299.66 1511.09,-283.81 1540.68,-272.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1541.88,-275.79 1549.97,-268.95 1539.38,-269.25 1541.88,-275.79\"/>\n",
       "</g>\n",
       "<!-- 1759433540080 -->\n",
       "<g id=\"node148\" class=\"node\">\n",
       "<title>1759433540080</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1444,-392 1278,-392 1278,-372 1444,-372 1444,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1361\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540080&#45;&gt;1759433524704 -->\n",
       "<g id=\"edge154\" class=\"edge\">\n",
       "<title>1759433540080&#45;&gt;1759433524704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1370.06,-371.62C1378.71,-362.68 1391.94,-348.99 1402.47,-338.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1404.71,-340.82 1409.14,-331.2 1399.67,-335.96 1404.71,-340.82\"/>\n",
       "</g>\n",
       "<!-- 1759433540800&#45;&gt;1759433540080 -->\n",
       "<g id=\"edge155\" class=\"edge\">\n",
       "<title>1759433540800&#45;&gt;1759433540080</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1289.98,-427.59C1302.73,-419.59 1321.29,-407.93 1336.24,-398.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1338.01,-401.57 1344.62,-393.28 1334.29,-395.64 1338.01,-401.57\"/>\n",
       "</g>\n",
       "<!-- 1759433540992 -->\n",
       "<g id=\"node150\" class=\"node\">\n",
       "<title>1759433540992</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1338,-504 1244,-504 1244,-484 1338,-484 1338,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1291\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540992&#45;&gt;1759433540800 -->\n",
       "<g id=\"edge156\" class=\"edge\">\n",
       "<title>1759433540992&#45;&gt;1759433540800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1288.21,-483.59C1286.22,-476.86 1283.45,-467.53 1280.97,-459.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1284.38,-458.34 1278.19,-449.75 1277.67,-460.33 1284.38,-458.34\"/>\n",
       "</g>\n",
       "<!-- 1759433541088&#45;&gt;1759433540992 -->\n",
       "<g id=\"edge157\" class=\"edge\">\n",
       "<title>1759433541088&#45;&gt;1759433540992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1301.97,-545.62C1300.2,-537.48 1297.58,-525.39 1295.35,-515.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1298.83,-514.62 1293.29,-505.59 1291.99,-516.11 1298.83,-514.62\"/>\n",
       "</g>\n",
       "<!-- 1759433541184 -->\n",
       "<g id=\"node152\" class=\"node\">\n",
       "<title>1759433541184</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1360,-628 1248,-628 1248,-608 1360,-608 1360,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1304\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433541184&#45;&gt;1759433541088 -->\n",
       "<g id=\"edge158\" class=\"edge\">\n",
       "<title>1759433541184&#45;&gt;1759433541088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1304,-607.62C1304,-599.56 1304,-587.65 1304,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1307.5,-577.63 1304,-567.63 1300.5,-577.63 1307.5,-577.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541280 -->\n",
       "<g id=\"node153\" class=\"node\">\n",
       "<title>1759433541280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1366,-690 1242,-690 1242,-670 1366,-670 1366,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"1304\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433541280&#45;&gt;1759433541184 -->\n",
       "<g id=\"edge159\" class=\"edge\">\n",
       "<title>1759433541280&#45;&gt;1759433541184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1304,-669.62C1304,-661.56 1304,-649.65 1304,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1307.5,-639.63 1304,-629.63 1300.5,-639.63 1307.5,-639.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541376&#45;&gt;1759433541280 -->\n",
       "<g id=\"edge160\" class=\"edge\">\n",
       "<title>1759433541376&#45;&gt;1759433541280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1349.56,-731.62C1341.59,-722.77 1329.43,-709.26 1319.68,-698.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1322.51,-696.33 1313.21,-691.24 1317.3,-701.01 1322.51,-696.33\"/>\n",
       "</g>\n",
       "<!-- 1759433524752 -->\n",
       "<g id=\"node155\" class=\"node\">\n",
       "<title>1759433524752</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1620,-330 1532,-330 1532,-310 1620,-310 1620,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524752&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge166\" class=\"edge\">\n",
       "<title>1759433524752&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-309.62C1576,-301.56 1576,-289.65 1576,-279.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-279.63 1576,-269.63 1572.5,-279.63 1579.5,-279.63\"/>\n",
       "</g>\n",
       "<!-- 1759433540896 -->\n",
       "<g id=\"node156\" class=\"node\">\n",
       "<title>1759433540896</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1659,-392 1493,-392 1493,-372 1659,-372 1659,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540896&#45;&gt;1759433524752 -->\n",
       "<g id=\"edge167\" class=\"edge\">\n",
       "<title>1759433540896&#45;&gt;1759433524752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-371.62C1576,-363.56 1576,-351.65 1576,-341.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-341.63 1576,-331.63 1572.5,-341.63 1579.5,-341.63\"/>\n",
       "</g>\n",
       "<!-- 1759433539648 -->\n",
       "<g id=\"node157\" class=\"node\">\n",
       "<title>1759433539648</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1656,-448 1496,-448 1496,-428 1656,-428 1656,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539648&#45;&gt;1759433540896 -->\n",
       "<g id=\"edge168\" class=\"edge\">\n",
       "<title>1759433539648&#45;&gt;1759433540896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-427.59C1576,-421.01 1576,-411.96 1576,-403.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-403.81 1576,-393.81 1572.5,-403.81 1579.5,-403.81\"/>\n",
       "</g>\n",
       "<!-- 1759433541328 -->\n",
       "<g id=\"node158\" class=\"node\">\n",
       "<title>1759433541328</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1623,-504 1529,-504 1529,-484 1623,-484 1623,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433541328&#45;&gt;1759433539648 -->\n",
       "<g id=\"edge169\" class=\"edge\">\n",
       "<title>1759433541328&#45;&gt;1759433539648</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-483.59C1576,-477.01 1576,-467.96 1576,-459.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-459.81 1576,-449.81 1572.5,-459.81 1579.5,-459.81\"/>\n",
       "</g>\n",
       "<!-- 1759433541424 -->\n",
       "<g id=\"node159\" class=\"node\">\n",
       "<title>1759433541424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1644,-566 1508,-566 1508,-546 1644,-546 1644,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433541424&#45;&gt;1759433541328 -->\n",
       "<g id=\"edge170\" class=\"edge\">\n",
       "<title>1759433541424&#45;&gt;1759433541328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-545.62C1576,-537.56 1576,-525.65 1576,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-515.63 1576,-505.63 1572.5,-515.63 1579.5,-515.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541520 -->\n",
       "<g id=\"node160\" class=\"node\">\n",
       "<title>1759433541520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1632,-628 1520,-628 1520,-608 1632,-608 1632,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433541520&#45;&gt;1759433541424 -->\n",
       "<g id=\"edge171\" class=\"edge\">\n",
       "<title>1759433541520&#45;&gt;1759433541424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-607.62C1576,-599.56 1576,-587.65 1576,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-577.63 1576,-567.63 1572.5,-577.63 1579.5,-577.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541616 -->\n",
       "<g id=\"node161\" class=\"node\">\n",
       "<title>1759433541616</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1638,-690 1514,-690 1514,-670 1638,-670 1638,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433541616&#45;&gt;1759433541520 -->\n",
       "<g id=\"edge172\" class=\"edge\">\n",
       "<title>1759433541616&#45;&gt;1759433541520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-669.62C1576,-661.56 1576,-649.65 1576,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-639.63 1576,-629.63 1572.5,-639.63 1579.5,-639.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541712&#45;&gt;1759433541616 -->\n",
       "<g id=\"edge173\" class=\"edge\">\n",
       "<title>1759433541712&#45;&gt;1759433541616</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1532.97,-731.62C1540.42,-722.86 1551.75,-709.53 1560.91,-698.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1563.46,-701.15 1567.27,-691.27 1558.13,-696.62 1563.46,-701.15\"/>\n",
       "</g>\n",
       "<!-- 1759433496032 -->\n",
       "<g id=\"node163\" class=\"node\">\n",
       "<title>1759433496032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1759,-628 1659,-628 1659,-608 1759,-608 1759,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1709\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433496032&#45;&gt;1759433541424 -->\n",
       "<g id=\"edge175\" class=\"edge\">\n",
       "<title>1759433496032&#45;&gt;1759433541424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1688.22,-607.62C1666.45,-597.81 1632.02,-582.27 1607.04,-571\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1608.57,-567.85 1598.01,-566.93 1605.69,-574.23 1608.57,-567.85\"/>\n",
       "</g>\n",
       "<!-- 1759421856640 -->\n",
       "<g id=\"node164\" class=\"node\">\n",
       "<title>1759421856640</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1780,-696 1656,-696 1656,-664 1780,-664 1780,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool7.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421856640&#45;&gt;1759433496032 -->\n",
       "<g id=\"edge176\" class=\"edge\">\n",
       "<title>1759421856640&#45;&gt;1759433496032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1715.68,-663.55C1714.59,-656.26 1713.27,-647.45 1712.09,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1715.58,-639.27 1710.64,-629.9 1708.66,-640.31 1715.58,-639.27\"/>\n",
       "</g>\n",
       "<!-- 1759433495888 -->\n",
       "<g id=\"node165\" class=\"node\">\n",
       "<title>1759433495888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1496,-628 1396,-628 1396,-608 1496,-608 1496,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1446\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433495888&#45;&gt;1759433541424 -->\n",
       "<g id=\"edge177\" class=\"edge\">\n",
       "<title>1759433495888&#45;&gt;1759433541424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1466.31,-607.62C1487.59,-597.81 1521.24,-582.27 1545.66,-571\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1546.84,-574.31 1554.46,-566.94 1543.91,-567.96 1546.84,-574.31\"/>\n",
       "</g>\n",
       "<!-- 1759421856720 -->\n",
       "<g id=\"node166\" class=\"node\">\n",
       "<title>1759421856720</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1496,-696 1384,-696 1384,-664 1496,-664 1496,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1440\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool7.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1440\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856720&#45;&gt;1759433495888 -->\n",
       "<g id=\"edge178\" class=\"edge\">\n",
       "<title>1759421856720&#45;&gt;1759433495888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1441.55,-663.55C1442.27,-656.26 1443.15,-647.45 1443.94,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1447.4,-640.21 1444.91,-629.91 1440.43,-639.52 1447.4,-640.21\"/>\n",
       "</g>\n",
       "<!-- 1759433495696 -->\n",
       "<g id=\"node167\" class=\"node\">\n",
       "<title>1759433495696</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1754,-504 1654,-504 1654,-484 1754,-484 1754,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1704\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433495696&#45;&gt;1759433539648 -->\n",
       "<g id=\"edge179\" class=\"edge\">\n",
       "<title>1759433495696&#45;&gt;1759433539648</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1681.7,-483.59C1661.64,-475.13 1631.88,-462.58 1609.05,-452.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1610.44,-449.73 1599.86,-449.07 1607.72,-456.18 1610.44,-449.73\"/>\n",
       "</g>\n",
       "<!-- 1759421856800 -->\n",
       "<g id=\"node168\" class=\"node\">\n",
       "<title>1759421856800</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1774,-572 1662,-572 1662,-540 1774,-540 1774,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool7.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856800&#45;&gt;1759433495696 -->\n",
       "<g id=\"edge180\" class=\"edge\">\n",
       "<title>1759421856800&#45;&gt;1759433495696</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1714.39,-539.55C1712.69,-532.26 1710.64,-523.45 1708.81,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1712.22,-514.83 1706.54,-505.88 1705.4,-516.42 1712.22,-514.83\"/>\n",
       "</g>\n",
       "<!-- 1759433494784 -->\n",
       "<g id=\"node169\" class=\"node\">\n",
       "<title>1759433494784</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1501,-504 1401,-504 1401,-484 1501,-484 1501,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1451\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494784&#45;&gt;1759433539648 -->\n",
       "<g id=\"edge181\" class=\"edge\">\n",
       "<title>1759433494784&#45;&gt;1759433539648</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1472.77,-483.59C1492.37,-475.13 1521.43,-462.58 1543.72,-452.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1544.88,-456.26 1552.67,-449.08 1542.1,-449.83 1544.88,-456.26\"/>\n",
       "</g>\n",
       "<!-- 1759421856880 -->\n",
       "<g id=\"node170\" class=\"node\">\n",
       "<title>1759421856880</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1490,-572 1390,-572 1390,-540 1490,-540 1490,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1440\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool7.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1440\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856880&#45;&gt;1759433494784 -->\n",
       "<g id=\"edge182\" class=\"edge\">\n",
       "<title>1759421856880&#45;&gt;1759433494784</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1442.83,-539.55C1444.17,-532.26 1445.78,-523.45 1447.22,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1450.64,-516.36 1449,-505.9 1443.76,-515.1 1450.64,-516.36\"/>\n",
       "</g>\n",
       "<!-- 1759433293680 -->\n",
       "<g id=\"node171\" class=\"node\">\n",
       "<title>1759433293680</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1950,-268 1850,-268 1850,-248 1950,-248 1950,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"1900\" y=\"-254.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433293680&#45;&gt;1759433524320 -->\n",
       "<g id=\"edge183\" class=\"edge\">\n",
       "<title>1759433293680&#45;&gt;1759433524320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1857.83,-247.56C1811.24,-237.13 1736.1,-220.31 1685.42,-208.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1686.37,-205.58 1675.84,-206.82 1684.84,-212.42 1686.37,-205.58\"/>\n",
       "</g>\n",
       "<!-- 1759421857200 -->\n",
       "<g id=\"node172\" class=\"node\">\n",
       "<title>1759421857200</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2178,-336 2048,-336 2048,-304 2178,-304 2178,-336\"/>\n",
       "<text text-anchor=\"middle\" x=\"2113\" y=\"-322.5\" font-family=\"monospace\" font-size=\"10.00\">decoder.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2113\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\"> (768, 32, 8)</text>\n",
       "</g>\n",
       "<!-- 1759421857200&#45;&gt;1759433293680 -->\n",
       "<g id=\"edge184\" class=\"edge\">\n",
       "<title>1759421857200&#45;&gt;1759433293680</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2058.15,-303.55C2022.82,-293.6 1977.49,-280.83 1944.42,-271.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1945.74,-268.25 1935.17,-268.91 1943.85,-274.99 1945.74,-268.25\"/>\n",
       "</g>\n",
       "<!-- 1759433293776 -->\n",
       "<g id=\"node173\" class=\"node\">\n",
       "<title>1759433293776</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1827,-144 1727,-144 1727,-124 1827,-124 1827,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433293776&#45;&gt;1759433524176 -->\n",
       "<g id=\"edge185\" class=\"edge\">\n",
       "<title>1759433293776&#45;&gt;1759433524176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1777,-123.59C1777,-117.01 1777,-107.96 1777,-99.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1780.5,-99.81 1777,-89.81 1773.5,-99.81 1780.5,-99.81\"/>\n",
       "</g>\n",
       "<!-- 1759421857680 -->\n",
       "<g id=\"node174\" class=\"node\">\n",
       "<title>1759421857680</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1836,-212 1718,-212 1718,-180 1836,-180 1836,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-198.5\" font-family=\"monospace\" font-size=\"10.00\">activation.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\"> (3, 32, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421857680&#45;&gt;1759433293776 -->\n",
       "<g id=\"edge186\" class=\"edge\">\n",
       "<title>1759421857680&#45;&gt;1759433293776</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1777,-179.55C1777,-172.34 1777,-163.66 1777,-155.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1780.5,-155.92 1777,-145.92 1773.5,-155.92 1780.5,-155.92\"/>\n",
       "</g>\n",
       "<!-- 1759433292768 -->\n",
       "<g id=\"node175\" class=\"node\">\n",
       "<title>1759433292768</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1951,-144 1851,-144 1851,-124 1951,-124 1951,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"1901\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433292768&#45;&gt;1759433524176 -->\n",
       "<g id=\"edge187\" class=\"edge\">\n",
       "<title>1759433292768&#45;&gt;1759433524176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1879.4,-123.59C1860.05,-115.17 1831.4,-102.69 1809.32,-93.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1810.72,-89.87 1800.16,-89.08 1807.93,-96.29 1810.72,-89.87\"/>\n",
       "</g>\n",
       "<!-- 1759421956160 -->\n",
       "<g id=\"node176\" class=\"node\">\n",
       "<title>1759421956160</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1960,-212 1854,-212 1854,-180 1960,-180 1960,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"1907\" y=\"-198.5\" font-family=\"monospace\" font-size=\"10.00\">activation.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1907\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 1759421956160&#45;&gt;1759433292768 -->\n",
       "<g id=\"edge188\" class=\"edge\">\n",
       "<title>1759421956160&#45;&gt;1759433292768</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1905.45,-179.55C1904.73,-172.26 1903.85,-163.45 1903.06,-155.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1906.57,-155.52 1902.09,-145.91 1899.6,-156.21 1906.57,-155.52\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x199a66903a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcc12216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, n_epochs, filename):\n",
    "    \n",
    "    # 用于跟踪训练过程中的训练损失\n",
    "    train_losses = []\n",
    "    # 用于跟踪训练过程中的验证损失\n",
    "    valid_losses = []\n",
    "    # 用于跟踪训练过程中的测试损失\n",
    "    test_losses = []\n",
    "    # 用于跟踪每个 epoch 的平均训练损失\n",
    "    avg_train_losses = []\n",
    "    # 用于跟踪每个 epoch 的平均验证损失\n",
    "    avg_valid_losses = [] \n",
    "    # 用于跟踪每个 epoch 的平均测试损失\n",
    "    avg_test_losses = [] \n",
    "    \n",
    "    min_loss = np.inf\n",
    "    \n",
    "    # 初始化 early_stopping 对象\n",
    "    #patience = 10\n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # 训练模型 #\n",
    "        ###################\n",
    "        model.train() # 准备模型进行训练\n",
    "        for batch, (data, target_power, target_status) in enumerate(train_loader, 1):\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 清除所有优化变量的梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 反向传播：计算损失相对于模型参数的梯度\n",
    "            loss.backward()\n",
    "            # 执行单次优化步骤（参数更新）\n",
    "            optimizer.step()\n",
    "            # 记录训练损失\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # 验证模型 #\n",
    "        ######################\n",
    "        model.eval() # 准备模型进行评估\n",
    "        for data, target_power, target_status in valid_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 记录验证损失\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        ##################    \n",
    "        # 测试模型 #\n",
    "        ##################\n",
    "        model.eval() # 准备模型进行评估\n",
    "        for data, target_power, target_status in test_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 记录测试损失\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        # 打印训练/验证统计信息\n",
    "        # 计算一个 epoch 的平均损失\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        test_loss = np.average(test_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'test_loss: {test_loss:.5f} ')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # 清空列表以跟踪下一个 epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        # early_stopping 需要验证损失来检查是否有减少，\n",
    "        # 如果有减少，它将创建当前模型的检查点\n",
    "        #early_stopping(valid_loss, model)\n",
    "        #if (early_stopping.early_stop and (epoch > 80)):\n",
    "        #    break\n",
    "        \n",
    "        if valid_loss < min_loss:\n",
    "            print(f'验证损失减少 ({min_loss:.6f} --> {valid_loss:.6f}). 正在保存模型...')\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            min_loss = valid_loss\n",
    "        \n",
    "    # 加载具有最佳模型的最后一个检查点\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    return  model, avg_train_losses, avg_valid_losses, avg_test_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682e424",
   "metadata": {},
   "source": [
    "这段代码定义了一个训练模型的函数train_model，用于训练神经网络模型。在训练过程中，该函数会迭代多个epoch，每个epoch会对训练集、验证集和测试集进行一次前向传播和反向传播，并计算损失值。在每个epoch结束时，该函数会输出该epoch的平均训练损失、平均验证损失和平均测试损失，并将最好的模型保存下来。该函数的返回值包括训练好的模型和每个epoch的平均损失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e36dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_activation(model, loader, a):\n",
    "    x_true = []\n",
    "    s_true = []\n",
    "    p_true = []\n",
    "    s_hat = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, p, s in loader:\n",
    "            x = x.unsqueeze(1).cuda()\n",
    "            p = p.permute(0,2,1)[:,a,:]\n",
    "            s = s.permute(0,2,1)[:,a,:]\n",
    "            \n",
    "            sh = model(x)\n",
    "            sh = torch.sigmoid(sh[:,a,:])\n",
    "            \n",
    "            s_hat.append(sh.contiguous().view(-1).detach().cpu().numpy())\n",
    "            \n",
    "            x_true.append(x[:,:,BORDER:-BORDER].contiguous().view(-1).detach().cpu().numpy())\n",
    "            s_true.append(s.contiguous().view(-1).detach().cpu().numpy())\n",
    "            p_true.append(p.contiguous().view(-1).detach().cpu().numpy())\n",
    "    x_true = np.hstack(x_true)\n",
    "    s_true = np.hstack(s_true)\n",
    "    p_true = np.hstack(p_true)\n",
    "    s_hat = np.hstack(s_hat)\n",
    "\n",
    "    return x_true, p_true, s_true, s_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792b787",
   "metadata": {},
   "source": [
    "这段代码是一个函数，用于评估模型在给定数据集上的激活结果。\n",
    "\n",
    "首先，在函数内部定义了四个空列表：x_true、s_true、p_true和s_hat，用于存储计算结果。\n",
    "\n",
    "接下来，将模型设置为评估模式，即model.eval()。\n",
    "\n",
    "然后，通过一个循环遍历数据加载器loader中的每个批次。在每个批次中，执行以下操作：\n",
    "\n",
    "将输入数据x从(batch_size, sequence_length)的形状转换为(batch_size, 1, sequence_length)的形状，并将其移到GPU上（假设有可用的CUDA设备）。\n",
    "使用permute函数对位置编码p和状态s进行维度转换，将其从(batch_size, num_features, sequence_length)的形状转换为(batch_size, sequence_length)的形状，并选择第a个特征。\n",
    "将处理后的输入数据x输入到模型中，得到输出sh。使用sigmoid函数对输出进行激活，只选择第a个特征。\n",
    "将激活结果sh展平并转换为NumPy数组，然后将其添加到s_hat列表中。\n",
    "对输入数据x、位置编码p和状态s进行类似的处理，将它们展平并转换为NumPy数组，然后分别添加到x_true、s_true和p_true列表中。\n",
    "在循环结束后，将x_true、s_true、p_true和s_hat列表中的所有元素堆叠起来，得到最终的结果x_true、p_true、s_true和s_hat。\n",
    "\n",
    "最后，将这四个结果作为函数的返回值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d506ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['fridge', 'dish_washer', 'washing_machine']\n",
    "THRESHOLD = [50., 10., 20.]\n",
    "MIN_ON = [1., 30., 30.]\n",
    "MIN_OFF = [1., 30., 3.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*4   #输出时间序列长度1440\n",
    "BORDER = 16   #边界宽度\n",
    "#输入为SEQ_LEN+BORDER*2-2=1470\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bbe78b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            aggregate  kettle     fridge  washing_machine  \\\n",
      "datetime                                                                    \n",
      "2013-04-12 00:00:00+00:00  166.416672     0.0   0.000000              0.0   \n",
      "2013-04-12 00:01:00+00:00  166.816666     0.0   0.000000              0.0   \n",
      "2013-04-12 00:02:00+00:00  168.083328     0.0   0.000000              0.0   \n",
      "2013-04-12 00:03:00+00:00  167.199997     0.0   0.000000              0.0   \n",
      "2013-04-12 00:04:00+00:00  167.383331     0.0   0.000000              0.0   \n",
      "...                               ...     ...        ...              ...   \n",
      "2014-12-14 23:56:00+00:00  210.449997     0.0  78.066666              0.0   \n",
      "2014-12-14 23:57:00+00:00  208.533340     0.0  79.699997              0.0   \n",
      "2014-12-14 23:58:00+00:00  209.333328     0.0  76.900002              0.0   \n",
      "2014-12-14 23:59:00+00:00  186.516663     0.0  50.049999              0.0   \n",
      "2014-12-15 00:00:00+00:00  165.399994     0.0   0.000000              0.0   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2013-04-12 00:00:00+00:00        0.0          0.0  \n",
      "2013-04-12 00:01:00+00:00        0.0          0.0  \n",
      "2013-04-12 00:02:00+00:00        0.0          0.0  \n",
      "2013-04-12 00:03:00+00:00        0.0          0.0  \n",
      "2013-04-12 00:04:00+00:00        0.0          0.0  \n",
      "...                              ...          ...  \n",
      "2014-12-14 23:56:00+00:00        0.0          0.0  \n",
      "2014-12-14 23:57:00+00:00        0.0          0.0  \n",
      "2014-12-14 23:58:00+00:00        0.0          0.0  \n",
      "2014-12-14 23:59:00+00:00        0.0          0.0  \n",
      "2014-12-15 00:00:00+00:00        0.0          0.0  \n",
      "\n",
      "[881281 rows x 6 columns]\n",
      "                            aggregate  kettle     fridge  washing_machine  \\\n",
      "datetime                                                                    \n",
      "2013-05-22 00:00:00+00:00  198.866669     0.0  94.283333              0.0   \n",
      "2013-05-22 00:01:00+00:00  196.866669     0.0  91.216667              0.0   \n",
      "2013-05-22 00:02:00+00:00  195.016663     0.0  89.433334              0.0   \n",
      "2013-05-22 00:03:00+00:00  193.600006     0.0  88.250000              0.0   \n",
      "2013-05-22 00:04:00+00:00  192.983337     0.0  86.866669              0.0   \n",
      "...                               ...     ...        ...              ...   \n",
      "2013-10-03 06:12:00+00:00  131.350006     0.0  10.100000              0.0   \n",
      "2013-10-03 06:13:00+00:00  131.050003     0.0  10.000000              0.0   \n",
      "2013-10-03 06:14:00+00:00  130.050003     0.0  10.000000              0.0   \n",
      "2013-10-03 06:15:00+00:00  130.100006     0.0  10.000000              0.0   \n",
      "2013-10-03 06:16:00+00:00  129.616669     0.0  10.000000              0.0   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2013-05-22 00:00:00+00:00        0.0          0.0  \n",
      "2013-05-22 00:01:00+00:00        0.0          0.0  \n",
      "2013-05-22 00:02:00+00:00        0.0          0.0  \n",
      "2013-05-22 00:03:00+00:00        0.0          0.0  \n",
      "2013-05-22 00:04:00+00:00        0.0          0.0  \n",
      "...                              ...          ...  \n",
      "2013-10-03 06:12:00+00:00        0.0          0.0  \n",
      "2013-10-03 06:13:00+00:00        0.0          0.0  \n",
      "2013-10-03 06:14:00+00:00        0.0          0.0  \n",
      "2013-10-03 06:15:00+00:00        0.0          0.0  \n",
      "2013-10-03 06:16:00+00:00        0.0          0.0  \n",
      "\n",
      "[193337 rows x 6 columns]\n",
      "                            aggregate  kettle  fridge  washing_machine  \\\n",
      "datetime                                                                 \n",
      "2013-02-27 20:35:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "2013-02-27 20:36:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "2013-02-27 20:37:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "2013-02-27 20:38:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "2013-02-27 20:39:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "...                               ...     ...     ...              ...   \n",
      "2013-04-01 06:11:00+00:00  137.100006     0.0     0.0              0.0   \n",
      "2013-04-01 06:12:00+00:00  136.800003     0.0     0.0              0.0   \n",
      "2013-04-01 06:13:00+00:00  136.766663     0.0     0.0              0.0   \n",
      "2013-04-01 06:14:00+00:00  136.866669     0.0     0.0              0.0   \n",
      "2013-04-01 06:15:00+00:00  136.899994     0.0     0.0              0.0   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2013-02-27 20:35:00+00:00        0.0          0.0  \n",
      "2013-02-27 20:36:00+00:00        0.0          0.0  \n",
      "2013-02-27 20:37:00+00:00        0.0          0.0  \n",
      "2013-02-27 20:38:00+00:00        0.0          0.0  \n",
      "2013-02-27 20:39:00+00:00        0.0          0.0  \n",
      "...                              ...          ...  \n",
      "2013-04-01 06:11:00+00:00        0.0          0.0  \n",
      "2013-04-01 06:12:00+00:00        0.0          0.0  \n",
      "2013-04-01 06:13:00+00:00        0.0          0.0  \n",
      "2013-04-01 06:14:00+00:00        0.0          0.0  \n",
      "2013-04-01 06:15:00+00:00        0.0          0.0  \n",
      "\n",
      "[46661 rows x 6 columns]\n",
      "                             aggregate  kettle     fridge  washing_machine  \\\n",
      "datetime                                                                     \n",
      "2013-03-09 14:40:00+00:00   630.150940     0.0  98.000000              0.0   \n",
      "2013-03-09 14:41:00+00:00   628.833313     0.0  96.933334              0.0   \n",
      "2013-03-09 14:42:00+00:00  2171.550049     0.0  95.900002              0.0   \n",
      "2013-03-09 14:43:00+00:00  3000.566650     0.0  94.716667              0.0   \n",
      "2013-03-09 14:44:00+00:00  1205.650024     0.0  94.716667              0.0   \n",
      "...                                ...     ...        ...              ...   \n",
      "2013-09-24 06:11:00+00:00   572.299988     0.0   0.000000              0.0   \n",
      "2013-09-24 06:12:00+00:00   570.099976     0.0   0.000000              0.0   \n",
      "2013-09-24 06:13:00+00:00   626.700012     0.0   0.000000              0.0   \n",
      "2013-09-24 06:14:00+00:00   579.233337     0.0   0.000000              0.0   \n",
      "2013-09-24 06:15:00+00:00   589.366638     0.0   0.000000              0.0   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2013-03-09 14:40:00+00:00        0.0          0.0  \n",
      "2013-03-09 14:41:00+00:00        0.0          0.0  \n",
      "2013-03-09 14:42:00+00:00        0.0          0.0  \n",
      "2013-03-09 14:43:00+00:00        0.0          0.0  \n",
      "2013-03-09 14:44:00+00:00        0.0          0.0  \n",
      "...                              ...          ...  \n",
      "2013-09-24 06:11:00+00:00        0.0          0.0  \n",
      "2013-09-24 06:12:00+00:00        0.0          0.0  \n",
      "2013-09-24 06:13:00+00:00        0.0          0.0  \n",
      "2013-09-24 06:14:00+00:00        0.0          0.0  \n",
      "2013-09-24 06:15:00+00:00        0.0          0.0  \n",
      "\n",
      "[286056 rows x 6 columns]\n",
      "                             aggregate  kettle      fridge  washing_machine  \\\n",
      "datetime                                                                      \n",
      "2014-06-29 16:23:00+00:00   769.000000     0.0  106.800003       225.000000   \n",
      "2014-06-29 16:24:00+00:00  1026.633301     0.0  106.300003       389.633331   \n",
      "2014-06-29 16:25:00+00:00  2263.383301     0.0  106.000000      1312.466675   \n",
      "2014-06-29 16:26:00+00:00   957.500000     0.0  106.500000       235.699997   \n",
      "2014-06-29 16:27:00+00:00  1905.300049     0.0  105.300003      1058.266724   \n",
      "...                                ...     ...         ...              ...   \n",
      "2014-08-31 23:56:00+00:00   490.000000     0.0    0.000000        15.000000   \n",
      "2014-08-31 23:57:00+00:00   487.133331     0.0    0.000000        14.800000   \n",
      "2014-08-31 23:58:00+00:00   487.399994     0.0    0.000000        14.800000   \n",
      "2014-08-31 23:59:00+00:00   488.399994     0.0    0.000000        15.000000   \n",
      "2014-09-01 00:00:00+00:00   491.483337     0.0    0.000000        15.000000   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2014-06-29 16:23:00+00:00   0.000000          0.0  \n",
      "2014-06-29 16:24:00+00:00   0.000000          0.0  \n",
      "2014-06-29 16:25:00+00:00   0.000000          0.0  \n",
      "2014-06-29 16:26:00+00:00   0.000000          0.0  \n",
      "2014-06-29 16:27:00+00:00   0.000000          0.0  \n",
      "...                              ...          ...  \n",
      "2014-08-31 23:56:00+00:00  50.700001          0.0  \n",
      "2014-08-31 23:57:00+00:00  50.400002          0.0  \n",
      "2014-08-31 23:58:00+00:00  50.299999          0.0  \n",
      "2014-08-31 23:59:00+00:00  50.299999          0.0  \n",
      "2014-09-01 00:00:00+00:00  50.000000          0.0  \n",
      "\n",
      "[91178 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "ds_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "for i in range(5):\n",
    "    ds = pd.read_feather('./UKDALE_%d_train.feather' %(i+1))\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    print(ds)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    ds_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(ds_meter[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf685d51",
   "metadata": {},
   "source": [
    "这段代码首先定义了一些常量，包括家电的名称、阈值、最小开启时间和最小关闭时间，以及一些数据处理相关的参数。然后，通过循环读取5个训练数据集，并将其存储在ds_meter、ds_appliance和ds_status列表中。其中，ds_meter存储了总用电量数据，ds_appliance存储了各个家电的用电量数据，ds_status存储了家电的状态数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1296741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2013-04-12 00:00:00+00:00    166.416672\n",
       "2013-04-12 00:01:00+00:00    166.816666\n",
       "2013-04-12 00:02:00+00:00    168.083328\n",
       "2013-04-12 00:03:00+00:00    167.199997\n",
       "2013-04-12 00:04:00+00:00    167.383331\n",
       "                                ...    \n",
       "2014-12-14 23:56:00+00:00    210.449997\n",
       "2014-12-14 23:57:00+00:00    208.533340\n",
       "2014-12-14 23:58:00+00:00    209.333328\n",
       "2014-12-14 23:59:00+00:00    186.516663\n",
       "2014-12-15 00:00:00+00:00    165.399994\n",
       "Name: aggregate, Length: 881281, dtype: float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_meter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21507fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (i, value) for i, value in enumerate(ds_meter[0][:10000])\n",
    "]\n",
    "\n",
    "# 提取序列号和值\n",
    "indices = [d[0] for d in data]\n",
    "values = [d[1] for d in data]\n",
    "\n",
    "# 创建折线图\n",
    "plt.plot(indices, values)\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('aggregate')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Power(W)')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "# 设置分辨率为600ppi\n",
    "dpi = 600\n",
    "\n",
    "# 设置图表尺寸，以英寸为单位\n",
    "#fig = plt.figure(figsize=(8, 6))  # 这里将宽度设置为8英寸，高度设置为6英寸，你可以根据需要进行调整\n",
    "\n",
    "# 选择文件路径和文件名，保存为PNG格式\n",
    "save_path = r'D:\\NILM\\绘图\\image.png'\n",
    "\n",
    "# 导出图片\n",
    "#plt.savefig(save_path, dpi=dpi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745aa91a",
   "metadata": {},
   "source": [
    "接下来，代码统计了ds_status[1]中状态变化为1的次数，并对ds_status[1]进行了描述性统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4017137",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_house_train = [Power(ds_meter[i][:int(0.8*ds_len[i])], \n",
    "                        ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                        ds_status[i][:int(0.8*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, True) for i in range(5+0)]\n",
    "\n",
    "ds_house_valid = [Power(ds_meter[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        ds_appliance[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])],\n",
    "                        ds_status[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_test  = [Power(ds_meter[i][int(0.9*ds_len[i]):], \n",
    "                        ds_appliance[i][int(0.9*ds_len[i]):],\n",
    "                        ds_status[i][int(0.9*ds_len[i]):], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_total  = [Power(ds_meter[i], ds_appliance[i], ds_status[i], \n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_train_seen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                ds_house_train[1], \n",
    "                                                #ds_house_train[2], \n",
    "                                                #ds_house_train[3],\n",
    "                                                ds_house_train[4]\n",
    "                                                ])\n",
    "ds_valid_seen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                #ds_house_valid[1], \n",
    "                                                #ds_house_valid[2], \n",
    "                                                #ds_house_valid[3], \n",
    "                                                #ds_house_valid[4]\n",
    "                                                ])\n",
    "\n",
    "dl_train_seen = DataLoader(dataset = ds_train_seen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_seen = DataLoader(dataset = ds_valid_seen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_seen = DataLoader(dataset = ds_house_test[0], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "ds_train_unseen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                  #ds_house_train[1], \n",
    "                                                  #ds_house_train[2], \n",
    "                                                  #ds_house_train[3], \n",
    "                                                  ds_house_train[4]\n",
    "                                                  ])\n",
    "ds_valid_unseen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                  #ds_house_valid[1], \n",
    "                                                  #ds_house_valid[2], \n",
    "                                                  #ds_house_valid[3], \n",
    "                                                  ds_house_valid[4]\n",
    "                                                  ])\n",
    "dl_train_unseen = DataLoader(dataset = ds_train_unseen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_unseen = DataLoader(dataset = ds_valid_unseen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_unseen = DataLoader(dataset = ds_house_total[1], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_test[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_valid[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695de8f",
   "metadata": {},
   "source": [
    "这段代码定义了多个数据集和数据加载器，用于训练和测试神经网络模型。首先，代码根据数据集的长度将数据集分为训练集、验证集和测试集，并将每个家庭的数据存储在不同的数据集中。然后，代码使用`ConcatDataset`将不同家庭的数据集合并成一个数据集，并将其传递给`DataLoader`进行批量加载。其中，`dl_train_seen`和`dl_valid_seen`是已知家电的训练集和验证集数据加载器，`dl_test_seen`是已知家电的测试集数据加载器，`dl_train_unseen`和`dl_valid_unseen`是未知家电的训练集和验证集数据加载器，`dl_test_unseen`是未知家电的测试集数据加载器。\n",
    "\n",
    "最后，代码定义了多个用于单独加载每个家庭数据的数据加载器`dl_house_test`、`dl_house_valid`和`dl_house_total`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bf31e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/500] train_loss: 0.39283 valid_loss: 0.27883 test_loss: 0.28490 \n",
      "验证损失减少 (inf --> 0.278825). 正在保存模型...\n",
      "[  2/500] train_loss: 0.22540 valid_loss: 0.20778 test_loss: 0.21709 \n",
      "验证损失减少 (0.278825 --> 0.207784). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17745 valid_loss: 0.17634 test_loss: 0.18568 \n",
      "验证损失减少 (0.207784 --> 0.176336). 正在保存模型...\n",
      "[  4/500] train_loss: 0.16008 valid_loss: 0.16067 test_loss: 0.17210 \n",
      "验证损失减少 (0.176336 --> 0.160670). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14533 valid_loss: 0.14967 test_loss: 0.16252 \n",
      "验证损失减少 (0.160670 --> 0.149669). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14200 valid_loss: 0.14455 test_loss: 0.15767 \n",
      "验证损失减少 (0.149669 --> 0.144555). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13873 valid_loss: 0.13860 test_loss: 0.15205 \n",
      "验证损失减少 (0.144555 --> 0.138603). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13396 valid_loss: 0.13433 test_loss: 0.14964 \n",
      "验证损失减少 (0.138603 --> 0.134326). 正在保存模型...\n",
      "[  9/500] train_loss: 0.12893 valid_loss: 0.13106 test_loss: 0.14788 \n",
      "验证损失减少 (0.134326 --> 0.131065). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12794 valid_loss: 0.12968 test_loss: 0.14628 \n",
      "验证损失减少 (0.131065 --> 0.129685). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12573 valid_loss: 0.12756 test_loss: 0.14278 \n",
      "验证损失减少 (0.129685 --> 0.127560). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12389 valid_loss: 0.12465 test_loss: 0.13924 \n",
      "验证损失减少 (0.127560 --> 0.124651). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12144 valid_loss: 0.12457 test_loss: 0.14030 \n",
      "验证损失减少 (0.124651 --> 0.124571). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.11591 valid_loss: 0.12274 test_loss: 0.13787 \n",
      "验证损失减少 (0.124571 --> 0.122735). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11821 valid_loss: 0.12445 test_loss: 0.13783 \n",
      "[ 16/500] train_loss: 0.11983 valid_loss: 0.11957 test_loss: 0.13393 \n",
      "验证损失减少 (0.122735 --> 0.119568). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11477 valid_loss: 0.11909 test_loss: 0.13328 \n",
      "验证损失减少 (0.119568 --> 0.119091). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11215 valid_loss: 0.11644 test_loss: 0.13230 \n",
      "验证损失减少 (0.119091 --> 0.116444). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.10926 valid_loss: 0.11814 test_loss: 0.13531 \n",
      "[ 20/500] train_loss: 0.11357 valid_loss: 0.11831 test_loss: 0.13292 \n",
      "[ 21/500] train_loss: 0.11111 valid_loss: 0.11525 test_loss: 0.13025 \n",
      "验证损失减少 (0.116444 --> 0.115249). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.10911 valid_loss: 0.11454 test_loss: 0.12958 \n",
      "验证损失减少 (0.115249 --> 0.114538). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10633 valid_loss: 0.11480 test_loss: 0.12908 \n",
      "[ 24/500] train_loss: 0.10647 valid_loss: 0.11307 test_loss: 0.12915 \n",
      "验证损失减少 (0.114538 --> 0.113072). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10659 valid_loss: 0.11128 test_loss: 0.12429 \n",
      "验证损失减少 (0.113072 --> 0.111279). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10516 valid_loss: 0.11216 test_loss: 0.12592 \n",
      "[ 27/500] train_loss: 0.10563 valid_loss: 0.11153 test_loss: 0.12568 \n",
      "[ 28/500] train_loss: 0.10463 valid_loss: 0.11162 test_loss: 0.12379 \n",
      "[ 29/500] train_loss: 0.10235 valid_loss: 0.11051 test_loss: 0.12443 \n",
      "验证损失减少 (0.111279 --> 0.110510). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10328 valid_loss: 0.10982 test_loss: 0.12384 \n",
      "验证损失减少 (0.110510 --> 0.109822). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.09998 valid_loss: 0.10605 test_loss: 0.12085 \n",
      "验证损失减少 (0.109822 --> 0.106055). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.10131 valid_loss: 0.10728 test_loss: 0.12218 \n",
      "[ 33/500] train_loss: 0.10039 valid_loss: 0.10746 test_loss: 0.12061 \n",
      "[ 34/500] train_loss: 0.09752 valid_loss: 0.10456 test_loss: 0.11835 \n",
      "验证损失减少 (0.106055 --> 0.104565). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.10003 valid_loss: 0.10532 test_loss: 0.11913 \n",
      "[ 36/500] train_loss: 0.09968 valid_loss: 0.10619 test_loss: 0.11887 \n",
      "[ 37/500] train_loss: 0.09767 valid_loss: 0.10711 test_loss: 0.12035 \n",
      "[ 38/500] train_loss: 0.09921 valid_loss: 0.10162 test_loss: 0.11589 \n",
      "验证损失减少 (0.104565 --> 0.101620). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.09581 valid_loss: 0.10070 test_loss: 0.11539 \n",
      "验证损失减少 (0.101620 --> 0.100697). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09927 valid_loss: 0.10422 test_loss: 0.11847 \n",
      "[ 41/500] train_loss: 0.09493 valid_loss: 0.10027 test_loss: 0.11431 \n",
      "验证损失减少 (0.100697 --> 0.100271). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.09486 valid_loss: 0.10498 test_loss: 0.11906 \n",
      "[ 43/500] train_loss: 0.09575 valid_loss: 0.10218 test_loss: 0.11439 \n",
      "[ 44/500] train_loss: 0.09323 valid_loss: 0.10095 test_loss: 0.11432 \n",
      "[ 45/500] train_loss: 0.09296 valid_loss: 0.10070 test_loss: 0.11433 \n",
      "[ 46/500] train_loss: 0.09513 valid_loss: 0.09983 test_loss: 0.11403 \n",
      "验证损失减少 (0.100271 --> 0.099829). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.09417 valid_loss: 0.10029 test_loss: 0.11383 \n",
      "[ 48/500] train_loss: 0.09306 valid_loss: 0.09840 test_loss: 0.11273 \n",
      "验证损失减少 (0.099829 --> 0.098405). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09411 valid_loss: 0.09910 test_loss: 0.10991 \n",
      "[ 50/500] train_loss: 0.09227 valid_loss: 0.09781 test_loss: 0.11160 \n",
      "验证损失减少 (0.098405 --> 0.097814). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.09088 valid_loss: 0.09975 test_loss: 0.11078 \n",
      "[ 52/500] train_loss: 0.09107 valid_loss: 0.09724 test_loss: 0.11074 \n",
      "验证损失减少 (0.097814 --> 0.097243). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.09130 valid_loss: 0.09841 test_loss: 0.11216 \n",
      "[ 54/500] train_loss: 0.09093 valid_loss: 0.09646 test_loss: 0.10981 \n",
      "验证损失减少 (0.097243 --> 0.096461). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.08970 valid_loss: 0.09789 test_loss: 0.11074 \n",
      "[ 56/500] train_loss: 0.08975 valid_loss: 0.09992 test_loss: 0.11084 \n",
      "[ 57/500] train_loss: 0.08865 valid_loss: 0.09551 test_loss: 0.10780 \n",
      "验证损失减少 (0.096461 --> 0.095507). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.08971 valid_loss: 0.09470 test_loss: 0.10884 \n",
      "验证损失减少 (0.095507 --> 0.094696). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.08788 valid_loss: 0.09758 test_loss: 0.10877 \n",
      "[ 60/500] train_loss: 0.08766 valid_loss: 0.09308 test_loss: 0.10682 \n",
      "验证损失减少 (0.094696 --> 0.093083). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.08842 valid_loss: 0.09466 test_loss: 0.10794 \n",
      "[ 62/500] train_loss: 0.08967 valid_loss: 0.09404 test_loss: 0.10737 \n",
      "[ 63/500] train_loss: 0.08797 valid_loss: 0.09377 test_loss: 0.10750 \n",
      "[ 64/500] train_loss: 0.08732 valid_loss: 0.09460 test_loss: 0.10816 \n",
      "[ 65/500] train_loss: 0.08818 valid_loss: 0.09541 test_loss: 0.10740 \n",
      "[ 66/500] train_loss: 0.08665 valid_loss: 0.09712 test_loss: 0.10826 \n",
      "[ 67/500] train_loss: 0.08770 valid_loss: 0.09633 test_loss: 0.10727 \n",
      "[ 68/500] train_loss: 0.08759 valid_loss: 0.09423 test_loss: 0.10672 \n",
      "[ 69/500] train_loss: 0.08391 valid_loss: 0.09413 test_loss: 0.10654 \n",
      "[ 70/500] train_loss: 0.08807 valid_loss: 0.09463 test_loss: 0.10667 \n",
      "[ 71/500] train_loss: 0.08568 valid_loss: 0.09337 test_loss: 0.10564 \n",
      "[ 72/500] train_loss: 0.08504 valid_loss: 0.09512 test_loss: 0.10556 \n",
      "[ 73/500] train_loss: 0.08512 valid_loss: 0.09133 test_loss: 0.10532 \n",
      "验证损失减少 (0.093083 --> 0.091333). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.08506 valid_loss: 0.09383 test_loss: 0.10453 \n",
      "[ 75/500] train_loss: 0.08522 valid_loss: 0.09351 test_loss: 0.10551 \n",
      "[ 76/500] train_loss: 0.08388 valid_loss: 0.09026 test_loss: 0.10339 \n",
      "验证损失减少 (0.091333 --> 0.090258). 正在保存模型...\n",
      "[ 77/500] train_loss: 0.08345 valid_loss: 0.09149 test_loss: 0.10353 \n",
      "[ 78/500] train_loss: 0.08374 valid_loss: 0.09035 test_loss: 0.10393 \n",
      "[ 79/500] train_loss: 0.08451 valid_loss: 0.09179 test_loss: 0.10378 \n",
      "[ 80/500] train_loss: 0.08508 valid_loss: 0.09003 test_loss: 0.10444 \n",
      "验证损失减少 (0.090258 --> 0.090026). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.08288 valid_loss: 0.09447 test_loss: 0.10495 \n",
      "[ 82/500] train_loss: 0.08467 valid_loss: 0.09399 test_loss: 0.10401 \n",
      "[ 83/500] train_loss: 0.08139 valid_loss: 0.09015 test_loss: 0.10389 \n",
      "[ 84/500] train_loss: 0.08272 valid_loss: 0.09037 test_loss: 0.10181 \n",
      "[ 85/500] train_loss: 0.08262 valid_loss: 0.08837 test_loss: 0.10178 \n",
      "验证损失减少 (0.090026 --> 0.088367). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08189 valid_loss: 0.09047 test_loss: 0.10293 \n",
      "[ 87/500] train_loss: 0.08517 valid_loss: 0.08893 test_loss: 0.10312 \n",
      "[ 88/500] train_loss: 0.08342 valid_loss: 0.08900 test_loss: 0.10314 \n",
      "[ 89/500] train_loss: 0.08261 valid_loss: 0.08942 test_loss: 0.10221 \n",
      "[ 90/500] train_loss: 0.08372 valid_loss: 0.08909 test_loss: 0.10164 \n",
      "[ 91/500] train_loss: 0.08198 valid_loss: 0.08861 test_loss: 0.10158 \n",
      "[ 92/500] train_loss: 0.07994 valid_loss: 0.08903 test_loss: 0.10176 \n",
      "[ 93/500] train_loss: 0.07993 valid_loss: 0.08876 test_loss: 0.10183 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 94/500] train_loss: 0.08186 valid_loss: 0.08691 test_loss: 0.10015 \n",
      "验证损失减少 (0.088367 --> 0.086911). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.07998 valid_loss: 0.08809 test_loss: 0.10172 \n",
      "[ 96/500] train_loss: 0.08211 valid_loss: 0.08970 test_loss: 0.10115 \n",
      "[ 97/500] train_loss: 0.07844 valid_loss: 0.08744 test_loss: 0.10061 \n",
      "[ 98/500] train_loss: 0.07912 valid_loss: 0.08999 test_loss: 0.10102 \n",
      "[ 99/500] train_loss: 0.07939 valid_loss: 0.08608 test_loss: 0.09984 \n",
      "验证损失减少 (0.086911 --> 0.086080). 正在保存模型...\n",
      "[100/500] train_loss: 0.07880 valid_loss: 0.08831 test_loss: 0.10063 \n",
      "[101/500] train_loss: 0.08009 valid_loss: 0.08721 test_loss: 0.10117 \n",
      "[102/500] train_loss: 0.07781 valid_loss: 0.08887 test_loss: 0.09942 \n",
      "[103/500] train_loss: 0.07739 valid_loss: 0.08556 test_loss: 0.09941 \n",
      "验证损失减少 (0.086080 --> 0.085562). 正在保存模型...\n",
      "[104/500] train_loss: 0.07867 valid_loss: 0.08569 test_loss: 0.09880 \n",
      "[105/500] train_loss: 0.07781 valid_loss: 0.08904 test_loss: 0.09822 \n",
      "[106/500] train_loss: 0.07840 valid_loss: 0.08663 test_loss: 0.09920 \n",
      "[107/500] train_loss: 0.07684 valid_loss: 0.08729 test_loss: 0.09987 \n",
      "[108/500] train_loss: 0.07742 valid_loss: 0.08743 test_loss: 0.10027 \n",
      "[109/500] train_loss: 0.07841 valid_loss: 0.09258 test_loss: 0.09943 \n",
      "[110/500] train_loss: 0.07923 valid_loss: 0.08562 test_loss: 0.09907 \n",
      "[111/500] train_loss: 0.07527 valid_loss: 0.08624 test_loss: 0.09971 \n",
      "[112/500] train_loss: 0.07745 valid_loss: 0.08653 test_loss: 0.09902 \n",
      "[113/500] train_loss: 0.07665 valid_loss: 0.08888 test_loss: 0.10046 \n",
      "[114/500] train_loss: 0.08009 valid_loss: 0.08585 test_loss: 0.09963 \n",
      "[115/500] train_loss: 0.07675 valid_loss: 0.08569 test_loss: 0.10002 \n",
      "[116/500] train_loss: 0.07833 valid_loss: 0.08450 test_loss: 0.09915 \n",
      "验证损失减少 (0.085562 --> 0.084501). 正在保存模型...\n",
      "[117/500] train_loss: 0.07776 valid_loss: 0.08617 test_loss: 0.09882 \n",
      "[118/500] train_loss: 0.07638 valid_loss: 0.08543 test_loss: 0.09837 \n",
      "[119/500] train_loss: 0.07625 valid_loss: 0.08627 test_loss: 0.10001 \n",
      "[120/500] train_loss: 0.07710 valid_loss: 0.08483 test_loss: 0.09695 \n",
      "[121/500] train_loss: 0.07559 valid_loss: 0.08384 test_loss: 0.09753 \n",
      "验证损失减少 (0.084501 --> 0.083841). 正在保存模型...\n",
      "[122/500] train_loss: 0.07735 valid_loss: 0.08349 test_loss: 0.09754 \n",
      "验证损失减少 (0.083841 --> 0.083493). 正在保存模型...\n",
      "[123/500] train_loss: 0.07743 valid_loss: 0.08323 test_loss: 0.09706 \n",
      "验证损失减少 (0.083493 --> 0.083225). 正在保存模型...\n",
      "[124/500] train_loss: 0.07615 valid_loss: 0.08532 test_loss: 0.09730 \n",
      "[125/500] train_loss: 0.07749 valid_loss: 0.08281 test_loss: 0.09578 \n",
      "验证损失减少 (0.083225 --> 0.082807). 正在保存模型...\n",
      "[126/500] train_loss: 0.07606 valid_loss: 0.08327 test_loss: 0.09662 \n",
      "[127/500] train_loss: 0.07439 valid_loss: 0.08428 test_loss: 0.09652 \n",
      "[128/500] train_loss: 0.07560 valid_loss: 0.08452 test_loss: 0.09663 \n",
      "[129/500] train_loss: 0.07410 valid_loss: 0.08355 test_loss: 0.09738 \n",
      "[130/500] train_loss: 0.07363 valid_loss: 0.08547 test_loss: 0.09693 \n",
      "[131/500] train_loss: 0.07380 valid_loss: 0.08540 test_loss: 0.09772 \n",
      "[132/500] train_loss: 0.07408 valid_loss: 0.08538 test_loss: 0.09719 \n",
      "[133/500] train_loss: 0.07527 valid_loss: 0.08277 test_loss: 0.09658 \n",
      "验证损失减少 (0.082807 --> 0.082770). 正在保存模型...\n",
      "[134/500] train_loss: 0.07407 valid_loss: 0.08222 test_loss: 0.09559 \n",
      "验证损失减少 (0.082770 --> 0.082224). 正在保存模型...\n",
      "[135/500] train_loss: 0.07446 valid_loss: 0.08217 test_loss: 0.09613 \n",
      "验证损失减少 (0.082224 --> 0.082170). 正在保存模型...\n",
      "[136/500] train_loss: 0.07393 valid_loss: 0.08320 test_loss: 0.09733 \n",
      "[137/500] train_loss: 0.07548 valid_loss: 0.08309 test_loss: 0.09751 \n",
      "[138/500] train_loss: 0.07376 valid_loss: 0.08160 test_loss: 0.09539 \n",
      "验证损失减少 (0.082170 --> 0.081601). 正在保存模型...\n",
      "[139/500] train_loss: 0.07164 valid_loss: 0.08211 test_loss: 0.09607 \n",
      "[140/500] train_loss: 0.07502 valid_loss: 0.08314 test_loss: 0.09739 \n",
      "[141/500] train_loss: 0.07383 valid_loss: 0.08191 test_loss: 0.09520 \n",
      "[142/500] train_loss: 0.07292 valid_loss: 0.08610 test_loss: 0.09489 \n",
      "[143/500] train_loss: 0.07287 valid_loss: 0.08205 test_loss: 0.09600 \n",
      "[144/500] train_loss: 0.07334 valid_loss: 0.08175 test_loss: 0.09482 \n",
      "[145/500] train_loss: 0.07247 valid_loss: 0.08437 test_loss: 0.09478 \n",
      "[146/500] train_loss: 0.07341 valid_loss: 0.08340 test_loss: 0.09569 \n",
      "[147/500] train_loss: 0.07284 valid_loss: 0.08270 test_loss: 0.09633 \n",
      "[148/500] train_loss: 0.07081 valid_loss: 0.08064 test_loss: 0.09661 \n",
      "验证损失减少 (0.081601 --> 0.080638). 正在保存模型...\n",
      "[149/500] train_loss: 0.07213 valid_loss: 0.08179 test_loss: 0.09585 \n",
      "[150/500] train_loss: 0.07308 valid_loss: 0.08116 test_loss: 0.09532 \n",
      "[151/500] train_loss: 0.07286 valid_loss: 0.08319 test_loss: 0.09410 \n",
      "[152/500] train_loss: 0.07218 valid_loss: 0.08190 test_loss: 0.09375 \n",
      "[153/500] train_loss: 0.07109 valid_loss: 0.08141 test_loss: 0.09571 \n",
      "[154/500] train_loss: 0.07174 valid_loss: 0.07986 test_loss: 0.09447 \n",
      "验证损失减少 (0.080638 --> 0.079860). 正在保存模型...\n",
      "[155/500] train_loss: 0.07171 valid_loss: 0.08078 test_loss: 0.09439 \n",
      "[156/500] train_loss: 0.07029 valid_loss: 0.09141 test_loss: 0.09575 \n",
      "[157/500] train_loss: 0.07188 valid_loss: 0.08205 test_loss: 0.09351 \n",
      "[158/500] train_loss: 0.07141 valid_loss: 0.08113 test_loss: 0.09481 \n",
      "[159/500] train_loss: 0.07100 valid_loss: 0.08473 test_loss: 0.09408 \n",
      "[160/500] train_loss: 0.06930 valid_loss: 0.08122 test_loss: 0.09420 \n",
      "[161/500] train_loss: 0.06894 valid_loss: 0.08083 test_loss: 0.09290 \n",
      "[162/500] train_loss: 0.07139 valid_loss: 0.08451 test_loss: 0.09425 \n",
      "[163/500] train_loss: 0.07274 valid_loss: 0.08458 test_loss: 0.09337 \n",
      "[164/500] train_loss: 0.07052 valid_loss: 0.08073 test_loss: 0.09396 \n",
      "[165/500] train_loss: 0.06829 valid_loss: 0.08029 test_loss: 0.09330 \n",
      "[166/500] train_loss: 0.07154 valid_loss: 0.07986 test_loss: 0.09370 \n",
      "验证损失减少 (0.079860 --> 0.079855). 正在保存模型...\n",
      "[167/500] train_loss: 0.06999 valid_loss: 0.08545 test_loss: 0.09341 \n",
      "[168/500] train_loss: 0.06922 valid_loss: 0.08331 test_loss: 0.09260 \n",
      "[169/500] train_loss: 0.07101 valid_loss: 0.08019 test_loss: 0.09249 \n",
      "[170/500] train_loss: 0.07120 valid_loss: 0.08021 test_loss: 0.09264 \n",
      "[171/500] train_loss: 0.07094 valid_loss: 0.08215 test_loss: 0.09435 \n",
      "[172/500] train_loss: 0.07008 valid_loss: 0.08036 test_loss: 0.09417 \n",
      "[173/500] train_loss: 0.06982 valid_loss: 0.08019 test_loss: 0.09389 \n",
      "[174/500] train_loss: 0.07025 valid_loss: 0.07757 test_loss: 0.09303 \n",
      "验证损失减少 (0.079855 --> 0.077568). 正在保存模型...\n",
      "[175/500] train_loss: 0.06956 valid_loss: 0.08048 test_loss: 0.09484 \n",
      "[176/500] train_loss: 0.06938 valid_loss: 0.07914 test_loss: 0.09237 \n",
      "[177/500] train_loss: 0.06929 valid_loss: 0.08058 test_loss: 0.09247 \n",
      "[178/500] train_loss: 0.06939 valid_loss: 0.07935 test_loss: 0.09362 \n",
      "[179/500] train_loss: 0.06933 valid_loss: 0.08247 test_loss: 0.09179 \n",
      "[180/500] train_loss: 0.06932 valid_loss: 0.08613 test_loss: 0.09231 \n",
      "[181/500] train_loss: 0.06892 valid_loss: 0.08055 test_loss: 0.09301 \n",
      "[182/500] train_loss: 0.06882 valid_loss: 0.07968 test_loss: 0.09307 \n",
      "[183/500] train_loss: 0.06863 valid_loss: 0.08146 test_loss: 0.09349 \n",
      "[184/500] train_loss: 0.06817 valid_loss: 0.08307 test_loss: 0.09187 \n",
      "[185/500] train_loss: 0.06719 valid_loss: 0.07932 test_loss: 0.09180 \n",
      "[186/500] train_loss: 0.06870 valid_loss: 0.07746 test_loss: 0.09191 \n",
      "验证损失减少 (0.077568 --> 0.077458). 正在保存模型...\n",
      "[187/500] train_loss: 0.06610 valid_loss: 0.07900 test_loss: 0.09205 \n",
      "[188/500] train_loss: 0.06973 valid_loss: 0.07967 test_loss: 0.09212 \n",
      "[189/500] train_loss: 0.06570 valid_loss: 0.08126 test_loss: 0.09187 \n",
      "[190/500] train_loss: 0.06700 valid_loss: 0.08039 test_loss: 0.09192 \n",
      "[191/500] train_loss: 0.06730 valid_loss: 0.08261 test_loss: 0.09446 \n",
      "[192/500] train_loss: 0.06744 valid_loss: 0.07979 test_loss: 0.09147 \n",
      "[193/500] train_loss: 0.06692 valid_loss: 0.08324 test_loss: 0.09205 \n",
      "[194/500] train_loss: 0.06790 valid_loss: 0.07895 test_loss: 0.09255 \n",
      "[195/500] train_loss: 0.06747 valid_loss: 0.08625 test_loss: 0.09302 \n",
      "[196/500] train_loss: 0.06732 valid_loss: 0.07972 test_loss: 0.09144 \n",
      "[197/500] train_loss: 0.06681 valid_loss: 0.07977 test_loss: 0.09242 \n",
      "[198/500] train_loss: 0.06911 valid_loss: 0.08027 test_loss: 0.09329 \n",
      "[199/500] train_loss: 0.06867 valid_loss: 0.07925 test_loss: 0.09123 \n",
      "[200/500] train_loss: 0.06885 valid_loss: 0.07875 test_loss: 0.09188 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201/500] train_loss: 0.06606 valid_loss: 0.07804 test_loss: 0.09078 \n",
      "[202/500] train_loss: 0.06546 valid_loss: 0.07949 test_loss: 0.09321 \n",
      "[203/500] train_loss: 0.06623 valid_loss: 0.07998 test_loss: 0.09139 \n",
      "[204/500] train_loss: 0.06536 valid_loss: 0.07855 test_loss: 0.09034 \n",
      "[205/500] train_loss: 0.06459 valid_loss: 0.08053 test_loss: 0.09213 \n",
      "[206/500] train_loss: 0.06616 valid_loss: 0.07897 test_loss: 0.09073 \n",
      "[207/500] train_loss: 0.06476 valid_loss: 0.08251 test_loss: 0.09022 \n",
      "[208/500] train_loss: 0.06771 valid_loss: 0.07931 test_loss: 0.09215 \n",
      "[209/500] train_loss: 0.06721 valid_loss: 0.07826 test_loss: 0.09128 \n",
      "[210/500] train_loss: 0.06517 valid_loss: 0.07951 test_loss: 0.09089 \n",
      "[211/500] train_loss: 0.06650 valid_loss: 0.08517 test_loss: 0.09376 \n",
      "[212/500] train_loss: 0.06679 valid_loss: 0.07981 test_loss: 0.09169 \n",
      "[213/500] train_loss: 0.06618 valid_loss: 0.07973 test_loss: 0.09270 \n",
      "[214/500] train_loss: 0.06671 valid_loss: 0.07880 test_loss: 0.09103 \n",
      "[215/500] train_loss: 0.06603 valid_loss: 0.07811 test_loss: 0.09081 \n",
      "[216/500] train_loss: 0.06615 valid_loss: 0.07936 test_loss: 0.09155 \n",
      "[217/500] train_loss: 0.06741 valid_loss: 0.07868 test_loss: 0.09092 \n",
      "[218/500] train_loss: 0.06678 valid_loss: 0.07898 test_loss: 0.09104 \n",
      "[219/500] train_loss: 0.06435 valid_loss: 0.07908 test_loss: 0.09274 \n",
      "[220/500] train_loss: 0.06480 valid_loss: 0.08023 test_loss: 0.09271 \n",
      "[221/500] train_loss: 0.06407 valid_loss: 0.07814 test_loss: 0.09175 \n",
      "[222/500] train_loss: 0.06626 valid_loss: 0.07806 test_loss: 0.09145 \n",
      "[223/500] train_loss: 0.06498 valid_loss: 0.07847 test_loss: 0.09163 \n",
      "[224/500] train_loss: 0.06706 valid_loss: 0.07775 test_loss: 0.08936 \n",
      "[225/500] train_loss: 0.06695 valid_loss: 0.07768 test_loss: 0.09066 \n",
      "[226/500] train_loss: 0.06493 valid_loss: 0.07854 test_loss: 0.09142 \n",
      "[227/500] train_loss: 0.06556 valid_loss: 0.07800 test_loss: 0.09092 \n",
      "[228/500] train_loss: 0.06561 valid_loss: 0.07891 test_loss: 0.09042 \n",
      "[229/500] train_loss: 0.06598 valid_loss: 0.08307 test_loss: 0.09185 \n",
      "[230/500] train_loss: 0.06605 valid_loss: 0.08188 test_loss: 0.09078 \n",
      "[231/500] train_loss: 0.06455 valid_loss: 0.07859 test_loss: 0.09311 \n",
      "[232/500] train_loss: 0.06356 valid_loss: 0.07850 test_loss: 0.09166 \n",
      "[233/500] train_loss: 0.06672 valid_loss: 0.08348 test_loss: 0.09079 \n",
      "[234/500] train_loss: 0.06635 valid_loss: 0.07953 test_loss: 0.09153 \n",
      "[235/500] train_loss: 0.06560 valid_loss: 0.07913 test_loss: 0.09009 \n",
      "[236/500] train_loss: 0.06379 valid_loss: 0.08123 test_loss: 0.09078 \n",
      "[237/500] train_loss: 0.06332 valid_loss: 0.07745 test_loss: 0.09056 \n",
      "验证损失减少 (0.077458 --> 0.077453). 正在保存模型...\n",
      "[238/500] train_loss: 0.06465 valid_loss: 0.07940 test_loss: 0.09126 \n",
      "[239/500] train_loss: 0.06398 valid_loss: 0.07845 test_loss: 0.09114 \n",
      "[240/500] train_loss: 0.06254 valid_loss: 0.07999 test_loss: 0.09039 \n",
      "[241/500] train_loss: 0.06403 valid_loss: 0.07787 test_loss: 0.09115 \n",
      "[242/500] train_loss: 0.06191 valid_loss: 0.07738 test_loss: 0.09082 \n",
      "验证损失减少 (0.077453 --> 0.077376). 正在保存模型...\n",
      "[243/500] train_loss: 0.06355 valid_loss: 0.07930 test_loss: 0.09233 \n",
      "[244/500] train_loss: 0.06448 valid_loss: 0.07998 test_loss: 0.09126 \n",
      "[245/500] train_loss: 0.06442 valid_loss: 0.07737 test_loss: 0.09006 \n",
      "验证损失减少 (0.077376 --> 0.077373). 正在保存模型...\n",
      "[246/500] train_loss: 0.06364 valid_loss: 0.07781 test_loss: 0.09053 \n",
      "[247/500] train_loss: 0.06334 valid_loss: 0.07832 test_loss: 0.08987 \n",
      "[248/500] train_loss: 0.06399 valid_loss: 0.07905 test_loss: 0.09058 \n",
      "[249/500] train_loss: 0.06233 valid_loss: 0.07896 test_loss: 0.09105 \n",
      "[250/500] train_loss: 0.06479 valid_loss: 0.08035 test_loss: 0.08889 \n",
      "[251/500] train_loss: 0.06322 valid_loss: 0.07790 test_loss: 0.09123 \n",
      "[252/500] train_loss: 0.06197 valid_loss: 0.08723 test_loss: 0.09056 \n",
      "[253/500] train_loss: 0.06212 valid_loss: 0.07859 test_loss: 0.09078 \n",
      "[254/500] train_loss: 0.06302 valid_loss: 0.08021 test_loss: 0.09033 \n",
      "[255/500] train_loss: 0.06346 valid_loss: 0.08388 test_loss: 0.09046 \n",
      "[256/500] train_loss: 0.06175 valid_loss: 0.07782 test_loss: 0.09159 \n",
      "[257/500] train_loss: 0.06237 valid_loss: 0.07981 test_loss: 0.09046 \n",
      "[258/500] train_loss: 0.06150 valid_loss: 0.08213 test_loss: 0.09025 \n",
      "[259/500] train_loss: 0.06407 valid_loss: 0.07950 test_loss: 0.09228 \n",
      "[260/500] train_loss: 0.06203 valid_loss: 0.07660 test_loss: 0.09052 \n",
      "验证损失减少 (0.077373 --> 0.076598). 正在保存模型...\n",
      "[261/500] train_loss: 0.06170 valid_loss: 0.07714 test_loss: 0.09087 \n",
      "[262/500] train_loss: 0.06357 valid_loss: 0.07849 test_loss: 0.09084 \n",
      "[263/500] train_loss: 0.06136 valid_loss: 0.07792 test_loss: 0.08967 \n",
      "[264/500] train_loss: 0.06028 valid_loss: 0.08004 test_loss: 0.09050 \n",
      "[265/500] train_loss: 0.06306 valid_loss: 0.07947 test_loss: 0.08906 \n",
      "[266/500] train_loss: 0.06241 valid_loss: 0.07641 test_loss: 0.08966 \n",
      "验证损失减少 (0.076598 --> 0.076408). 正在保存模型...\n",
      "[267/500] train_loss: 0.06223 valid_loss: 0.07626 test_loss: 0.08912 \n",
      "验证损失减少 (0.076408 --> 0.076256). 正在保存模型...\n",
      "[268/500] train_loss: 0.06125 valid_loss: 0.07648 test_loss: 0.08854 \n",
      "[269/500] train_loss: 0.06217 valid_loss: 0.07668 test_loss: 0.08877 \n",
      "[270/500] train_loss: 0.06111 valid_loss: 0.07957 test_loss: 0.09128 \n",
      "[271/500] train_loss: 0.06230 valid_loss: 0.08064 test_loss: 0.08903 \n",
      "[272/500] train_loss: 0.06179 valid_loss: 0.07736 test_loss: 0.08798 \n",
      "[273/500] train_loss: 0.06279 valid_loss: 0.07766 test_loss: 0.09052 \n",
      "[274/500] train_loss: 0.06249 valid_loss: 0.07794 test_loss: 0.08914 \n",
      "[275/500] train_loss: 0.06094 valid_loss: 0.07737 test_loss: 0.09059 \n",
      "[276/500] train_loss: 0.06224 valid_loss: 0.07780 test_loss: 0.08964 \n",
      "[277/500] train_loss: 0.06207 valid_loss: 0.07739 test_loss: 0.08956 \n",
      "[278/500] train_loss: 0.06134 valid_loss: 0.07682 test_loss: 0.08915 \n",
      "[279/500] train_loss: 0.06084 valid_loss: 0.07754 test_loss: 0.09050 \n",
      "[280/500] train_loss: 0.05966 valid_loss: 0.08475 test_loss: 0.08916 \n",
      "[281/500] train_loss: 0.06050 valid_loss: 0.08014 test_loss: 0.08999 \n",
      "[282/500] train_loss: 0.06125 valid_loss: 0.07766 test_loss: 0.09018 \n",
      "[283/500] train_loss: 0.06249 valid_loss: 0.08304 test_loss: 0.09048 \n",
      "[284/500] train_loss: 0.06074 valid_loss: 0.08311 test_loss: 0.08925 \n",
      "[285/500] train_loss: 0.06153 valid_loss: 0.07692 test_loss: 0.08836 \n",
      "[286/500] train_loss: 0.05977 valid_loss: 0.07772 test_loss: 0.08913 \n",
      "[287/500] train_loss: 0.06043 valid_loss: 0.07722 test_loss: 0.08938 \n",
      "[288/500] train_loss: 0.06283 valid_loss: 0.07628 test_loss: 0.08892 \n",
      "[289/500] train_loss: 0.06048 valid_loss: 0.08139 test_loss: 0.08886 \n",
      "[290/500] train_loss: 0.06101 valid_loss: 0.08232 test_loss: 0.09068 \n",
      "[291/500] train_loss: 0.05983 valid_loss: 0.09565 test_loss: 0.08990 \n",
      "[292/500] train_loss: 0.06063 valid_loss: 0.08769 test_loss: 0.08880 \n",
      "[293/500] train_loss: 0.06094 valid_loss: 0.09236 test_loss: 0.08879 \n",
      "[294/500] train_loss: 0.05949 valid_loss: 0.08471 test_loss: 0.08857 \n",
      "[295/500] train_loss: 0.05952 valid_loss: 0.08663 test_loss: 0.08846 \n",
      "[296/500] train_loss: 0.05986 valid_loss: 0.07982 test_loss: 0.08877 \n",
      "[297/500] train_loss: 0.05896 valid_loss: 0.07964 test_loss: 0.08790 \n",
      "[298/500] train_loss: 0.05875 valid_loss: 0.07717 test_loss: 0.08966 \n",
      "[299/500] train_loss: 0.06030 valid_loss: 0.08727 test_loss: 0.08847 \n",
      "[300/500] train_loss: 0.06478 valid_loss: 0.07965 test_loss: 0.08863 \n",
      "[301/500] train_loss: 0.06118 valid_loss: 0.08009 test_loss: 0.09011 \n",
      "[302/500] train_loss: 0.06010 valid_loss: 0.07901 test_loss: 0.08795 \n",
      "[303/500] train_loss: 0.06182 valid_loss: 0.07768 test_loss: 0.08852 \n",
      "[304/500] train_loss: 0.06153 valid_loss: 0.07811 test_loss: 0.09008 \n",
      "[305/500] train_loss: 0.05948 valid_loss: 0.08158 test_loss: 0.08979 \n",
      "[306/500] train_loss: 0.06117 valid_loss: 0.07863 test_loss: 0.08922 \n",
      "[307/500] train_loss: 0.05841 valid_loss: 0.07713 test_loss: 0.08937 \n",
      "[308/500] train_loss: 0.06187 valid_loss: 0.07850 test_loss: 0.08966 \n",
      "[309/500] train_loss: 0.05946 valid_loss: 0.07792 test_loss: 0.08978 \n",
      "[310/500] train_loss: 0.05877 valid_loss: 0.07833 test_loss: 0.09035 \n",
      "[311/500] train_loss: 0.05854 valid_loss: 0.07685 test_loss: 0.08970 \n",
      "[312/500] train_loss: 0.05918 valid_loss: 0.07673 test_loss: 0.08941 \n",
      "[313/500] train_loss: 0.05896 valid_loss: 0.07617 test_loss: 0.08844 \n",
      "验证损失减少 (0.076256 --> 0.076165). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[314/500] train_loss: 0.05843 valid_loss: 0.07949 test_loss: 0.09132 \n",
      "[315/500] train_loss: 0.05787 valid_loss: 0.07713 test_loss: 0.08929 \n",
      "[316/500] train_loss: 0.05967 valid_loss: 0.07660 test_loss: 0.08961 \n",
      "[317/500] train_loss: 0.05928 valid_loss: 0.07929 test_loss: 0.08943 \n",
      "[318/500] train_loss: 0.05877 valid_loss: 0.07670 test_loss: 0.08963 \n",
      "[319/500] train_loss: 0.05985 valid_loss: 0.07575 test_loss: 0.08787 \n",
      "验证损失减少 (0.076165 --> 0.075754). 正在保存模型...\n",
      "[320/500] train_loss: 0.06065 valid_loss: 0.08078 test_loss: 0.08816 \n",
      "[321/500] train_loss: 0.06037 valid_loss: 0.08014 test_loss: 0.08890 \n",
      "[322/500] train_loss: 0.05926 valid_loss: 0.07741 test_loss: 0.08881 \n",
      "[323/500] train_loss: 0.05853 valid_loss: 0.07859 test_loss: 0.08997 \n",
      "[324/500] train_loss: 0.05878 valid_loss: 0.08058 test_loss: 0.08993 \n",
      "[325/500] train_loss: 0.05656 valid_loss: 0.08046 test_loss: 0.08894 \n",
      "[326/500] train_loss: 0.05825 valid_loss: 0.07864 test_loss: 0.08922 \n",
      "[327/500] train_loss: 0.05873 valid_loss: 0.07632 test_loss: 0.08855 \n",
      "[328/500] train_loss: 0.05776 valid_loss: 0.07812 test_loss: 0.09020 \n",
      "[329/500] train_loss: 0.05914 valid_loss: 0.07969 test_loss: 0.08783 \n",
      "[330/500] train_loss: 0.05864 valid_loss: 0.07791 test_loss: 0.08757 \n",
      "[331/500] train_loss: 0.05957 valid_loss: 0.07735 test_loss: 0.08740 \n",
      "[332/500] train_loss: 0.05814 valid_loss: 0.07690 test_loss: 0.08869 \n",
      "[333/500] train_loss: 0.05770 valid_loss: 0.07722 test_loss: 0.08914 \n",
      "[334/500] train_loss: 0.05829 valid_loss: 0.07616 test_loss: 0.08882 \n",
      "[335/500] train_loss: 0.05830 valid_loss: 0.07699 test_loss: 0.08946 \n",
      "[336/500] train_loss: 0.05724 valid_loss: 0.07791 test_loss: 0.09071 \n",
      "[337/500] train_loss: 0.05764 valid_loss: 0.07742 test_loss: 0.08887 \n",
      "[338/500] train_loss: 0.05880 valid_loss: 0.07702 test_loss: 0.08842 \n",
      "[339/500] train_loss: 0.05926 valid_loss: 0.07786 test_loss: 0.08913 \n",
      "[340/500] train_loss: 0.05746 valid_loss: 0.07680 test_loss: 0.08881 \n",
      "[341/500] train_loss: 0.05797 valid_loss: 0.08477 test_loss: 0.08750 \n",
      "[342/500] train_loss: 0.05770 valid_loss: 0.07856 test_loss: 0.08793 \n",
      "[343/500] train_loss: 0.05653 valid_loss: 0.07791 test_loss: 0.08821 \n",
      "[344/500] train_loss: 0.05833 valid_loss: 0.07528 test_loss: 0.08820 \n",
      "验证损失减少 (0.075754 --> 0.075276). 正在保存模型...\n",
      "[345/500] train_loss: 0.05796 valid_loss: 0.07696 test_loss: 0.08933 \n",
      "[346/500] train_loss: 0.05823 valid_loss: 0.07632 test_loss: 0.08817 \n",
      "[347/500] train_loss: 0.05682 valid_loss: 0.07696 test_loss: 0.08764 \n",
      "[348/500] train_loss: 0.05629 valid_loss: 0.07619 test_loss: 0.08788 \n",
      "[349/500] train_loss: 0.05798 valid_loss: 0.07643 test_loss: 0.08778 \n",
      "[350/500] train_loss: 0.05686 valid_loss: 0.07954 test_loss: 0.08751 \n",
      "[351/500] train_loss: 0.05618 valid_loss: 0.08064 test_loss: 0.08770 \n",
      "[352/500] train_loss: 0.05799 valid_loss: 0.08319 test_loss: 0.08890 \n",
      "[353/500] train_loss: 0.05812 valid_loss: 0.08341 test_loss: 0.08862 \n",
      "[354/500] train_loss: 0.05869 valid_loss: 0.07536 test_loss: 0.08813 \n",
      "[355/500] train_loss: 0.06019 valid_loss: 0.07985 test_loss: 0.08831 \n",
      "[356/500] train_loss: 0.05815 valid_loss: 0.07714 test_loss: 0.08843 \n",
      "[357/500] train_loss: 0.05816 valid_loss: 0.07702 test_loss: 0.08864 \n",
      "[358/500] train_loss: 0.05697 valid_loss: 0.07583 test_loss: 0.08744 \n",
      "[359/500] train_loss: 0.05605 valid_loss: 0.07792 test_loss: 0.08773 \n",
      "[360/500] train_loss: 0.05729 valid_loss: 0.08139 test_loss: 0.08799 \n",
      "[361/500] train_loss: 0.05675 valid_loss: 0.07635 test_loss: 0.08944 \n",
      "[362/500] train_loss: 0.05715 valid_loss: 0.08817 test_loss: 0.08838 \n",
      "[363/500] train_loss: 0.05720 valid_loss: 0.07920 test_loss: 0.08797 \n",
      "[364/500] train_loss: 0.05644 valid_loss: 0.07746 test_loss: 0.08799 \n",
      "[365/500] train_loss: 0.05677 valid_loss: 0.08650 test_loss: 0.08887 \n",
      "[366/500] train_loss: 0.05699 valid_loss: 0.08579 test_loss: 0.08656 \n",
      "[367/500] train_loss: 0.05820 valid_loss: 0.09801 test_loss: 0.08701 \n",
      "[368/500] train_loss: 0.05773 valid_loss: 0.07669 test_loss: 0.08761 \n",
      "[369/500] train_loss: 0.05585 valid_loss: 0.09176 test_loss: 0.08724 \n",
      "[370/500] train_loss: 0.05743 valid_loss: 0.07688 test_loss: 0.08795 \n",
      "[371/500] train_loss: 0.05738 valid_loss: 0.07720 test_loss: 0.08764 \n",
      "[372/500] train_loss: 0.05763 valid_loss: 0.09477 test_loss: 0.08652 \n",
      "[373/500] train_loss: 0.05673 valid_loss: 0.08998 test_loss: 0.08620 \n",
      "[374/500] train_loss: 0.05533 valid_loss: 0.08449 test_loss: 0.08742 \n",
      "[375/500] train_loss: 0.05602 valid_loss: 0.07889 test_loss: 0.08806 \n",
      "[376/500] train_loss: 0.05614 valid_loss: 0.08729 test_loss: 0.08700 \n",
      "[377/500] train_loss: 0.05761 valid_loss: 0.09521 test_loss: 0.08767 \n",
      "[378/500] train_loss: 0.05656 valid_loss: 0.09076 test_loss: 0.08712 \n",
      "[379/500] train_loss: 0.05642 valid_loss: 0.08175 test_loss: 0.08750 \n",
      "[380/500] train_loss: 0.05605 valid_loss: 0.09068 test_loss: 0.08698 \n",
      "[381/500] train_loss: 0.05822 valid_loss: 0.07535 test_loss: 0.08802 \n",
      "[382/500] train_loss: 0.05917 valid_loss: 0.07479 test_loss: 0.08795 \n",
      "验证损失减少 (0.075276 --> 0.074792). 正在保存模型...\n",
      "[383/500] train_loss: 0.05770 valid_loss: 0.07482 test_loss: 0.08707 \n",
      "[384/500] train_loss: 0.05484 valid_loss: 0.07550 test_loss: 0.08785 \n",
      "[385/500] train_loss: 0.05575 valid_loss: 0.07622 test_loss: 0.08906 \n",
      "[386/500] train_loss: 0.05587 valid_loss: 0.07418 test_loss: 0.08650 \n",
      "验证损失减少 (0.074792 --> 0.074181). 正在保存模型...\n",
      "[387/500] train_loss: 0.05505 valid_loss: 0.07419 test_loss: 0.08763 \n",
      "[388/500] train_loss: 0.05604 valid_loss: 0.07518 test_loss: 0.08805 \n",
      "[389/500] train_loss: 0.05601 valid_loss: 0.07588 test_loss: 0.08804 \n",
      "[390/500] train_loss: 0.05589 valid_loss: 0.07570 test_loss: 0.08859 \n",
      "[391/500] train_loss: 0.05557 valid_loss: 0.07773 test_loss: 0.08815 \n",
      "[392/500] train_loss: 0.05442 valid_loss: 0.07858 test_loss: 0.08743 \n",
      "[393/500] train_loss: 0.05531 valid_loss: 0.07568 test_loss: 0.08796 \n",
      "[394/500] train_loss: 0.05565 valid_loss: 0.07638 test_loss: 0.08868 \n",
      "[395/500] train_loss: 0.05522 valid_loss: 0.07528 test_loss: 0.08773 \n",
      "[396/500] train_loss: 0.05710 valid_loss: 0.07448 test_loss: 0.08801 \n",
      "[397/500] train_loss: 0.05506 valid_loss: 0.07673 test_loss: 0.08660 \n",
      "[398/500] train_loss: 0.05432 valid_loss: 0.07580 test_loss: 0.08816 \n",
      "[399/500] train_loss: 0.05525 valid_loss: 0.07543 test_loss: 0.08752 \n",
      "[400/500] train_loss: 0.05613 valid_loss: 0.07482 test_loss: 0.08693 \n",
      "[401/500] train_loss: 0.05494 valid_loss: 0.07567 test_loss: 0.08714 \n",
      "[402/500] train_loss: 0.05560 valid_loss: 0.07497 test_loss: 0.08638 \n",
      "[403/500] train_loss: 0.05526 valid_loss: 0.07511 test_loss: 0.08742 \n",
      "[404/500] train_loss: 0.05302 valid_loss: 0.07507 test_loss: 0.08781 \n",
      "[405/500] train_loss: 0.05585 valid_loss: 0.07469 test_loss: 0.08679 \n",
      "[406/500] train_loss: 0.05469 valid_loss: 0.07595 test_loss: 0.08733 \n",
      "[407/500] train_loss: 0.05535 valid_loss: 0.07944 test_loss: 0.08709 \n",
      "[408/500] train_loss: 0.05561 valid_loss: 0.08323 test_loss: 0.08746 \n",
      "[409/500] train_loss: 0.05565 valid_loss: 0.08381 test_loss: 0.08795 \n",
      "[410/500] train_loss: 0.05548 valid_loss: 0.08492 test_loss: 0.08783 \n",
      "[411/500] train_loss: 0.05427 valid_loss: 0.07825 test_loss: 0.08788 \n",
      "[412/500] train_loss: 0.05408 valid_loss: 0.07728 test_loss: 0.08768 \n",
      "[413/500] train_loss: 0.05478 valid_loss: 0.07716 test_loss: 0.08774 \n",
      "[414/500] train_loss: 0.05560 valid_loss: 0.07698 test_loss: 0.08742 \n",
      "[415/500] train_loss: 0.05442 valid_loss: 0.07608 test_loss: 0.08749 \n",
      "[416/500] train_loss: 0.05377 valid_loss: 0.07735 test_loss: 0.08761 \n",
      "[417/500] train_loss: 0.05344 valid_loss: 0.07782 test_loss: 0.08754 \n",
      "[418/500] train_loss: 0.05315 valid_loss: 0.07514 test_loss: 0.08720 \n",
      "[419/500] train_loss: 0.05407 valid_loss: 0.07494 test_loss: 0.08726 \n",
      "[420/500] train_loss: 0.05559 valid_loss: 0.07517 test_loss: 0.08859 \n",
      "[421/500] train_loss: 0.05215 valid_loss: 0.07525 test_loss: 0.08753 \n",
      "[422/500] train_loss: 0.05446 valid_loss: 0.07532 test_loss: 0.08837 \n",
      "[423/500] train_loss: 0.05358 valid_loss: 0.07475 test_loss: 0.08872 \n",
      "[424/500] train_loss: 0.05435 valid_loss: 0.07880 test_loss: 0.08751 \n",
      "[425/500] train_loss: 0.05642 valid_loss: 0.07453 test_loss: 0.08711 \n",
      "[426/500] train_loss: 0.05470 valid_loss: 0.08030 test_loss: 0.08908 \n",
      "[427/500] train_loss: 0.05360 valid_loss: 0.07701 test_loss: 0.08818 \n",
      "[428/500] train_loss: 0.05356 valid_loss: 0.07850 test_loss: 0.08667 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[429/500] train_loss: 0.05341 valid_loss: 0.08183 test_loss: 0.08780 \n",
      "[430/500] train_loss: 0.05467 valid_loss: 0.07623 test_loss: 0.08851 \n",
      "[431/500] train_loss: 0.05419 valid_loss: 0.07852 test_loss: 0.08735 \n",
      "[432/500] train_loss: 0.05350 valid_loss: 0.07657 test_loss: 0.08787 \n",
      "[433/500] train_loss: 0.05472 valid_loss: 0.07717 test_loss: 0.08806 \n",
      "[434/500] train_loss: 0.05371 valid_loss: 0.07574 test_loss: 0.08782 \n",
      "[435/500] train_loss: 0.05370 valid_loss: 0.07672 test_loss: 0.08692 \n",
      "[436/500] train_loss: 0.05470 valid_loss: 0.08176 test_loss: 0.08768 \n",
      "[437/500] train_loss: 0.05374 valid_loss: 0.07471 test_loss: 0.08804 \n",
      "[438/500] train_loss: 0.05189 valid_loss: 0.07604 test_loss: 0.08825 \n",
      "[439/500] train_loss: 0.05306 valid_loss: 0.08560 test_loss: 0.08811 \n",
      "[440/500] train_loss: 0.05374 valid_loss: 0.07415 test_loss: 0.08668 \n",
      "验证损失减少 (0.074181 --> 0.074145). 正在保存模型...\n",
      "[441/500] train_loss: 0.05354 valid_loss: 0.07735 test_loss: 0.08994 \n",
      "[442/500] train_loss: 0.05300 valid_loss: 0.07799 test_loss: 0.08980 \n",
      "[443/500] train_loss: 0.05311 valid_loss: 0.08525 test_loss: 0.08888 \n",
      "[444/500] train_loss: 0.05326 valid_loss: 0.09603 test_loss: 0.08951 \n",
      "[445/500] train_loss: 0.05368 valid_loss: 0.07694 test_loss: 0.08780 \n",
      "[446/500] train_loss: 0.05409 valid_loss: 0.08798 test_loss: 0.08954 \n",
      "[447/500] train_loss: 0.05321 valid_loss: 0.09366 test_loss: 0.08963 \n",
      "[448/500] train_loss: 0.05162 valid_loss: 0.07566 test_loss: 0.08921 \n",
      "[449/500] train_loss: 0.05446 valid_loss: 0.08368 test_loss: 0.09013 \n",
      "[450/500] train_loss: 0.05413 valid_loss: 0.07779 test_loss: 0.08791 \n",
      "[451/500] train_loss: 0.05232 valid_loss: 0.07639 test_loss: 0.08937 \n",
      "[452/500] train_loss: 0.05339 valid_loss: 0.07664 test_loss: 0.08805 \n",
      "[453/500] train_loss: 0.05444 valid_loss: 0.08941 test_loss: 0.08752 \n",
      "[454/500] train_loss: 0.05346 valid_loss: 0.10277 test_loss: 0.08754 \n",
      "[455/500] train_loss: 0.05321 valid_loss: 0.08093 test_loss: 0.08851 \n",
      "[456/500] train_loss: 0.05197 valid_loss: 0.10339 test_loss: 0.08939 \n",
      "[457/500] train_loss: 0.05328 valid_loss: 0.09218 test_loss: 0.08675 \n",
      "[458/500] train_loss: 0.05393 valid_loss: 0.08669 test_loss: 0.08924 \n",
      "[459/500] train_loss: 0.05373 valid_loss: 0.09816 test_loss: 0.08803 \n",
      "[460/500] train_loss: 0.05150 valid_loss: 0.08811 test_loss: 0.08959 \n",
      "[461/500] train_loss: 0.05233 valid_loss: 0.08225 test_loss: 0.08854 \n",
      "[462/500] train_loss: 0.05363 valid_loss: 0.08320 test_loss: 0.08790 \n",
      "[463/500] train_loss: 0.05243 valid_loss: 0.09102 test_loss: 0.08814 \n",
      "[464/500] train_loss: 0.05207 valid_loss: 0.08287 test_loss: 0.08750 \n",
      "[465/500] train_loss: 0.05286 valid_loss: 0.07570 test_loss: 0.08746 \n",
      "[466/500] train_loss: 0.05296 valid_loss: 0.07921 test_loss: 0.08650 \n",
      "[467/500] train_loss: 0.05207 valid_loss: 0.07742 test_loss: 0.08665 \n",
      "[468/500] train_loss: 0.05293 valid_loss: 0.07921 test_loss: 0.08769 \n",
      "[469/500] train_loss: 0.05183 valid_loss: 0.07614 test_loss: 0.08993 \n",
      "[470/500] train_loss: 0.05345 valid_loss: 0.07550 test_loss: 0.08802 \n",
      "[471/500] train_loss: 0.05380 valid_loss: 0.08233 test_loss: 0.08829 \n",
      "[472/500] train_loss: 0.05077 valid_loss: 0.08707 test_loss: 0.08936 \n",
      "[473/500] train_loss: 0.05175 valid_loss: 0.07914 test_loss: 0.08801 \n",
      "[474/500] train_loss: 0.05288 valid_loss: 0.07770 test_loss: 0.08906 \n",
      "[475/500] train_loss: 0.05241 valid_loss: 0.10499 test_loss: 0.08880 \n",
      "[476/500] train_loss: 0.05160 valid_loss: 0.09809 test_loss: 0.08818 \n",
      "[477/500] train_loss: 0.05145 valid_loss: 0.07910 test_loss: 0.08742 \n",
      "[478/500] train_loss: 0.05143 valid_loss: 0.07976 test_loss: 0.08791 \n",
      "[479/500] train_loss: 0.05237 valid_loss: 0.08360 test_loss: 0.08751 \n",
      "[480/500] train_loss: 0.05290 valid_loss: 0.08371 test_loss: 0.08992 \n",
      "[481/500] train_loss: 0.05219 valid_loss: 0.11384 test_loss: 0.08782 \n",
      "[482/500] train_loss: 0.05373 valid_loss: 0.09037 test_loss: 0.08919 \n",
      "[483/500] train_loss: 0.05300 valid_loss: 0.07793 test_loss: 0.08808 \n",
      "[484/500] train_loss: 0.05139 valid_loss: 0.07601 test_loss: 0.08979 \n",
      "[485/500] train_loss: 0.05181 valid_loss: 0.07780 test_loss: 0.08904 \n",
      "[486/500] train_loss: 0.05256 valid_loss: 0.07476 test_loss: 0.08686 \n",
      "[487/500] train_loss: 0.05279 valid_loss: 0.09710 test_loss: 0.08845 \n",
      "[488/500] train_loss: 0.05142 valid_loss: 0.09003 test_loss: 0.08832 \n",
      "[489/500] train_loss: 0.05325 valid_loss: 0.09680 test_loss: 0.08951 \n",
      "[490/500] train_loss: 0.05227 valid_loss: 0.08424 test_loss: 0.08918 \n",
      "[491/500] train_loss: 0.05143 valid_loss: 0.09125 test_loss: 0.08818 \n",
      "[492/500] train_loss: 0.05143 valid_loss: 0.11224 test_loss: 0.08789 \n",
      "[493/500] train_loss: 0.05051 valid_loss: 0.08245 test_loss: 0.08978 \n",
      "[494/500] train_loss: 0.05256 valid_loss: 0.08726 test_loss: 0.08964 \n",
      "[495/500] train_loss: 0.05323 valid_loss: 0.07994 test_loss: 0.08707 \n",
      "[496/500] train_loss: 0.05352 valid_loss: 0.07933 test_loss: 0.08764 \n",
      "[497/500] train_loss: 0.05102 valid_loss: 0.10840 test_loss: 0.08881 \n",
      "[498/500] train_loss: 0.05036 valid_loss: 0.08832 test_loss: 0.08847 \n",
      "[499/500] train_loss: 0.05087 valid_loss: 0.08162 test_loss: 0.08664 \n",
      "[500/500] train_loss: 0.05104 valid_loss: 0.08891 test_loss: 0.08834 \n",
      "TRAINING MODEL 1\n",
      "[  1/500] train_loss: 0.35785 valid_loss: 0.25884 test_loss: 0.26560 \n",
      "验证损失减少 (inf --> 0.258838). 正在保存模型...\n",
      "[  2/500] train_loss: 0.20505 valid_loss: 0.19485 test_loss: 0.20359 \n",
      "验证损失减少 (0.258838 --> 0.194850). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17059 valid_loss: 0.16986 test_loss: 0.17921 \n",
      "验证损失减少 (0.194850 --> 0.169865). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15842 valid_loss: 0.16000 test_loss: 0.17150 \n",
      "验证损失减少 (0.169865 --> 0.160002). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15087 valid_loss: 0.15057 test_loss: 0.16185 \n",
      "验证损失减少 (0.160002 --> 0.150574). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14586 valid_loss: 0.14438 test_loss: 0.15635 \n",
      "验证损失减少 (0.150574 --> 0.144379). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13685 valid_loss: 0.14167 test_loss: 0.15539 \n",
      "验证损失减少 (0.144379 --> 0.141668). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13519 valid_loss: 0.13727 test_loss: 0.14934 \n",
      "验证损失减少 (0.141668 --> 0.137274). 正在保存模型...\n",
      "[  9/500] train_loss: 0.12964 valid_loss: 0.13428 test_loss: 0.14625 \n",
      "验证损失减少 (0.137274 --> 0.134276). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12681 valid_loss: 0.13188 test_loss: 0.14790 \n",
      "验证损失减少 (0.134276 --> 0.131884). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12353 valid_loss: 0.12868 test_loss: 0.14480 \n",
      "验证损失减少 (0.131884 --> 0.128683). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12471 valid_loss: 0.12821 test_loss: 0.14330 \n",
      "验证损失减少 (0.128683 --> 0.128212). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12015 valid_loss: 0.12589 test_loss: 0.14075 \n",
      "验证损失减少 (0.128212 --> 0.125893). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.11996 valid_loss: 0.12097 test_loss: 0.13589 \n",
      "验证损失减少 (0.125893 --> 0.120975). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11713 valid_loss: 0.12052 test_loss: 0.13467 \n",
      "验证损失减少 (0.120975 --> 0.120523). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11574 valid_loss: 0.12140 test_loss: 0.13515 \n",
      "[ 17/500] train_loss: 0.11595 valid_loss: 0.11842 test_loss: 0.13287 \n",
      "验证损失减少 (0.120523 --> 0.118420). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11197 valid_loss: 0.11621 test_loss: 0.13040 \n",
      "验证损失减少 (0.118420 --> 0.116213). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11107 valid_loss: 0.11592 test_loss: 0.12864 \n",
      "验证损失减少 (0.116213 --> 0.115920). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.10796 valid_loss: 0.11512 test_loss: 0.12992 \n",
      "验证损失减少 (0.115920 --> 0.115121). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.10876 valid_loss: 0.11559 test_loss: 0.12953 \n",
      "[ 22/500] train_loss: 0.10825 valid_loss: 0.11509 test_loss: 0.12883 \n",
      "验证损失减少 (0.115121 --> 0.115093). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10712 valid_loss: 0.11160 test_loss: 0.12469 \n",
      "验证损失减少 (0.115093 --> 0.111604). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10724 valid_loss: 0.10990 test_loss: 0.12616 \n",
      "验证损失减少 (0.111604 --> 0.109899). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10555 valid_loss: 0.11064 test_loss: 0.12393 \n",
      "[ 26/500] train_loss: 0.10848 valid_loss: 0.11610 test_loss: 0.12809 \n",
      "[ 27/500] train_loss: 0.10665 valid_loss: 0.10908 test_loss: 0.12323 \n",
      "验证损失减少 (0.109899 --> 0.109075). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10480 valid_loss: 0.10759 test_loss: 0.12018 \n",
      "验证损失减少 (0.109075 --> 0.107592). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.09991 valid_loss: 0.10910 test_loss: 0.12177 \n",
      "[ 30/500] train_loss: 0.10311 valid_loss: 0.10631 test_loss: 0.12030 \n",
      "验证损失减少 (0.107592 --> 0.106311). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 31/500] train_loss: 0.10418 valid_loss: 0.10646 test_loss: 0.11941 \n",
      "[ 32/500] train_loss: 0.10203 valid_loss: 0.10585 test_loss: 0.12047 \n",
      "验证损失减少 (0.106311 --> 0.105848). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10008 valid_loss: 0.10628 test_loss: 0.12040 \n",
      "[ 34/500] train_loss: 0.10043 valid_loss: 0.10389 test_loss: 0.11729 \n",
      "验证损失减少 (0.105848 --> 0.103886). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.09946 valid_loss: 0.10426 test_loss: 0.11531 \n",
      "[ 36/500] train_loss: 0.09561 valid_loss: 0.10647 test_loss: 0.11565 \n",
      "[ 37/500] train_loss: 0.09752 valid_loss: 0.10400 test_loss: 0.11868 \n",
      "[ 38/500] train_loss: 0.09514 valid_loss: 0.10199 test_loss: 0.11392 \n",
      "验证损失减少 (0.103886 --> 0.101990). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.09738 valid_loss: 0.10122 test_loss: 0.11443 \n",
      "验证损失减少 (0.101990 --> 0.101217). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09786 valid_loss: 0.10358 test_loss: 0.11613 \n",
      "[ 41/500] train_loss: 0.09661 valid_loss: 0.10066 test_loss: 0.11266 \n",
      "验证损失减少 (0.101217 --> 0.100660). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.09587 valid_loss: 0.10050 test_loss: 0.11430 \n",
      "验证损失减少 (0.100660 --> 0.100502). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.09596 valid_loss: 0.10106 test_loss: 0.11342 \n",
      "[ 44/500] train_loss: 0.09472 valid_loss: 0.10195 test_loss: 0.11293 \n",
      "[ 45/500] train_loss: 0.09432 valid_loss: 0.09886 test_loss: 0.11161 \n",
      "验证损失减少 (0.100502 --> 0.098863). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.09233 valid_loss: 0.10195 test_loss: 0.11421 \n",
      "[ 47/500] train_loss: 0.09209 valid_loss: 0.09987 test_loss: 0.11248 \n",
      "[ 48/500] train_loss: 0.09326 valid_loss: 0.09779 test_loss: 0.10929 \n",
      "验证损失减少 (0.098863 --> 0.097790). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09102 valid_loss: 0.09889 test_loss: 0.11080 \n",
      "[ 50/500] train_loss: 0.09119 valid_loss: 0.09806 test_loss: 0.11064 \n",
      "[ 51/500] train_loss: 0.09131 valid_loss: 0.10457 test_loss: 0.11229 \n",
      "[ 52/500] train_loss: 0.09101 valid_loss: 0.09554 test_loss: 0.10796 \n",
      "验证损失减少 (0.097790 --> 0.095543). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.09154 valid_loss: 0.09919 test_loss: 0.10995 \n",
      "[ 54/500] train_loss: 0.09009 valid_loss: 0.09565 test_loss: 0.10898 \n",
      "[ 55/500] train_loss: 0.08996 valid_loss: 0.09691 test_loss: 0.10739 \n",
      "[ 56/500] train_loss: 0.08889 valid_loss: 0.09608 test_loss: 0.10799 \n",
      "[ 57/500] train_loss: 0.09030 valid_loss: 0.09582 test_loss: 0.10778 \n",
      "[ 58/500] train_loss: 0.08754 valid_loss: 0.09616 test_loss: 0.11012 \n",
      "[ 59/500] train_loss: 0.08840 valid_loss: 0.09558 test_loss: 0.10702 \n",
      "[ 60/500] train_loss: 0.08741 valid_loss: 0.09840 test_loss: 0.10880 \n",
      "[ 61/500] train_loss: 0.08603 valid_loss: 0.09450 test_loss: 0.10737 \n",
      "验证损失减少 (0.095543 --> 0.094504). 正在保存模型...\n",
      "[ 62/500] train_loss: 0.08883 valid_loss: 0.09344 test_loss: 0.10648 \n",
      "验证损失减少 (0.094504 --> 0.093441). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.08703 valid_loss: 0.09411 test_loss: 0.10745 \n",
      "[ 64/500] train_loss: 0.08960 valid_loss: 0.09286 test_loss: 0.10517 \n",
      "验证损失减少 (0.093441 --> 0.092855). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.08716 valid_loss: 0.09353 test_loss: 0.10556 \n",
      "[ 66/500] train_loss: 0.08550 valid_loss: 0.09350 test_loss: 0.10630 \n",
      "[ 67/500] train_loss: 0.08725 valid_loss: 0.09393 test_loss: 0.10649 \n",
      "[ 68/500] train_loss: 0.08593 valid_loss: 0.09195 test_loss: 0.10567 \n",
      "验证损失减少 (0.092855 --> 0.091955). 正在保存模型...\n",
      "[ 69/500] train_loss: 0.08242 valid_loss: 0.09328 test_loss: 0.10505 \n",
      "[ 70/500] train_loss: 0.08474 valid_loss: 0.09242 test_loss: 0.10508 \n",
      "[ 71/500] train_loss: 0.08631 valid_loss: 0.09187 test_loss: 0.10480 \n",
      "验证损失减少 (0.091955 --> 0.091874). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.08658 valid_loss: 0.09257 test_loss: 0.10379 \n",
      "[ 73/500] train_loss: 0.08549 valid_loss: 0.09031 test_loss: 0.10280 \n",
      "验证损失减少 (0.091874 --> 0.090312). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.08254 valid_loss: 0.09583 test_loss: 0.10493 \n",
      "[ 75/500] train_loss: 0.08479 valid_loss: 0.09310 test_loss: 0.10504 \n",
      "[ 76/500] train_loss: 0.08279 valid_loss: 0.09042 test_loss: 0.10455 \n",
      "[ 77/500] train_loss: 0.08232 valid_loss: 0.08972 test_loss: 0.10310 \n",
      "验证损失减少 (0.090312 --> 0.089723). 正在保存模型...\n",
      "[ 78/500] train_loss: 0.08201 valid_loss: 0.09047 test_loss: 0.10341 \n",
      "[ 79/500] train_loss: 0.08148 valid_loss: 0.08977 test_loss: 0.10242 \n",
      "[ 80/500] train_loss: 0.08130 valid_loss: 0.08984 test_loss: 0.10320 \n",
      "[ 81/500] train_loss: 0.08137 valid_loss: 0.09332 test_loss: 0.10427 \n",
      "[ 82/500] train_loss: 0.08372 valid_loss: 0.08934 test_loss: 0.10354 \n",
      "验证损失减少 (0.089723 --> 0.089345). 正在保存模型...\n",
      "[ 83/500] train_loss: 0.08248 valid_loss: 0.09000 test_loss: 0.10367 \n",
      "[ 84/500] train_loss: 0.08177 valid_loss: 0.08895 test_loss: 0.10117 \n",
      "验证损失减少 (0.089345 --> 0.088946). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.07955 valid_loss: 0.09018 test_loss: 0.10057 \n",
      "[ 86/500] train_loss: 0.08094 valid_loss: 0.08975 test_loss: 0.10029 \n",
      "[ 87/500] train_loss: 0.08099 valid_loss: 0.08867 test_loss: 0.10117 \n",
      "验证损失减少 (0.088946 --> 0.088674). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.08147 valid_loss: 0.08968 test_loss: 0.10204 \n",
      "[ 89/500] train_loss: 0.08080 valid_loss: 0.08831 test_loss: 0.09924 \n",
      "验证损失减少 (0.088674 --> 0.088309). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.08409 valid_loss: 0.08878 test_loss: 0.09944 \n",
      "[ 91/500] train_loss: 0.07919 valid_loss: 0.08926 test_loss: 0.10074 \n",
      "[ 92/500] train_loss: 0.07969 valid_loss: 0.08931 test_loss: 0.09885 \n",
      "[ 93/500] train_loss: 0.07819 valid_loss: 0.08912 test_loss: 0.10162 \n",
      "[ 94/500] train_loss: 0.08065 valid_loss: 0.08658 test_loss: 0.09819 \n",
      "验证损失减少 (0.088309 --> 0.086582). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.07903 valid_loss: 0.08654 test_loss: 0.09801 \n",
      "验证损失减少 (0.086582 --> 0.086535). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.07969 valid_loss: 0.08603 test_loss: 0.09846 \n",
      "验证损失减少 (0.086535 --> 0.086025). 正在保存模型...\n",
      "[ 97/500] train_loss: 0.07995 valid_loss: 0.08616 test_loss: 0.09924 \n",
      "[ 98/500] train_loss: 0.07953 valid_loss: 0.08706 test_loss: 0.09836 \n",
      "[ 99/500] train_loss: 0.07948 valid_loss: 0.08975 test_loss: 0.09866 \n",
      "[100/500] train_loss: 0.07815 valid_loss: 0.09033 test_loss: 0.09810 \n",
      "[101/500] train_loss: 0.07732 valid_loss: 0.08667 test_loss: 0.09771 \n",
      "[102/500] train_loss: 0.07721 valid_loss: 0.08518 test_loss: 0.09883 \n",
      "验证损失减少 (0.086025 --> 0.085179). 正在保存模型...\n",
      "[103/500] train_loss: 0.07582 valid_loss: 0.08872 test_loss: 0.09880 \n",
      "[104/500] train_loss: 0.07619 valid_loss: 0.08682 test_loss: 0.09796 \n",
      "[105/500] train_loss: 0.07710 valid_loss: 0.08835 test_loss: 0.09729 \n",
      "[106/500] train_loss: 0.07622 valid_loss: 0.08625 test_loss: 0.09881 \n",
      "[107/500] train_loss: 0.08114 valid_loss: 0.08551 test_loss: 0.09671 \n",
      "[108/500] train_loss: 0.07761 valid_loss: 0.09067 test_loss: 0.09680 \n",
      "[109/500] train_loss: 0.07674 valid_loss: 0.08998 test_loss: 0.09629 \n",
      "[110/500] train_loss: 0.07603 valid_loss: 0.08572 test_loss: 0.09908 \n",
      "[111/500] train_loss: 0.07549 valid_loss: 0.08667 test_loss: 0.09894 \n",
      "[112/500] train_loss: 0.08026 valid_loss: 0.08837 test_loss: 0.09656 \n",
      "[113/500] train_loss: 0.07662 valid_loss: 0.08508 test_loss: 0.09561 \n",
      "验证损失减少 (0.085179 --> 0.085081). 正在保存模型...\n",
      "[114/500] train_loss: 0.07795 valid_loss: 0.08580 test_loss: 0.09586 \n",
      "[115/500] train_loss: 0.07665 valid_loss: 0.08652 test_loss: 0.09799 \n",
      "[116/500] train_loss: 0.07627 valid_loss: 0.08456 test_loss: 0.09599 \n",
      "验证损失减少 (0.085081 --> 0.084564). 正在保存模型...\n",
      "[117/500] train_loss: 0.07599 valid_loss: 0.08418 test_loss: 0.09583 \n",
      "验证损失减少 (0.084564 --> 0.084183). 正在保存模型...\n",
      "[118/500] train_loss: 0.07705 valid_loss: 0.08428 test_loss: 0.09656 \n",
      "[119/500] train_loss: 0.07650 valid_loss: 0.08947 test_loss: 0.09925 \n",
      "[120/500] train_loss: 0.07624 valid_loss: 0.08775 test_loss: 0.09518 \n",
      "[121/500] train_loss: 0.07506 valid_loss: 0.08343 test_loss: 0.09509 \n",
      "验证损失减少 (0.084183 --> 0.083428). 正在保存模型...\n",
      "[122/500] train_loss: 0.07475 valid_loss: 0.08773 test_loss: 0.09355 \n",
      "[123/500] train_loss: 0.07642 valid_loss: 0.09598 test_loss: 0.09823 \n",
      "[124/500] train_loss: 0.07505 valid_loss: 0.09750 test_loss: 0.09403 \n",
      "[125/500] train_loss: 0.07433 valid_loss: 0.08589 test_loss: 0.09433 \n",
      "[126/500] train_loss: 0.07542 valid_loss: 0.09049 test_loss: 0.09547 \n",
      "[127/500] train_loss: 0.07421 valid_loss: 0.09088 test_loss: 0.09489 \n",
      "[128/500] train_loss: 0.07393 valid_loss: 0.08775 test_loss: 0.09357 \n",
      "[129/500] train_loss: 0.07395 valid_loss: 0.09335 test_loss: 0.09387 \n",
      "[130/500] train_loss: 0.07272 valid_loss: 0.08960 test_loss: 0.09431 \n",
      "[131/500] train_loss: 0.07196 valid_loss: 0.09319 test_loss: 0.09400 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132/500] train_loss: 0.07466 valid_loss: 0.08508 test_loss: 0.09329 \n",
      "[133/500] train_loss: 0.07192 valid_loss: 0.08140 test_loss: 0.09352 \n",
      "验证损失减少 (0.083428 --> 0.081398). 正在保存模型...\n",
      "[134/500] train_loss: 0.07174 valid_loss: 0.08108 test_loss: 0.09269 \n",
      "验证损失减少 (0.081398 --> 0.081083). 正在保存模型...\n",
      "[135/500] train_loss: 0.07356 valid_loss: 0.08674 test_loss: 0.09399 \n",
      "[136/500] train_loss: 0.07405 valid_loss: 0.08639 test_loss: 0.09364 \n",
      "[137/500] train_loss: 0.07438 valid_loss: 0.08425 test_loss: 0.09429 \n",
      "[138/500] train_loss: 0.07340 valid_loss: 0.08594 test_loss: 0.09251 \n",
      "[139/500] train_loss: 0.07566 valid_loss: 0.09072 test_loss: 0.09263 \n",
      "[140/500] train_loss: 0.07213 valid_loss: 0.08200 test_loss: 0.09198 \n",
      "[141/500] train_loss: 0.07232 valid_loss: 0.08225 test_loss: 0.09228 \n",
      "[142/500] train_loss: 0.07406 valid_loss: 0.08086 test_loss: 0.09201 \n",
      "验证损失减少 (0.081083 --> 0.080860). 正在保存模型...\n",
      "[143/500] train_loss: 0.07163 valid_loss: 0.08360 test_loss: 0.09484 \n",
      "[144/500] train_loss: 0.07310 valid_loss: 0.08552 test_loss: 0.09206 \n",
      "[145/500] train_loss: 0.07447 valid_loss: 0.08584 test_loss: 0.09252 \n",
      "[146/500] train_loss: 0.07079 valid_loss: 0.08224 test_loss: 0.09167 \n",
      "[147/500] train_loss: 0.07005 valid_loss: 0.08614 test_loss: 0.09153 \n",
      "[148/500] train_loss: 0.06933 valid_loss: 0.08623 test_loss: 0.09048 \n",
      "[149/500] train_loss: 0.07026 valid_loss: 0.08333 test_loss: 0.09169 \n",
      "[150/500] train_loss: 0.07210 valid_loss: 0.08667 test_loss: 0.09193 \n",
      "[151/500] train_loss: 0.06996 valid_loss: 0.08521 test_loss: 0.09247 \n",
      "[152/500] train_loss: 0.07270 valid_loss: 0.08119 test_loss: 0.09180 \n",
      "[153/500] train_loss: 0.07155 valid_loss: 0.08388 test_loss: 0.09162 \n",
      "[154/500] train_loss: 0.07006 valid_loss: 0.08187 test_loss: 0.09224 \n",
      "[155/500] train_loss: 0.06884 valid_loss: 0.08412 test_loss: 0.09133 \n",
      "[156/500] train_loss: 0.06937 valid_loss: 0.08088 test_loss: 0.09146 \n",
      "[157/500] train_loss: 0.07123 valid_loss: 0.08171 test_loss: 0.09185 \n",
      "[158/500] train_loss: 0.06922 valid_loss: 0.08124 test_loss: 0.09192 \n",
      "[159/500] train_loss: 0.07231 valid_loss: 0.08432 test_loss: 0.09067 \n",
      "[160/500] train_loss: 0.07038 valid_loss: 0.08129 test_loss: 0.09064 \n",
      "[161/500] train_loss: 0.07067 valid_loss: 0.08210 test_loss: 0.09163 \n",
      "[162/500] train_loss: 0.07012 valid_loss: 0.08525 test_loss: 0.09154 \n",
      "[163/500] train_loss: 0.06824 valid_loss: 0.08296 test_loss: 0.09199 \n",
      "[164/500] train_loss: 0.07074 valid_loss: 0.08056 test_loss: 0.09217 \n",
      "验证损失减少 (0.080860 --> 0.080561). 正在保存模型...\n",
      "[165/500] train_loss: 0.07030 valid_loss: 0.08107 test_loss: 0.09138 \n",
      "[166/500] train_loss: 0.06938 valid_loss: 0.08562 test_loss: 0.09239 \n",
      "[167/500] train_loss: 0.07163 valid_loss: 0.08125 test_loss: 0.09166 \n",
      "[168/500] train_loss: 0.07027 valid_loss: 0.08118 test_loss: 0.09035 \n",
      "[169/500] train_loss: 0.06977 valid_loss: 0.08267 test_loss: 0.09170 \n",
      "[170/500] train_loss: 0.06721 valid_loss: 0.08196 test_loss: 0.09105 \n",
      "[171/500] train_loss: 0.06818 valid_loss: 0.08100 test_loss: 0.09149 \n",
      "[172/500] train_loss: 0.06873 valid_loss: 0.07911 test_loss: 0.09099 \n",
      "验证损失减少 (0.080561 --> 0.079105). 正在保存模型...\n",
      "[173/500] train_loss: 0.07026 valid_loss: 0.08305 test_loss: 0.09201 \n",
      "[174/500] train_loss: 0.06864 valid_loss: 0.08436 test_loss: 0.09090 \n",
      "[175/500] train_loss: 0.06705 valid_loss: 0.08201 test_loss: 0.09018 \n",
      "[176/500] train_loss: 0.06910 valid_loss: 0.07937 test_loss: 0.08965 \n",
      "[177/500] train_loss: 0.06902 valid_loss: 0.08073 test_loss: 0.08963 \n",
      "[178/500] train_loss: 0.06869 valid_loss: 0.08519 test_loss: 0.09004 \n",
      "[179/500] train_loss: 0.06878 valid_loss: 0.08079 test_loss: 0.09082 \n",
      "[180/500] train_loss: 0.06883 valid_loss: 0.07881 test_loss: 0.08952 \n",
      "验证损失减少 (0.079105 --> 0.078809). 正在保存模型...\n",
      "[181/500] train_loss: 0.06686 valid_loss: 0.08055 test_loss: 0.08897 \n",
      "[182/500] train_loss: 0.06881 valid_loss: 0.07870 test_loss: 0.08880 \n",
      "验证损失减少 (0.078809 --> 0.078705). 正在保存模型...\n",
      "[183/500] train_loss: 0.06712 valid_loss: 0.08344 test_loss: 0.08926 \n",
      "[184/500] train_loss: 0.06840 valid_loss: 0.07864 test_loss: 0.08860 \n",
      "验证损失减少 (0.078705 --> 0.078637). 正在保存模型...\n",
      "[185/500] train_loss: 0.06678 valid_loss: 0.08494 test_loss: 0.08881 \n",
      "[186/500] train_loss: 0.06600 valid_loss: 0.08019 test_loss: 0.09125 \n",
      "[187/500] train_loss: 0.06680 valid_loss: 0.08551 test_loss: 0.09073 \n",
      "[188/500] train_loss: 0.06736 valid_loss: 0.08301 test_loss: 0.08885 \n",
      "[189/500] train_loss: 0.06853 valid_loss: 0.09286 test_loss: 0.09044 \n",
      "[190/500] train_loss: 0.06633 valid_loss: 0.08214 test_loss: 0.09077 \n",
      "[191/500] train_loss: 0.06762 valid_loss: 0.09382 test_loss: 0.08890 \n",
      "[192/500] train_loss: 0.07033 valid_loss: 0.08493 test_loss: 0.08952 \n",
      "[193/500] train_loss: 0.06632 valid_loss: 0.08298 test_loss: 0.08942 \n",
      "[194/500] train_loss: 0.06631 valid_loss: 0.08228 test_loss: 0.08967 \n",
      "[195/500] train_loss: 0.06611 valid_loss: 0.09003 test_loss: 0.08849 \n",
      "[196/500] train_loss: 0.06671 valid_loss: 0.09128 test_loss: 0.08817 \n",
      "[197/500] train_loss: 0.06488 valid_loss: 0.08319 test_loss: 0.08904 \n",
      "[198/500] train_loss: 0.06543 valid_loss: 0.08280 test_loss: 0.08943 \n",
      "[199/500] train_loss: 0.06492 valid_loss: 0.08388 test_loss: 0.08924 \n",
      "[200/500] train_loss: 0.06670 valid_loss: 0.08352 test_loss: 0.08904 \n",
      "[201/500] train_loss: 0.06692 valid_loss: 0.08278 test_loss: 0.08921 \n",
      "[202/500] train_loss: 0.06508 valid_loss: 0.08042 test_loss: 0.08761 \n",
      "[203/500] train_loss: 0.06650 valid_loss: 0.08253 test_loss: 0.08951 \n",
      "[204/500] train_loss: 0.06870 valid_loss: 0.08298 test_loss: 0.08768 \n",
      "[205/500] train_loss: 0.06398 valid_loss: 0.07995 test_loss: 0.09010 \n",
      "[206/500] train_loss: 0.06403 valid_loss: 0.07936 test_loss: 0.08869 \n",
      "[207/500] train_loss: 0.06558 valid_loss: 0.08449 test_loss: 0.08952 \n",
      "[208/500] train_loss: 0.06851 valid_loss: 0.08592 test_loss: 0.08829 \n",
      "[209/500] train_loss: 0.06588 valid_loss: 0.07883 test_loss: 0.08844 \n",
      "[210/500] train_loss: 0.06680 valid_loss: 0.09077 test_loss: 0.08874 \n",
      "[211/500] train_loss: 0.06450 valid_loss: 0.08124 test_loss: 0.08931 \n",
      "[212/500] train_loss: 0.06710 valid_loss: 0.08164 test_loss: 0.08835 \n",
      "[213/500] train_loss: 0.06503 valid_loss: 0.08293 test_loss: 0.08758 \n",
      "[214/500] train_loss: 0.06579 valid_loss: 0.08736 test_loss: 0.08909 \n",
      "[215/500] train_loss: 0.06634 valid_loss: 0.08277 test_loss: 0.08819 \n",
      "[216/500] train_loss: 0.06459 valid_loss: 0.08594 test_loss: 0.08802 \n",
      "[217/500] train_loss: 0.06555 valid_loss: 0.07938 test_loss: 0.08843 \n",
      "[218/500] train_loss: 0.06322 valid_loss: 0.08163 test_loss: 0.08823 \n",
      "[219/500] train_loss: 0.06378 valid_loss: 0.08165 test_loss: 0.08755 \n",
      "[220/500] train_loss: 0.06408 valid_loss: 0.08049 test_loss: 0.08778 \n",
      "[221/500] train_loss: 0.06420 valid_loss: 0.08251 test_loss: 0.08874 \n",
      "[222/500] train_loss: 0.06344 valid_loss: 0.08636 test_loss: 0.08763 \n",
      "[223/500] train_loss: 0.06563 valid_loss: 0.07774 test_loss: 0.08811 \n",
      "验证损失减少 (0.078637 --> 0.077740). 正在保存模型...\n",
      "[224/500] train_loss: 0.06466 valid_loss: 0.08416 test_loss: 0.08641 \n",
      "[225/500] train_loss: 0.06276 valid_loss: 0.07772 test_loss: 0.08798 \n",
      "验证损失减少 (0.077740 --> 0.077722). 正在保存模型...\n",
      "[226/500] train_loss: 0.06267 valid_loss: 0.08452 test_loss: 0.08783 \n",
      "[227/500] train_loss: 0.06351 valid_loss: 0.08246 test_loss: 0.08714 \n",
      "[228/500] train_loss: 0.06265 valid_loss: 0.08115 test_loss: 0.08703 \n",
      "[229/500] train_loss: 0.06219 valid_loss: 0.08520 test_loss: 0.08741 \n",
      "[230/500] train_loss: 0.06547 valid_loss: 0.07811 test_loss: 0.08695 \n",
      "[231/500] train_loss: 0.06289 valid_loss: 0.08476 test_loss: 0.08644 \n",
      "[232/500] train_loss: 0.06275 valid_loss: 0.08064 test_loss: 0.08783 \n",
      "[233/500] train_loss: 0.06291 valid_loss: 0.08450 test_loss: 0.08670 \n",
      "[234/500] train_loss: 0.06321 valid_loss: 0.07690 test_loss: 0.08762 \n",
      "验证损失减少 (0.077722 --> 0.076902). 正在保存模型...\n",
      "[235/500] train_loss: 0.06278 valid_loss: 0.08451 test_loss: 0.08791 \n",
      "[236/500] train_loss: 0.06335 valid_loss: 0.07617 test_loss: 0.08843 \n",
      "验证损失减少 (0.076902 --> 0.076174). 正在保存模型...\n",
      "[237/500] train_loss: 0.06401 valid_loss: 0.07900 test_loss: 0.08830 \n",
      "[238/500] train_loss: 0.06257 valid_loss: 0.08177 test_loss: 0.08845 \n",
      "[239/500] train_loss: 0.06389 valid_loss: 0.07926 test_loss: 0.08863 \n",
      "[240/500] train_loss: 0.06284 valid_loss: 0.08189 test_loss: 0.08755 \n",
      "[241/500] train_loss: 0.06369 valid_loss: 0.08132 test_loss: 0.08699 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[242/500] train_loss: 0.06400 valid_loss: 0.08989 test_loss: 0.08722 \n",
      "[243/500] train_loss: 0.06385 valid_loss: 0.08315 test_loss: 0.08620 \n",
      "[244/500] train_loss: 0.06213 valid_loss: 0.08961 test_loss: 0.08688 \n",
      "[245/500] train_loss: 0.06279 valid_loss: 0.07806 test_loss: 0.08919 \n",
      "[246/500] train_loss: 0.06476 valid_loss: 0.08703 test_loss: 0.08821 \n",
      "[247/500] train_loss: 0.06302 valid_loss: 0.07856 test_loss: 0.08743 \n",
      "[248/500] train_loss: 0.06106 valid_loss: 0.09089 test_loss: 0.08817 \n",
      "[249/500] train_loss: 0.06203 valid_loss: 0.08580 test_loss: 0.08843 \n",
      "[250/500] train_loss: 0.06289 valid_loss: 0.08721 test_loss: 0.08716 \n",
      "[251/500] train_loss: 0.06152 valid_loss: 0.08302 test_loss: 0.08704 \n",
      "[252/500] train_loss: 0.06169 valid_loss: 0.08355 test_loss: 0.08712 \n",
      "[253/500] train_loss: 0.06240 valid_loss: 0.08615 test_loss: 0.08507 \n",
      "[254/500] train_loss: 0.06206 valid_loss: 0.07930 test_loss: 0.08618 \n",
      "[255/500] train_loss: 0.06235 valid_loss: 0.08220 test_loss: 0.08670 \n",
      "[256/500] train_loss: 0.06264 valid_loss: 0.09100 test_loss: 0.08769 \n",
      "[257/500] train_loss: 0.06171 valid_loss: 0.08697 test_loss: 0.08643 \n",
      "[258/500] train_loss: 0.06348 valid_loss: 0.09208 test_loss: 0.08714 \n",
      "[259/500] train_loss: 0.06099 valid_loss: 0.08683 test_loss: 0.08642 \n",
      "[260/500] train_loss: 0.06165 valid_loss: 0.08550 test_loss: 0.08655 \n",
      "[261/500] train_loss: 0.06151 valid_loss: 0.07827 test_loss: 0.08632 \n",
      "[262/500] train_loss: 0.06063 valid_loss: 0.10121 test_loss: 0.08727 \n",
      "[263/500] train_loss: 0.06130 valid_loss: 0.09771 test_loss: 0.08710 \n",
      "[264/500] train_loss: 0.06203 valid_loss: 0.10026 test_loss: 0.08735 \n",
      "[265/500] train_loss: 0.06077 valid_loss: 0.09300 test_loss: 0.08832 \n",
      "[266/500] train_loss: 0.06075 valid_loss: 0.07915 test_loss: 0.08702 \n",
      "[267/500] train_loss: 0.06070 valid_loss: 0.08273 test_loss: 0.08677 \n",
      "[268/500] train_loss: 0.06101 valid_loss: 0.09398 test_loss: 0.08746 \n",
      "[269/500] train_loss: 0.06010 valid_loss: 0.08973 test_loss: 0.08639 \n",
      "[270/500] train_loss: 0.06087 valid_loss: 0.08074 test_loss: 0.08659 \n",
      "[271/500] train_loss: 0.06198 valid_loss: 0.10282 test_loss: 0.08534 \n",
      "[272/500] train_loss: 0.06049 valid_loss: 0.09106 test_loss: 0.08685 \n",
      "[273/500] train_loss: 0.06033 valid_loss: 0.09623 test_loss: 0.08620 \n",
      "[274/500] train_loss: 0.06020 valid_loss: 0.09428 test_loss: 0.08717 \n",
      "[275/500] train_loss: 0.06140 valid_loss: 0.08910 test_loss: 0.08656 \n",
      "[276/500] train_loss: 0.06094 valid_loss: 0.10542 test_loss: 0.08686 \n",
      "[277/500] train_loss: 0.06032 valid_loss: 0.07767 test_loss: 0.08831 \n",
      "[278/500] train_loss: 0.06007 valid_loss: 0.08854 test_loss: 0.08767 \n",
      "[279/500] train_loss: 0.06152 valid_loss: 0.08544 test_loss: 0.08713 \n",
      "[280/500] train_loss: 0.06184 valid_loss: 0.07884 test_loss: 0.08589 \n",
      "[281/500] train_loss: 0.05959 valid_loss: 0.07463 test_loss: 0.08652 \n",
      "验证损失减少 (0.076174 --> 0.074625). 正在保存模型...\n",
      "[282/500] train_loss: 0.06054 valid_loss: 0.07622 test_loss: 0.08852 \n",
      "[283/500] train_loss: 0.06068 valid_loss: 0.08475 test_loss: 0.08818 \n",
      "[284/500] train_loss: 0.05987 valid_loss: 0.09328 test_loss: 0.08915 \n",
      "[285/500] train_loss: 0.06110 valid_loss: 0.08886 test_loss: 0.08855 \n",
      "[286/500] train_loss: 0.05974 valid_loss: 0.09263 test_loss: 0.08695 \n",
      "[287/500] train_loss: 0.06056 valid_loss: 0.08740 test_loss: 0.08706 \n",
      "[288/500] train_loss: 0.06178 valid_loss: 0.09843 test_loss: 0.08757 \n",
      "[289/500] train_loss: 0.06041 valid_loss: 0.08634 test_loss: 0.08715 \n",
      "[290/500] train_loss: 0.05989 valid_loss: 0.08294 test_loss: 0.08780 \n",
      "[291/500] train_loss: 0.05990 valid_loss: 0.08144 test_loss: 0.08627 \n",
      "[292/500] train_loss: 0.06046 valid_loss: 0.08630 test_loss: 0.08811 \n",
      "[293/500] train_loss: 0.05823 valid_loss: 0.09831 test_loss: 0.08724 \n",
      "[294/500] train_loss: 0.06007 valid_loss: 0.09226 test_loss: 0.08662 \n",
      "[295/500] train_loss: 0.06133 valid_loss: 0.07546 test_loss: 0.08711 \n",
      "[296/500] train_loss: 0.05858 valid_loss: 0.07962 test_loss: 0.08653 \n",
      "[297/500] train_loss: 0.06082 valid_loss: 0.08097 test_loss: 0.08712 \n",
      "[298/500] train_loss: 0.06060 valid_loss: 0.08204 test_loss: 0.08659 \n",
      "[299/500] train_loss: 0.05845 valid_loss: 0.07793 test_loss: 0.08657 \n",
      "[300/500] train_loss: 0.05992 valid_loss: 0.07707 test_loss: 0.08705 \n",
      "[301/500] train_loss: 0.05991 valid_loss: 0.07499 test_loss: 0.08638 \n",
      "[302/500] train_loss: 0.05823 valid_loss: 0.07798 test_loss: 0.08674 \n",
      "[303/500] train_loss: 0.05924 valid_loss: 0.09521 test_loss: 0.08675 \n",
      "[304/500] train_loss: 0.05872 valid_loss: 0.07506 test_loss: 0.08647 \n",
      "[305/500] train_loss: 0.05837 valid_loss: 0.08670 test_loss: 0.08868 \n",
      "[306/500] train_loss: 0.06026 valid_loss: 0.08569 test_loss: 0.08779 \n",
      "[307/500] train_loss: 0.05919 valid_loss: 0.07811 test_loss: 0.08815 \n",
      "[308/500] train_loss: 0.05698 valid_loss: 0.07654 test_loss: 0.08733 \n",
      "[309/500] train_loss: 0.05662 valid_loss: 0.07799 test_loss: 0.08672 \n",
      "[310/500] train_loss: 0.05796 valid_loss: 0.08108 test_loss: 0.08618 \n",
      "[311/500] train_loss: 0.05933 valid_loss: 0.08155 test_loss: 0.08859 \n",
      "[312/500] train_loss: 0.05796 valid_loss: 0.08457 test_loss: 0.08904 \n",
      "[313/500] train_loss: 0.05745 valid_loss: 0.08025 test_loss: 0.08692 \n",
      "[314/500] train_loss: 0.05742 valid_loss: 0.07803 test_loss: 0.08645 \n",
      "[315/500] train_loss: 0.05771 valid_loss: 0.07891 test_loss: 0.08660 \n",
      "[316/500] train_loss: 0.05903 valid_loss: 0.07996 test_loss: 0.08747 \n",
      "[317/500] train_loss: 0.05744 valid_loss: 0.09481 test_loss: 0.08629 \n",
      "[318/500] train_loss: 0.05793 valid_loss: 0.07579 test_loss: 0.08631 \n",
      "[319/500] train_loss: 0.05826 valid_loss: 0.08202 test_loss: 0.08692 \n",
      "[320/500] train_loss: 0.05966 valid_loss: 0.08409 test_loss: 0.08688 \n",
      "[321/500] train_loss: 0.05736 valid_loss: 0.07972 test_loss: 0.08631 \n",
      "[322/500] train_loss: 0.05782 valid_loss: 0.07484 test_loss: 0.08546 \n",
      "[323/500] train_loss: 0.05662 valid_loss: 0.08586 test_loss: 0.08562 \n",
      "[324/500] train_loss: 0.05668 valid_loss: 0.07544 test_loss: 0.08638 \n",
      "[325/500] train_loss: 0.05611 valid_loss: 0.08387 test_loss: 0.08691 \n",
      "[326/500] train_loss: 0.05735 valid_loss: 0.09657 test_loss: 0.08944 \n",
      "[327/500] train_loss: 0.05632 valid_loss: 0.10908 test_loss: 0.08684 \n",
      "[328/500] train_loss: 0.05757 valid_loss: 0.08884 test_loss: 0.08637 \n",
      "[329/500] train_loss: 0.05816 valid_loss: 0.09053 test_loss: 0.08739 \n",
      "[330/500] train_loss: 0.05913 valid_loss: 0.08050 test_loss: 0.08661 \n",
      "[331/500] train_loss: 0.05763 valid_loss: 0.11788 test_loss: 0.08691 \n",
      "[332/500] train_loss: 0.05600 valid_loss: 0.10035 test_loss: 0.08611 \n",
      "[333/500] train_loss: 0.05799 valid_loss: 0.08681 test_loss: 0.08575 \n",
      "[334/500] train_loss: 0.05755 valid_loss: 0.07956 test_loss: 0.08691 \n",
      "[335/500] train_loss: 0.05490 valid_loss: 0.09809 test_loss: 0.08656 \n",
      "[336/500] train_loss: 0.05817 valid_loss: 0.10524 test_loss: 0.08677 \n",
      "[337/500] train_loss: 0.05725 valid_loss: 0.10696 test_loss: 0.08634 \n",
      "[338/500] train_loss: 0.05635 valid_loss: 0.09094 test_loss: 0.08600 \n",
      "[339/500] train_loss: 0.05794 valid_loss: 0.09657 test_loss: 0.08673 \n",
      "[340/500] train_loss: 0.05644 valid_loss: 0.09243 test_loss: 0.08618 \n",
      "[341/500] train_loss: 0.05711 valid_loss: 0.08410 test_loss: 0.08702 \n",
      "[342/500] train_loss: 0.05801 valid_loss: 0.08694 test_loss: 0.08552 \n",
      "[343/500] train_loss: 0.05806 valid_loss: 0.08613 test_loss: 0.08733 \n",
      "[344/500] train_loss: 0.05662 valid_loss: 0.08970 test_loss: 0.08714 \n",
      "[345/500] train_loss: 0.05682 valid_loss: 0.07895 test_loss: 0.08598 \n",
      "[346/500] train_loss: 0.05681 valid_loss: 0.08371 test_loss: 0.08635 \n",
      "[347/500] train_loss: 0.05693 valid_loss: 0.08221 test_loss: 0.08633 \n",
      "[348/500] train_loss: 0.05716 valid_loss: 0.09956 test_loss: 0.08835 \n",
      "[349/500] train_loss: 0.05757 valid_loss: 0.09201 test_loss: 0.08687 \n",
      "[350/500] train_loss: 0.05678 valid_loss: 0.09198 test_loss: 0.08631 \n",
      "[351/500] train_loss: 0.05838 valid_loss: 0.09046 test_loss: 0.08651 \n",
      "[352/500] train_loss: 0.05748 valid_loss: 0.09936 test_loss: 0.08787 \n",
      "[353/500] train_loss: 0.05691 valid_loss: 0.08912 test_loss: 0.08665 \n",
      "[354/500] train_loss: 0.05549 valid_loss: 0.08962 test_loss: 0.08645 \n",
      "[355/500] train_loss: 0.05690 valid_loss: 0.10096 test_loss: 0.08605 \n",
      "[356/500] train_loss: 0.05628 valid_loss: 0.07985 test_loss: 0.08650 \n",
      "[357/500] train_loss: 0.05798 valid_loss: 0.09192 test_loss: 0.08678 \n",
      "[358/500] train_loss: 0.05824 valid_loss: 0.08864 test_loss: 0.08743 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[359/500] train_loss: 0.05736 valid_loss: 0.08277 test_loss: 0.08791 \n",
      "[360/500] train_loss: 0.05563 valid_loss: 0.08032 test_loss: 0.08700 \n",
      "[361/500] train_loss: 0.05594 valid_loss: 0.09186 test_loss: 0.08814 \n",
      "[362/500] train_loss: 0.05606 valid_loss: 0.08558 test_loss: 0.08680 \n",
      "[363/500] train_loss: 0.05599 valid_loss: 0.08358 test_loss: 0.08749 \n",
      "[364/500] train_loss: 0.05482 valid_loss: 0.10450 test_loss: 0.08785 \n",
      "[365/500] train_loss: 0.05642 valid_loss: 0.09192 test_loss: 0.08772 \n",
      "[366/500] train_loss: 0.05486 valid_loss: 0.10713 test_loss: 0.08670 \n",
      "[367/500] train_loss: 0.05456 valid_loss: 0.08085 test_loss: 0.08656 \n",
      "[368/500] train_loss: 0.05723 valid_loss: 0.08808 test_loss: 0.08608 \n",
      "[369/500] train_loss: 0.05393 valid_loss: 0.09189 test_loss: 0.08685 \n",
      "[370/500] train_loss: 0.05409 valid_loss: 0.07968 test_loss: 0.08523 \n",
      "[371/500] train_loss: 0.05599 valid_loss: 0.08703 test_loss: 0.08669 \n",
      "[372/500] train_loss: 0.05606 valid_loss: 0.09427 test_loss: 0.08620 \n",
      "[373/500] train_loss: 0.05533 valid_loss: 0.08189 test_loss: 0.08616 \n",
      "[374/500] train_loss: 0.05644 valid_loss: 0.08647 test_loss: 0.08533 \n",
      "[375/500] train_loss: 0.05424 valid_loss: 0.07750 test_loss: 0.08786 \n",
      "[376/500] train_loss: 0.05482 valid_loss: 0.08626 test_loss: 0.08477 \n",
      "[377/500] train_loss: 0.05630 valid_loss: 0.09287 test_loss: 0.08502 \n",
      "[378/500] train_loss: 0.05720 valid_loss: 0.09071 test_loss: 0.08781 \n",
      "[379/500] train_loss: 0.05554 valid_loss: 0.09169 test_loss: 0.08637 \n",
      "[380/500] train_loss: 0.05490 valid_loss: 0.09047 test_loss: 0.08852 \n",
      "[381/500] train_loss: 0.05559 valid_loss: 0.10567 test_loss: 0.08655 \n",
      "[382/500] train_loss: 0.05452 valid_loss: 0.10318 test_loss: 0.08673 \n",
      "[383/500] train_loss: 0.05651 valid_loss: 0.08121 test_loss: 0.08684 \n",
      "[384/500] train_loss: 0.05588 valid_loss: 0.08331 test_loss: 0.08820 \n",
      "[385/500] train_loss: 0.05494 valid_loss: 0.08826 test_loss: 0.08787 \n",
      "[386/500] train_loss: 0.05441 valid_loss: 0.07633 test_loss: 0.08659 \n",
      "[387/500] train_loss: 0.05430 valid_loss: 0.09006 test_loss: 0.08691 \n",
      "[388/500] train_loss: 0.05397 valid_loss: 0.08499 test_loss: 0.08693 \n",
      "[389/500] train_loss: 0.05514 valid_loss: 0.08965 test_loss: 0.08644 \n",
      "[390/500] train_loss: 0.05465 valid_loss: 0.07935 test_loss: 0.08633 \n",
      "[391/500] train_loss: 0.05510 valid_loss: 0.10913 test_loss: 0.08600 \n",
      "[392/500] train_loss: 0.05476 valid_loss: 0.09003 test_loss: 0.08570 \n",
      "[393/500] train_loss: 0.05405 valid_loss: 0.08157 test_loss: 0.08678 \n",
      "[394/500] train_loss: 0.05444 valid_loss: 0.07668 test_loss: 0.08563 \n",
      "[395/500] train_loss: 0.05428 valid_loss: 0.08721 test_loss: 0.08608 \n",
      "[396/500] train_loss: 0.05403 valid_loss: 0.07817 test_loss: 0.08650 \n",
      "[397/500] train_loss: 0.05393 valid_loss: 0.08716 test_loss: 0.08645 \n",
      "[398/500] train_loss: 0.05483 valid_loss: 0.08938 test_loss: 0.08544 \n",
      "[399/500] train_loss: 0.05339 valid_loss: 0.08143 test_loss: 0.08581 \n",
      "[400/500] train_loss: 0.05437 valid_loss: 0.08692 test_loss: 0.08516 \n",
      "[401/500] train_loss: 0.05495 valid_loss: 0.09100 test_loss: 0.08533 \n",
      "[402/500] train_loss: 0.05410 valid_loss: 0.08585 test_loss: 0.08688 \n",
      "[403/500] train_loss: 0.05565 valid_loss: 0.08103 test_loss: 0.08692 \n",
      "[404/500] train_loss: 0.05633 valid_loss: 0.07504 test_loss: 0.08720 \n",
      "[405/500] train_loss: 0.05220 valid_loss: 0.09565 test_loss: 0.08753 \n",
      "[406/500] train_loss: 0.05491 valid_loss: 0.10203 test_loss: 0.08673 \n",
      "[407/500] train_loss: 0.05362 valid_loss: 0.10816 test_loss: 0.08592 \n",
      "[408/500] train_loss: 0.05429 valid_loss: 0.09265 test_loss: 0.08758 \n",
      "[409/500] train_loss: 0.05434 valid_loss: 0.08799 test_loss: 0.08897 \n",
      "[410/500] train_loss: 0.05628 valid_loss: 0.08892 test_loss: 0.08687 \n",
      "[411/500] train_loss: 0.05446 valid_loss: 0.11200 test_loss: 0.08630 \n",
      "[412/500] train_loss: 0.05470 valid_loss: 0.09285 test_loss: 0.08640 \n",
      "[413/500] train_loss: 0.05414 valid_loss: 0.09739 test_loss: 0.08751 \n",
      "[414/500] train_loss: 0.05424 valid_loss: 0.08023 test_loss: 0.08599 \n",
      "[415/500] train_loss: 0.05284 valid_loss: 0.10905 test_loss: 0.08600 \n",
      "[416/500] train_loss: 0.05440 valid_loss: 0.10947 test_loss: 0.08550 \n",
      "[417/500] train_loss: 0.05370 valid_loss: 0.09123 test_loss: 0.08579 \n",
      "[418/500] train_loss: 0.05342 valid_loss: 0.12407 test_loss: 0.08587 \n",
      "[419/500] train_loss: 0.05362 valid_loss: 0.08837 test_loss: 0.08727 \n",
      "[420/500] train_loss: 0.05318 valid_loss: 0.11506 test_loss: 0.08728 \n",
      "[421/500] train_loss: 0.05216 valid_loss: 0.09786 test_loss: 0.08484 \n",
      "[422/500] train_loss: 0.05409 valid_loss: 0.09021 test_loss: 0.08458 \n",
      "[423/500] train_loss: 0.05319 valid_loss: 0.10357 test_loss: 0.08547 \n",
      "[424/500] train_loss: 0.05369 valid_loss: 0.10372 test_loss: 0.08579 \n",
      "[425/500] train_loss: 0.05451 valid_loss: 0.08642 test_loss: 0.08581 \n",
      "[426/500] train_loss: 0.05263 valid_loss: 0.09046 test_loss: 0.08717 \n",
      "[427/500] train_loss: 0.05368 valid_loss: 0.09270 test_loss: 0.08534 \n",
      "[428/500] train_loss: 0.05262 valid_loss: 0.08537 test_loss: 0.08738 \n",
      "[429/500] train_loss: 0.05384 valid_loss: 0.09239 test_loss: 0.08592 \n",
      "[430/500] train_loss: 0.05281 valid_loss: 0.09647 test_loss: 0.08660 \n",
      "[431/500] train_loss: 0.05315 valid_loss: 0.09159 test_loss: 0.08746 \n",
      "[432/500] train_loss: 0.05435 valid_loss: 0.11194 test_loss: 0.08765 \n",
      "[433/500] train_loss: 0.05154 valid_loss: 0.08185 test_loss: 0.08773 \n",
      "[434/500] train_loss: 0.05330 valid_loss: 0.08168 test_loss: 0.08743 \n",
      "[435/500] train_loss: 0.05337 valid_loss: 0.07747 test_loss: 0.08714 \n",
      "[436/500] train_loss: 0.05421 valid_loss: 0.09152 test_loss: 0.08675 \n",
      "[437/500] train_loss: 0.05192 valid_loss: 0.09024 test_loss: 0.08803 \n",
      "[438/500] train_loss: 0.05329 valid_loss: 0.10181 test_loss: 0.08680 \n",
      "[439/500] train_loss: 0.05261 valid_loss: 0.08352 test_loss: 0.08649 \n",
      "[440/500] train_loss: 0.05166 valid_loss: 0.10838 test_loss: 0.08699 \n",
      "[441/500] train_loss: 0.05248 valid_loss: 0.10440 test_loss: 0.08749 \n",
      "[442/500] train_loss: 0.05436 valid_loss: 0.08274 test_loss: 0.08697 \n",
      "[443/500] train_loss: 0.05127 valid_loss: 0.08546 test_loss: 0.08783 \n",
      "[444/500] train_loss: 0.05272 valid_loss: 0.09115 test_loss: 0.08752 \n",
      "[445/500] train_loss: 0.05308 valid_loss: 0.08884 test_loss: 0.08712 \n",
      "[446/500] train_loss: 0.05243 valid_loss: 0.10840 test_loss: 0.08692 \n",
      "[447/500] train_loss: 0.05283 valid_loss: 0.08413 test_loss: 0.08703 \n",
      "[448/500] train_loss: 0.05147 valid_loss: 0.08655 test_loss: 0.08642 \n",
      "[449/500] train_loss: 0.05148 valid_loss: 0.10574 test_loss: 0.08681 \n",
      "[450/500] train_loss: 0.05394 valid_loss: 0.10204 test_loss: 0.08639 \n",
      "[451/500] train_loss: 0.05325 valid_loss: 0.09052 test_loss: 0.08656 \n",
      "[452/500] train_loss: 0.05191 valid_loss: 0.08730 test_loss: 0.08541 \n",
      "[453/500] train_loss: 0.05167 valid_loss: 0.09048 test_loss: 0.08734 \n",
      "[454/500] train_loss: 0.05218 valid_loss: 0.10067 test_loss: 0.08705 \n",
      "[455/500] train_loss: 0.05154 valid_loss: 0.08471 test_loss: 0.08735 \n",
      "[456/500] train_loss: 0.05381 valid_loss: 0.09287 test_loss: 0.08793 \n",
      "[457/500] train_loss: 0.05171 valid_loss: 0.08188 test_loss: 0.08659 \n",
      "[458/500] train_loss: 0.05374 valid_loss: 0.08346 test_loss: 0.08617 \n",
      "[459/500] train_loss: 0.05133 valid_loss: 0.08515 test_loss: 0.08651 \n",
      "[460/500] train_loss: 0.05292 valid_loss: 0.09736 test_loss: 0.08687 \n",
      "[461/500] train_loss: 0.05134 valid_loss: 0.08895 test_loss: 0.08761 \n",
      "[462/500] train_loss: 0.05124 valid_loss: 0.08567 test_loss: 0.08673 \n",
      "[463/500] train_loss: 0.05165 valid_loss: 0.09013 test_loss: 0.08720 \n",
      "[464/500] train_loss: 0.05235 valid_loss: 0.08443 test_loss: 0.08731 \n",
      "[465/500] train_loss: 0.05049 valid_loss: 0.08252 test_loss: 0.08597 \n",
      "[466/500] train_loss: 0.05289 valid_loss: 0.07789 test_loss: 0.08738 \n",
      "[467/500] train_loss: 0.05037 valid_loss: 0.07800 test_loss: 0.08676 \n",
      "[468/500] train_loss: 0.05036 valid_loss: 0.10463 test_loss: 0.08735 \n",
      "[469/500] train_loss: 0.05168 valid_loss: 0.08243 test_loss: 0.08803 \n",
      "[470/500] train_loss: 0.05080 valid_loss: 0.08017 test_loss: 0.08670 \n",
      "[471/500] train_loss: 0.05090 valid_loss: 0.08862 test_loss: 0.08872 \n",
      "[472/500] train_loss: 0.05090 valid_loss: 0.07465 test_loss: 0.08737 \n",
      "[473/500] train_loss: 0.05084 valid_loss: 0.08539 test_loss: 0.08801 \n",
      "[474/500] train_loss: 0.05173 valid_loss: 0.08510 test_loss: 0.08646 \n",
      "[475/500] train_loss: 0.04969 valid_loss: 0.07873 test_loss: 0.08621 \n",
      "[476/500] train_loss: 0.05177 valid_loss: 0.08464 test_loss: 0.08715 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[477/500] train_loss: 0.05309 valid_loss: 0.09191 test_loss: 0.08706 \n",
      "[478/500] train_loss: 0.05350 valid_loss: 0.10823 test_loss: 0.08679 \n",
      "[479/500] train_loss: 0.05156 valid_loss: 0.08830 test_loss: 0.08650 \n",
      "[480/500] train_loss: 0.05157 valid_loss: 0.09902 test_loss: 0.08555 \n",
      "[481/500] train_loss: 0.05171 valid_loss: 0.09049 test_loss: 0.08678 \n",
      "[482/500] train_loss: 0.04978 valid_loss: 0.11481 test_loss: 0.08695 \n",
      "[483/500] train_loss: 0.05153 valid_loss: 0.09528 test_loss: 0.08650 \n",
      "[484/500] train_loss: 0.05212 valid_loss: 0.12229 test_loss: 0.08732 \n",
      "[485/500] train_loss: 0.05003 valid_loss: 0.09673 test_loss: 0.08611 \n",
      "[486/500] train_loss: 0.05150 valid_loss: 0.10911 test_loss: 0.08762 \n",
      "[487/500] train_loss: 0.05136 valid_loss: 0.11705 test_loss: 0.08695 \n",
      "[488/500] train_loss: 0.05263 valid_loss: 0.10295 test_loss: 0.08801 \n",
      "[489/500] train_loss: 0.05213 valid_loss: 0.10946 test_loss: 0.08730 \n",
      "[490/500] train_loss: 0.04985 valid_loss: 0.09113 test_loss: 0.08752 \n",
      "[491/500] train_loss: 0.05216 valid_loss: 0.11023 test_loss: 0.08729 \n",
      "[492/500] train_loss: 0.05115 valid_loss: 0.10130 test_loss: 0.08615 \n",
      "[493/500] train_loss: 0.05266 valid_loss: 0.07646 test_loss: 0.08830 \n",
      "[494/500] train_loss: 0.05182 valid_loss: 0.09518 test_loss: 0.08848 \n",
      "[495/500] train_loss: 0.05086 valid_loss: 0.07966 test_loss: 0.08651 \n",
      "[496/500] train_loss: 0.05067 valid_loss: 0.08653 test_loss: 0.08738 \n",
      "[497/500] train_loss: 0.05132 valid_loss: 0.09002 test_loss: 0.08701 \n",
      "[498/500] train_loss: 0.05044 valid_loss: 0.09170 test_loss: 0.08705 \n",
      "[499/500] train_loss: 0.05067 valid_loss: 0.09232 test_loss: 0.08676 \n",
      "[500/500] train_loss: 0.04970 valid_loss: 0.10207 test_loss: 0.08737 \n",
      "TRAINING MODEL 2\n",
      "[  1/500] train_loss: 0.39198 valid_loss: 0.27813 test_loss: 0.28123 \n",
      "验证损失减少 (inf --> 0.278128). 正在保存模型...\n",
      "[  2/500] train_loss: 0.22014 valid_loss: 0.20688 test_loss: 0.21259 \n",
      "验证损失减少 (0.278128 --> 0.206878). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17863 valid_loss: 0.17743 test_loss: 0.18556 \n",
      "验证损失减少 (0.206878 --> 0.177430). 正在保存模型...\n",
      "[  4/500] train_loss: 0.16056 valid_loss: 0.16294 test_loss: 0.17325 \n",
      "验证损失减少 (0.177430 --> 0.162944). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15064 valid_loss: 0.15466 test_loss: 0.16688 \n",
      "验证损失减少 (0.162944 --> 0.154657). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14336 valid_loss: 0.14633 test_loss: 0.15965 \n",
      "验证损失减少 (0.154657 --> 0.146328). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13766 valid_loss: 0.14156 test_loss: 0.15598 \n",
      "验证损失减少 (0.146328 --> 0.141563). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13421 valid_loss: 0.13650 test_loss: 0.15204 \n",
      "验证损失减少 (0.141563 --> 0.136504). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13265 valid_loss: 0.13270 test_loss: 0.14792 \n",
      "验证损失减少 (0.136504 --> 0.132701). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12824 valid_loss: 0.12931 test_loss: 0.14466 \n",
      "验证损失减少 (0.132701 --> 0.129305). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12552 valid_loss: 0.12705 test_loss: 0.14432 \n",
      "验证损失减少 (0.129305 --> 0.127045). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12315 valid_loss: 0.12681 test_loss: 0.14264 \n",
      "验证损失减少 (0.127045 --> 0.126811). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12328 valid_loss: 0.12616 test_loss: 0.14347 \n",
      "验证损失减少 (0.126811 --> 0.126158). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.12235 valid_loss: 0.12422 test_loss: 0.14083 \n",
      "验证损失减少 (0.126158 --> 0.124221). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11938 valid_loss: 0.12358 test_loss: 0.14151 \n",
      "验证损失减少 (0.124221 --> 0.123581). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11433 valid_loss: 0.11925 test_loss: 0.13683 \n",
      "验证损失减少 (0.123581 --> 0.119250). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11547 valid_loss: 0.11854 test_loss: 0.13641 \n",
      "验证损失减少 (0.119250 --> 0.118539). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11329 valid_loss: 0.11443 test_loss: 0.13150 \n",
      "验证损失减少 (0.118539 --> 0.114435). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11323 valid_loss: 0.11851 test_loss: 0.13392 \n",
      "[ 20/500] train_loss: 0.10957 valid_loss: 0.11388 test_loss: 0.13099 \n",
      "验证损失减少 (0.114435 --> 0.113875). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.11047 valid_loss: 0.11412 test_loss: 0.12954 \n",
      "[ 22/500] train_loss: 0.10846 valid_loss: 0.11333 test_loss: 0.13067 \n",
      "验证损失减少 (0.113875 --> 0.113328). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10844 valid_loss: 0.11378 test_loss: 0.12853 \n",
      "[ 24/500] train_loss: 0.10993 valid_loss: 0.11158 test_loss: 0.12636 \n",
      "验证损失减少 (0.113328 --> 0.111577). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10891 valid_loss: 0.11437 test_loss: 0.13027 \n",
      "[ 26/500] train_loss: 0.10474 valid_loss: 0.11028 test_loss: 0.12495 \n",
      "验证损失减少 (0.111577 --> 0.110284). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.10487 valid_loss: 0.10889 test_loss: 0.12511 \n",
      "验证损失减少 (0.110284 --> 0.108895). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10436 valid_loss: 0.10902 test_loss: 0.12456 \n",
      "[ 29/500] train_loss: 0.10232 valid_loss: 0.10734 test_loss: 0.12279 \n",
      "验证损失减少 (0.108895 --> 0.107336). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10334 valid_loss: 0.10885 test_loss: 0.12200 \n",
      "[ 31/500] train_loss: 0.10130 valid_loss: 0.10622 test_loss: 0.12102 \n",
      "验证损失减少 (0.107336 --> 0.106223). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.10098 valid_loss: 0.10710 test_loss: 0.12160 \n",
      "[ 33/500] train_loss: 0.09970 valid_loss: 0.10342 test_loss: 0.11912 \n",
      "验证损失减少 (0.106223 --> 0.103422). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.09763 valid_loss: 0.10731 test_loss: 0.12276 \n",
      "[ 35/500] train_loss: 0.10015 valid_loss: 0.10233 test_loss: 0.11629 \n",
      "验证损失减少 (0.103422 --> 0.102328). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09936 valid_loss: 0.10496 test_loss: 0.11784 \n",
      "[ 37/500] train_loss: 0.09772 valid_loss: 0.10248 test_loss: 0.11675 \n",
      "[ 38/500] train_loss: 0.10091 valid_loss: 0.10448 test_loss: 0.11945 \n",
      "[ 39/500] train_loss: 0.09573 valid_loss: 0.10135 test_loss: 0.11586 \n",
      "验证损失减少 (0.102328 --> 0.101346). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09890 valid_loss: 0.10355 test_loss: 0.11741 \n",
      "[ 41/500] train_loss: 0.09640 valid_loss: 0.10416 test_loss: 0.11912 \n",
      "[ 42/500] train_loss: 0.09376 valid_loss: 0.10204 test_loss: 0.11494 \n",
      "[ 43/500] train_loss: 0.09593 valid_loss: 0.10549 test_loss: 0.11591 \n",
      "[ 44/500] train_loss: 0.09476 valid_loss: 0.09862 test_loss: 0.11313 \n",
      "验证损失减少 (0.101346 --> 0.098617). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.09265 valid_loss: 0.10159 test_loss: 0.11359 \n",
      "[ 46/500] train_loss: 0.09373 valid_loss: 0.09903 test_loss: 0.11305 \n",
      "[ 47/500] train_loss: 0.09188 valid_loss: 0.09783 test_loss: 0.11321 \n",
      "验证损失减少 (0.098617 --> 0.097834). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.09286 valid_loss: 0.10194 test_loss: 0.11506 \n",
      "[ 49/500] train_loss: 0.09414 valid_loss: 0.10051 test_loss: 0.11294 \n",
      "[ 50/500] train_loss: 0.09288 valid_loss: 0.09932 test_loss: 0.11226 \n",
      "[ 51/500] train_loss: 0.09085 valid_loss: 0.09980 test_loss: 0.11321 \n",
      "[ 52/500] train_loss: 0.08852 valid_loss: 0.09737 test_loss: 0.11096 \n",
      "验证损失减少 (0.097834 --> 0.097369). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.09327 valid_loss: 0.09586 test_loss: 0.10925 \n",
      "验证损失减少 (0.097369 --> 0.095864). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.09208 valid_loss: 0.09575 test_loss: 0.10962 \n",
      "验证损失减少 (0.095864 --> 0.095750). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.08996 valid_loss: 0.09661 test_loss: 0.10979 \n",
      "[ 56/500] train_loss: 0.08947 valid_loss: 0.09455 test_loss: 0.10769 \n",
      "验证损失减少 (0.095750 --> 0.094549). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.09018 valid_loss: 0.09544 test_loss: 0.10830 \n",
      "[ 58/500] train_loss: 0.08879 valid_loss: 0.09637 test_loss: 0.10783 \n",
      "[ 59/500] train_loss: 0.09099 valid_loss: 0.09375 test_loss: 0.10676 \n",
      "验证损失减少 (0.094549 --> 0.093754). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.08653 valid_loss: 0.09585 test_loss: 0.10775 \n",
      "[ 61/500] train_loss: 0.08920 valid_loss: 0.09512 test_loss: 0.10620 \n",
      "[ 62/500] train_loss: 0.08804 valid_loss: 0.09506 test_loss: 0.10690 \n",
      "[ 63/500] train_loss: 0.08831 valid_loss: 0.09420 test_loss: 0.10693 \n",
      "[ 64/500] train_loss: 0.09015 valid_loss: 0.09399 test_loss: 0.10528 \n",
      "[ 65/500] train_loss: 0.08763 valid_loss: 0.09360 test_loss: 0.10665 \n",
      "验证损失减少 (0.093754 --> 0.093596). 正在保存模型...\n",
      "[ 66/500] train_loss: 0.08483 valid_loss: 0.09493 test_loss: 0.10538 \n",
      "[ 67/500] train_loss: 0.08750 valid_loss: 0.09264 test_loss: 0.10549 \n",
      "验证损失减少 (0.093596 --> 0.092636). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.08566 valid_loss: 0.09276 test_loss: 0.10571 \n",
      "[ 69/500] train_loss: 0.08709 valid_loss: 0.09404 test_loss: 0.10431 \n",
      "[ 70/500] train_loss: 0.08250 valid_loss: 0.09259 test_loss: 0.10360 \n",
      "验证损失减少 (0.092636 --> 0.092593). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.08372 valid_loss: 0.09368 test_loss: 0.10479 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 72/500] train_loss: 0.08377 valid_loss: 0.09151 test_loss: 0.10360 \n",
      "验证损失减少 (0.092593 --> 0.091507). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.08326 valid_loss: 0.09145 test_loss: 0.10251 \n",
      "验证损失减少 (0.091507 --> 0.091454). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.08300 valid_loss: 0.09218 test_loss: 0.10294 \n",
      "[ 75/500] train_loss: 0.08309 valid_loss: 0.09303 test_loss: 0.10405 \n",
      "[ 76/500] train_loss: 0.08452 valid_loss: 0.09280 test_loss: 0.10366 \n",
      "[ 77/500] train_loss: 0.08441 valid_loss: 0.09307 test_loss: 0.10230 \n",
      "[ 78/500] train_loss: 0.08309 valid_loss: 0.09089 test_loss: 0.10141 \n",
      "验证损失减少 (0.091454 --> 0.090886). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08062 valid_loss: 0.09216 test_loss: 0.10312 \n",
      "[ 80/500] train_loss: 0.08228 valid_loss: 0.09181 test_loss: 0.10192 \n",
      "[ 81/500] train_loss: 0.08177 valid_loss: 0.09106 test_loss: 0.10024 \n",
      "[ 82/500] train_loss: 0.08094 valid_loss: 0.09034 test_loss: 0.10148 \n",
      "验证损失减少 (0.090886 --> 0.090340). 正在保存模型...\n",
      "[ 83/500] train_loss: 0.08315 valid_loss: 0.08935 test_loss: 0.10012 \n",
      "验证损失减少 (0.090340 --> 0.089353). 正在保存模型...\n",
      "[ 84/500] train_loss: 0.08250 valid_loss: 0.09272 test_loss: 0.10192 \n",
      "[ 85/500] train_loss: 0.08219 valid_loss: 0.09305 test_loss: 0.10098 \n",
      "[ 86/500] train_loss: 0.08235 valid_loss: 0.09222 test_loss: 0.10237 \n",
      "[ 87/500] train_loss: 0.08318 valid_loss: 0.08879 test_loss: 0.10003 \n",
      "验证损失减少 (0.089353 --> 0.088787). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.08188 valid_loss: 0.09018 test_loss: 0.10188 \n",
      "[ 89/500] train_loss: 0.08181 valid_loss: 0.09116 test_loss: 0.10185 \n",
      "[ 90/500] train_loss: 0.08386 valid_loss: 0.09123 test_loss: 0.10038 \n",
      "[ 91/500] train_loss: 0.08081 valid_loss: 0.09067 test_loss: 0.10256 \n",
      "[ 92/500] train_loss: 0.07950 valid_loss: 0.08999 test_loss: 0.09943 \n",
      "[ 93/500] train_loss: 0.08163 valid_loss: 0.08805 test_loss: 0.09964 \n",
      "验证损失减少 (0.088787 --> 0.088045). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.08051 valid_loss: 0.08842 test_loss: 0.09837 \n",
      "[ 95/500] train_loss: 0.08047 valid_loss: 0.08856 test_loss: 0.09787 \n",
      "[ 96/500] train_loss: 0.07909 valid_loss: 0.08680 test_loss: 0.09912 \n",
      "验证损失减少 (0.088045 --> 0.086802). 正在保存模型...\n",
      "[ 97/500] train_loss: 0.08053 valid_loss: 0.08936 test_loss: 0.09927 \n",
      "[ 98/500] train_loss: 0.07743 valid_loss: 0.08701 test_loss: 0.09892 \n",
      "[ 99/500] train_loss: 0.07880 valid_loss: 0.09013 test_loss: 0.09943 \n",
      "[100/500] train_loss: 0.07917 valid_loss: 0.08530 test_loss: 0.09644 \n",
      "验证损失减少 (0.086802 --> 0.085296). 正在保存模型...\n",
      "[101/500] train_loss: 0.07926 valid_loss: 0.08751 test_loss: 0.09929 \n",
      "[102/500] train_loss: 0.07762 valid_loss: 0.08740 test_loss: 0.09677 \n",
      "[103/500] train_loss: 0.07912 valid_loss: 0.08982 test_loss: 0.10150 \n",
      "[104/500] train_loss: 0.07948 valid_loss: 0.09298 test_loss: 0.09731 \n",
      "[105/500] train_loss: 0.07891 valid_loss: 0.08689 test_loss: 0.09766 \n",
      "[106/500] train_loss: 0.07787 valid_loss: 0.08942 test_loss: 0.09865 \n",
      "[107/500] train_loss: 0.07771 valid_loss: 0.08889 test_loss: 0.09745 \n",
      "[108/500] train_loss: 0.07749 valid_loss: 0.08730 test_loss: 0.09709 \n",
      "[109/500] train_loss: 0.07956 valid_loss: 0.08770 test_loss: 0.09758 \n",
      "[110/500] train_loss: 0.07732 valid_loss: 0.08655 test_loss: 0.09666 \n",
      "[111/500] train_loss: 0.07920 valid_loss: 0.08694 test_loss: 0.09690 \n",
      "[112/500] train_loss: 0.07613 valid_loss: 0.08659 test_loss: 0.09738 \n",
      "[113/500] train_loss: 0.07631 valid_loss: 0.08914 test_loss: 0.09948 \n",
      "[114/500] train_loss: 0.07623 valid_loss: 0.08757 test_loss: 0.09717 \n",
      "[115/500] train_loss: 0.07587 valid_loss: 0.08675 test_loss: 0.09694 \n",
      "[116/500] train_loss: 0.07905 valid_loss: 0.08365 test_loss: 0.09634 \n",
      "验证损失减少 (0.085296 --> 0.083648). 正在保存模型...\n",
      "[117/500] train_loss: 0.07481 valid_loss: 0.08734 test_loss: 0.09745 \n",
      "[118/500] train_loss: 0.07399 valid_loss: 0.08399 test_loss: 0.09490 \n",
      "[119/500] train_loss: 0.07442 valid_loss: 0.08556 test_loss: 0.09668 \n",
      "[120/500] train_loss: 0.07665 valid_loss: 0.08430 test_loss: 0.09556 \n",
      "[121/500] train_loss: 0.07597 valid_loss: 0.08528 test_loss: 0.09548 \n",
      "[122/500] train_loss: 0.07588 valid_loss: 0.08406 test_loss: 0.09507 \n",
      "[123/500] train_loss: 0.07502 valid_loss: 0.08712 test_loss: 0.09744 \n",
      "[124/500] train_loss: 0.07585 valid_loss: 0.08447 test_loss: 0.09619 \n",
      "[125/500] train_loss: 0.07376 valid_loss: 0.08680 test_loss: 0.09638 \n",
      "[126/500] train_loss: 0.07633 valid_loss: 0.08405 test_loss: 0.09429 \n",
      "[127/500] train_loss: 0.07595 valid_loss: 0.08526 test_loss: 0.09502 \n",
      "[128/500] train_loss: 0.07507 valid_loss: 0.08861 test_loss: 0.09583 \n",
      "[129/500] train_loss: 0.07459 valid_loss: 0.08424 test_loss: 0.09529 \n",
      "[130/500] train_loss: 0.07397 valid_loss: 0.08610 test_loss: 0.09553 \n",
      "[131/500] train_loss: 0.07378 valid_loss: 0.08590 test_loss: 0.09437 \n",
      "[132/500] train_loss: 0.07289 valid_loss: 0.08560 test_loss: 0.09545 \n",
      "[133/500] train_loss: 0.07243 valid_loss: 0.08585 test_loss: 0.09344 \n",
      "[134/500] train_loss: 0.07392 valid_loss: 0.08530 test_loss: 0.09435 \n",
      "[135/500] train_loss: 0.07131 valid_loss: 0.09596 test_loss: 0.09451 \n",
      "[136/500] train_loss: 0.07093 valid_loss: 0.08370 test_loss: 0.09395 \n",
      "[137/500] train_loss: 0.07270 valid_loss: 0.08840 test_loss: 0.09614 \n",
      "[138/500] train_loss: 0.07164 valid_loss: 0.08915 test_loss: 0.09284 \n",
      "[139/500] train_loss: 0.07231 valid_loss: 0.09126 test_loss: 0.09401 \n",
      "[140/500] train_loss: 0.07151 valid_loss: 0.09007 test_loss: 0.09389 \n",
      "[141/500] train_loss: 0.07203 valid_loss: 0.08742 test_loss: 0.09416 \n",
      "[142/500] train_loss: 0.07552 valid_loss: 0.08994 test_loss: 0.09367 \n",
      "[143/500] train_loss: 0.07184 valid_loss: 0.08921 test_loss: 0.09444 \n",
      "[144/500] train_loss: 0.07170 valid_loss: 0.08629 test_loss: 0.09491 \n",
      "[145/500] train_loss: 0.07280 valid_loss: 0.08350 test_loss: 0.09362 \n",
      "验证损失减少 (0.083648 --> 0.083499). 正在保存模型...\n",
      "[146/500] train_loss: 0.07060 valid_loss: 0.08757 test_loss: 0.09534 \n",
      "[147/500] train_loss: 0.07051 valid_loss: 0.08607 test_loss: 0.09376 \n",
      "[148/500] train_loss: 0.07099 valid_loss: 0.08950 test_loss: 0.09442 \n",
      "[149/500] train_loss: 0.07225 valid_loss: 0.08489 test_loss: 0.09407 \n",
      "[150/500] train_loss: 0.07189 valid_loss: 0.08889 test_loss: 0.09346 \n",
      "[151/500] train_loss: 0.07217 valid_loss: 0.08361 test_loss: 0.09355 \n",
      "[152/500] train_loss: 0.07035 valid_loss: 0.09094 test_loss: 0.09391 \n",
      "[153/500] train_loss: 0.07146 valid_loss: 0.09466 test_loss: 0.09199 \n",
      "[154/500] train_loss: 0.07043 valid_loss: 0.08522 test_loss: 0.09421 \n",
      "[155/500] train_loss: 0.07140 valid_loss: 0.08125 test_loss: 0.09293 \n",
      "验证损失减少 (0.083499 --> 0.081254). 正在保存模型...\n",
      "[156/500] train_loss: 0.07235 valid_loss: 0.08160 test_loss: 0.09232 \n",
      "[157/500] train_loss: 0.07223 valid_loss: 0.08131 test_loss: 0.09248 \n",
      "[158/500] train_loss: 0.07228 valid_loss: 0.08185 test_loss: 0.09343 \n",
      "[159/500] train_loss: 0.07090 valid_loss: 0.08161 test_loss: 0.09377 \n",
      "[160/500] train_loss: 0.06985 valid_loss: 0.08233 test_loss: 0.09303 \n",
      "[161/500] train_loss: 0.07273 valid_loss: 0.08310 test_loss: 0.09660 \n",
      "[162/500] train_loss: 0.06845 valid_loss: 0.08239 test_loss: 0.09338 \n",
      "[163/500] train_loss: 0.07145 valid_loss: 0.08172 test_loss: 0.09218 \n",
      "[164/500] train_loss: 0.07140 valid_loss: 0.08093 test_loss: 0.09141 \n",
      "验证损失减少 (0.081254 --> 0.080935). 正在保存模型...\n",
      "[165/500] train_loss: 0.06926 valid_loss: 0.08005 test_loss: 0.09184 \n",
      "验证损失减少 (0.080935 --> 0.080047). 正在保存模型...\n",
      "[166/500] train_loss: 0.07173 valid_loss: 0.08254 test_loss: 0.09336 \n",
      "[167/500] train_loss: 0.06995 valid_loss: 0.08460 test_loss: 0.09183 \n",
      "[168/500] train_loss: 0.07121 valid_loss: 0.08680 test_loss: 0.09203 \n",
      "[169/500] train_loss: 0.07113 valid_loss: 0.08389 test_loss: 0.09240 \n",
      "[170/500] train_loss: 0.07078 valid_loss: 0.08262 test_loss: 0.09370 \n",
      "[171/500] train_loss: 0.07107 valid_loss: 0.08124 test_loss: 0.09248 \n",
      "[172/500] train_loss: 0.06847 valid_loss: 0.08542 test_loss: 0.09218 \n",
      "[173/500] train_loss: 0.06869 valid_loss: 0.08214 test_loss: 0.09239 \n",
      "[174/500] train_loss: 0.07019 valid_loss: 0.08360 test_loss: 0.09124 \n",
      "[175/500] train_loss: 0.07103 valid_loss: 0.08106 test_loss: 0.09085 \n",
      "[176/500] train_loss: 0.07006 valid_loss: 0.08388 test_loss: 0.09321 \n",
      "[177/500] train_loss: 0.07130 valid_loss: 0.09005 test_loss: 0.09164 \n",
      "[178/500] train_loss: 0.06833 valid_loss: 0.08357 test_loss: 0.09122 \n",
      "[179/500] train_loss: 0.06956 valid_loss: 0.08439 test_loss: 0.09283 \n",
      "[180/500] train_loss: 0.06644 valid_loss: 0.08443 test_loss: 0.09204 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181/500] train_loss: 0.06938 valid_loss: 0.07937 test_loss: 0.09237 \n",
      "验证损失减少 (0.080047 --> 0.079374). 正在保存模型...\n",
      "[182/500] train_loss: 0.07051 valid_loss: 0.08116 test_loss: 0.09067 \n",
      "[183/500] train_loss: 0.06667 valid_loss: 0.07973 test_loss: 0.09122 \n",
      "[184/500] train_loss: 0.06725 valid_loss: 0.08232 test_loss: 0.09218 \n",
      "[185/500] train_loss: 0.07202 valid_loss: 0.08007 test_loss: 0.09207 \n",
      "[186/500] train_loss: 0.06840 valid_loss: 0.08341 test_loss: 0.09318 \n",
      "[187/500] train_loss: 0.06853 valid_loss: 0.08090 test_loss: 0.08967 \n",
      "[188/500] train_loss: 0.06718 valid_loss: 0.08137 test_loss: 0.09100 \n",
      "[189/500] train_loss: 0.07050 valid_loss: 0.08064 test_loss: 0.09334 \n",
      "[190/500] train_loss: 0.06673 valid_loss: 0.07918 test_loss: 0.09042 \n",
      "验证损失减少 (0.079374 --> 0.079176). 正在保存模型...\n",
      "[191/500] train_loss: 0.06618 valid_loss: 0.07984 test_loss: 0.09132 \n",
      "[192/500] train_loss: 0.06642 valid_loss: 0.07948 test_loss: 0.09175 \n",
      "[193/500] train_loss: 0.06708 valid_loss: 0.08047 test_loss: 0.09111 \n",
      "[194/500] train_loss: 0.06767 valid_loss: 0.08254 test_loss: 0.09144 \n",
      "[195/500] train_loss: 0.06744 valid_loss: 0.07948 test_loss: 0.09050 \n",
      "[196/500] train_loss: 0.06597 valid_loss: 0.08077 test_loss: 0.09080 \n",
      "[197/500] train_loss: 0.06882 valid_loss: 0.08207 test_loss: 0.08921 \n",
      "[198/500] train_loss: 0.06580 valid_loss: 0.08115 test_loss: 0.09081 \n",
      "[199/500] train_loss: 0.06707 valid_loss: 0.08040 test_loss: 0.09174 \n",
      "[200/500] train_loss: 0.06622 valid_loss: 0.08012 test_loss: 0.09178 \n",
      "[201/500] train_loss: 0.06850 valid_loss: 0.07910 test_loss: 0.09022 \n",
      "验证损失减少 (0.079176 --> 0.079101). 正在保存模型...\n",
      "[202/500] train_loss: 0.06742 valid_loss: 0.08014 test_loss: 0.09348 \n",
      "[203/500] train_loss: 0.06876 valid_loss: 0.07960 test_loss: 0.09303 \n",
      "[204/500] train_loss: 0.06797 valid_loss: 0.07874 test_loss: 0.09048 \n",
      "验证损失减少 (0.079101 --> 0.078739). 正在保存模型...\n",
      "[205/500] train_loss: 0.06605 valid_loss: 0.07927 test_loss: 0.09088 \n",
      "[206/500] train_loss: 0.06576 valid_loss: 0.07839 test_loss: 0.09057 \n",
      "验证损失减少 (0.078739 --> 0.078385). 正在保存模型...\n",
      "[207/500] train_loss: 0.06537 valid_loss: 0.08146 test_loss: 0.09035 \n",
      "[208/500] train_loss: 0.06598 valid_loss: 0.07909 test_loss: 0.09138 \n",
      "[209/500] train_loss: 0.06543 valid_loss: 0.07995 test_loss: 0.08949 \n",
      "[210/500] train_loss: 0.06593 valid_loss: 0.08137 test_loss: 0.09003 \n",
      "[211/500] train_loss: 0.06670 valid_loss: 0.07819 test_loss: 0.08993 \n",
      "验证损失减少 (0.078385 --> 0.078193). 正在保存模型...\n",
      "[212/500] train_loss: 0.06461 valid_loss: 0.08006 test_loss: 0.08983 \n",
      "[213/500] train_loss: 0.06719 valid_loss: 0.08119 test_loss: 0.08952 \n",
      "[214/500] train_loss: 0.06350 valid_loss: 0.07966 test_loss: 0.08939 \n",
      "[215/500] train_loss: 0.06670 valid_loss: 0.07745 test_loss: 0.08929 \n",
      "验证损失减少 (0.078193 --> 0.077449). 正在保存模型...\n",
      "[216/500] train_loss: 0.06524 valid_loss: 0.08237 test_loss: 0.09162 \n",
      "[217/500] train_loss: 0.06442 valid_loss: 0.07994 test_loss: 0.09014 \n",
      "[218/500] train_loss: 0.06439 valid_loss: 0.08168 test_loss: 0.08964 \n",
      "[219/500] train_loss: 0.06432 valid_loss: 0.07850 test_loss: 0.09004 \n",
      "[220/500] train_loss: 0.06444 valid_loss: 0.08126 test_loss: 0.09071 \n",
      "[221/500] train_loss: 0.06563 valid_loss: 0.08757 test_loss: 0.09205 \n",
      "[222/500] train_loss: 0.06469 valid_loss: 0.08523 test_loss: 0.08951 \n",
      "[223/500] train_loss: 0.06633 valid_loss: 0.08370 test_loss: 0.09153 \n",
      "[224/500] train_loss: 0.06628 valid_loss: 0.08715 test_loss: 0.09196 \n",
      "[225/500] train_loss: 0.06562 valid_loss: 0.08280 test_loss: 0.09039 \n",
      "[226/500] train_loss: 0.06400 valid_loss: 0.08946 test_loss: 0.09062 \n",
      "[227/500] train_loss: 0.06487 valid_loss: 0.08874 test_loss: 0.09040 \n",
      "[228/500] train_loss: 0.06478 valid_loss: 0.08630 test_loss: 0.09104 \n",
      "[229/500] train_loss: 0.06495 valid_loss: 0.08613 test_loss: 0.08898 \n",
      "[230/500] train_loss: 0.06348 valid_loss: 0.08735 test_loss: 0.08955 \n",
      "[231/500] train_loss: 0.06465 valid_loss: 0.08408 test_loss: 0.09107 \n",
      "[232/500] train_loss: 0.06180 valid_loss: 0.08007 test_loss: 0.08972 \n",
      "[233/500] train_loss: 0.06353 valid_loss: 0.09339 test_loss: 0.09044 \n",
      "[234/500] train_loss: 0.06367 valid_loss: 0.09766 test_loss: 0.09085 \n",
      "[235/500] train_loss: 0.06348 valid_loss: 0.09058 test_loss: 0.09091 \n",
      "[236/500] train_loss: 0.06529 valid_loss: 0.08523 test_loss: 0.08881 \n",
      "[237/500] train_loss: 0.06467 valid_loss: 0.08091 test_loss: 0.08956 \n",
      "[238/500] train_loss: 0.06484 valid_loss: 0.07932 test_loss: 0.08943 \n",
      "[239/500] train_loss: 0.06372 valid_loss: 0.07762 test_loss: 0.08950 \n",
      "[240/500] train_loss: 0.06512 valid_loss: 0.07714 test_loss: 0.08840 \n",
      "验证损失减少 (0.077449 --> 0.077139). 正在保存模型...\n",
      "[241/500] train_loss: 0.06359 valid_loss: 0.08305 test_loss: 0.09028 \n",
      "[242/500] train_loss: 0.06667 valid_loss: 0.08055 test_loss: 0.08873 \n",
      "[243/500] train_loss: 0.06370 valid_loss: 0.08063 test_loss: 0.08888 \n",
      "[244/500] train_loss: 0.06432 valid_loss: 0.07948 test_loss: 0.08887 \n",
      "[245/500] train_loss: 0.06466 valid_loss: 0.08167 test_loss: 0.09104 \n",
      "[246/500] train_loss: 0.06489 valid_loss: 0.07822 test_loss: 0.08943 \n",
      "[247/500] train_loss: 0.06223 valid_loss: 0.07962 test_loss: 0.08932 \n",
      "[248/500] train_loss: 0.06250 valid_loss: 0.08621 test_loss: 0.08858 \n",
      "[249/500] train_loss: 0.06354 valid_loss: 0.08118 test_loss: 0.09002 \n",
      "[250/500] train_loss: 0.06421 valid_loss: 0.07831 test_loss: 0.09003 \n",
      "[251/500] train_loss: 0.06672 valid_loss: 0.08621 test_loss: 0.08960 \n",
      "[252/500] train_loss: 0.06524 valid_loss: 0.08204 test_loss: 0.08911 \n",
      "[253/500] train_loss: 0.06297 valid_loss: 0.08307 test_loss: 0.08891 \n",
      "[254/500] train_loss: 0.06290 valid_loss: 0.08089 test_loss: 0.08907 \n",
      "[255/500] train_loss: 0.06331 valid_loss: 0.07848 test_loss: 0.08971 \n",
      "[256/500] train_loss: 0.06129 valid_loss: 0.08620 test_loss: 0.08950 \n",
      "[257/500] train_loss: 0.06262 valid_loss: 0.07919 test_loss: 0.08862 \n",
      "[258/500] train_loss: 0.06313 valid_loss: 0.08026 test_loss: 0.08891 \n",
      "[259/500] train_loss: 0.06130 valid_loss: 0.07737 test_loss: 0.08938 \n",
      "[260/500] train_loss: 0.06304 valid_loss: 0.07912 test_loss: 0.09022 \n",
      "[261/500] train_loss: 0.06068 valid_loss: 0.07635 test_loss: 0.08912 \n",
      "验证损失减少 (0.077139 --> 0.076353). 正在保存模型...\n",
      "[262/500] train_loss: 0.06278 valid_loss: 0.07769 test_loss: 0.08956 \n",
      "[263/500] train_loss: 0.06317 valid_loss: 0.07835 test_loss: 0.09031 \n",
      "[264/500] train_loss: 0.06463 valid_loss: 0.07905 test_loss: 0.08767 \n",
      "[265/500] train_loss: 0.06336 valid_loss: 0.07954 test_loss: 0.08896 \n",
      "[266/500] train_loss: 0.06207 valid_loss: 0.07910 test_loss: 0.08833 \n",
      "[267/500] train_loss: 0.06243 valid_loss: 0.07645 test_loss: 0.08812 \n",
      "[268/500] train_loss: 0.06124 valid_loss: 0.08008 test_loss: 0.08881 \n",
      "[269/500] train_loss: 0.06433 valid_loss: 0.08134 test_loss: 0.08811 \n",
      "[270/500] train_loss: 0.06312 valid_loss: 0.08220 test_loss: 0.08923 \n",
      "[271/500] train_loss: 0.06264 valid_loss: 0.07558 test_loss: 0.08822 \n",
      "验证损失减少 (0.076353 --> 0.075576). 正在保存模型...\n",
      "[272/500] train_loss: 0.06177 valid_loss: 0.07986 test_loss: 0.09001 \n",
      "[273/500] train_loss: 0.06133 valid_loss: 0.08300 test_loss: 0.09020 \n",
      "[274/500] train_loss: 0.06173 valid_loss: 0.08221 test_loss: 0.08925 \n",
      "[275/500] train_loss: 0.06229 valid_loss: 0.08352 test_loss: 0.09069 \n",
      "[276/500] train_loss: 0.06153 valid_loss: 0.08230 test_loss: 0.08871 \n",
      "[277/500] train_loss: 0.06145 valid_loss: 0.09152 test_loss: 0.09078 \n",
      "[278/500] train_loss: 0.06266 valid_loss: 0.07824 test_loss: 0.08926 \n",
      "[279/500] train_loss: 0.05924 valid_loss: 0.07765 test_loss: 0.09133 \n",
      "[280/500] train_loss: 0.06270 valid_loss: 0.07945 test_loss: 0.08964 \n",
      "[281/500] train_loss: 0.06204 valid_loss: 0.08195 test_loss: 0.08984 \n",
      "[282/500] train_loss: 0.06001 valid_loss: 0.08226 test_loss: 0.08907 \n",
      "[283/500] train_loss: 0.06175 valid_loss: 0.08830 test_loss: 0.08833 \n",
      "[284/500] train_loss: 0.05972 valid_loss: 0.08211 test_loss: 0.08954 \n",
      "[285/500] train_loss: 0.06343 valid_loss: 0.08156 test_loss: 0.08873 \n",
      "[286/500] train_loss: 0.06046 valid_loss: 0.07595 test_loss: 0.08927 \n",
      "[287/500] train_loss: 0.05919 valid_loss: 0.07717 test_loss: 0.08969 \n",
      "[288/500] train_loss: 0.06090 valid_loss: 0.08108 test_loss: 0.09100 \n",
      "[289/500] train_loss: 0.05987 valid_loss: 0.07901 test_loss: 0.09162 \n",
      "[290/500] train_loss: 0.06153 valid_loss: 0.07678 test_loss: 0.08918 \n",
      "[291/500] train_loss: 0.06002 valid_loss: 0.08463 test_loss: 0.08876 \n",
      "[292/500] train_loss: 0.05998 valid_loss: 0.08262 test_loss: 0.08870 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293/500] train_loss: 0.06110 valid_loss: 0.08070 test_loss: 0.08858 \n",
      "[294/500] train_loss: 0.06137 valid_loss: 0.08134 test_loss: 0.08827 \n",
      "[295/500] train_loss: 0.06194 valid_loss: 0.08968 test_loss: 0.09026 \n",
      "[296/500] train_loss: 0.06197 valid_loss: 0.09254 test_loss: 0.08959 \n",
      "[297/500] train_loss: 0.05897 valid_loss: 0.08055 test_loss: 0.08947 \n",
      "[298/500] train_loss: 0.06207 valid_loss: 0.07623 test_loss: 0.09056 \n",
      "[299/500] train_loss: 0.05905 valid_loss: 0.08387 test_loss: 0.08927 \n",
      "[300/500] train_loss: 0.06043 valid_loss: 0.08049 test_loss: 0.08893 \n",
      "[301/500] train_loss: 0.06211 valid_loss: 0.07670 test_loss: 0.08896 \n",
      "[302/500] train_loss: 0.06199 valid_loss: 0.07977 test_loss: 0.08871 \n",
      "[303/500] train_loss: 0.06023 valid_loss: 0.07732 test_loss: 0.08788 \n",
      "[304/500] train_loss: 0.05984 valid_loss: 0.07981 test_loss: 0.08939 \n",
      "[305/500] train_loss: 0.05950 valid_loss: 0.08074 test_loss: 0.09001 \n",
      "[306/500] train_loss: 0.06168 valid_loss: 0.07945 test_loss: 0.08727 \n",
      "[307/500] train_loss: 0.06066 valid_loss: 0.07606 test_loss: 0.08738 \n",
      "[308/500] train_loss: 0.06141 valid_loss: 0.07749 test_loss: 0.08872 \n",
      "[309/500] train_loss: 0.06005 valid_loss: 0.07797 test_loss: 0.08980 \n",
      "[310/500] train_loss: 0.06097 valid_loss: 0.07541 test_loss: 0.08810 \n",
      "验证损失减少 (0.075576 --> 0.075413). 正在保存模型...\n",
      "[311/500] train_loss: 0.06106 valid_loss: 0.07922 test_loss: 0.08891 \n",
      "[312/500] train_loss: 0.05996 valid_loss: 0.08261 test_loss: 0.08781 \n",
      "[313/500] train_loss: 0.05931 valid_loss: 0.08849 test_loss: 0.08863 \n",
      "[314/500] train_loss: 0.05707 valid_loss: 0.08039 test_loss: 0.08776 \n",
      "[315/500] train_loss: 0.06050 valid_loss: 0.07824 test_loss: 0.08853 \n",
      "[316/500] train_loss: 0.05902 valid_loss: 0.08793 test_loss: 0.08809 \n",
      "[317/500] train_loss: 0.05899 valid_loss: 0.07589 test_loss: 0.08794 \n",
      "[318/500] train_loss: 0.05814 valid_loss: 0.07483 test_loss: 0.08748 \n",
      "验证损失减少 (0.075413 --> 0.074829). 正在保存模型...\n",
      "[319/500] train_loss: 0.05867 valid_loss: 0.07397 test_loss: 0.08720 \n",
      "验证损失减少 (0.074829 --> 0.073965). 正在保存模型...\n",
      "[320/500] train_loss: 0.05787 valid_loss: 0.07550 test_loss: 0.08792 \n",
      "[321/500] train_loss: 0.05741 valid_loss: 0.07538 test_loss: 0.08786 \n",
      "[322/500] train_loss: 0.05866 valid_loss: 0.07557 test_loss: 0.08804 \n",
      "[323/500] train_loss: 0.05945 valid_loss: 0.07652 test_loss: 0.08790 \n",
      "[324/500] train_loss: 0.05997 valid_loss: 0.07661 test_loss: 0.08884 \n",
      "[325/500] train_loss: 0.05850 valid_loss: 0.07652 test_loss: 0.08949 \n",
      "[326/500] train_loss: 0.05975 valid_loss: 0.07560 test_loss: 0.08696 \n",
      "[327/500] train_loss: 0.06112 valid_loss: 0.07893 test_loss: 0.08861 \n",
      "[328/500] train_loss: 0.05934 valid_loss: 0.08366 test_loss: 0.08797 \n",
      "[329/500] train_loss: 0.05833 valid_loss: 0.07531 test_loss: 0.08774 \n",
      "[330/500] train_loss: 0.05963 valid_loss: 0.07532 test_loss: 0.08801 \n",
      "[331/500] train_loss: 0.06140 valid_loss: 0.07715 test_loss: 0.08741 \n",
      "[332/500] train_loss: 0.05951 valid_loss: 0.08074 test_loss: 0.08598 \n",
      "[333/500] train_loss: 0.05774 valid_loss: 0.07663 test_loss: 0.08922 \n",
      "[334/500] train_loss: 0.05864 valid_loss: 0.07533 test_loss: 0.08833 \n",
      "[335/500] train_loss: 0.05978 valid_loss: 0.07614 test_loss: 0.08834 \n",
      "[336/500] train_loss: 0.05827 valid_loss: 0.07702 test_loss: 0.09020 \n",
      "[337/500] train_loss: 0.05742 valid_loss: 0.07643 test_loss: 0.08807 \n",
      "[338/500] train_loss: 0.05787 valid_loss: 0.07655 test_loss: 0.08843 \n",
      "[339/500] train_loss: 0.05746 valid_loss: 0.08347 test_loss: 0.08648 \n",
      "[340/500] train_loss: 0.05908 valid_loss: 0.08039 test_loss: 0.08898 \n",
      "[341/500] train_loss: 0.05796 valid_loss: 0.08039 test_loss: 0.08769 \n",
      "[342/500] train_loss: 0.05656 valid_loss: 0.08647 test_loss: 0.08732 \n",
      "[343/500] train_loss: 0.05770 valid_loss: 0.08340 test_loss: 0.08749 \n",
      "[344/500] train_loss: 0.05947 valid_loss: 0.07683 test_loss: 0.08812 \n",
      "[345/500] train_loss: 0.05809 valid_loss: 0.07794 test_loss: 0.08596 \n",
      "[346/500] train_loss: 0.05705 valid_loss: 0.07652 test_loss: 0.08944 \n",
      "[347/500] train_loss: 0.05739 valid_loss: 0.07541 test_loss: 0.08714 \n",
      "[348/500] train_loss: 0.05873 valid_loss: 0.08203 test_loss: 0.08813 \n",
      "[349/500] train_loss: 0.05769 valid_loss: 0.07748 test_loss: 0.08754 \n",
      "[350/500] train_loss: 0.05901 valid_loss: 0.07510 test_loss: 0.08747 \n",
      "[351/500] train_loss: 0.05806 valid_loss: 0.07985 test_loss: 0.08792 \n",
      "[352/500] train_loss: 0.05784 valid_loss: 0.07673 test_loss: 0.08789 \n",
      "[353/500] train_loss: 0.05859 valid_loss: 0.08488 test_loss: 0.08716 \n",
      "[354/500] train_loss: 0.05876 valid_loss: 0.07571 test_loss: 0.08714 \n",
      "[355/500] train_loss: 0.05828 valid_loss: 0.08654 test_loss: 0.08819 \n",
      "[356/500] train_loss: 0.05768 valid_loss: 0.07669 test_loss: 0.08937 \n",
      "[357/500] train_loss: 0.05858 valid_loss: 0.07627 test_loss: 0.08747 \n",
      "[358/500] train_loss: 0.05600 valid_loss: 0.08877 test_loss: 0.08623 \n",
      "[359/500] train_loss: 0.05740 valid_loss: 0.07899 test_loss: 0.08840 \n",
      "[360/500] train_loss: 0.05665 valid_loss: 0.07737 test_loss: 0.08715 \n",
      "[361/500] train_loss: 0.05689 valid_loss: 0.07886 test_loss: 0.08778 \n",
      "[362/500] train_loss: 0.05794 valid_loss: 0.08370 test_loss: 0.08633 \n",
      "[363/500] train_loss: 0.05722 valid_loss: 0.08261 test_loss: 0.08733 \n",
      "[364/500] train_loss: 0.05611 valid_loss: 0.08841 test_loss: 0.08787 \n",
      "[365/500] train_loss: 0.05488 valid_loss: 0.08501 test_loss: 0.08709 \n",
      "[366/500] train_loss: 0.05844 valid_loss: 0.07805 test_loss: 0.08603 \n",
      "[367/500] train_loss: 0.05589 valid_loss: 0.07431 test_loss: 0.08616 \n",
      "[368/500] train_loss: 0.05746 valid_loss: 0.07611 test_loss: 0.08601 \n",
      "[369/500] train_loss: 0.05770 valid_loss: 0.07698 test_loss: 0.08948 \n",
      "[370/500] train_loss: 0.05755 valid_loss: 0.07842 test_loss: 0.08724 \n",
      "[371/500] train_loss: 0.05594 valid_loss: 0.07648 test_loss: 0.08612 \n",
      "[372/500] train_loss: 0.05732 valid_loss: 0.07496 test_loss: 0.08689 \n",
      "[373/500] train_loss: 0.05606 valid_loss: 0.08938 test_loss: 0.08601 \n",
      "[374/500] train_loss: 0.05570 valid_loss: 0.09447 test_loss: 0.08647 \n",
      "[375/500] train_loss: 0.05869 valid_loss: 0.08890 test_loss: 0.08606 \n",
      "[376/500] train_loss: 0.05785 valid_loss: 0.07947 test_loss: 0.08590 \n",
      "[377/500] train_loss: 0.05562 valid_loss: 0.10707 test_loss: 0.08673 \n",
      "[378/500] train_loss: 0.05444 valid_loss: 0.08523 test_loss: 0.08779 \n",
      "[379/500] train_loss: 0.05665 valid_loss: 0.08481 test_loss: 0.08719 \n",
      "[380/500] train_loss: 0.05656 valid_loss: 0.08519 test_loss: 0.08681 \n",
      "[381/500] train_loss: 0.05679 valid_loss: 0.08339 test_loss: 0.08583 \n",
      "[382/500] train_loss: 0.05757 valid_loss: 0.08221 test_loss: 0.08522 \n",
      "[383/500] train_loss: 0.05575 valid_loss: 0.07630 test_loss: 0.08874 \n",
      "[384/500] train_loss: 0.05511 valid_loss: 0.10140 test_loss: 0.08764 \n",
      "[385/500] train_loss: 0.05662 valid_loss: 0.09187 test_loss: 0.08636 \n",
      "[386/500] train_loss: 0.05632 valid_loss: 0.08209 test_loss: 0.08608 \n",
      "[387/500] train_loss: 0.05750 valid_loss: 0.08173 test_loss: 0.08598 \n",
      "[388/500] train_loss: 0.05654 valid_loss: 0.07720 test_loss: 0.08644 \n",
      "[389/500] train_loss: 0.05681 valid_loss: 0.08759 test_loss: 0.08702 \n",
      "[390/500] train_loss: 0.05412 valid_loss: 0.07637 test_loss: 0.08791 \n",
      "[391/500] train_loss: 0.05494 valid_loss: 0.08530 test_loss: 0.08744 \n",
      "[392/500] train_loss: 0.05552 valid_loss: 0.09164 test_loss: 0.08786 \n",
      "[393/500] train_loss: 0.05651 valid_loss: 0.08427 test_loss: 0.08718 \n",
      "[394/500] train_loss: 0.05556 valid_loss: 0.09310 test_loss: 0.08556 \n",
      "[395/500] train_loss: 0.05744 valid_loss: 0.08417 test_loss: 0.08768 \n",
      "[396/500] train_loss: 0.05606 valid_loss: 0.08676 test_loss: 0.08730 \n",
      "[397/500] train_loss: 0.05552 valid_loss: 0.08240 test_loss: 0.08693 \n",
      "[398/500] train_loss: 0.05587 valid_loss: 0.07634 test_loss: 0.08879 \n",
      "[399/500] train_loss: 0.05717 valid_loss: 0.08333 test_loss: 0.08765 \n",
      "[400/500] train_loss: 0.05314 valid_loss: 0.08391 test_loss: 0.08860 \n",
      "[401/500] train_loss: 0.05526 valid_loss: 0.08535 test_loss: 0.08792 \n",
      "[402/500] train_loss: 0.05666 valid_loss: 0.08662 test_loss: 0.08864 \n",
      "[403/500] train_loss: 0.05580 valid_loss: 0.07982 test_loss: 0.08776 \n",
      "[404/500] train_loss: 0.05515 valid_loss: 0.07887 test_loss: 0.08737 \n",
      "[405/500] train_loss: 0.05517 valid_loss: 0.08039 test_loss: 0.08660 \n",
      "[406/500] train_loss: 0.05472 valid_loss: 0.07777 test_loss: 0.08555 \n",
      "[407/500] train_loss: 0.05438 valid_loss: 0.08112 test_loss: 0.08558 \n",
      "[408/500] train_loss: 0.05517 valid_loss: 0.08560 test_loss: 0.08644 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[409/500] train_loss: 0.05543 valid_loss: 0.07578 test_loss: 0.08673 \n",
      "[410/500] train_loss: 0.05626 valid_loss: 0.07693 test_loss: 0.08640 \n",
      "[411/500] train_loss: 0.05523 valid_loss: 0.07658 test_loss: 0.08689 \n",
      "[412/500] train_loss: 0.05491 valid_loss: 0.07486 test_loss: 0.08578 \n",
      "[413/500] train_loss: 0.05598 valid_loss: 0.07545 test_loss: 0.08777 \n",
      "[414/500] train_loss: 0.05379 valid_loss: 0.07945 test_loss: 0.08747 \n",
      "[415/500] train_loss: 0.05424 valid_loss: 0.09182 test_loss: 0.08766 \n",
      "[416/500] train_loss: 0.05459 valid_loss: 0.07610 test_loss: 0.08666 \n",
      "[417/500] train_loss: 0.05697 valid_loss: 0.07574 test_loss: 0.08640 \n",
      "[418/500] train_loss: 0.05339 valid_loss: 0.08535 test_loss: 0.08768 \n",
      "[419/500] train_loss: 0.05526 valid_loss: 0.08276 test_loss: 0.08832 \n",
      "[420/500] train_loss: 0.05627 valid_loss: 0.08482 test_loss: 0.08869 \n",
      "[421/500] train_loss: 0.05645 valid_loss: 0.08501 test_loss: 0.08655 \n",
      "[422/500] train_loss: 0.05464 valid_loss: 0.08165 test_loss: 0.08613 \n",
      "[423/500] train_loss: 0.05497 valid_loss: 0.08994 test_loss: 0.08817 \n",
      "[424/500] train_loss: 0.05465 valid_loss: 0.08357 test_loss: 0.08808 \n",
      "[425/500] train_loss: 0.05385 valid_loss: 0.07617 test_loss: 0.08675 \n",
      "[426/500] train_loss: 0.05520 valid_loss: 0.08502 test_loss: 0.08668 \n",
      "[427/500] train_loss: 0.05445 valid_loss: 0.08295 test_loss: 0.08839 \n",
      "[428/500] train_loss: 0.05555 valid_loss: 0.07797 test_loss: 0.08611 \n",
      "[429/500] train_loss: 0.05537 valid_loss: 0.08740 test_loss: 0.08797 \n",
      "[430/500] train_loss: 0.05422 valid_loss: 0.09275 test_loss: 0.08822 \n",
      "[431/500] train_loss: 0.05384 valid_loss: 0.09698 test_loss: 0.08733 \n",
      "[432/500] train_loss: 0.05534 valid_loss: 0.09292 test_loss: 0.08693 \n",
      "[433/500] train_loss: 0.05581 valid_loss: 0.07618 test_loss: 0.08704 \n",
      "[434/500] train_loss: 0.05426 valid_loss: 0.08219 test_loss: 0.08651 \n",
      "[435/500] train_loss: 0.05325 valid_loss: 0.08465 test_loss: 0.08630 \n",
      "[436/500] train_loss: 0.05342 valid_loss: 0.08737 test_loss: 0.08690 \n",
      "[437/500] train_loss: 0.05413 valid_loss: 0.08796 test_loss: 0.08713 \n",
      "[438/500] train_loss: 0.05376 valid_loss: 0.07971 test_loss: 0.08746 \n",
      "[439/500] train_loss: 0.05420 valid_loss: 0.09715 test_loss: 0.08643 \n",
      "[440/500] train_loss: 0.05304 valid_loss: 0.08616 test_loss: 0.08629 \n",
      "[441/500] train_loss: 0.05534 valid_loss: 0.09733 test_loss: 0.08721 \n",
      "[442/500] train_loss: 0.05337 valid_loss: 0.08423 test_loss: 0.08691 \n",
      "[443/500] train_loss: 0.05490 valid_loss: 0.08857 test_loss: 0.08674 \n",
      "[444/500] train_loss: 0.05392 valid_loss: 0.09018 test_loss: 0.08644 \n",
      "[445/500] train_loss: 0.05522 valid_loss: 0.07827 test_loss: 0.08710 \n",
      "[446/500] train_loss: 0.05421 valid_loss: 0.07604 test_loss: 0.08751 \n",
      "[447/500] train_loss: 0.05536 valid_loss: 0.07858 test_loss: 0.08946 \n",
      "[448/500] train_loss: 0.05380 valid_loss: 0.08681 test_loss: 0.08702 \n",
      "[449/500] train_loss: 0.05387 valid_loss: 0.09290 test_loss: 0.08697 \n",
      "[450/500] train_loss: 0.05324 valid_loss: 0.07905 test_loss: 0.08638 \n",
      "[451/500] train_loss: 0.05714 valid_loss: 0.09109 test_loss: 0.08743 \n",
      "[452/500] train_loss: 0.05316 valid_loss: 0.09371 test_loss: 0.08783 \n",
      "[453/500] train_loss: 0.05383 valid_loss: 0.08204 test_loss: 0.08767 \n",
      "[454/500] train_loss: 0.05308 valid_loss: 0.08001 test_loss: 0.08661 \n",
      "[455/500] train_loss: 0.05563 valid_loss: 0.08525 test_loss: 0.08773 \n",
      "[456/500] train_loss: 0.05303 valid_loss: 0.08202 test_loss: 0.08677 \n",
      "[457/500] train_loss: 0.05190 valid_loss: 0.07703 test_loss: 0.08772 \n",
      "[458/500] train_loss: 0.05315 valid_loss: 0.07508 test_loss: 0.08576 \n",
      "[459/500] train_loss: 0.05258 valid_loss: 0.07716 test_loss: 0.08778 \n",
      "[460/500] train_loss: 0.05320 valid_loss: 0.07772 test_loss: 0.08507 \n",
      "[461/500] train_loss: 0.05466 valid_loss: 0.08686 test_loss: 0.08585 \n",
      "[462/500] train_loss: 0.05203 valid_loss: 0.09013 test_loss: 0.08599 \n",
      "[463/500] train_loss: 0.05240 valid_loss: 0.07969 test_loss: 0.08727 \n",
      "[464/500] train_loss: 0.05291 valid_loss: 0.07657 test_loss: 0.08816 \n",
      "[465/500] train_loss: 0.05256 valid_loss: 0.08339 test_loss: 0.08624 \n",
      "[466/500] train_loss: 0.05250 valid_loss: 0.07618 test_loss: 0.08687 \n",
      "[467/500] train_loss: 0.05356 valid_loss: 0.10378 test_loss: 0.08769 \n",
      "[468/500] train_loss: 0.05228 valid_loss: 0.09465 test_loss: 0.08736 \n",
      "[469/500] train_loss: 0.05533 valid_loss: 0.08942 test_loss: 0.08652 \n",
      "[470/500] train_loss: 0.05275 valid_loss: 0.08928 test_loss: 0.08643 \n",
      "[471/500] train_loss: 0.05178 valid_loss: 0.08319 test_loss: 0.08590 \n",
      "[472/500] train_loss: 0.05388 valid_loss: 0.08470 test_loss: 0.08732 \n",
      "[473/500] train_loss: 0.05287 valid_loss: 0.08343 test_loss: 0.08574 \n",
      "[474/500] train_loss: 0.05279 valid_loss: 0.08064 test_loss: 0.08780 \n",
      "[475/500] train_loss: 0.05349 valid_loss: 0.08195 test_loss: 0.08904 \n",
      "[476/500] train_loss: 0.05256 valid_loss: 0.08447 test_loss: 0.08624 \n",
      "[477/500] train_loss: 0.05165 valid_loss: 0.07538 test_loss: 0.08681 \n",
      "[478/500] train_loss: 0.05278 valid_loss: 0.08236 test_loss: 0.08812 \n",
      "[479/500] train_loss: 0.05371 valid_loss: 0.09893 test_loss: 0.08709 \n",
      "[480/500] train_loss: 0.05147 valid_loss: 0.09648 test_loss: 0.08805 \n",
      "[481/500] train_loss: 0.05247 valid_loss: 0.09752 test_loss: 0.08737 \n",
      "[482/500] train_loss: 0.05264 valid_loss: 0.08421 test_loss: 0.08712 \n",
      "[483/500] train_loss: 0.05082 valid_loss: 0.07574 test_loss: 0.08693 \n",
      "[484/500] train_loss: 0.05161 valid_loss: 0.09638 test_loss: 0.08798 \n",
      "[485/500] train_loss: 0.05292 valid_loss: 0.08499 test_loss: 0.08862 \n",
      "[486/500] train_loss: 0.05280 valid_loss: 0.08535 test_loss: 0.08759 \n",
      "[487/500] train_loss: 0.05265 valid_loss: 0.10244 test_loss: 0.08606 \n",
      "[488/500] train_loss: 0.05271 valid_loss: 0.09044 test_loss: 0.08739 \n",
      "[489/500] train_loss: 0.05305 valid_loss: 0.08432 test_loss: 0.08580 \n",
      "[490/500] train_loss: 0.05246 valid_loss: 0.08590 test_loss: 0.08833 \n",
      "[491/500] train_loss: 0.05289 valid_loss: 0.08185 test_loss: 0.08698 \n",
      "[492/500] train_loss: 0.05367 valid_loss: 0.09028 test_loss: 0.08772 \n",
      "[493/500] train_loss: 0.05129 valid_loss: 0.09317 test_loss: 0.08734 \n",
      "[494/500] train_loss: 0.05133 valid_loss: 0.09499 test_loss: 0.08684 \n",
      "[495/500] train_loss: 0.05162 valid_loss: 0.08992 test_loss: 0.08794 \n",
      "[496/500] train_loss: 0.05202 valid_loss: 0.08676 test_loss: 0.08862 \n",
      "[497/500] train_loss: 0.05057 valid_loss: 0.08802 test_loss: 0.08834 \n",
      "[498/500] train_loss: 0.05125 valid_loss: 0.09969 test_loss: 0.08713 \n",
      "[499/500] train_loss: 0.05326 valid_loss: 0.10456 test_loss: 0.08835 \n",
      "[500/500] train_loss: 0.05107 valid_loss: 0.11972 test_loss: 0.08826 \n",
      "TRAINING MODEL 3\n",
      "[  1/500] train_loss: 0.39832 valid_loss: 0.28169 test_loss: 0.28726 \n",
      "验证损失减少 (inf --> 0.281688). 正在保存模型...\n",
      "[  2/500] train_loss: 0.22593 valid_loss: 0.20292 test_loss: 0.20981 \n",
      "验证损失减少 (0.281688 --> 0.202920). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17569 valid_loss: 0.17713 test_loss: 0.18645 \n",
      "验证损失减少 (0.202920 --> 0.177128). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15663 valid_loss: 0.16280 test_loss: 0.17243 \n",
      "验证损失减少 (0.177128 --> 0.162798). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14746 valid_loss: 0.15406 test_loss: 0.16582 \n",
      "验证损失减少 (0.162798 --> 0.154059). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14727 valid_loss: 0.14684 test_loss: 0.15812 \n",
      "验证损失减少 (0.154059 --> 0.146837). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13577 valid_loss: 0.14063 test_loss: 0.15224 \n",
      "验证损失减少 (0.146837 --> 0.140631). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13229 valid_loss: 0.13984 test_loss: 0.15289 \n",
      "验证损失减少 (0.140631 --> 0.139841). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13156 valid_loss: 0.13493 test_loss: 0.14901 \n",
      "验证损失减少 (0.139841 --> 0.134926). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12842 valid_loss: 0.13467 test_loss: 0.14921 \n",
      "验证损失减少 (0.134926 --> 0.134674). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12445 valid_loss: 0.12648 test_loss: 0.14050 \n",
      "验证损失减少 (0.134674 --> 0.126484). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12382 valid_loss: 0.12875 test_loss: 0.14249 \n",
      "[ 13/500] train_loss: 0.11945 valid_loss: 0.12723 test_loss: 0.13911 \n",
      "[ 14/500] train_loss: 0.11685 valid_loss: 0.12457 test_loss: 0.13846 \n",
      "验证损失减少 (0.126484 --> 0.124571). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11675 valid_loss: 0.12274 test_loss: 0.13668 \n",
      "验证损失减少 (0.124571 --> 0.122737). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11529 valid_loss: 0.11913 test_loss: 0.13349 \n",
      "验证损失减少 (0.122737 --> 0.119128). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11673 valid_loss: 0.11807 test_loss: 0.13190 \n",
      "验证损失减少 (0.119128 --> 0.118069). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18/500] train_loss: 0.11309 valid_loss: 0.11850 test_loss: 0.13337 \n",
      "[ 19/500] train_loss: 0.11537 valid_loss: 0.11560 test_loss: 0.13062 \n",
      "验证损失减少 (0.118069 --> 0.115598). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11101 valid_loss: 0.11669 test_loss: 0.13068 \n",
      "[ 21/500] train_loss: 0.11330 valid_loss: 0.11599 test_loss: 0.13073 \n",
      "[ 22/500] train_loss: 0.11242 valid_loss: 0.11843 test_loss: 0.13019 \n",
      "[ 23/500] train_loss: 0.10955 valid_loss: 0.11302 test_loss: 0.12699 \n",
      "验证损失减少 (0.115598 --> 0.113023). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10949 valid_loss: 0.11304 test_loss: 0.12736 \n",
      "[ 25/500] train_loss: 0.10695 valid_loss: 0.11139 test_loss: 0.12458 \n",
      "验证损失减少 (0.113023 --> 0.111393). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10675 valid_loss: 0.11238 test_loss: 0.12629 \n",
      "[ 27/500] train_loss: 0.10339 valid_loss: 0.11147 test_loss: 0.12407 \n",
      "[ 28/500] train_loss: 0.10127 valid_loss: 0.11196 test_loss: 0.12477 \n",
      "[ 29/500] train_loss: 0.10387 valid_loss: 0.11255 test_loss: 0.12652 \n",
      "[ 30/500] train_loss: 0.10422 valid_loss: 0.10833 test_loss: 0.12254 \n",
      "验证损失减少 (0.111393 --> 0.108326). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.10089 valid_loss: 0.10792 test_loss: 0.12272 \n",
      "验证损失减少 (0.108326 --> 0.107918). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.10494 valid_loss: 0.10696 test_loss: 0.11996 \n",
      "验证损失减少 (0.107918 --> 0.106964). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10125 valid_loss: 0.10738 test_loss: 0.12074 \n",
      "[ 34/500] train_loss: 0.09792 valid_loss: 0.10949 test_loss: 0.12145 \n",
      "[ 35/500] train_loss: 0.10116 valid_loss: 0.10575 test_loss: 0.11904 \n",
      "验证损失减少 (0.106964 --> 0.105754). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09976 valid_loss: 0.10727 test_loss: 0.12108 \n",
      "[ 37/500] train_loss: 0.09795 valid_loss: 0.10724 test_loss: 0.11976 \n",
      "[ 38/500] train_loss: 0.09876 valid_loss: 0.10635 test_loss: 0.11770 \n",
      "[ 39/500] train_loss: 0.09945 valid_loss: 0.10545 test_loss: 0.11768 \n",
      "验证损失减少 (0.105754 --> 0.105450). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09772 valid_loss: 0.10593 test_loss: 0.11755 \n",
      "[ 41/500] train_loss: 0.09891 valid_loss: 0.10389 test_loss: 0.11549 \n",
      "验证损失减少 (0.105450 --> 0.103887). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.09668 valid_loss: 0.10422 test_loss: 0.11628 \n",
      "[ 43/500] train_loss: 0.09474 valid_loss: 0.10480 test_loss: 0.11708 \n",
      "[ 44/500] train_loss: 0.09535 valid_loss: 0.10175 test_loss: 0.11513 \n",
      "验证损失减少 (0.103887 --> 0.101749). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.09494 valid_loss: 0.10284 test_loss: 0.11346 \n",
      "[ 46/500] train_loss: 0.09622 valid_loss: 0.10095 test_loss: 0.11388 \n",
      "验证损失减少 (0.101749 --> 0.100947). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.09399 valid_loss: 0.10153 test_loss: 0.11503 \n",
      "[ 48/500] train_loss: 0.09443 valid_loss: 0.10237 test_loss: 0.11270 \n",
      "[ 49/500] train_loss: 0.09077 valid_loss: 0.10023 test_loss: 0.11191 \n",
      "验证损失减少 (0.100947 --> 0.100229). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.09281 valid_loss: 0.10093 test_loss: 0.11150 \n",
      "[ 51/500] train_loss: 0.09357 valid_loss: 0.09825 test_loss: 0.11027 \n",
      "验证损失减少 (0.100229 --> 0.098245). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.09250 valid_loss: 0.09869 test_loss: 0.11111 \n",
      "[ 53/500] train_loss: 0.09137 valid_loss: 0.09964 test_loss: 0.10937 \n",
      "[ 54/500] train_loss: 0.09066 valid_loss: 0.10089 test_loss: 0.11070 \n",
      "[ 55/500] train_loss: 0.09067 valid_loss: 0.09805 test_loss: 0.11101 \n",
      "验证损失减少 (0.098245 --> 0.098055). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.08961 valid_loss: 0.09898 test_loss: 0.10997 \n",
      "[ 57/500] train_loss: 0.09209 valid_loss: 0.09672 test_loss: 0.10863 \n",
      "验证损失减少 (0.098055 --> 0.096721). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.09012 valid_loss: 0.09609 test_loss: 0.10838 \n",
      "验证损失减少 (0.096721 --> 0.096095). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.09090 valid_loss: 0.09820 test_loss: 0.10867 \n",
      "[ 60/500] train_loss: 0.08930 valid_loss: 0.09621 test_loss: 0.10886 \n",
      "[ 61/500] train_loss: 0.09088 valid_loss: 0.09497 test_loss: 0.10730 \n",
      "验证损失减少 (0.096095 --> 0.094966). 正在保存模型...\n",
      "[ 62/500] train_loss: 0.08940 valid_loss: 0.09669 test_loss: 0.10970 \n",
      "[ 63/500] train_loss: 0.09042 valid_loss: 0.09750 test_loss: 0.10757 \n",
      "[ 64/500] train_loss: 0.08796 valid_loss: 0.09394 test_loss: 0.10909 \n",
      "验证损失减少 (0.094966 --> 0.093938). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.08839 valid_loss: 0.09486 test_loss: 0.10637 \n",
      "[ 66/500] train_loss: 0.08859 valid_loss: 0.09612 test_loss: 0.10617 \n",
      "[ 67/500] train_loss: 0.08827 valid_loss: 0.09364 test_loss: 0.10543 \n",
      "验证损失减少 (0.093938 --> 0.093640). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.08652 valid_loss: 0.09563 test_loss: 0.10758 \n",
      "[ 69/500] train_loss: 0.08778 valid_loss: 0.09256 test_loss: 0.10429 \n",
      "验证损失减少 (0.093640 --> 0.092559). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.08532 valid_loss: 0.09556 test_loss: 0.10529 \n",
      "[ 71/500] train_loss: 0.08532 valid_loss: 0.09455 test_loss: 0.10586 \n",
      "[ 72/500] train_loss: 0.08564 valid_loss: 0.09473 test_loss: 0.10698 \n",
      "[ 73/500] train_loss: 0.08495 valid_loss: 0.09206 test_loss: 0.10541 \n",
      "验证损失减少 (0.092559 --> 0.092059). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.08551 valid_loss: 0.09109 test_loss: 0.10288 \n",
      "验证损失减少 (0.092059 --> 0.091090). 正在保存模型...\n",
      "[ 75/500] train_loss: 0.08394 valid_loss: 0.09345 test_loss: 0.10256 \n",
      "[ 76/500] train_loss: 0.08621 valid_loss: 0.09271 test_loss: 0.10272 \n",
      "[ 77/500] train_loss: 0.08417 valid_loss: 0.09432 test_loss: 0.10412 \n",
      "[ 78/500] train_loss: 0.08511 valid_loss: 0.09147 test_loss: 0.10242 \n",
      "[ 79/500] train_loss: 0.08299 valid_loss: 0.09063 test_loss: 0.10329 \n",
      "验证损失减少 (0.091090 --> 0.090631). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.08354 valid_loss: 0.09216 test_loss: 0.10350 \n",
      "[ 81/500] train_loss: 0.08212 valid_loss: 0.08987 test_loss: 0.10163 \n",
      "验证损失减少 (0.090631 --> 0.089865). 正在保存模型...\n",
      "[ 82/500] train_loss: 0.08352 valid_loss: 0.09174 test_loss: 0.10321 \n",
      "[ 83/500] train_loss: 0.08318 valid_loss: 0.09143 test_loss: 0.10219 \n",
      "[ 84/500] train_loss: 0.08368 valid_loss: 0.09025 test_loss: 0.10286 \n",
      "[ 85/500] train_loss: 0.08297 valid_loss: 0.08980 test_loss: 0.10051 \n",
      "验证损失减少 (0.089865 --> 0.089802). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08490 valid_loss: 0.09093 test_loss: 0.10155 \n",
      "[ 87/500] train_loss: 0.08355 valid_loss: 0.09049 test_loss: 0.10292 \n",
      "[ 88/500] train_loss: 0.08347 valid_loss: 0.09347 test_loss: 0.10203 \n",
      "[ 89/500] train_loss: 0.08086 valid_loss: 0.08872 test_loss: 0.10053 \n",
      "验证损失减少 (0.089802 --> 0.088722). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.08147 valid_loss: 0.08956 test_loss: 0.10197 \n",
      "[ 91/500] train_loss: 0.08165 valid_loss: 0.08987 test_loss: 0.10426 \n",
      "[ 92/500] train_loss: 0.08112 valid_loss: 0.08965 test_loss: 0.10023 \n",
      "[ 93/500] train_loss: 0.07930 valid_loss: 0.08859 test_loss: 0.09915 \n",
      "验证损失减少 (0.088722 --> 0.088590). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.07997 valid_loss: 0.08771 test_loss: 0.09986 \n",
      "验证损失减少 (0.088590 --> 0.087705). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.08117 valid_loss: 0.08952 test_loss: 0.10121 \n",
      "[ 96/500] train_loss: 0.08151 valid_loss: 0.08529 test_loss: 0.09891 \n",
      "验证损失减少 (0.087705 --> 0.085291). 正在保存模型...\n",
      "[ 97/500] train_loss: 0.08063 valid_loss: 0.08721 test_loss: 0.09953 \n",
      "[ 98/500] train_loss: 0.07892 valid_loss: 0.08869 test_loss: 0.10168 \n",
      "[ 99/500] train_loss: 0.07956 valid_loss: 0.08673 test_loss: 0.09990 \n",
      "[100/500] train_loss: 0.07828 valid_loss: 0.08886 test_loss: 0.10083 \n",
      "[101/500] train_loss: 0.07975 valid_loss: 0.08905 test_loss: 0.09944 \n",
      "[102/500] train_loss: 0.07989 valid_loss: 0.08820 test_loss: 0.10116 \n",
      "[103/500] train_loss: 0.07943 valid_loss: 0.08716 test_loss: 0.09818 \n",
      "[104/500] train_loss: 0.08167 valid_loss: 0.08781 test_loss: 0.09928 \n",
      "[105/500] train_loss: 0.07938 valid_loss: 0.08469 test_loss: 0.09739 \n",
      "验证损失减少 (0.085291 --> 0.084689). 正在保存模型...\n",
      "[106/500] train_loss: 0.07840 valid_loss: 0.08895 test_loss: 0.10121 \n",
      "[107/500] train_loss: 0.08039 valid_loss: 0.08525 test_loss: 0.09846 \n",
      "[108/500] train_loss: 0.07760 valid_loss: 0.08928 test_loss: 0.10006 \n",
      "[109/500] train_loss: 0.08003 valid_loss: 0.08356 test_loss: 0.09752 \n",
      "验证损失减少 (0.084689 --> 0.083559). 正在保存模型...\n",
      "[110/500] train_loss: 0.07650 valid_loss: 0.08636 test_loss: 0.09807 \n",
      "[111/500] train_loss: 0.07750 valid_loss: 0.08683 test_loss: 0.09777 \n",
      "[112/500] train_loss: 0.07882 valid_loss: 0.08703 test_loss: 0.09783 \n",
      "[113/500] train_loss: 0.07654 valid_loss: 0.08558 test_loss: 0.09824 \n",
      "[114/500] train_loss: 0.07891 valid_loss: 0.08362 test_loss: 0.09643 \n",
      "[115/500] train_loss: 0.07739 valid_loss: 0.08439 test_loss: 0.09790 \n",
      "[116/500] train_loss: 0.07702 valid_loss: 0.08371 test_loss: 0.09644 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117/500] train_loss: 0.07640 valid_loss: 0.08969 test_loss: 0.09704 \n",
      "[118/500] train_loss: 0.07706 valid_loss: 0.08522 test_loss: 0.09747 \n",
      "[119/500] train_loss: 0.07546 valid_loss: 0.08589 test_loss: 0.09736 \n",
      "[120/500] train_loss: 0.07618 valid_loss: 0.08315 test_loss: 0.09529 \n",
      "验证损失减少 (0.083559 --> 0.083154). 正在保存模型...\n",
      "[121/500] train_loss: 0.07681 valid_loss: 0.08599 test_loss: 0.09747 \n",
      "[122/500] train_loss: 0.07408 valid_loss: 0.08352 test_loss: 0.09644 \n",
      "[123/500] train_loss: 0.07605 valid_loss: 0.08332 test_loss: 0.09600 \n",
      "[124/500] train_loss: 0.07637 valid_loss: 0.08483 test_loss: 0.09410 \n",
      "[125/500] train_loss: 0.07669 valid_loss: 0.08331 test_loss: 0.09479 \n",
      "[126/500] train_loss: 0.07623 valid_loss: 0.08594 test_loss: 0.09635 \n",
      "[127/500] train_loss: 0.07633 valid_loss: 0.08286 test_loss: 0.09786 \n",
      "验证损失减少 (0.083154 --> 0.082856). 正在保存模型...\n",
      "[128/500] train_loss: 0.07645 valid_loss: 0.08415 test_loss: 0.09677 \n",
      "[129/500] train_loss: 0.07487 valid_loss: 0.08338 test_loss: 0.09671 \n",
      "[130/500] train_loss: 0.07504 valid_loss: 0.08500 test_loss: 0.09431 \n",
      "[131/500] train_loss: 0.07187 valid_loss: 0.08228 test_loss: 0.09541 \n",
      "验证损失减少 (0.082856 --> 0.082280). 正在保存模型...\n",
      "[132/500] train_loss: 0.07434 valid_loss: 0.08423 test_loss: 0.09623 \n",
      "[133/500] train_loss: 0.07232 valid_loss: 0.08244 test_loss: 0.09417 \n",
      "[134/500] train_loss: 0.07440 valid_loss: 0.08267 test_loss: 0.09653 \n",
      "[135/500] train_loss: 0.07435 valid_loss: 0.08316 test_loss: 0.09634 \n",
      "[136/500] train_loss: 0.07522 valid_loss: 0.08130 test_loss: 0.09432 \n",
      "验证损失减少 (0.082280 --> 0.081303). 正在保存模型...\n",
      "[137/500] train_loss: 0.07321 valid_loss: 0.08352 test_loss: 0.09590 \n",
      "[138/500] train_loss: 0.07449 valid_loss: 0.08224 test_loss: 0.09589 \n",
      "[139/500] train_loss: 0.07515 valid_loss: 0.08141 test_loss: 0.09462 \n",
      "[140/500] train_loss: 0.07324 valid_loss: 0.08239 test_loss: 0.09574 \n",
      "[141/500] train_loss: 0.07207 valid_loss: 0.08146 test_loss: 0.09552 \n",
      "[142/500] train_loss: 0.07229 valid_loss: 0.08287 test_loss: 0.09455 \n",
      "[143/500] train_loss: 0.06989 valid_loss: 0.08289 test_loss: 0.09533 \n",
      "[144/500] train_loss: 0.07587 valid_loss: 0.08175 test_loss: 0.09261 \n",
      "[145/500] train_loss: 0.07686 valid_loss: 0.08127 test_loss: 0.09305 \n",
      "验证损失减少 (0.081303 --> 0.081271). 正在保存模型...\n",
      "[146/500] train_loss: 0.07184 valid_loss: 0.08275 test_loss: 0.09488 \n",
      "[147/500] train_loss: 0.07223 valid_loss: 0.08393 test_loss: 0.09464 \n",
      "[148/500] train_loss: 0.07297 valid_loss: 0.08245 test_loss: 0.09357 \n",
      "[149/500] train_loss: 0.07271 valid_loss: 0.08036 test_loss: 0.09301 \n",
      "验证损失减少 (0.081271 --> 0.080364). 正在保存模型...\n",
      "[150/500] train_loss: 0.07135 valid_loss: 0.08231 test_loss: 0.09390 \n",
      "[151/500] train_loss: 0.07029 valid_loss: 0.08185 test_loss: 0.09368 \n",
      "[152/500] train_loss: 0.07128 valid_loss: 0.07979 test_loss: 0.09162 \n",
      "验证损失减少 (0.080364 --> 0.079789). 正在保存模型...\n",
      "[153/500] train_loss: 0.07208 valid_loss: 0.08097 test_loss: 0.09381 \n",
      "[154/500] train_loss: 0.07072 valid_loss: 0.08132 test_loss: 0.09350 \n",
      "[155/500] train_loss: 0.07277 valid_loss: 0.08131 test_loss: 0.09381 \n",
      "[156/500] train_loss: 0.07207 valid_loss: 0.07951 test_loss: 0.09295 \n",
      "验证损失减少 (0.079789 --> 0.079507). 正在保存模型...\n",
      "[157/500] train_loss: 0.06946 valid_loss: 0.08085 test_loss: 0.09365 \n",
      "[158/500] train_loss: 0.07058 valid_loss: 0.08128 test_loss: 0.09314 \n",
      "[159/500] train_loss: 0.07180 valid_loss: 0.08434 test_loss: 0.09372 \n",
      "[160/500] train_loss: 0.07135 valid_loss: 0.08057 test_loss: 0.09274 \n",
      "[161/500] train_loss: 0.07016 valid_loss: 0.08107 test_loss: 0.09382 \n",
      "[162/500] train_loss: 0.06932 valid_loss: 0.08148 test_loss: 0.09303 \n",
      "[163/500] train_loss: 0.07129 valid_loss: 0.08160 test_loss: 0.09180 \n",
      "[164/500] train_loss: 0.07181 valid_loss: 0.08031 test_loss: 0.09183 \n",
      "[165/500] train_loss: 0.07159 valid_loss: 0.08048 test_loss: 0.09387 \n",
      "[166/500] train_loss: 0.06987 valid_loss: 0.08112 test_loss: 0.09367 \n",
      "[167/500] train_loss: 0.07231 valid_loss: 0.08058 test_loss: 0.09382 \n",
      "[168/500] train_loss: 0.07191 valid_loss: 0.08172 test_loss: 0.09255 \n",
      "[169/500] train_loss: 0.06841 valid_loss: 0.08005 test_loss: 0.09204 \n",
      "[170/500] train_loss: 0.07105 valid_loss: 0.08084 test_loss: 0.09134 \n",
      "[171/500] train_loss: 0.06974 valid_loss: 0.07971 test_loss: 0.09253 \n",
      "[172/500] train_loss: 0.07003 valid_loss: 0.07975 test_loss: 0.09265 \n",
      "[173/500] train_loss: 0.06941 valid_loss: 0.08141 test_loss: 0.09320 \n",
      "[174/500] train_loss: 0.06921 valid_loss: 0.08088 test_loss: 0.09269 \n",
      "[175/500] train_loss: 0.07097 valid_loss: 0.08335 test_loss: 0.09138 \n",
      "[176/500] train_loss: 0.06849 valid_loss: 0.07962 test_loss: 0.09100 \n",
      "[177/500] train_loss: 0.06984 valid_loss: 0.08175 test_loss: 0.09463 \n",
      "[178/500] train_loss: 0.07034 valid_loss: 0.07720 test_loss: 0.09057 \n",
      "验证损失减少 (0.079507 --> 0.077203). 正在保存模型...\n",
      "[179/500] train_loss: 0.06957 valid_loss: 0.07884 test_loss: 0.09139 \n",
      "[180/500] train_loss: 0.06889 valid_loss: 0.08011 test_loss: 0.09258 \n",
      "[181/500] train_loss: 0.06994 valid_loss: 0.08054 test_loss: 0.09191 \n",
      "[182/500] train_loss: 0.06762 valid_loss: 0.08128 test_loss: 0.09228 \n",
      "[183/500] train_loss: 0.06797 valid_loss: 0.08010 test_loss: 0.09129 \n",
      "[184/500] train_loss: 0.06880 valid_loss: 0.08165 test_loss: 0.09130 \n",
      "[185/500] train_loss: 0.06763 valid_loss: 0.08345 test_loss: 0.09224 \n",
      "[186/500] train_loss: 0.06835 valid_loss: 0.07916 test_loss: 0.09016 \n",
      "[187/500] train_loss: 0.06780 valid_loss: 0.08073 test_loss: 0.09120 \n",
      "[188/500] train_loss: 0.06940 valid_loss: 0.08186 test_loss: 0.09017 \n",
      "[189/500] train_loss: 0.06642 valid_loss: 0.08262 test_loss: 0.08931 \n",
      "[190/500] train_loss: 0.06723 valid_loss: 0.08506 test_loss: 0.09140 \n",
      "[191/500] train_loss: 0.06816 valid_loss: 0.07867 test_loss: 0.09027 \n",
      "[192/500] train_loss: 0.06695 valid_loss: 0.08052 test_loss: 0.09011 \n",
      "[193/500] train_loss: 0.06782 valid_loss: 0.07713 test_loss: 0.09019 \n",
      "验证损失减少 (0.077203 --> 0.077130). 正在保存模型...\n",
      "[194/500] train_loss: 0.06774 valid_loss: 0.07879 test_loss: 0.09045 \n",
      "[195/500] train_loss: 0.06852 valid_loss: 0.07790 test_loss: 0.09099 \n",
      "[196/500] train_loss: 0.06727 valid_loss: 0.07994 test_loss: 0.09175 \n",
      "[197/500] train_loss: 0.06832 valid_loss: 0.07774 test_loss: 0.09066 \n",
      "[198/500] train_loss: 0.06776 valid_loss: 0.07819 test_loss: 0.09055 \n",
      "[199/500] train_loss: 0.06854 valid_loss: 0.07654 test_loss: 0.08984 \n",
      "验证损失减少 (0.077130 --> 0.076545). 正在保存模型...\n",
      "[200/500] train_loss: 0.06688 valid_loss: 0.07772 test_loss: 0.09176 \n",
      "[201/500] train_loss: 0.06505 valid_loss: 0.07859 test_loss: 0.09038 \n",
      "[202/500] train_loss: 0.06812 valid_loss: 0.08264 test_loss: 0.09023 \n",
      "[203/500] train_loss: 0.06891 valid_loss: 0.08084 test_loss: 0.09065 \n",
      "[204/500] train_loss: 0.06765 valid_loss: 0.07983 test_loss: 0.09199 \n",
      "[205/500] train_loss: 0.06568 valid_loss: 0.07880 test_loss: 0.09217 \n",
      "[206/500] train_loss: 0.06780 valid_loss: 0.07956 test_loss: 0.09273 \n",
      "[207/500] train_loss: 0.06674 valid_loss: 0.07868 test_loss: 0.09071 \n",
      "[208/500] train_loss: 0.06456 valid_loss: 0.07827 test_loss: 0.09033 \n",
      "[209/500] train_loss: 0.06674 valid_loss: 0.07839 test_loss: 0.09090 \n",
      "[210/500] train_loss: 0.06631 valid_loss: 0.07783 test_loss: 0.09194 \n",
      "[211/500] train_loss: 0.06713 valid_loss: 0.07616 test_loss: 0.09074 \n",
      "验证损失减少 (0.076545 --> 0.076157). 正在保存模型...\n",
      "[212/500] train_loss: 0.06721 valid_loss: 0.08141 test_loss: 0.09060 \n",
      "[213/500] train_loss: 0.06742 valid_loss: 0.07844 test_loss: 0.08993 \n",
      "[214/500] train_loss: 0.06552 valid_loss: 0.07716 test_loss: 0.09086 \n",
      "[215/500] train_loss: 0.06510 valid_loss: 0.07756 test_loss: 0.08992 \n",
      "[216/500] train_loss: 0.06379 valid_loss: 0.08042 test_loss: 0.09109 \n",
      "[217/500] train_loss: 0.06465 valid_loss: 0.08092 test_loss: 0.08999 \n",
      "[218/500] train_loss: 0.06562 valid_loss: 0.07832 test_loss: 0.09072 \n",
      "[219/500] train_loss: 0.06728 valid_loss: 0.07807 test_loss: 0.08952 \n",
      "[220/500] train_loss: 0.06752 valid_loss: 0.07799 test_loss: 0.08928 \n",
      "[221/500] train_loss: 0.06615 valid_loss: 0.07701 test_loss: 0.09039 \n",
      "[222/500] train_loss: 0.06506 valid_loss: 0.07721 test_loss: 0.08946 \n",
      "[223/500] train_loss: 0.06464 valid_loss: 0.07834 test_loss: 0.09143 \n",
      "[224/500] train_loss: 0.06611 valid_loss: 0.07729 test_loss: 0.09083 \n",
      "[225/500] train_loss: 0.06365 valid_loss: 0.07704 test_loss: 0.08999 \n",
      "[226/500] train_loss: 0.06498 valid_loss: 0.07725 test_loss: 0.09067 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[227/500] train_loss: 0.06502 valid_loss: 0.07844 test_loss: 0.09035 \n",
      "[228/500] train_loss: 0.06596 valid_loss: 0.07778 test_loss: 0.08998 \n",
      "[229/500] train_loss: 0.06719 valid_loss: 0.07753 test_loss: 0.09073 \n",
      "[230/500] train_loss: 0.06482 valid_loss: 0.07637 test_loss: 0.08897 \n",
      "[231/500] train_loss: 0.06522 valid_loss: 0.07779 test_loss: 0.09052 \n",
      "[232/500] train_loss: 0.06616 valid_loss: 0.08715 test_loss: 0.09084 \n",
      "[233/500] train_loss: 0.06547 valid_loss: 0.08142 test_loss: 0.09087 \n",
      "[234/500] train_loss: 0.06405 valid_loss: 0.07584 test_loss: 0.08861 \n",
      "验证损失减少 (0.076157 --> 0.075841). 正在保存模型...\n",
      "[235/500] train_loss: 0.06517 valid_loss: 0.07692 test_loss: 0.08893 \n",
      "[236/500] train_loss: 0.06564 valid_loss: 0.07751 test_loss: 0.08947 \n",
      "[237/500] train_loss: 0.06395 valid_loss: 0.07707 test_loss: 0.08942 \n",
      "[238/500] train_loss: 0.06561 valid_loss: 0.07670 test_loss: 0.09085 \n",
      "[239/500] train_loss: 0.06308 valid_loss: 0.07897 test_loss: 0.09007 \n",
      "[240/500] train_loss: 0.06243 valid_loss: 0.07791 test_loss: 0.08983 \n",
      "[241/500] train_loss: 0.06362 valid_loss: 0.07676 test_loss: 0.09028 \n",
      "[242/500] train_loss: 0.06380 valid_loss: 0.07791 test_loss: 0.08931 \n",
      "[243/500] train_loss: 0.06532 valid_loss: 0.07693 test_loss: 0.08738 \n",
      "[244/500] train_loss: 0.06329 valid_loss: 0.07663 test_loss: 0.08907 \n",
      "[245/500] train_loss: 0.06226 valid_loss: 0.07808 test_loss: 0.08892 \n",
      "[246/500] train_loss: 0.06273 valid_loss: 0.07690 test_loss: 0.08963 \n",
      "[247/500] train_loss: 0.06424 valid_loss: 0.07777 test_loss: 0.09103 \n",
      "[248/500] train_loss: 0.06236 valid_loss: 0.07712 test_loss: 0.08939 \n",
      "[249/500] train_loss: 0.06283 valid_loss: 0.07873 test_loss: 0.08987 \n",
      "[250/500] train_loss: 0.06242 valid_loss: 0.07806 test_loss: 0.08842 \n",
      "[251/500] train_loss: 0.06189 valid_loss: 0.07693 test_loss: 0.08799 \n",
      "[252/500] train_loss: 0.06337 valid_loss: 0.07865 test_loss: 0.08965 \n",
      "[253/500] train_loss: 0.06578 valid_loss: 0.07651 test_loss: 0.08864 \n",
      "[254/500] train_loss: 0.06293 valid_loss: 0.08063 test_loss: 0.08857 \n",
      "[255/500] train_loss: 0.06164 valid_loss: 0.07683 test_loss: 0.08724 \n",
      "[256/500] train_loss: 0.06190 valid_loss: 0.07754 test_loss: 0.08747 \n",
      "[257/500] train_loss: 0.06216 valid_loss: 0.07681 test_loss: 0.08797 \n",
      "[258/500] train_loss: 0.06270 valid_loss: 0.07735 test_loss: 0.08972 \n",
      "[259/500] train_loss: 0.06336 valid_loss: 0.07609 test_loss: 0.08878 \n",
      "[260/500] train_loss: 0.06071 valid_loss: 0.07551 test_loss: 0.08782 \n",
      "验证损失减少 (0.075841 --> 0.075515). 正在保存模型...\n",
      "[261/500] train_loss: 0.06388 valid_loss: 0.07640 test_loss: 0.08988 \n",
      "[262/500] train_loss: 0.06281 valid_loss: 0.07742 test_loss: 0.09141 \n",
      "[263/500] train_loss: 0.06163 valid_loss: 0.07698 test_loss: 0.08921 \n",
      "[264/500] train_loss: 0.06244 valid_loss: 0.07645 test_loss: 0.08993 \n",
      "[265/500] train_loss: 0.06184 valid_loss: 0.07850 test_loss: 0.08911 \n",
      "[266/500] train_loss: 0.06258 valid_loss: 0.07652 test_loss: 0.08917 \n",
      "[267/500] train_loss: 0.06206 valid_loss: 0.07648 test_loss: 0.08905 \n",
      "[268/500] train_loss: 0.06206 valid_loss: 0.08073 test_loss: 0.08947 \n",
      "[269/500] train_loss: 0.06224 valid_loss: 0.07735 test_loss: 0.09018 \n",
      "[270/500] train_loss: 0.06334 valid_loss: 0.07595 test_loss: 0.08791 \n",
      "[271/500] train_loss: 0.06240 valid_loss: 0.08921 test_loss: 0.08827 \n",
      "[272/500] train_loss: 0.06378 valid_loss: 0.07997 test_loss: 0.08742 \n",
      "[273/500] train_loss: 0.06078 valid_loss: 0.07622 test_loss: 0.08793 \n",
      "[274/500] train_loss: 0.06132 valid_loss: 0.07771 test_loss: 0.08774 \n",
      "[275/500] train_loss: 0.06493 valid_loss: 0.07679 test_loss: 0.08909 \n",
      "[276/500] train_loss: 0.06202 valid_loss: 0.07545 test_loss: 0.08985 \n",
      "验证损失减少 (0.075515 --> 0.075454). 正在保存模型...\n",
      "[277/500] train_loss: 0.06250 valid_loss: 0.07505 test_loss: 0.08856 \n",
      "验证损失减少 (0.075454 --> 0.075048). 正在保存模型...\n",
      "[278/500] train_loss: 0.06217 valid_loss: 0.07510 test_loss: 0.08721 \n",
      "[279/500] train_loss: 0.06112 valid_loss: 0.07617 test_loss: 0.08826 \n",
      "[280/500] train_loss: 0.06164 valid_loss: 0.07634 test_loss: 0.08766 \n",
      "[281/500] train_loss: 0.05894 valid_loss: 0.07650 test_loss: 0.08884 \n",
      "[282/500] train_loss: 0.06167 valid_loss: 0.08072 test_loss: 0.08811 \n",
      "[283/500] train_loss: 0.06007 valid_loss: 0.07587 test_loss: 0.08828 \n",
      "[284/500] train_loss: 0.06066 valid_loss: 0.07951 test_loss: 0.08809 \n",
      "[285/500] train_loss: 0.06060 valid_loss: 0.07752 test_loss: 0.08921 \n",
      "[286/500] train_loss: 0.06224 valid_loss: 0.07607 test_loss: 0.08858 \n",
      "[287/500] train_loss: 0.06210 valid_loss: 0.07734 test_loss: 0.09042 \n",
      "[288/500] train_loss: 0.06150 valid_loss: 0.07649 test_loss: 0.09060 \n",
      "[289/500] train_loss: 0.06266 valid_loss: 0.07986 test_loss: 0.08917 \n",
      "[290/500] train_loss: 0.06099 valid_loss: 0.07423 test_loss: 0.08797 \n",
      "验证损失减少 (0.075048 --> 0.074229). 正在保存模型...\n",
      "[291/500] train_loss: 0.06129 valid_loss: 0.07513 test_loss: 0.08829 \n",
      "[292/500] train_loss: 0.06126 valid_loss: 0.07640 test_loss: 0.08754 \n",
      "[293/500] train_loss: 0.06153 valid_loss: 0.07465 test_loss: 0.08799 \n",
      "[294/500] train_loss: 0.06165 valid_loss: 0.07575 test_loss: 0.08764 \n",
      "[295/500] train_loss: 0.05971 valid_loss: 0.07505 test_loss: 0.08878 \n",
      "[296/500] train_loss: 0.06045 valid_loss: 0.08295 test_loss: 0.08750 \n",
      "[297/500] train_loss: 0.05991 valid_loss: 0.07757 test_loss: 0.08895 \n",
      "[298/500] train_loss: 0.06010 valid_loss: 0.07515 test_loss: 0.08785 \n",
      "[299/500] train_loss: 0.06150 valid_loss: 0.07409 test_loss: 0.08808 \n",
      "验证损失减少 (0.074229 --> 0.074086). 正在保存模型...\n",
      "[300/500] train_loss: 0.06032 valid_loss: 0.07521 test_loss: 0.08779 \n",
      "[301/500] train_loss: 0.06021 valid_loss: 0.07561 test_loss: 0.08853 \n",
      "[302/500] train_loss: 0.06014 valid_loss: 0.07540 test_loss: 0.08773 \n",
      "[303/500] train_loss: 0.06160 valid_loss: 0.07846 test_loss: 0.08830 \n",
      "[304/500] train_loss: 0.05899 valid_loss: 0.07522 test_loss: 0.08719 \n",
      "[305/500] train_loss: 0.05901 valid_loss: 0.07903 test_loss: 0.08767 \n",
      "[306/500] train_loss: 0.06063 valid_loss: 0.07461 test_loss: 0.08704 \n",
      "[307/500] train_loss: 0.06023 valid_loss: 0.07598 test_loss: 0.08785 \n",
      "[308/500] train_loss: 0.05980 valid_loss: 0.07976 test_loss: 0.08845 \n",
      "[309/500] train_loss: 0.05948 valid_loss: 0.07556 test_loss: 0.08827 \n",
      "[310/500] train_loss: 0.06076 valid_loss: 0.07878 test_loss: 0.08766 \n",
      "[311/500] train_loss: 0.05957 valid_loss: 0.07632 test_loss: 0.08863 \n",
      "[312/500] train_loss: 0.06108 valid_loss: 0.07396 test_loss: 0.08839 \n",
      "验证损失减少 (0.074086 --> 0.073959). 正在保存模型...\n",
      "[313/500] train_loss: 0.05873 valid_loss: 0.07665 test_loss: 0.08928 \n",
      "[314/500] train_loss: 0.06335 valid_loss: 0.08504 test_loss: 0.08849 \n",
      "[315/500] train_loss: 0.06077 valid_loss: 0.07563 test_loss: 0.08704 \n",
      "[316/500] train_loss: 0.05867 valid_loss: 0.07488 test_loss: 0.08767 \n",
      "[317/500] train_loss: 0.05926 valid_loss: 0.07532 test_loss: 0.08874 \n",
      "[318/500] train_loss: 0.05965 valid_loss: 0.07607 test_loss: 0.08815 \n",
      "[319/500] train_loss: 0.06015 valid_loss: 0.07395 test_loss: 0.08784 \n",
      "验证损失减少 (0.073959 --> 0.073953). 正在保存模型...\n",
      "[320/500] train_loss: 0.05815 valid_loss: 0.07543 test_loss: 0.08847 \n",
      "[321/500] train_loss: 0.05849 valid_loss: 0.07596 test_loss: 0.08785 \n",
      "[322/500] train_loss: 0.06050 valid_loss: 0.07535 test_loss: 0.08877 \n",
      "[323/500] train_loss: 0.06040 valid_loss: 0.07828 test_loss: 0.08779 \n",
      "[324/500] train_loss: 0.05756 valid_loss: 0.08031 test_loss: 0.08761 \n",
      "[325/500] train_loss: 0.05812 valid_loss: 0.07523 test_loss: 0.08924 \n",
      "[326/500] train_loss: 0.05937 valid_loss: 0.07624 test_loss: 0.08893 \n",
      "[327/500] train_loss: 0.05993 valid_loss: 0.07408 test_loss: 0.08693 \n",
      "[328/500] train_loss: 0.05890 valid_loss: 0.07486 test_loss: 0.08821 \n",
      "[329/500] train_loss: 0.06043 valid_loss: 0.07450 test_loss: 0.08871 \n",
      "[330/500] train_loss: 0.05900 valid_loss: 0.07435 test_loss: 0.08826 \n",
      "[331/500] train_loss: 0.05810 valid_loss: 0.07691 test_loss: 0.08918 \n",
      "[332/500] train_loss: 0.05961 valid_loss: 0.07412 test_loss: 0.08812 \n",
      "[333/500] train_loss: 0.05896 valid_loss: 0.08058 test_loss: 0.08887 \n",
      "[334/500] train_loss: 0.05879 valid_loss: 0.07488 test_loss: 0.08756 \n",
      "[335/500] train_loss: 0.05902 valid_loss: 0.07948 test_loss: 0.08904 \n",
      "[336/500] train_loss: 0.05755 valid_loss: 0.08572 test_loss: 0.08775 \n",
      "[337/500] train_loss: 0.05857 valid_loss: 0.07666 test_loss: 0.08739 \n",
      "[338/500] train_loss: 0.05831 valid_loss: 0.07656 test_loss: 0.08793 \n",
      "[339/500] train_loss: 0.05697 valid_loss: 0.07832 test_loss: 0.08753 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[340/500] train_loss: 0.05702 valid_loss: 0.07802 test_loss: 0.08669 \n",
      "[341/500] train_loss: 0.05709 valid_loss: 0.07656 test_loss: 0.08954 \n",
      "[342/500] train_loss: 0.05898 valid_loss: 0.07801 test_loss: 0.08891 \n",
      "[343/500] train_loss: 0.05764 valid_loss: 0.08070 test_loss: 0.08945 \n",
      "[344/500] train_loss: 0.05828 valid_loss: 0.08317 test_loss: 0.08806 \n",
      "[345/500] train_loss: 0.05792 valid_loss: 0.07660 test_loss: 0.08854 \n",
      "[346/500] train_loss: 0.05788 valid_loss: 0.08001 test_loss: 0.08930 \n",
      "[347/500] train_loss: 0.05876 valid_loss: 0.07820 test_loss: 0.08825 \n",
      "[348/500] train_loss: 0.05778 valid_loss: 0.08272 test_loss: 0.08920 \n",
      "[349/500] train_loss: 0.05843 valid_loss: 0.08456 test_loss: 0.08927 \n",
      "[350/500] train_loss: 0.05592 valid_loss: 0.07549 test_loss: 0.08952 \n",
      "[351/500] train_loss: 0.05894 valid_loss: 0.08193 test_loss: 0.08740 \n",
      "[352/500] train_loss: 0.05778 valid_loss: 0.07731 test_loss: 0.08686 \n",
      "[353/500] train_loss: 0.05716 valid_loss: 0.07524 test_loss: 0.08882 \n",
      "[354/500] train_loss: 0.05906 valid_loss: 0.07493 test_loss: 0.08827 \n",
      "[355/500] train_loss: 0.05769 valid_loss: 0.07440 test_loss: 0.08910 \n",
      "[356/500] train_loss: 0.05726 valid_loss: 0.07508 test_loss: 0.08761 \n",
      "[357/500] train_loss: 0.05885 valid_loss: 0.07459 test_loss: 0.08772 \n",
      "[358/500] train_loss: 0.05695 valid_loss: 0.07626 test_loss: 0.08965 \n",
      "[359/500] train_loss: 0.05883 valid_loss: 0.07351 test_loss: 0.08781 \n",
      "验证损失减少 (0.073953 --> 0.073508). 正在保存模型...\n",
      "[360/500] train_loss: 0.05659 valid_loss: 0.07377 test_loss: 0.08732 \n",
      "[361/500] train_loss: 0.05709 valid_loss: 0.07523 test_loss: 0.08835 \n",
      "[362/500] train_loss: 0.05837 valid_loss: 0.07610 test_loss: 0.08801 \n",
      "[363/500] train_loss: 0.05617 valid_loss: 0.07537 test_loss: 0.08985 \n",
      "[364/500] train_loss: 0.05689 valid_loss: 0.07634 test_loss: 0.08913 \n",
      "[365/500] train_loss: 0.05735 valid_loss: 0.07555 test_loss: 0.08887 \n",
      "[366/500] train_loss: 0.05805 valid_loss: 0.07690 test_loss: 0.09047 \n",
      "[367/500] train_loss: 0.05701 valid_loss: 0.07528 test_loss: 0.08778 \n",
      "[368/500] train_loss: 0.05728 valid_loss: 0.07979 test_loss: 0.08807 \n",
      "[369/500] train_loss: 0.05754 valid_loss: 0.09041 test_loss: 0.08820 \n",
      "[370/500] train_loss: 0.05815 valid_loss: 0.08074 test_loss: 0.08677 \n",
      "[371/500] train_loss: 0.05620 valid_loss: 0.07701 test_loss: 0.08742 \n",
      "[372/500] train_loss: 0.05628 valid_loss: 0.07617 test_loss: 0.08709 \n",
      "[373/500] train_loss: 0.05675 valid_loss: 0.07464 test_loss: 0.08862 \n",
      "[374/500] train_loss: 0.05835 valid_loss: 0.07305 test_loss: 0.08642 \n",
      "验证损失减少 (0.073508 --> 0.073054). 正在保存模型...\n",
      "[375/500] train_loss: 0.05605 valid_loss: 0.07440 test_loss: 0.08755 \n",
      "[376/500] train_loss: 0.05710 valid_loss: 0.08289 test_loss: 0.08700 \n",
      "[377/500] train_loss: 0.05582 valid_loss: 0.07795 test_loss: 0.08835 \n",
      "[378/500] train_loss: 0.05608 valid_loss: 0.07923 test_loss: 0.08800 \n",
      "[379/500] train_loss: 0.05502 valid_loss: 0.07507 test_loss: 0.08746 \n",
      "[380/500] train_loss: 0.05644 valid_loss: 0.08852 test_loss: 0.08753 \n",
      "[381/500] train_loss: 0.05625 valid_loss: 0.08069 test_loss: 0.08845 \n",
      "[382/500] train_loss: 0.05697 valid_loss: 0.07982 test_loss: 0.08713 \n",
      "[383/500] train_loss: 0.05622 valid_loss: 0.07553 test_loss: 0.08820 \n",
      "[384/500] train_loss: 0.05588 valid_loss: 0.08052 test_loss: 0.08651 \n",
      "[385/500] train_loss: 0.05538 valid_loss: 0.07610 test_loss: 0.08677 \n",
      "[386/500] train_loss: 0.05541 valid_loss: 0.08000 test_loss: 0.08760 \n",
      "[387/500] train_loss: 0.05583 valid_loss: 0.08582 test_loss: 0.08747 \n",
      "[388/500] train_loss: 0.05539 valid_loss: 0.08364 test_loss: 0.08861 \n",
      "[389/500] train_loss: 0.05657 valid_loss: 0.08634 test_loss: 0.08794 \n",
      "[390/500] train_loss: 0.05567 valid_loss: 0.07839 test_loss: 0.08698 \n",
      "[391/500] train_loss: 0.05477 valid_loss: 0.08211 test_loss: 0.08775 \n",
      "[392/500] train_loss: 0.05578 valid_loss: 0.08967 test_loss: 0.08601 \n",
      "[393/500] train_loss: 0.05626 valid_loss: 0.08721 test_loss: 0.08773 \n",
      "[394/500] train_loss: 0.05599 valid_loss: 0.08233 test_loss: 0.08664 \n",
      "[395/500] train_loss: 0.05705 valid_loss: 0.08708 test_loss: 0.08735 \n",
      "[396/500] train_loss: 0.05471 valid_loss: 0.09132 test_loss: 0.08706 \n",
      "[397/500] train_loss: 0.05618 valid_loss: 0.07995 test_loss: 0.08686 \n",
      "[398/500] train_loss: 0.05653 valid_loss: 0.07708 test_loss: 0.08786 \n",
      "[399/500] train_loss: 0.05651 valid_loss: 0.08330 test_loss: 0.08820 \n",
      "[400/500] train_loss: 0.05510 valid_loss: 0.08190 test_loss: 0.08764 \n",
      "[401/500] train_loss: 0.05633 valid_loss: 0.08217 test_loss: 0.08802 \n",
      "[402/500] train_loss: 0.05587 valid_loss: 0.09339 test_loss: 0.08823 \n",
      "[403/500] train_loss: 0.05669 valid_loss: 0.07912 test_loss: 0.08834 \n",
      "[404/500] train_loss: 0.05412 valid_loss: 0.07841 test_loss: 0.08798 \n",
      "[405/500] train_loss: 0.05460 valid_loss: 0.07924 test_loss: 0.08671 \n",
      "[406/500] train_loss: 0.05539 valid_loss: 0.07757 test_loss: 0.08765 \n",
      "[407/500] train_loss: 0.05564 valid_loss: 0.07511 test_loss: 0.08869 \n",
      "[408/500] train_loss: 0.05412 valid_loss: 0.08329 test_loss: 0.08711 \n",
      "[409/500] train_loss: 0.05518 valid_loss: 0.08379 test_loss: 0.08773 \n",
      "[410/500] train_loss: 0.05594 valid_loss: 0.08845 test_loss: 0.08658 \n",
      "[411/500] train_loss: 0.05553 valid_loss: 0.07767 test_loss: 0.08569 \n",
      "[412/500] train_loss: 0.05735 valid_loss: 0.08717 test_loss: 0.08779 \n",
      "[413/500] train_loss: 0.05673 valid_loss: 0.08344 test_loss: 0.08792 \n",
      "[414/500] train_loss: 0.05691 valid_loss: 0.08312 test_loss: 0.08696 \n",
      "[415/500] train_loss: 0.05585 valid_loss: 0.09348 test_loss: 0.08825 \n",
      "[416/500] train_loss: 0.05582 valid_loss: 0.07688 test_loss: 0.08707 \n",
      "[417/500] train_loss: 0.05479 valid_loss: 0.07609 test_loss: 0.08854 \n",
      "[418/500] train_loss: 0.05540 valid_loss: 0.07496 test_loss: 0.08868 \n",
      "[419/500] train_loss: 0.05490 valid_loss: 0.07677 test_loss: 0.08759 \n",
      "[420/500] train_loss: 0.05538 valid_loss: 0.08215 test_loss: 0.08680 \n",
      "[421/500] train_loss: 0.05529 valid_loss: 0.07919 test_loss: 0.08725 \n",
      "[422/500] train_loss: 0.05564 valid_loss: 0.07557 test_loss: 0.08756 \n",
      "[423/500] train_loss: 0.05390 valid_loss: 0.07655 test_loss: 0.08769 \n",
      "[424/500] train_loss: 0.05448 valid_loss: 0.07432 test_loss: 0.08708 \n",
      "[425/500] train_loss: 0.05417 valid_loss: 0.07796 test_loss: 0.08952 \n",
      "[426/500] train_loss: 0.05296 valid_loss: 0.07459 test_loss: 0.08871 \n",
      "[427/500] train_loss: 0.05473 valid_loss: 0.07430 test_loss: 0.08691 \n",
      "[428/500] train_loss: 0.05564 valid_loss: 0.07449 test_loss: 0.08812 \n",
      "[429/500] train_loss: 0.05622 valid_loss: 0.07631 test_loss: 0.08775 \n",
      "[430/500] train_loss: 0.05403 valid_loss: 0.07468 test_loss: 0.08760 \n",
      "[431/500] train_loss: 0.05359 valid_loss: 0.07709 test_loss: 0.08749 \n",
      "[432/500] train_loss: 0.05415 valid_loss: 0.07637 test_loss: 0.08719 \n",
      "[433/500] train_loss: 0.05533 valid_loss: 0.07535 test_loss: 0.08694 \n",
      "[434/500] train_loss: 0.05298 valid_loss: 0.08222 test_loss: 0.08795 \n",
      "[435/500] train_loss: 0.05342 valid_loss: 0.08884 test_loss: 0.08752 \n",
      "[436/500] train_loss: 0.05288 valid_loss: 0.07658 test_loss: 0.08791 \n",
      "[437/500] train_loss: 0.05485 valid_loss: 0.07694 test_loss: 0.08682 \n",
      "[438/500] train_loss: 0.05329 valid_loss: 0.07792 test_loss: 0.08888 \n",
      "[439/500] train_loss: 0.05283 valid_loss: 0.08190 test_loss: 0.08822 \n",
      "[440/500] train_loss: 0.05508 valid_loss: 0.08007 test_loss: 0.08774 \n",
      "[441/500] train_loss: 0.05449 valid_loss: 0.08198 test_loss: 0.08787 \n",
      "[442/500] train_loss: 0.05405 valid_loss: 0.07965 test_loss: 0.08910 \n",
      "[443/500] train_loss: 0.05435 valid_loss: 0.09342 test_loss: 0.08928 \n",
      "[444/500] train_loss: 0.05311 valid_loss: 0.08030 test_loss: 0.08745 \n",
      "[445/500] train_loss: 0.05415 valid_loss: 0.08586 test_loss: 0.08812 \n",
      "[446/500] train_loss: 0.05384 valid_loss: 0.08257 test_loss: 0.08781 \n",
      "[447/500] train_loss: 0.05242 valid_loss: 0.08224 test_loss: 0.08897 \n",
      "[448/500] train_loss: 0.05162 valid_loss: 0.08375 test_loss: 0.08822 \n",
      "[449/500] train_loss: 0.05374 valid_loss: 0.07531 test_loss: 0.08656 \n",
      "[450/500] train_loss: 0.05314 valid_loss: 0.08132 test_loss: 0.08647 \n",
      "[451/500] train_loss: 0.05479 valid_loss: 0.07444 test_loss: 0.08712 \n",
      "[452/500] train_loss: 0.05322 valid_loss: 0.08155 test_loss: 0.08777 \n",
      "[453/500] train_loss: 0.05238 valid_loss: 0.07770 test_loss: 0.08773 \n",
      "[454/500] train_loss: 0.05273 valid_loss: 0.07423 test_loss: 0.08828 \n",
      "[455/500] train_loss: 0.05297 valid_loss: 0.07352 test_loss: 0.08681 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[456/500] train_loss: 0.05406 valid_loss: 0.07282 test_loss: 0.08812 \n",
      "验证损失减少 (0.073054 --> 0.072821). 正在保存模型...\n",
      "[457/500] train_loss: 0.05397 valid_loss: 0.07667 test_loss: 0.08681 \n",
      "[458/500] train_loss: 0.05130 valid_loss: 0.07335 test_loss: 0.08731 \n",
      "[459/500] train_loss: 0.05374 valid_loss: 0.07348 test_loss: 0.08701 \n",
      "[460/500] train_loss: 0.05271 valid_loss: 0.07409 test_loss: 0.08811 \n",
      "[461/500] train_loss: 0.05161 valid_loss: 0.07861 test_loss: 0.08810 \n",
      "[462/500] train_loss: 0.05321 valid_loss: 0.07573 test_loss: 0.08738 \n",
      "[463/500] train_loss: 0.05344 valid_loss: 0.07368 test_loss: 0.08654 \n",
      "[464/500] train_loss: 0.05255 valid_loss: 0.07605 test_loss: 0.08759 \n",
      "[465/500] train_loss: 0.05262 valid_loss: 0.07525 test_loss: 0.08812 \n",
      "[466/500] train_loss: 0.05372 valid_loss: 0.07375 test_loss: 0.08611 \n",
      "[467/500] train_loss: 0.05384 valid_loss: 0.07623 test_loss: 0.08755 \n",
      "[468/500] train_loss: 0.05351 valid_loss: 0.07452 test_loss: 0.08664 \n",
      "[469/500] train_loss: 0.05392 valid_loss: 0.07330 test_loss: 0.08644 \n",
      "[470/500] train_loss: 0.05436 valid_loss: 0.08148 test_loss: 0.08676 \n",
      "[471/500] train_loss: 0.05232 valid_loss: 0.07530 test_loss: 0.08605 \n",
      "[472/500] train_loss: 0.05139 valid_loss: 0.07426 test_loss: 0.08885 \n",
      "[473/500] train_loss: 0.05157 valid_loss: 0.07369 test_loss: 0.08682 \n",
      "[474/500] train_loss: 0.05295 valid_loss: 0.07409 test_loss: 0.08702 \n",
      "[475/500] train_loss: 0.05415 valid_loss: 0.07379 test_loss: 0.08590 \n",
      "[476/500] train_loss: 0.05303 valid_loss: 0.07358 test_loss: 0.08745 \n",
      "[477/500] train_loss: 0.05307 valid_loss: 0.07428 test_loss: 0.08813 \n",
      "[478/500] train_loss: 0.05218 valid_loss: 0.07433 test_loss: 0.08780 \n",
      "[479/500] train_loss: 0.05277 valid_loss: 0.07499 test_loss: 0.08940 \n",
      "[480/500] train_loss: 0.05144 valid_loss: 0.07410 test_loss: 0.08836 \n",
      "[481/500] train_loss: 0.05330 valid_loss: 0.07420 test_loss: 0.08896 \n",
      "[482/500] train_loss: 0.05036 valid_loss: 0.07438 test_loss: 0.08889 \n",
      "[483/500] train_loss: 0.05141 valid_loss: 0.07385 test_loss: 0.08698 \n",
      "[484/500] train_loss: 0.05347 valid_loss: 0.07361 test_loss: 0.08722 \n",
      "[485/500] train_loss: 0.05264 valid_loss: 0.07367 test_loss: 0.08777 \n",
      "[486/500] train_loss: 0.05119 valid_loss: 0.07505 test_loss: 0.08965 \n",
      "[487/500] train_loss: 0.05245 valid_loss: 0.07433 test_loss: 0.08919 \n",
      "[488/500] train_loss: 0.05145 valid_loss: 0.07295 test_loss: 0.08685 \n",
      "[489/500] train_loss: 0.05175 valid_loss: 0.07531 test_loss: 0.08922 \n",
      "[490/500] train_loss: 0.05409 valid_loss: 0.07222 test_loss: 0.08681 \n",
      "验证损失减少 (0.072821 --> 0.072222). 正在保存模型...\n",
      "[491/500] train_loss: 0.05131 valid_loss: 0.07370 test_loss: 0.08846 \n",
      "[492/500] train_loss: 0.05054 valid_loss: 0.07507 test_loss: 0.08926 \n",
      "[493/500] train_loss: 0.05160 valid_loss: 0.07390 test_loss: 0.08848 \n",
      "[494/500] train_loss: 0.05275 valid_loss: 0.07519 test_loss: 0.08968 \n",
      "[495/500] train_loss: 0.05166 valid_loss: 0.07303 test_loss: 0.08785 \n",
      "[496/500] train_loss: 0.05314 valid_loss: 0.07395 test_loss: 0.08807 \n",
      "[497/500] train_loss: 0.05273 valid_loss: 0.07501 test_loss: 0.08788 \n",
      "[498/500] train_loss: 0.05020 valid_loss: 0.07477 test_loss: 0.08805 \n",
      "[499/500] train_loss: 0.05270 valid_loss: 0.07311 test_loss: 0.08780 \n",
      "[500/500] train_loss: 0.05176 valid_loss: 0.07468 test_loss: 0.08848 \n",
      "TRAINING MODEL 4\n",
      "[  1/500] train_loss: 0.40902 valid_loss: 0.28741 test_loss: 0.29367 \n",
      "验证损失减少 (inf --> 0.287409). 正在保存模型...\n",
      "[  2/500] train_loss: 0.22474 valid_loss: 0.20797 test_loss: 0.21429 \n",
      "验证损失减少 (0.287409 --> 0.207970). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17964 valid_loss: 0.17691 test_loss: 0.18538 \n",
      "验证损失减少 (0.207970 --> 0.176906). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15729 valid_loss: 0.16006 test_loss: 0.17019 \n",
      "验证损失减少 (0.176906 --> 0.160056). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14789 valid_loss: 0.15223 test_loss: 0.16301 \n",
      "验证损失减少 (0.160056 --> 0.152235). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14262 valid_loss: 0.14741 test_loss: 0.15971 \n",
      "验证损失减少 (0.152235 --> 0.147410). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13765 valid_loss: 0.14444 test_loss: 0.15676 \n",
      "验证损失减少 (0.147410 --> 0.144439). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13177 valid_loss: 0.13592 test_loss: 0.14875 \n",
      "验证损失减少 (0.144439 --> 0.135917). 正在保存模型...\n",
      "[  9/500] train_loss: 0.12907 valid_loss: 0.13377 test_loss: 0.14506 \n",
      "验证损失减少 (0.135917 --> 0.133769). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12687 valid_loss: 0.12935 test_loss: 0.14315 \n",
      "验证损失减少 (0.133769 --> 0.129348). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12536 valid_loss: 0.13192 test_loss: 0.14400 \n",
      "[ 12/500] train_loss: 0.12343 valid_loss: 0.12623 test_loss: 0.13787 \n",
      "验证损失减少 (0.129348 --> 0.126226). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12041 valid_loss: 0.12376 test_loss: 0.13768 \n",
      "验证损失减少 (0.126226 --> 0.123756). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.11774 valid_loss: 0.12335 test_loss: 0.13637 \n",
      "验证损失减少 (0.123756 --> 0.123347). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11869 valid_loss: 0.12392 test_loss: 0.13674 \n",
      "[ 16/500] train_loss: 0.11574 valid_loss: 0.12118 test_loss: 0.13464 \n",
      "验证损失减少 (0.123347 --> 0.121179). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11425 valid_loss: 0.11891 test_loss: 0.13209 \n",
      "验证损失减少 (0.121179 --> 0.118915). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11504 valid_loss: 0.11906 test_loss: 0.13047 \n",
      "[ 19/500] train_loss: 0.10939 valid_loss: 0.11467 test_loss: 0.12820 \n",
      "验证损失减少 (0.118915 --> 0.114671). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11218 valid_loss: 0.11596 test_loss: 0.12827 \n",
      "[ 21/500] train_loss: 0.11025 valid_loss: 0.11264 test_loss: 0.12654 \n",
      "验证损失减少 (0.114671 --> 0.112644). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.10687 valid_loss: 0.11340 test_loss: 0.12999 \n",
      "[ 23/500] train_loss: 0.10910 valid_loss: 0.11203 test_loss: 0.12509 \n",
      "验证损失减少 (0.112644 --> 0.112034). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10699 valid_loss: 0.11092 test_loss: 0.12465 \n",
      "验证损失减少 (0.112034 --> 0.110925). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10442 valid_loss: 0.11096 test_loss: 0.12501 \n",
      "[ 26/500] train_loss: 0.10623 valid_loss: 0.11149 test_loss: 0.12357 \n",
      "[ 27/500] train_loss: 0.10293 valid_loss: 0.11089 test_loss: 0.12300 \n",
      "验证损失减少 (0.110925 --> 0.110893). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10136 valid_loss: 0.10744 test_loss: 0.12146 \n",
      "验证损失减少 (0.110893 --> 0.107437). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10489 valid_loss: 0.10663 test_loss: 0.12114 \n",
      "验证损失减少 (0.107437 --> 0.106628). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10330 valid_loss: 0.10862 test_loss: 0.12204 \n",
      "[ 31/500] train_loss: 0.10177 valid_loss: 0.11069 test_loss: 0.12102 \n",
      "[ 32/500] train_loss: 0.10092 valid_loss: 0.10538 test_loss: 0.12019 \n",
      "验证损失减少 (0.106628 --> 0.105375). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10063 valid_loss: 0.10599 test_loss: 0.11861 \n",
      "[ 34/500] train_loss: 0.09926 valid_loss: 0.10721 test_loss: 0.11956 \n",
      "[ 35/500] train_loss: 0.10121 valid_loss: 0.10586 test_loss: 0.11837 \n",
      "[ 36/500] train_loss: 0.09919 valid_loss: 0.10603 test_loss: 0.11745 \n",
      "[ 37/500] train_loss: 0.09608 valid_loss: 0.10411 test_loss: 0.11528 \n",
      "验证损失减少 (0.105375 --> 0.104106). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.09426 valid_loss: 0.10194 test_loss: 0.11504 \n",
      "验证损失减少 (0.104106 --> 0.101943). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.09530 valid_loss: 0.10037 test_loss: 0.11385 \n",
      "验证损失减少 (0.101943 --> 0.100373). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09601 valid_loss: 0.10121 test_loss: 0.11419 \n",
      "[ 41/500] train_loss: 0.09391 valid_loss: 0.10222 test_loss: 0.11356 \n",
      "[ 42/500] train_loss: 0.09413 valid_loss: 0.09831 test_loss: 0.11155 \n",
      "验证损失减少 (0.100373 --> 0.098314). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.09438 valid_loss: 0.10154 test_loss: 0.11245 \n",
      "[ 44/500] train_loss: 0.09556 valid_loss: 0.09704 test_loss: 0.11060 \n",
      "验证损失减少 (0.098314 --> 0.097036). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.09493 valid_loss: 0.09905 test_loss: 0.11233 \n",
      "[ 46/500] train_loss: 0.09446 valid_loss: 0.09957 test_loss: 0.11092 \n",
      "[ 47/500] train_loss: 0.09253 valid_loss: 0.09728 test_loss: 0.11247 \n",
      "[ 48/500] train_loss: 0.09232 valid_loss: 0.09702 test_loss: 0.10953 \n",
      "验证损失减少 (0.097036 --> 0.097022). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09278 valid_loss: 0.09816 test_loss: 0.10859 \n",
      "[ 50/500] train_loss: 0.09104 valid_loss: 0.09838 test_loss: 0.10751 \n",
      "[ 51/500] train_loss: 0.09195 valid_loss: 0.09578 test_loss: 0.10860 \n",
      "验证损失减少 (0.097022 --> 0.095781). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.08914 valid_loss: 0.09551 test_loss: 0.10724 \n",
      "验证损失减少 (0.095781 --> 0.095507). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.08930 valid_loss: 0.09993 test_loss: 0.10875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54/500] train_loss: 0.09331 valid_loss: 0.09560 test_loss: 0.10708 \n",
      "[ 55/500] train_loss: 0.08819 valid_loss: 0.09685 test_loss: 0.10813 \n",
      "[ 56/500] train_loss: 0.08914 valid_loss: 0.09674 test_loss: 0.10664 \n",
      "[ 57/500] train_loss: 0.09011 valid_loss: 0.09778 test_loss: 0.10656 \n",
      "[ 58/500] train_loss: 0.09028 valid_loss: 0.09315 test_loss: 0.10526 \n",
      "验证损失减少 (0.095507 --> 0.093151). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.08885 valid_loss: 0.09378 test_loss: 0.10532 \n",
      "[ 60/500] train_loss: 0.08532 valid_loss: 0.09205 test_loss: 0.10358 \n",
      "验证损失减少 (0.093151 --> 0.092050). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.08441 valid_loss: 0.09268 test_loss: 0.10465 \n",
      "[ 62/500] train_loss: 0.08671 valid_loss: 0.09244 test_loss: 0.10348 \n",
      "[ 63/500] train_loss: 0.08529 valid_loss: 0.09462 test_loss: 0.10673 \n",
      "[ 64/500] train_loss: 0.08669 valid_loss: 0.09233 test_loss: 0.10271 \n",
      "[ 65/500] train_loss: 0.08700 valid_loss: 0.09316 test_loss: 0.10398 \n",
      "[ 66/500] train_loss: 0.08643 valid_loss: 0.09147 test_loss: 0.10304 \n",
      "验证损失减少 (0.092050 --> 0.091468). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.08666 valid_loss: 0.09171 test_loss: 0.10300 \n",
      "[ 68/500] train_loss: 0.08669 valid_loss: 0.09313 test_loss: 0.10341 \n",
      "[ 69/500] train_loss: 0.08467 valid_loss: 0.09105 test_loss: 0.10217 \n",
      "验证损失减少 (0.091468 --> 0.091050). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.08593 valid_loss: 0.09354 test_loss: 0.10197 \n",
      "[ 71/500] train_loss: 0.08325 valid_loss: 0.09069 test_loss: 0.10235 \n",
      "验证损失减少 (0.091050 --> 0.090693). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.08296 valid_loss: 0.09291 test_loss: 0.10121 \n",
      "[ 73/500] train_loss: 0.08481 valid_loss: 0.09373 test_loss: 0.10349 \n",
      "[ 74/500] train_loss: 0.08820 valid_loss: 0.09447 test_loss: 0.10650 \n",
      "[ 75/500] train_loss: 0.08338 valid_loss: 0.09091 test_loss: 0.10130 \n",
      "[ 76/500] train_loss: 0.08432 valid_loss: 0.09162 test_loss: 0.10060 \n",
      "[ 77/500] train_loss: 0.08367 valid_loss: 0.09061 test_loss: 0.10252 \n",
      "验证损失减少 (0.090693 --> 0.090612). 正在保存模型...\n",
      "[ 78/500] train_loss: 0.08283 valid_loss: 0.09068 test_loss: 0.10164 \n",
      "[ 79/500] train_loss: 0.08407 valid_loss: 0.09220 test_loss: 0.10199 \n",
      "[ 80/500] train_loss: 0.08474 valid_loss: 0.08974 test_loss: 0.10120 \n",
      "验证损失减少 (0.090612 --> 0.089737). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.08159 valid_loss: 0.09066 test_loss: 0.10204 \n",
      "[ 82/500] train_loss: 0.08233 valid_loss: 0.09040 test_loss: 0.10053 \n",
      "[ 83/500] train_loss: 0.08267 valid_loss: 0.08996 test_loss: 0.10180 \n",
      "[ 84/500] train_loss: 0.08312 valid_loss: 0.08909 test_loss: 0.09937 \n",
      "验证损失减少 (0.089737 --> 0.089093). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.08253 valid_loss: 0.08797 test_loss: 0.09864 \n",
      "验证损失减少 (0.089093 --> 0.087975). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08008 valid_loss: 0.09008 test_loss: 0.09982 \n",
      "[ 87/500] train_loss: 0.08209 valid_loss: 0.08833 test_loss: 0.09796 \n",
      "[ 88/500] train_loss: 0.07967 valid_loss: 0.08742 test_loss: 0.09998 \n",
      "验证损失减少 (0.087975 --> 0.087415). 正在保存模型...\n",
      "[ 89/500] train_loss: 0.08072 valid_loss: 0.09049 test_loss: 0.09848 \n",
      "[ 90/500] train_loss: 0.08311 valid_loss: 0.08943 test_loss: 0.09766 \n",
      "[ 91/500] train_loss: 0.08089 valid_loss: 0.08710 test_loss: 0.09788 \n",
      "验证损失减少 (0.087415 --> 0.087099). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.08131 valid_loss: 0.09072 test_loss: 0.10020 \n",
      "[ 93/500] train_loss: 0.08136 valid_loss: 0.08775 test_loss: 0.09806 \n",
      "[ 94/500] train_loss: 0.07965 valid_loss: 0.08707 test_loss: 0.09721 \n",
      "验证损失减少 (0.087099 --> 0.087073). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.07921 valid_loss: 0.08744 test_loss: 0.09791 \n",
      "[ 96/500] train_loss: 0.07729 valid_loss: 0.08859 test_loss: 0.09847 \n",
      "[ 97/500] train_loss: 0.08063 valid_loss: 0.08705 test_loss: 0.09723 \n",
      "验证损失减少 (0.087073 --> 0.087049). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.07855 valid_loss: 0.08761 test_loss: 0.09743 \n",
      "[ 99/500] train_loss: 0.07820 valid_loss: 0.08665 test_loss: 0.09775 \n",
      "验证损失减少 (0.087049 --> 0.086653). 正在保存模型...\n",
      "[100/500] train_loss: 0.07698 valid_loss: 0.08614 test_loss: 0.09719 \n",
      "验证损失减少 (0.086653 --> 0.086139). 正在保存模型...\n",
      "[101/500] train_loss: 0.08174 valid_loss: 0.08830 test_loss: 0.09746 \n",
      "[102/500] train_loss: 0.07459 valid_loss: 0.08822 test_loss: 0.09708 \n",
      "[103/500] train_loss: 0.07776 valid_loss: 0.08778 test_loss: 0.09692 \n",
      "[104/500] train_loss: 0.07825 valid_loss: 0.08751 test_loss: 0.09646 \n",
      "[105/500] train_loss: 0.07859 valid_loss: 0.08622 test_loss: 0.09496 \n",
      "[106/500] train_loss: 0.07871 valid_loss: 0.08634 test_loss: 0.09651 \n",
      "[107/500] train_loss: 0.08029 valid_loss: 0.08524 test_loss: 0.09525 \n",
      "验证损失减少 (0.086139 --> 0.085237). 正在保存模型...\n",
      "[108/500] train_loss: 0.07636 valid_loss: 0.08493 test_loss: 0.09497 \n",
      "验证损失减少 (0.085237 --> 0.084926). 正在保存模型...\n",
      "[109/500] train_loss: 0.07771 valid_loss: 0.08557 test_loss: 0.09509 \n",
      "[110/500] train_loss: 0.07561 valid_loss: 0.08639 test_loss: 0.09659 \n",
      "[111/500] train_loss: 0.07400 valid_loss: 0.08658 test_loss: 0.09626 \n",
      "[112/500] train_loss: 0.07419 valid_loss: 0.08514 test_loss: 0.09613 \n",
      "[113/500] train_loss: 0.07786 valid_loss: 0.08689 test_loss: 0.09490 \n",
      "[114/500] train_loss: 0.07306 valid_loss: 0.08571 test_loss: 0.09553 \n",
      "[115/500] train_loss: 0.07442 valid_loss: 0.08450 test_loss: 0.09440 \n",
      "验证损失减少 (0.084926 --> 0.084500). 正在保存模型...\n",
      "[116/500] train_loss: 0.07517 valid_loss: 0.08716 test_loss: 0.09444 \n",
      "[117/500] train_loss: 0.07646 valid_loss: 0.08399 test_loss: 0.09571 \n",
      "验证损失减少 (0.084500 --> 0.083993). 正在保存模型...\n",
      "[118/500] train_loss: 0.07717 valid_loss: 0.08411 test_loss: 0.09526 \n",
      "[119/500] train_loss: 0.07348 valid_loss: 0.08361 test_loss: 0.09482 \n",
      "验证损失减少 (0.083993 --> 0.083613). 正在保存模型...\n",
      "[120/500] train_loss: 0.07746 valid_loss: 0.08397 test_loss: 0.09393 \n",
      "[121/500] train_loss: 0.07401 valid_loss: 0.08540 test_loss: 0.09504 \n",
      "[122/500] train_loss: 0.07503 valid_loss: 0.08500 test_loss: 0.09460 \n",
      "[123/500] train_loss: 0.07606 valid_loss: 0.08542 test_loss: 0.09603 \n",
      "[124/500] train_loss: 0.07485 valid_loss: 0.08445 test_loss: 0.09530 \n",
      "[125/500] train_loss: 0.07387 valid_loss: 0.08980 test_loss: 0.09458 \n",
      "[126/500] train_loss: 0.07577 valid_loss: 0.08279 test_loss: 0.09243 \n",
      "验证损失减少 (0.083613 --> 0.082785). 正在保存模型...\n",
      "[127/500] train_loss: 0.07336 valid_loss: 0.08409 test_loss: 0.09628 \n",
      "[128/500] train_loss: 0.07504 valid_loss: 0.08477 test_loss: 0.09407 \n",
      "[129/500] train_loss: 0.07299 valid_loss: 0.08356 test_loss: 0.09321 \n",
      "[130/500] train_loss: 0.07063 valid_loss: 0.08491 test_loss: 0.09373 \n",
      "[131/500] train_loss: 0.07488 valid_loss: 0.08229 test_loss: 0.09232 \n",
      "验证损失减少 (0.082785 --> 0.082289). 正在保存模型...\n",
      "[132/500] train_loss: 0.07421 valid_loss: 0.08484 test_loss: 0.09393 \n",
      "[133/500] train_loss: 0.07406 valid_loss: 0.08433 test_loss: 0.09246 \n",
      "[134/500] train_loss: 0.07369 valid_loss: 0.08450 test_loss: 0.09311 \n",
      "[135/500] train_loss: 0.07210 valid_loss: 0.08694 test_loss: 0.09405 \n",
      "[136/500] train_loss: 0.07187 valid_loss: 0.08262 test_loss: 0.09349 \n",
      "[137/500] train_loss: 0.07313 valid_loss: 0.08297 test_loss: 0.09298 \n",
      "[138/500] train_loss: 0.07196 valid_loss: 0.08245 test_loss: 0.09260 \n",
      "[139/500] train_loss: 0.07296 valid_loss: 0.08336 test_loss: 0.09429 \n",
      "[140/500] train_loss: 0.07384 valid_loss: 0.08360 test_loss: 0.09481 \n",
      "[141/500] train_loss: 0.07279 valid_loss: 0.08271 test_loss: 0.09278 \n",
      "[142/500] train_loss: 0.07188 valid_loss: 0.08429 test_loss: 0.09396 \n",
      "[143/500] train_loss: 0.07223 valid_loss: 0.08642 test_loss: 0.09341 \n",
      "[144/500] train_loss: 0.07186 valid_loss: 0.08319 test_loss: 0.09417 \n",
      "[145/500] train_loss: 0.07213 valid_loss: 0.08185 test_loss: 0.09312 \n",
      "验证损失减少 (0.082289 --> 0.081845). 正在保存模型...\n",
      "[146/500] train_loss: 0.06836 valid_loss: 0.08503 test_loss: 0.09251 \n",
      "[147/500] train_loss: 0.07245 valid_loss: 0.08134 test_loss: 0.09204 \n",
      "验证损失减少 (0.081845 --> 0.081339). 正在保存模型...\n",
      "[148/500] train_loss: 0.06947 valid_loss: 0.08627 test_loss: 0.09132 \n",
      "[149/500] train_loss: 0.07214 valid_loss: 0.08241 test_loss: 0.09045 \n",
      "[150/500] train_loss: 0.07142 valid_loss: 0.08350 test_loss: 0.09208 \n",
      "[151/500] train_loss: 0.07239 valid_loss: 0.08466 test_loss: 0.09075 \n",
      "[152/500] train_loss: 0.07092 valid_loss: 0.08309 test_loss: 0.09169 \n",
      "[153/500] train_loss: 0.06894 valid_loss: 0.08420 test_loss: 0.09183 \n",
      "[154/500] train_loss: 0.06989 valid_loss: 0.08286 test_loss: 0.09147 \n",
      "[155/500] train_loss: 0.07220 valid_loss: 0.08321 test_loss: 0.09102 \n",
      "[156/500] train_loss: 0.06713 valid_loss: 0.08149 test_loss: 0.09228 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157/500] train_loss: 0.06967 valid_loss: 0.08099 test_loss: 0.09161 \n",
      "验证损失减少 (0.081339 --> 0.080987). 正在保存模型...\n",
      "[158/500] train_loss: 0.07140 valid_loss: 0.08127 test_loss: 0.09110 \n",
      "[159/500] train_loss: 0.07125 valid_loss: 0.08197 test_loss: 0.09203 \n",
      "[160/500] train_loss: 0.06875 valid_loss: 0.08286 test_loss: 0.09270 \n",
      "[161/500] train_loss: 0.07206 valid_loss: 0.08049 test_loss: 0.09137 \n",
      "验证损失减少 (0.080987 --> 0.080489). 正在保存模型...\n",
      "[162/500] train_loss: 0.06881 valid_loss: 0.08167 test_loss: 0.09135 \n",
      "[163/500] train_loss: 0.07006 valid_loss: 0.08384 test_loss: 0.09191 \n",
      "[164/500] train_loss: 0.07036 valid_loss: 0.08471 test_loss: 0.09084 \n",
      "[165/500] train_loss: 0.07015 valid_loss: 0.08242 test_loss: 0.09086 \n",
      "[166/500] train_loss: 0.06788 valid_loss: 0.08454 test_loss: 0.09312 \n",
      "[167/500] train_loss: 0.07064 valid_loss: 0.08453 test_loss: 0.09151 \n",
      "[168/500] train_loss: 0.06931 valid_loss: 0.08364 test_loss: 0.09159 \n",
      "[169/500] train_loss: 0.06820 valid_loss: 0.07964 test_loss: 0.09120 \n",
      "验证损失减少 (0.080489 --> 0.079644). 正在保存模型...\n",
      "[170/500] train_loss: 0.06801 valid_loss: 0.08223 test_loss: 0.09193 \n",
      "[171/500] train_loss: 0.06905 valid_loss: 0.08160 test_loss: 0.09088 \n",
      "[172/500] train_loss: 0.06896 valid_loss: 0.08172 test_loss: 0.09007 \n",
      "[173/500] train_loss: 0.06842 valid_loss: 0.08580 test_loss: 0.09105 \n",
      "[174/500] train_loss: 0.07016 valid_loss: 0.08049 test_loss: 0.09001 \n",
      "[175/500] train_loss: 0.06987 valid_loss: 0.08632 test_loss: 0.08995 \n",
      "[176/500] train_loss: 0.06768 valid_loss: 0.08407 test_loss: 0.09093 \n",
      "[177/500] train_loss: 0.06723 valid_loss: 0.08184 test_loss: 0.09114 \n",
      "[178/500] train_loss: 0.06872 valid_loss: 0.08080 test_loss: 0.08931 \n",
      "[179/500] train_loss: 0.06896 valid_loss: 0.08301 test_loss: 0.08971 \n",
      "[180/500] train_loss: 0.06622 valid_loss: 0.08514 test_loss: 0.08972 \n",
      "[181/500] train_loss: 0.07003 valid_loss: 0.08417 test_loss: 0.09047 \n",
      "[182/500] train_loss: 0.06775 valid_loss: 0.08200 test_loss: 0.09003 \n",
      "[183/500] train_loss: 0.06911 valid_loss: 0.08813 test_loss: 0.08928 \n",
      "[184/500] train_loss: 0.06838 valid_loss: 0.08436 test_loss: 0.09042 \n",
      "[185/500] train_loss: 0.06743 valid_loss: 0.08151 test_loss: 0.08835 \n",
      "[186/500] train_loss: 0.06758 valid_loss: 0.08246 test_loss: 0.09052 \n",
      "[187/500] train_loss: 0.06751 valid_loss: 0.08673 test_loss: 0.08970 \n",
      "[188/500] train_loss: 0.06650 valid_loss: 0.08477 test_loss: 0.09170 \n",
      "[189/500] train_loss: 0.06532 valid_loss: 0.08140 test_loss: 0.09034 \n",
      "[190/500] train_loss: 0.06672 valid_loss: 0.08163 test_loss: 0.08959 \n",
      "[191/500] train_loss: 0.06827 valid_loss: 0.08431 test_loss: 0.08937 \n",
      "[192/500] train_loss: 0.06627 valid_loss: 0.08238 test_loss: 0.08879 \n",
      "[193/500] train_loss: 0.06536 valid_loss: 0.08564 test_loss: 0.08895 \n",
      "[194/500] train_loss: 0.06807 valid_loss: 0.08844 test_loss: 0.09021 \n",
      "[195/500] train_loss: 0.06766 valid_loss: 0.08343 test_loss: 0.08911 \n",
      "[196/500] train_loss: 0.06557 valid_loss: 0.08531 test_loss: 0.08936 \n",
      "[197/500] train_loss: 0.06673 valid_loss: 0.08384 test_loss: 0.08970 \n",
      "[198/500] train_loss: 0.06711 valid_loss: 0.08420 test_loss: 0.08802 \n",
      "[199/500] train_loss: 0.06616 valid_loss: 0.08665 test_loss: 0.08990 \n",
      "[200/500] train_loss: 0.06694 valid_loss: 0.08350 test_loss: 0.08783 \n",
      "[201/500] train_loss: 0.06569 valid_loss: 0.08303 test_loss: 0.09101 \n",
      "[202/500] train_loss: 0.06702 valid_loss: 0.08615 test_loss: 0.08912 \n",
      "[203/500] train_loss: 0.06602 valid_loss: 0.08937 test_loss: 0.08976 \n",
      "[204/500] train_loss: 0.06510 valid_loss: 0.09055 test_loss: 0.08951 \n",
      "[205/500] train_loss: 0.06548 valid_loss: 0.08120 test_loss: 0.08768 \n",
      "[206/500] train_loss: 0.06756 valid_loss: 0.08353 test_loss: 0.08867 \n",
      "[207/500] train_loss: 0.06515 valid_loss: 0.09362 test_loss: 0.08982 \n",
      "[208/500] train_loss: 0.06452 valid_loss: 0.08401 test_loss: 0.08932 \n",
      "[209/500] train_loss: 0.06777 valid_loss: 0.08359 test_loss: 0.08746 \n",
      "[210/500] train_loss: 0.06557 valid_loss: 0.08088 test_loss: 0.08876 \n",
      "[211/500] train_loss: 0.06645 valid_loss: 0.08104 test_loss: 0.08982 \n",
      "[212/500] train_loss: 0.06610 valid_loss: 0.08246 test_loss: 0.08867 \n",
      "[213/500] train_loss: 0.06584 valid_loss: 0.08184 test_loss: 0.08790 \n",
      "[214/500] train_loss: 0.06713 valid_loss: 0.07913 test_loss: 0.08765 \n",
      "验证损失减少 (0.079644 --> 0.079125). 正在保存模型...\n",
      "[215/500] train_loss: 0.06530 valid_loss: 0.08138 test_loss: 0.08807 \n",
      "[216/500] train_loss: 0.06427 valid_loss: 0.08228 test_loss: 0.09034 \n",
      "[217/500] train_loss: 0.06669 valid_loss: 0.08127 test_loss: 0.08803 \n",
      "[218/500] train_loss: 0.06661 valid_loss: 0.08113 test_loss: 0.08819 \n",
      "[219/500] train_loss: 0.06480 valid_loss: 0.08195 test_loss: 0.08789 \n",
      "[220/500] train_loss: 0.06535 valid_loss: 0.08083 test_loss: 0.08895 \n",
      "[221/500] train_loss: 0.06421 valid_loss: 0.08003 test_loss: 0.08810 \n",
      "[222/500] train_loss: 0.06539 valid_loss: 0.08232 test_loss: 0.08739 \n",
      "[223/500] train_loss: 0.06343 valid_loss: 0.08214 test_loss: 0.08757 \n",
      "[224/500] train_loss: 0.06540 valid_loss: 0.08579 test_loss: 0.08737 \n",
      "[225/500] train_loss: 0.06396 valid_loss: 0.07908 test_loss: 0.08772 \n",
      "验证损失减少 (0.079125 --> 0.079081). 正在保存模型...\n",
      "[226/500] train_loss: 0.06385 valid_loss: 0.08295 test_loss: 0.08766 \n",
      "[227/500] train_loss: 0.06464 valid_loss: 0.08667 test_loss: 0.08952 \n",
      "[228/500] train_loss: 0.06345 valid_loss: 0.08097 test_loss: 0.08874 \n",
      "[229/500] train_loss: 0.06510 valid_loss: 0.07796 test_loss: 0.08750 \n",
      "验证损失减少 (0.079081 --> 0.077960). 正在保存模型...\n",
      "[230/500] train_loss: 0.06441 valid_loss: 0.07921 test_loss: 0.08801 \n",
      "[231/500] train_loss: 0.06480 valid_loss: 0.08352 test_loss: 0.09094 \n",
      "[232/500] train_loss: 0.06497 valid_loss: 0.07985 test_loss: 0.08862 \n",
      "[233/500] train_loss: 0.06233 valid_loss: 0.08071 test_loss: 0.08911 \n",
      "[234/500] train_loss: 0.06246 valid_loss: 0.08019 test_loss: 0.08796 \n",
      "[235/500] train_loss: 0.06226 valid_loss: 0.07777 test_loss: 0.08697 \n",
      "验证损失减少 (0.077960 --> 0.077767). 正在保存模型...\n",
      "[236/500] train_loss: 0.06136 valid_loss: 0.07878 test_loss: 0.08794 \n",
      "[237/500] train_loss: 0.06640 valid_loss: 0.07767 test_loss: 0.08720 \n",
      "验证损失减少 (0.077767 --> 0.077666). 正在保存模型...\n",
      "[238/500] train_loss: 0.06389 valid_loss: 0.08085 test_loss: 0.08850 \n",
      "[239/500] train_loss: 0.06411 valid_loss: 0.08230 test_loss: 0.08788 \n",
      "[240/500] train_loss: 0.06249 valid_loss: 0.08112 test_loss: 0.08670 \n",
      "[241/500] train_loss: 0.06215 valid_loss: 0.07947 test_loss: 0.08761 \n",
      "[242/500] train_loss: 0.06262 valid_loss: 0.08101 test_loss: 0.08882 \n",
      "[243/500] train_loss: 0.06339 valid_loss: 0.08654 test_loss: 0.08779 \n",
      "[244/500] train_loss: 0.06347 valid_loss: 0.08555 test_loss: 0.08700 \n",
      "[245/500] train_loss: 0.06468 valid_loss: 0.07830 test_loss: 0.08802 \n",
      "[246/500] train_loss: 0.06444 valid_loss: 0.08273 test_loss: 0.08726 \n",
      "[247/500] train_loss: 0.06378 valid_loss: 0.08158 test_loss: 0.08843 \n",
      "[248/500] train_loss: 0.06361 valid_loss: 0.08247 test_loss: 0.08829 \n",
      "[249/500] train_loss: 0.06142 valid_loss: 0.07828 test_loss: 0.08847 \n",
      "[250/500] train_loss: 0.06192 valid_loss: 0.08541 test_loss: 0.08875 \n",
      "[251/500] train_loss: 0.06426 valid_loss: 0.08167 test_loss: 0.08742 \n",
      "[252/500] train_loss: 0.06414 valid_loss: 0.08191 test_loss: 0.08797 \n",
      "[253/500] train_loss: 0.06428 valid_loss: 0.08568 test_loss: 0.08692 \n",
      "[254/500] train_loss: 0.06277 valid_loss: 0.09253 test_loss: 0.08857 \n",
      "[255/500] train_loss: 0.06094 valid_loss: 0.08441 test_loss: 0.08746 \n",
      "[256/500] train_loss: 0.06173 valid_loss: 0.08287 test_loss: 0.08747 \n",
      "[257/500] train_loss: 0.06369 valid_loss: 0.07847 test_loss: 0.08760 \n",
      "[258/500] train_loss: 0.06324 valid_loss: 0.09917 test_loss: 0.08728 \n",
      "[259/500] train_loss: 0.06114 valid_loss: 0.09808 test_loss: 0.08904 \n",
      "[260/500] train_loss: 0.06122 valid_loss: 0.08392 test_loss: 0.08709 \n",
      "[261/500] train_loss: 0.06197 valid_loss: 0.07968 test_loss: 0.08845 \n",
      "[262/500] train_loss: 0.06154 valid_loss: 0.09244 test_loss: 0.08785 \n",
      "[263/500] train_loss: 0.06124 valid_loss: 0.08787 test_loss: 0.08757 \n",
      "[264/500] train_loss: 0.06187 valid_loss: 0.09808 test_loss: 0.08805 \n",
      "[265/500] train_loss: 0.06081 valid_loss: 0.08031 test_loss: 0.08756 \n",
      "[266/500] train_loss: 0.06294 valid_loss: 0.07790 test_loss: 0.08766 \n",
      "[267/500] train_loss: 0.06089 valid_loss: 0.08069 test_loss: 0.08599 \n",
      "[268/500] train_loss: 0.06153 valid_loss: 0.08101 test_loss: 0.08705 \n",
      "[269/500] train_loss: 0.05961 valid_loss: 0.07762 test_loss: 0.08629 \n",
      "验证损失减少 (0.077666 --> 0.077619). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[270/500] train_loss: 0.06008 valid_loss: 0.08802 test_loss: 0.08779 \n",
      "[271/500] train_loss: 0.06138 valid_loss: 0.08543 test_loss: 0.08689 \n",
      "[272/500] train_loss: 0.06026 valid_loss: 0.08107 test_loss: 0.08775 \n",
      "[273/500] train_loss: 0.06045 valid_loss: 0.08523 test_loss: 0.08976 \n",
      "[274/500] train_loss: 0.05987 valid_loss: 0.08073 test_loss: 0.08870 \n",
      "[275/500] train_loss: 0.06153 valid_loss: 0.08332 test_loss: 0.08657 \n",
      "[276/500] train_loss: 0.06209 valid_loss: 0.08270 test_loss: 0.08841 \n",
      "[277/500] train_loss: 0.06158 valid_loss: 0.09199 test_loss: 0.08745 \n",
      "[278/500] train_loss: 0.06024 valid_loss: 0.09966 test_loss: 0.08821 \n",
      "[279/500] train_loss: 0.06142 valid_loss: 0.10320 test_loss: 0.08730 \n",
      "[280/500] train_loss: 0.06054 valid_loss: 0.09400 test_loss: 0.08703 \n",
      "[281/500] train_loss: 0.06174 valid_loss: 0.10204 test_loss: 0.08814 \n",
      "[282/500] train_loss: 0.06013 valid_loss: 0.09712 test_loss: 0.08912 \n",
      "[283/500] train_loss: 0.05920 valid_loss: 0.09175 test_loss: 0.08567 \n",
      "[284/500] train_loss: 0.06154 valid_loss: 0.09325 test_loss: 0.08695 \n",
      "[285/500] train_loss: 0.06070 valid_loss: 0.09061 test_loss: 0.08803 \n",
      "[286/500] train_loss: 0.06126 valid_loss: 0.08505 test_loss: 0.08735 \n",
      "[287/500] train_loss: 0.06056 valid_loss: 0.08850 test_loss: 0.08631 \n",
      "[288/500] train_loss: 0.05987 valid_loss: 0.08326 test_loss: 0.08729 \n",
      "[289/500] train_loss: 0.05736 valid_loss: 0.08487 test_loss: 0.08759 \n",
      "[290/500] train_loss: 0.06038 valid_loss: 0.09504 test_loss: 0.08807 \n",
      "[291/500] train_loss: 0.05926 valid_loss: 0.07918 test_loss: 0.08705 \n",
      "[292/500] train_loss: 0.05916 valid_loss: 0.08555 test_loss: 0.08748 \n",
      "[293/500] train_loss: 0.05858 valid_loss: 0.08547 test_loss: 0.08750 \n",
      "[294/500] train_loss: 0.06075 valid_loss: 0.08157 test_loss: 0.08717 \n",
      "[295/500] train_loss: 0.05837 valid_loss: 0.08738 test_loss: 0.08793 \n",
      "[296/500] train_loss: 0.05915 valid_loss: 0.08568 test_loss: 0.08677 \n",
      "[297/500] train_loss: 0.05886 valid_loss: 0.08017 test_loss: 0.08793 \n",
      "[298/500] train_loss: 0.06036 valid_loss: 0.07575 test_loss: 0.08734 \n",
      "验证损失减少 (0.077619 --> 0.075752). 正在保存模型...\n",
      "[299/500] train_loss: 0.06022 valid_loss: 0.08359 test_loss: 0.08773 \n",
      "[300/500] train_loss: 0.06153 valid_loss: 0.09473 test_loss: 0.08894 \n",
      "[301/500] train_loss: 0.05956 valid_loss: 0.10206 test_loss: 0.08693 \n",
      "[302/500] train_loss: 0.05799 valid_loss: 0.09499 test_loss: 0.08859 \n",
      "[303/500] train_loss: 0.06066 valid_loss: 0.08518 test_loss: 0.08828 \n",
      "[304/500] train_loss: 0.05944 valid_loss: 0.09409 test_loss: 0.08606 \n",
      "[305/500] train_loss: 0.05899 valid_loss: 0.08118 test_loss: 0.08674 \n",
      "[306/500] train_loss: 0.05902 valid_loss: 0.08539 test_loss: 0.08718 \n",
      "[307/500] train_loss: 0.06006 valid_loss: 0.07820 test_loss: 0.08726 \n",
      "[308/500] train_loss: 0.05900 valid_loss: 0.08124 test_loss: 0.08658 \n",
      "[309/500] train_loss: 0.05947 valid_loss: 0.08891 test_loss: 0.08693 \n",
      "[310/500] train_loss: 0.05927 valid_loss: 0.10088 test_loss: 0.08697 \n",
      "[311/500] train_loss: 0.05684 valid_loss: 0.10094 test_loss: 0.08744 \n",
      "[312/500] train_loss: 0.05991 valid_loss: 0.09569 test_loss: 0.08649 \n",
      "[313/500] train_loss: 0.05867 valid_loss: 0.10540 test_loss: 0.08745 \n",
      "[314/500] train_loss: 0.05959 valid_loss: 0.08924 test_loss: 0.08598 \n",
      "[315/500] train_loss: 0.05886 valid_loss: 0.09178 test_loss: 0.08577 \n",
      "[316/500] train_loss: 0.06019 valid_loss: 0.09615 test_loss: 0.08546 \n",
      "[317/500] train_loss: 0.06069 valid_loss: 0.08778 test_loss: 0.08658 \n",
      "[318/500] train_loss: 0.05878 valid_loss: 0.09267 test_loss: 0.08764 \n",
      "[319/500] train_loss: 0.05873 valid_loss: 0.09738 test_loss: 0.08685 \n",
      "[320/500] train_loss: 0.05957 valid_loss: 0.08948 test_loss: 0.08815 \n",
      "[321/500] train_loss: 0.05917 valid_loss: 0.09593 test_loss: 0.08675 \n",
      "[322/500] train_loss: 0.05851 valid_loss: 0.09430 test_loss: 0.08765 \n",
      "[323/500] train_loss: 0.05809 valid_loss: 0.09802 test_loss: 0.08664 \n",
      "[324/500] train_loss: 0.05675 valid_loss: 0.09121 test_loss: 0.08513 \n",
      "[325/500] train_loss: 0.05832 valid_loss: 0.08778 test_loss: 0.08564 \n",
      "[326/500] train_loss: 0.05962 valid_loss: 0.09133 test_loss: 0.08543 \n",
      "[327/500] train_loss: 0.05819 valid_loss: 0.08908 test_loss: 0.08678 \n",
      "[328/500] train_loss: 0.05801 valid_loss: 0.09190 test_loss: 0.08564 \n",
      "[329/500] train_loss: 0.05816 valid_loss: 0.08892 test_loss: 0.08533 \n",
      "[330/500] train_loss: 0.05768 valid_loss: 0.07816 test_loss: 0.08608 \n",
      "[331/500] train_loss: 0.05863 valid_loss: 0.09525 test_loss: 0.08579 \n",
      "[332/500] train_loss: 0.05808 valid_loss: 0.08275 test_loss: 0.08459 \n",
      "[333/500] train_loss: 0.05709 valid_loss: 0.09032 test_loss: 0.08545 \n",
      "[334/500] train_loss: 0.05764 valid_loss: 0.07812 test_loss: 0.08453 \n",
      "[335/500] train_loss: 0.05638 valid_loss: 0.08402 test_loss: 0.08482 \n",
      "[336/500] train_loss: 0.05760 valid_loss: 0.07974 test_loss: 0.08492 \n",
      "[337/500] train_loss: 0.05780 valid_loss: 0.09429 test_loss: 0.08532 \n",
      "[338/500] train_loss: 0.05670 valid_loss: 0.08200 test_loss: 0.08516 \n",
      "[339/500] train_loss: 0.05943 valid_loss: 0.07944 test_loss: 0.08707 \n",
      "[340/500] train_loss: 0.05712 valid_loss: 0.08325 test_loss: 0.08654 \n",
      "[341/500] train_loss: 0.05692 valid_loss: 0.08392 test_loss: 0.08762 \n",
      "[342/500] train_loss: 0.05691 valid_loss: 0.09386 test_loss: 0.08495 \n",
      "[343/500] train_loss: 0.05731 valid_loss: 0.07701 test_loss: 0.08631 \n",
      "[344/500] train_loss: 0.05789 valid_loss: 0.07530 test_loss: 0.08658 \n",
      "验证损失减少 (0.075752 --> 0.075297). 正在保存模型...\n",
      "[345/500] train_loss: 0.05678 valid_loss: 0.08500 test_loss: 0.08772 \n",
      "[346/500] train_loss: 0.05714 valid_loss: 0.08219 test_loss: 0.08615 \n",
      "[347/500] train_loss: 0.05883 valid_loss: 0.08412 test_loss: 0.08703 \n",
      "[348/500] train_loss: 0.05788 valid_loss: 0.08336 test_loss: 0.08628 \n",
      "[349/500] train_loss: 0.05594 valid_loss: 0.09107 test_loss: 0.08667 \n",
      "[350/500] train_loss: 0.05684 valid_loss: 0.08414 test_loss: 0.08592 \n",
      "[351/500] train_loss: 0.05697 valid_loss: 0.09719 test_loss: 0.08613 \n",
      "[352/500] train_loss: 0.05615 valid_loss: 0.09346 test_loss: 0.08687 \n",
      "[353/500] train_loss: 0.05568 valid_loss: 0.10795 test_loss: 0.08649 \n",
      "[354/500] train_loss: 0.05702 valid_loss: 0.09394 test_loss: 0.08705 \n",
      "[355/500] train_loss: 0.05682 valid_loss: 0.08765 test_loss: 0.08549 \n",
      "[356/500] train_loss: 0.05692 valid_loss: 0.12440 test_loss: 0.08824 \n",
      "[357/500] train_loss: 0.05552 valid_loss: 0.09639 test_loss: 0.08748 \n",
      "[358/500] train_loss: 0.05639 valid_loss: 0.11652 test_loss: 0.08638 \n",
      "[359/500] train_loss: 0.05518 valid_loss: 0.11360 test_loss: 0.08658 \n",
      "[360/500] train_loss: 0.05614 valid_loss: 0.07641 test_loss: 0.08637 \n",
      "[361/500] train_loss: 0.05628 valid_loss: 0.08533 test_loss: 0.08592 \n",
      "[362/500] train_loss: 0.05675 valid_loss: 0.09243 test_loss: 0.08763 \n",
      "[363/500] train_loss: 0.05681 valid_loss: 0.08192 test_loss: 0.08655 \n",
      "[364/500] train_loss: 0.05669 valid_loss: 0.08607 test_loss: 0.08586 \n",
      "[365/500] train_loss: 0.05564 valid_loss: 0.08532 test_loss: 0.08646 \n",
      "[366/500] train_loss: 0.05534 valid_loss: 0.07680 test_loss: 0.08547 \n",
      "[367/500] train_loss: 0.05626 valid_loss: 0.08002 test_loss: 0.08764 \n",
      "[368/500] train_loss: 0.05615 valid_loss: 0.07681 test_loss: 0.08731 \n",
      "[369/500] train_loss: 0.05542 valid_loss: 0.07716 test_loss: 0.08764 \n",
      "[370/500] train_loss: 0.05718 valid_loss: 0.08428 test_loss: 0.08784 \n",
      "[371/500] train_loss: 0.05628 valid_loss: 0.09109 test_loss: 0.08746 \n",
      "[372/500] train_loss: 0.05537 valid_loss: 0.07819 test_loss: 0.08926 \n",
      "[373/500] train_loss: 0.05588 valid_loss: 0.08537 test_loss: 0.08669 \n",
      "[374/500] train_loss: 0.05515 valid_loss: 0.09992 test_loss: 0.08987 \n",
      "[375/500] train_loss: 0.05734 valid_loss: 0.07765 test_loss: 0.08836 \n",
      "[376/500] train_loss: 0.05633 valid_loss: 0.08362 test_loss: 0.08657 \n",
      "[377/500] train_loss: 0.05473 valid_loss: 0.07686 test_loss: 0.08634 \n",
      "[378/500] train_loss: 0.05547 valid_loss: 0.08256 test_loss: 0.08668 \n",
      "[379/500] train_loss: 0.05759 valid_loss: 0.07535 test_loss: 0.08623 \n",
      "[380/500] train_loss: 0.05423 valid_loss: 0.08002 test_loss: 0.08584 \n",
      "[381/500] train_loss: 0.05459 valid_loss: 0.08966 test_loss: 0.08617 \n",
      "[382/500] train_loss: 0.05741 valid_loss: 0.09329 test_loss: 0.08743 \n",
      "[383/500] train_loss: 0.05536 valid_loss: 0.08187 test_loss: 0.08696 \n",
      "[384/500] train_loss: 0.05490 valid_loss: 0.08802 test_loss: 0.08569 \n",
      "[385/500] train_loss: 0.05554 valid_loss: 0.08001 test_loss: 0.08447 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[386/500] train_loss: 0.05461 valid_loss: 0.09135 test_loss: 0.08542 \n",
      "[387/500] train_loss: 0.05631 valid_loss: 0.07955 test_loss: 0.08789 \n",
      "[388/500] train_loss: 0.05512 valid_loss: 0.07732 test_loss: 0.08685 \n",
      "[389/500] train_loss: 0.05451 valid_loss: 0.08222 test_loss: 0.08669 \n",
      "[390/500] train_loss: 0.05519 valid_loss: 0.08213 test_loss: 0.08617 \n",
      "[391/500] train_loss: 0.05584 valid_loss: 0.08358 test_loss: 0.08788 \n",
      "[392/500] train_loss: 0.05718 valid_loss: 0.08199 test_loss: 0.08687 \n",
      "[393/500] train_loss: 0.05422 valid_loss: 0.08937 test_loss: 0.08725 \n",
      "[394/500] train_loss: 0.05492 valid_loss: 0.09688 test_loss: 0.08707 \n",
      "[395/500] train_loss: 0.05586 valid_loss: 0.08231 test_loss: 0.08633 \n",
      "[396/500] train_loss: 0.05483 valid_loss: 0.08751 test_loss: 0.08767 \n",
      "[397/500] train_loss: 0.05318 valid_loss: 0.08041 test_loss: 0.08640 \n",
      "[398/500] train_loss: 0.05299 valid_loss: 0.07817 test_loss: 0.08842 \n",
      "[399/500] train_loss: 0.05588 valid_loss: 0.08034 test_loss: 0.08720 \n",
      "[400/500] train_loss: 0.05442 valid_loss: 0.09536 test_loss: 0.08800 \n",
      "[401/500] train_loss: 0.05504 valid_loss: 0.08461 test_loss: 0.08755 \n",
      "[402/500] train_loss: 0.05622 valid_loss: 0.08648 test_loss: 0.08609 \n",
      "[403/500] train_loss: 0.05516 valid_loss: 0.07880 test_loss: 0.08639 \n",
      "[404/500] train_loss: 0.05553 valid_loss: 0.08794 test_loss: 0.08868 \n",
      "[405/500] train_loss: 0.05318 valid_loss: 0.07693 test_loss: 0.08672 \n",
      "[406/500] train_loss: 0.05455 valid_loss: 0.08012 test_loss: 0.08721 \n",
      "[407/500] train_loss: 0.05275 valid_loss: 0.07912 test_loss: 0.08634 \n",
      "[408/500] train_loss: 0.05263 valid_loss: 0.07792 test_loss: 0.08711 \n",
      "[409/500] train_loss: 0.05548 valid_loss: 0.08151 test_loss: 0.08558 \n",
      "[410/500] train_loss: 0.05284 valid_loss: 0.09550 test_loss: 0.08784 \n",
      "[411/500] train_loss: 0.05538 valid_loss: 0.08650 test_loss: 0.08552 \n",
      "[412/500] train_loss: 0.05394 valid_loss: 0.10846 test_loss: 0.08635 \n",
      "[413/500] train_loss: 0.05429 valid_loss: 0.09071 test_loss: 0.08495 \n",
      "[414/500] train_loss: 0.05378 valid_loss: 0.08935 test_loss: 0.08734 \n",
      "[415/500] train_loss: 0.05311 valid_loss: 0.09132 test_loss: 0.08695 \n",
      "[416/500] train_loss: 0.05366 valid_loss: 0.08158 test_loss: 0.08522 \n",
      "[417/500] train_loss: 0.05294 valid_loss: 0.08242 test_loss: 0.08570 \n",
      "[418/500] train_loss: 0.05437 valid_loss: 0.08941 test_loss: 0.08585 \n",
      "[419/500] train_loss: 0.05364 valid_loss: 0.08150 test_loss: 0.08421 \n",
      "[420/500] train_loss: 0.05294 valid_loss: 0.08237 test_loss: 0.08691 \n",
      "[421/500] train_loss: 0.05393 valid_loss: 0.07927 test_loss: 0.08571 \n",
      "[422/500] train_loss: 0.05275 valid_loss: 0.07922 test_loss: 0.08632 \n",
      "[423/500] train_loss: 0.05362 valid_loss: 0.08074 test_loss: 0.08719 \n",
      "[424/500] train_loss: 0.05414 valid_loss: 0.07803 test_loss: 0.08593 \n",
      "[425/500] train_loss: 0.05428 valid_loss: 0.08038 test_loss: 0.08714 \n",
      "[426/500] train_loss: 0.05168 valid_loss: 0.07685 test_loss: 0.08739 \n",
      "[427/500] train_loss: 0.05475 valid_loss: 0.08173 test_loss: 0.08557 \n",
      "[428/500] train_loss: 0.05371 valid_loss: 0.08563 test_loss: 0.08651 \n",
      "[429/500] train_loss: 0.05280 valid_loss: 0.07979 test_loss: 0.08644 \n",
      "[430/500] train_loss: 0.05238 valid_loss: 0.07462 test_loss: 0.08614 \n",
      "验证损失减少 (0.075297 --> 0.074623). 正在保存模型...\n",
      "[431/500] train_loss: 0.05489 valid_loss: 0.07977 test_loss: 0.08518 \n",
      "[432/500] train_loss: 0.05389 valid_loss: 0.08212 test_loss: 0.08664 \n",
      "[433/500] train_loss: 0.05497 valid_loss: 0.09058 test_loss: 0.08582 \n",
      "[434/500] train_loss: 0.05477 valid_loss: 0.08510 test_loss: 0.08589 \n",
      "[435/500] train_loss: 0.05439 valid_loss: 0.11036 test_loss: 0.08732 \n",
      "[436/500] train_loss: 0.05178 valid_loss: 0.08601 test_loss: 0.08683 \n",
      "[437/500] train_loss: 0.05218 valid_loss: 0.08330 test_loss: 0.08819 \n",
      "[438/500] train_loss: 0.05352 valid_loss: 0.09326 test_loss: 0.08546 \n",
      "[439/500] train_loss: 0.05289 valid_loss: 0.09944 test_loss: 0.08622 \n",
      "[440/500] train_loss: 0.05321 valid_loss: 0.08308 test_loss: 0.08504 \n",
      "[441/500] train_loss: 0.05307 valid_loss: 0.09255 test_loss: 0.08619 \n",
      "[442/500] train_loss: 0.05353 valid_loss: 0.08717 test_loss: 0.08600 \n",
      "[443/500] train_loss: 0.05212 valid_loss: 0.08201 test_loss: 0.08687 \n",
      "[444/500] train_loss: 0.05313 valid_loss: 0.08787 test_loss: 0.08581 \n",
      "[445/500] train_loss: 0.05216 valid_loss: 0.07909 test_loss: 0.08725 \n",
      "[446/500] train_loss: 0.05400 valid_loss: 0.09817 test_loss: 0.08598 \n",
      "[447/500] train_loss: 0.05267 valid_loss: 0.09193 test_loss: 0.08513 \n",
      "[448/500] train_loss: 0.05282 valid_loss: 0.09556 test_loss: 0.08543 \n",
      "[449/500] train_loss: 0.05395 valid_loss: 0.09482 test_loss: 0.08496 \n",
      "[450/500] train_loss: 0.05360 valid_loss: 0.07587 test_loss: 0.08628 \n",
      "[451/500] train_loss: 0.05212 valid_loss: 0.08372 test_loss: 0.08576 \n",
      "[452/500] train_loss: 0.05473 valid_loss: 0.07924 test_loss: 0.08699 \n",
      "[453/500] train_loss: 0.05404 valid_loss: 0.08838 test_loss: 0.08692 \n",
      "[454/500] train_loss: 0.05251 valid_loss: 0.08260 test_loss: 0.08774 \n",
      "[455/500] train_loss: 0.05369 valid_loss: 0.08789 test_loss: 0.08648 \n",
      "[456/500] train_loss: 0.05311 valid_loss: 0.08503 test_loss: 0.08593 \n",
      "[457/500] train_loss: 0.05244 valid_loss: 0.08009 test_loss: 0.08738 \n",
      "[458/500] train_loss: 0.05167 valid_loss: 0.07581 test_loss: 0.08678 \n",
      "[459/500] train_loss: 0.05071 valid_loss: 0.07846 test_loss: 0.08566 \n",
      "[460/500] train_loss: 0.05112 valid_loss: 0.07586 test_loss: 0.08637 \n",
      "[461/500] train_loss: 0.05288 valid_loss: 0.08247 test_loss: 0.08609 \n",
      "[462/500] train_loss: 0.05135 valid_loss: 0.07966 test_loss: 0.08639 \n",
      "[463/500] train_loss: 0.05334 valid_loss: 0.07736 test_loss: 0.08588 \n",
      "[464/500] train_loss: 0.05288 valid_loss: 0.08134 test_loss: 0.08597 \n",
      "[465/500] train_loss: 0.05246 valid_loss: 0.07823 test_loss: 0.08700 \n",
      "[466/500] train_loss: 0.05044 valid_loss: 0.08004 test_loss: 0.08615 \n",
      "[467/500] train_loss: 0.05323 valid_loss: 0.09042 test_loss: 0.08648 \n",
      "[468/500] train_loss: 0.05127 valid_loss: 0.08937 test_loss: 0.08659 \n",
      "[469/500] train_loss: 0.05332 valid_loss: 0.07650 test_loss: 0.08717 \n",
      "[470/500] train_loss: 0.05070 valid_loss: 0.07411 test_loss: 0.08645 \n",
      "验证损失减少 (0.074623 --> 0.074106). 正在保存模型...\n",
      "[471/500] train_loss: 0.05316 valid_loss: 0.08803 test_loss: 0.08566 \n",
      "[472/500] train_loss: 0.05095 valid_loss: 0.08440 test_loss: 0.08559 \n",
      "[473/500] train_loss: 0.05155 valid_loss: 0.08297 test_loss: 0.08793 \n",
      "[474/500] train_loss: 0.05124 valid_loss: 0.08360 test_loss: 0.08686 \n",
      "[475/500] train_loss: 0.05224 valid_loss: 0.08943 test_loss: 0.08641 \n",
      "[476/500] train_loss: 0.05154 valid_loss: 0.07447 test_loss: 0.08649 \n",
      "[477/500] train_loss: 0.05210 valid_loss: 0.07606 test_loss: 0.08603 \n",
      "[478/500] train_loss: 0.05198 valid_loss: 0.08294 test_loss: 0.08629 \n",
      "[479/500] train_loss: 0.05129 valid_loss: 0.07553 test_loss: 0.08771 \n",
      "[480/500] train_loss: 0.05170 valid_loss: 0.07748 test_loss: 0.08655 \n",
      "[481/500] train_loss: 0.05411 valid_loss: 0.07471 test_loss: 0.08594 \n",
      "[482/500] train_loss: 0.05123 valid_loss: 0.08330 test_loss: 0.08577 \n",
      "[483/500] train_loss: 0.05076 valid_loss: 0.10005 test_loss: 0.08637 \n",
      "[484/500] train_loss: 0.05291 valid_loss: 0.07529 test_loss: 0.08540 \n",
      "[485/500] train_loss: 0.05158 valid_loss: 0.09899 test_loss: 0.08551 \n",
      "[486/500] train_loss: 0.05158 valid_loss: 0.08347 test_loss: 0.08557 \n",
      "[487/500] train_loss: 0.05247 valid_loss: 0.09332 test_loss: 0.08577 \n",
      "[488/500] train_loss: 0.05077 valid_loss: 0.09709 test_loss: 0.08842 \n",
      "[489/500] train_loss: 0.05164 valid_loss: 0.10009 test_loss: 0.08687 \n",
      "[490/500] train_loss: 0.05038 valid_loss: 0.08531 test_loss: 0.08623 \n",
      "[491/500] train_loss: 0.05333 valid_loss: 0.07796 test_loss: 0.08690 \n",
      "[492/500] train_loss: 0.04942 valid_loss: 0.07551 test_loss: 0.08719 \n",
      "[493/500] train_loss: 0.05187 valid_loss: 0.07558 test_loss: 0.08622 \n",
      "[494/500] train_loss: 0.05085 valid_loss: 0.08179 test_loss: 0.08656 \n",
      "[495/500] train_loss: 0.05380 valid_loss: 0.09266 test_loss: 0.08522 \n",
      "[496/500] train_loss: 0.05078 valid_loss: 0.08836 test_loss: 0.08648 \n",
      "[497/500] train_loss: 0.05169 valid_loss: 0.07588 test_loss: 0.08731 \n",
      "[498/500] train_loss: 0.05123 valid_loss: 0.07777 test_loss: 0.08795 \n",
      "[499/500] train_loss: 0.05313 valid_loss: 0.08117 test_loss: 0.08497 \n",
      "[500/500] train_loss: 0.04989 valid_loss: 0.07896 test_loss: 0.08537 \n",
      "TRAINING MODEL 5\n",
      "[  1/500] train_loss: 0.45319 valid_loss: 0.31319 test_loss: 0.31881 \n",
      "验证损失减少 (inf --> 0.313194). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2/500] train_loss: 0.24064 valid_loss: 0.21467 test_loss: 0.22099 \n",
      "验证损失减少 (0.313194 --> 0.214675). 正在保存模型...\n",
      "[  3/500] train_loss: 0.18558 valid_loss: 0.18111 test_loss: 0.19024 \n",
      "验证损失减少 (0.214675 --> 0.181108). 正在保存模型...\n",
      "[  4/500] train_loss: 0.16367 valid_loss: 0.16374 test_loss: 0.17221 \n",
      "验证损失减少 (0.181108 --> 0.163744). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15158 valid_loss: 0.15268 test_loss: 0.16478 \n",
      "验证损失减少 (0.163744 --> 0.152678). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14463 valid_loss: 0.14479 test_loss: 0.15637 \n",
      "验证损失减少 (0.152678 --> 0.144793). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13985 valid_loss: 0.14500 test_loss: 0.15922 \n",
      "[  8/500] train_loss: 0.13245 valid_loss: 0.13797 test_loss: 0.14881 \n",
      "验证损失减少 (0.144793 --> 0.137971). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13198 valid_loss: 0.13407 test_loss: 0.14685 \n",
      "验证损失减少 (0.137971 --> 0.134070). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12715 valid_loss: 0.12970 test_loss: 0.14413 \n",
      "验证损失减少 (0.134070 --> 0.129698). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12791 valid_loss: 0.13072 test_loss: 0.14451 \n",
      "[ 12/500] train_loss: 0.12322 valid_loss: 0.12604 test_loss: 0.13968 \n",
      "验证损失减少 (0.129698 --> 0.126043). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12415 valid_loss: 0.12761 test_loss: 0.14232 \n",
      "[ 14/500] train_loss: 0.12135 valid_loss: 0.12665 test_loss: 0.13925 \n",
      "[ 15/500] train_loss: 0.11698 valid_loss: 0.12235 test_loss: 0.13754 \n",
      "验证损失减少 (0.126043 --> 0.122354). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11840 valid_loss: 0.12202 test_loss: 0.13576 \n",
      "验证损失减少 (0.122354 --> 0.122015). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11515 valid_loss: 0.12290 test_loss: 0.13445 \n",
      "[ 18/500] train_loss: 0.11355 valid_loss: 0.11817 test_loss: 0.13182 \n",
      "验证损失减少 (0.122015 --> 0.118174). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11144 valid_loss: 0.11655 test_loss: 0.13057 \n",
      "验证损失减少 (0.118174 --> 0.116550). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11037 valid_loss: 0.11502 test_loss: 0.12838 \n",
      "验证损失减少 (0.116550 --> 0.115021). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.11082 valid_loss: 0.11506 test_loss: 0.12950 \n",
      "[ 22/500] train_loss: 0.10713 valid_loss: 0.11443 test_loss: 0.13028 \n",
      "验证损失减少 (0.115021 --> 0.114426). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.11157 valid_loss: 0.11488 test_loss: 0.12861 \n",
      "[ 24/500] train_loss: 0.10769 valid_loss: 0.11700 test_loss: 0.12900 \n",
      "[ 25/500] train_loss: 0.10818 valid_loss: 0.11389 test_loss: 0.12622 \n",
      "验证损失减少 (0.114426 --> 0.113892). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10608 valid_loss: 0.11133 test_loss: 0.12552 \n",
      "验证损失减少 (0.113892 --> 0.111332). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.10911 valid_loss: 0.10983 test_loss: 0.12448 \n",
      "验证损失减少 (0.111332 --> 0.109832). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10321 valid_loss: 0.10969 test_loss: 0.12461 \n",
      "验证损失减少 (0.109832 --> 0.109692). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10073 valid_loss: 0.11001 test_loss: 0.12236 \n",
      "[ 30/500] train_loss: 0.10359 valid_loss: 0.10872 test_loss: 0.12308 \n",
      "验证损失减少 (0.109692 --> 0.108717). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.10313 valid_loss: 0.10770 test_loss: 0.12052 \n",
      "验证损失减少 (0.108717 --> 0.107695). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.10174 valid_loss: 0.10895 test_loss: 0.12190 \n",
      "[ 33/500] train_loss: 0.10175 valid_loss: 0.10744 test_loss: 0.12008 \n",
      "验证损失减少 (0.107695 --> 0.107438). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.09990 valid_loss: 0.10805 test_loss: 0.11724 \n",
      "[ 35/500] train_loss: 0.09942 valid_loss: 0.10609 test_loss: 0.11764 \n",
      "验证损失减少 (0.107438 --> 0.106092). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09880 valid_loss: 0.10440 test_loss: 0.11629 \n",
      "验证损失减少 (0.106092 --> 0.104400). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.10109 valid_loss: 0.10134 test_loss: 0.11615 \n",
      "验证损失减少 (0.104400 --> 0.101336). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.09555 valid_loss: 0.10381 test_loss: 0.11644 \n",
      "[ 39/500] train_loss: 0.09692 valid_loss: 0.10222 test_loss: 0.11562 \n",
      "[ 40/500] train_loss: 0.09594 valid_loss: 0.10110 test_loss: 0.11396 \n",
      "验证损失减少 (0.101336 --> 0.101099). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.09596 valid_loss: 0.10307 test_loss: 0.11546 \n",
      "[ 42/500] train_loss: 0.09605 valid_loss: 0.10045 test_loss: 0.11426 \n",
      "验证损失减少 (0.101099 --> 0.100454). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.09324 valid_loss: 0.10187 test_loss: 0.11611 \n",
      "[ 44/500] train_loss: 0.09521 valid_loss: 0.10039 test_loss: 0.11256 \n",
      "验证损失减少 (0.100454 --> 0.100391). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.09420 valid_loss: 0.10057 test_loss: 0.11277 \n",
      "[ 46/500] train_loss: 0.09324 valid_loss: 0.10258 test_loss: 0.11439 \n",
      "[ 47/500] train_loss: 0.09316 valid_loss: 0.10066 test_loss: 0.11439 \n",
      "[ 48/500] train_loss: 0.09251 valid_loss: 0.09842 test_loss: 0.11132 \n",
      "验证损失减少 (0.100391 --> 0.098421). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09385 valid_loss: 0.10082 test_loss: 0.11157 \n",
      "[ 50/500] train_loss: 0.09543 valid_loss: 0.09670 test_loss: 0.10974 \n",
      "验证损失减少 (0.098421 --> 0.096705). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.08930 valid_loss: 0.10012 test_loss: 0.11078 \n",
      "[ 52/500] train_loss: 0.09075 valid_loss: 0.10087 test_loss: 0.11011 \n",
      "[ 53/500] train_loss: 0.08980 valid_loss: 0.09989 test_loss: 0.11108 \n",
      "[ 54/500] train_loss: 0.09167 valid_loss: 0.09761 test_loss: 0.10946 \n",
      "[ 55/500] train_loss: 0.09053 valid_loss: 0.09756 test_loss: 0.10884 \n",
      "[ 56/500] train_loss: 0.09081 valid_loss: 0.09612 test_loss: 0.10814 \n",
      "验证损失减少 (0.096705 --> 0.096124). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.08876 valid_loss: 0.09797 test_loss: 0.10852 \n",
      "[ 58/500] train_loss: 0.08933 valid_loss: 0.09540 test_loss: 0.10711 \n",
      "验证损失减少 (0.096124 --> 0.095396). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.08948 valid_loss: 0.09576 test_loss: 0.10787 \n",
      "[ 60/500] train_loss: 0.09043 valid_loss: 0.09435 test_loss: 0.10759 \n",
      "验证损失减少 (0.095396 --> 0.094354). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.08785 valid_loss: 0.09566 test_loss: 0.10904 \n",
      "[ 62/500] train_loss: 0.08515 valid_loss: 0.09380 test_loss: 0.10668 \n",
      "验证损失减少 (0.094354 --> 0.093800). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.08981 valid_loss: 0.09309 test_loss: 0.10541 \n",
      "验证损失减少 (0.093800 --> 0.093094). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.08879 valid_loss: 0.09598 test_loss: 0.10695 \n",
      "[ 65/500] train_loss: 0.08654 valid_loss: 0.09379 test_loss: 0.10511 \n",
      "[ 66/500] train_loss: 0.08669 valid_loss: 0.09163 test_loss: 0.10506 \n",
      "验证损失减少 (0.093094 --> 0.091629). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.08795 valid_loss: 0.09364 test_loss: 0.10353 \n",
      "[ 68/500] train_loss: 0.08916 valid_loss: 0.09316 test_loss: 0.10414 \n",
      "[ 69/500] train_loss: 0.08694 valid_loss: 0.09049 test_loss: 0.10365 \n",
      "验证损失减少 (0.091629 --> 0.090488). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.08451 valid_loss: 0.09186 test_loss: 0.10508 \n",
      "[ 71/500] train_loss: 0.08453 valid_loss: 0.09135 test_loss: 0.10403 \n",
      "[ 72/500] train_loss: 0.08568 valid_loss: 0.09457 test_loss: 0.10403 \n",
      "[ 73/500] train_loss: 0.08345 valid_loss: 0.09147 test_loss: 0.10438 \n",
      "[ 74/500] train_loss: 0.08272 valid_loss: 0.09296 test_loss: 0.10405 \n",
      "[ 75/500] train_loss: 0.08482 valid_loss: 0.08941 test_loss: 0.10250 \n",
      "验证损失减少 (0.090488 --> 0.089408). 正在保存模型...\n",
      "[ 76/500] train_loss: 0.08575 valid_loss: 0.09079 test_loss: 0.10275 \n",
      "[ 77/500] train_loss: 0.08617 valid_loss: 0.09212 test_loss: 0.10518 \n",
      "[ 78/500] train_loss: 0.08422 valid_loss: 0.09161 test_loss: 0.10311 \n",
      "[ 79/500] train_loss: 0.08403 valid_loss: 0.08955 test_loss: 0.10146 \n",
      "[ 80/500] train_loss: 0.08103 valid_loss: 0.09052 test_loss: 0.10250 \n",
      "[ 81/500] train_loss: 0.08210 valid_loss: 0.09201 test_loss: 0.10248 \n",
      "[ 82/500] train_loss: 0.08326 valid_loss: 0.09106 test_loss: 0.10392 \n",
      "[ 83/500] train_loss: 0.08294 valid_loss: 0.08770 test_loss: 0.10078 \n",
      "验证损失减少 (0.089408 --> 0.087699). 正在保存模型...\n",
      "[ 84/500] train_loss: 0.08007 valid_loss: 0.08996 test_loss: 0.10254 \n",
      "[ 85/500] train_loss: 0.08144 valid_loss: 0.08926 test_loss: 0.10173 \n",
      "[ 86/500] train_loss: 0.08056 valid_loss: 0.09086 test_loss: 0.10310 \n",
      "[ 87/500] train_loss: 0.07830 valid_loss: 0.08964 test_loss: 0.10096 \n",
      "[ 88/500] train_loss: 0.08076 valid_loss: 0.08883 test_loss: 0.10143 \n",
      "[ 89/500] train_loss: 0.08077 valid_loss: 0.08977 test_loss: 0.10214 \n",
      "[ 90/500] train_loss: 0.08464 valid_loss: 0.08839 test_loss: 0.09992 \n",
      "[ 91/500] train_loss: 0.08229 valid_loss: 0.08634 test_loss: 0.10010 \n",
      "验证损失减少 (0.087699 --> 0.086338). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.07965 valid_loss: 0.08724 test_loss: 0.10009 \n",
      "[ 93/500] train_loss: 0.08000 valid_loss: 0.08735 test_loss: 0.09875 \n",
      "[ 94/500] train_loss: 0.08095 valid_loss: 0.08928 test_loss: 0.10032 \n",
      "[ 95/500] train_loss: 0.08156 valid_loss: 0.08651 test_loss: 0.09890 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 96/500] train_loss: 0.08344 valid_loss: 0.08700 test_loss: 0.09893 \n",
      "[ 97/500] train_loss: 0.07965 valid_loss: 0.08632 test_loss: 0.10026 \n",
      "验证损失减少 (0.086338 --> 0.086316). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.08056 valid_loss: 0.08764 test_loss: 0.10018 \n",
      "[ 99/500] train_loss: 0.07897 valid_loss: 0.08706 test_loss: 0.09944 \n",
      "[100/500] train_loss: 0.07720 valid_loss: 0.08676 test_loss: 0.09875 \n",
      "[101/500] train_loss: 0.07786 valid_loss: 0.08616 test_loss: 0.09764 \n",
      "验证损失减少 (0.086316 --> 0.086156). 正在保存模型...\n",
      "[102/500] train_loss: 0.08026 valid_loss: 0.08786 test_loss: 0.10018 \n",
      "[103/500] train_loss: 0.07822 valid_loss: 0.08621 test_loss: 0.09758 \n",
      "[104/500] train_loss: 0.07658 valid_loss: 0.08728 test_loss: 0.09902 \n",
      "[105/500] train_loss: 0.07715 valid_loss: 0.08916 test_loss: 0.09771 \n",
      "[106/500] train_loss: 0.07802 valid_loss: 0.08666 test_loss: 0.09549 \n",
      "[107/500] train_loss: 0.07595 valid_loss: 0.08840 test_loss: 0.09774 \n",
      "[108/500] train_loss: 0.07565 valid_loss: 0.08544 test_loss: 0.09619 \n",
      "验证损失减少 (0.086156 --> 0.085437). 正在保存模型...\n",
      "[109/500] train_loss: 0.07783 valid_loss: 0.08522 test_loss: 0.09749 \n",
      "验证损失减少 (0.085437 --> 0.085218). 正在保存模型...\n",
      "[110/500] train_loss: 0.07751 valid_loss: 0.08675 test_loss: 0.09820 \n",
      "[111/500] train_loss: 0.07601 valid_loss: 0.08406 test_loss: 0.09657 \n",
      "验证损失减少 (0.085218 --> 0.084062). 正在保存模型...\n",
      "[112/500] train_loss: 0.07609 valid_loss: 0.08553 test_loss: 0.09710 \n",
      "[113/500] train_loss: 0.07815 valid_loss: 0.08528 test_loss: 0.09624 \n",
      "[114/500] train_loss: 0.07714 valid_loss: 0.08562 test_loss: 0.09698 \n",
      "[115/500] train_loss: 0.07581 valid_loss: 0.08574 test_loss: 0.09715 \n",
      "[116/500] train_loss: 0.07656 valid_loss: 0.08515 test_loss: 0.09632 \n",
      "[117/500] train_loss: 0.07500 valid_loss: 0.08449 test_loss: 0.09560 \n",
      "[118/500] train_loss: 0.07268 valid_loss: 0.08487 test_loss: 0.09549 \n",
      "[119/500] train_loss: 0.07380 valid_loss: 0.08314 test_loss: 0.09633 \n",
      "验证损失减少 (0.084062 --> 0.083143). 正在保存模型...\n",
      "[120/500] train_loss: 0.07608 valid_loss: 0.08503 test_loss: 0.09484 \n",
      "[121/500] train_loss: 0.07589 valid_loss: 0.08492 test_loss: 0.09573 \n",
      "[122/500] train_loss: 0.07643 valid_loss: 0.08863 test_loss: 0.09490 \n",
      "[123/500] train_loss: 0.07587 valid_loss: 0.08503 test_loss: 0.09518 \n",
      "[124/500] train_loss: 0.07355 valid_loss: 0.08354 test_loss: 0.09503 \n",
      "[125/500] train_loss: 0.07443 valid_loss: 0.08415 test_loss: 0.09641 \n",
      "[126/500] train_loss: 0.07676 valid_loss: 0.08351 test_loss: 0.09546 \n",
      "[127/500] train_loss: 0.07516 valid_loss: 0.08251 test_loss: 0.09465 \n",
      "验证损失减少 (0.083143 --> 0.082513). 正在保存模型...\n",
      "[128/500] train_loss: 0.07505 valid_loss: 0.08580 test_loss: 0.09574 \n",
      "[129/500] train_loss: 0.07545 valid_loss: 0.08441 test_loss: 0.09546 \n",
      "[130/500] train_loss: 0.07107 valid_loss: 0.08701 test_loss: 0.09597 \n",
      "[131/500] train_loss: 0.07380 valid_loss: 0.08342 test_loss: 0.09588 \n",
      "[132/500] train_loss: 0.07452 valid_loss: 0.08408 test_loss: 0.09523 \n",
      "[133/500] train_loss: 0.07269 valid_loss: 0.08612 test_loss: 0.09575 \n",
      "[134/500] train_loss: 0.07385 valid_loss: 0.08567 test_loss: 0.09693 \n",
      "[135/500] train_loss: 0.07377 valid_loss: 0.08509 test_loss: 0.09311 \n",
      "[136/500] train_loss: 0.07067 valid_loss: 0.08461 test_loss: 0.09357 \n",
      "[137/500] train_loss: 0.07189 valid_loss: 0.08225 test_loss: 0.09212 \n",
      "验证损失减少 (0.082513 --> 0.082251). 正在保存模型...\n",
      "[138/500] train_loss: 0.07346 valid_loss: 0.08732 test_loss: 0.09377 \n",
      "[139/500] train_loss: 0.07318 valid_loss: 0.08486 test_loss: 0.09397 \n",
      "[140/500] train_loss: 0.07495 valid_loss: 0.08408 test_loss: 0.09284 \n",
      "[141/500] train_loss: 0.07439 valid_loss: 0.08385 test_loss: 0.09331 \n",
      "[142/500] train_loss: 0.07198 valid_loss: 0.08377 test_loss: 0.09373 \n",
      "[143/500] train_loss: 0.07283 valid_loss: 0.08218 test_loss: 0.09319 \n",
      "验证损失减少 (0.082251 --> 0.082180). 正在保存模型...\n",
      "[144/500] train_loss: 0.07303 valid_loss: 0.08267 test_loss: 0.09401 \n",
      "[145/500] train_loss: 0.07149 valid_loss: 0.08240 test_loss: 0.09381 \n",
      "[146/500] train_loss: 0.07137 valid_loss: 0.08142 test_loss: 0.09252 \n",
      "验证损失减少 (0.082180 --> 0.081420). 正在保存模型...\n",
      "[147/500] train_loss: 0.07147 valid_loss: 0.08141 test_loss: 0.09208 \n",
      "验证损失减少 (0.081420 --> 0.081406). 正在保存模型...\n",
      "[148/500] train_loss: 0.07184 valid_loss: 0.08198 test_loss: 0.09363 \n",
      "[149/500] train_loss: 0.07078 valid_loss: 0.08262 test_loss: 0.09485 \n",
      "[150/500] train_loss: 0.07233 valid_loss: 0.08407 test_loss: 0.09325 \n",
      "[151/500] train_loss: 0.07200 valid_loss: 0.08358 test_loss: 0.09213 \n",
      "[152/500] train_loss: 0.07154 valid_loss: 0.08164 test_loss: 0.09294 \n",
      "[153/500] train_loss: 0.07293 valid_loss: 0.08134 test_loss: 0.09137 \n",
      "验证损失减少 (0.081406 --> 0.081343). 正在保存模型...\n",
      "[154/500] train_loss: 0.06967 valid_loss: 0.08387 test_loss: 0.09213 \n",
      "[155/500] train_loss: 0.06989 valid_loss: 0.08317 test_loss: 0.09298 \n",
      "[156/500] train_loss: 0.06895 valid_loss: 0.08048 test_loss: 0.09136 \n",
      "验证损失减少 (0.081343 --> 0.080480). 正在保存模型...\n",
      "[157/500] train_loss: 0.06979 valid_loss: 0.08160 test_loss: 0.09308 \n",
      "[158/500] train_loss: 0.06880 valid_loss: 0.08122 test_loss: 0.09178 \n",
      "[159/500] train_loss: 0.07000 valid_loss: 0.08234 test_loss: 0.09115 \n",
      "[160/500] train_loss: 0.07220 valid_loss: 0.08053 test_loss: 0.09169 \n",
      "[161/500] train_loss: 0.07041 valid_loss: 0.08041 test_loss: 0.09217 \n",
      "验证损失减少 (0.080480 --> 0.080413). 正在保存模型...\n",
      "[162/500] train_loss: 0.06940 valid_loss: 0.08079 test_loss: 0.09146 \n",
      "[163/500] train_loss: 0.06852 valid_loss: 0.08046 test_loss: 0.09146 \n",
      "[164/500] train_loss: 0.07103 valid_loss: 0.08036 test_loss: 0.09074 \n",
      "验证损失减少 (0.080413 --> 0.080360). 正在保存模型...\n",
      "[165/500] train_loss: 0.06905 valid_loss: 0.08199 test_loss: 0.09100 \n",
      "[166/500] train_loss: 0.06955 valid_loss: 0.08111 test_loss: 0.09018 \n",
      "[167/500] train_loss: 0.06977 valid_loss: 0.08264 test_loss: 0.09198 \n",
      "[168/500] train_loss: 0.06833 valid_loss: 0.07991 test_loss: 0.09195 \n",
      "验证损失减少 (0.080360 --> 0.079915). 正在保存模型...\n",
      "[169/500] train_loss: 0.06946 valid_loss: 0.08204 test_loss: 0.09071 \n",
      "[170/500] train_loss: 0.06975 valid_loss: 0.08487 test_loss: 0.09075 \n",
      "[171/500] train_loss: 0.06912 valid_loss: 0.08075 test_loss: 0.09144 \n",
      "[172/500] train_loss: 0.06935 valid_loss: 0.07999 test_loss: 0.09141 \n",
      "[173/500] train_loss: 0.06919 valid_loss: 0.07989 test_loss: 0.08988 \n",
      "验证损失减少 (0.079915 --> 0.079888). 正在保存模型...\n",
      "[174/500] train_loss: 0.06894 valid_loss: 0.08131 test_loss: 0.09076 \n",
      "[175/500] train_loss: 0.06815 valid_loss: 0.08096 test_loss: 0.09085 \n",
      "[176/500] train_loss: 0.06831 valid_loss: 0.08114 test_loss: 0.09118 \n",
      "[177/500] train_loss: 0.06979 valid_loss: 0.08212 test_loss: 0.08985 \n",
      "[178/500] train_loss: 0.06914 valid_loss: 0.07991 test_loss: 0.09026 \n",
      "[179/500] train_loss: 0.06860 valid_loss: 0.07894 test_loss: 0.08946 \n",
      "验证损失减少 (0.079888 --> 0.078938). 正在保存模型...\n",
      "[180/500] train_loss: 0.06765 valid_loss: 0.08063 test_loss: 0.08893 \n",
      "[181/500] train_loss: 0.06771 valid_loss: 0.08007 test_loss: 0.08943 \n",
      "[182/500] train_loss: 0.06722 valid_loss: 0.08704 test_loss: 0.09131 \n",
      "[183/500] train_loss: 0.06915 valid_loss: 0.08020 test_loss: 0.08899 \n",
      "[184/500] train_loss: 0.06647 valid_loss: 0.08391 test_loss: 0.08979 \n",
      "[185/500] train_loss: 0.06919 valid_loss: 0.07890 test_loss: 0.08977 \n",
      "验证损失减少 (0.078938 --> 0.078895). 正在保存模型...\n",
      "[186/500] train_loss: 0.06684 valid_loss: 0.08110 test_loss: 0.09106 \n",
      "[187/500] train_loss: 0.06609 valid_loss: 0.07922 test_loss: 0.08978 \n",
      "[188/500] train_loss: 0.06885 valid_loss: 0.07950 test_loss: 0.08896 \n",
      "[189/500] train_loss: 0.06632 valid_loss: 0.07904 test_loss: 0.09034 \n",
      "[190/500] train_loss: 0.06722 valid_loss: 0.08004 test_loss: 0.09058 \n",
      "[191/500] train_loss: 0.06643 valid_loss: 0.08035 test_loss: 0.08991 \n",
      "[192/500] train_loss: 0.06705 valid_loss: 0.07904 test_loss: 0.08961 \n",
      "[193/500] train_loss: 0.06649 valid_loss: 0.08205 test_loss: 0.09009 \n",
      "[194/500] train_loss: 0.06801 valid_loss: 0.07965 test_loss: 0.08884 \n",
      "[195/500] train_loss: 0.06691 valid_loss: 0.07965 test_loss: 0.08951 \n",
      "[196/500] train_loss: 0.06615 valid_loss: 0.07965 test_loss: 0.09025 \n",
      "[197/500] train_loss: 0.06579 valid_loss: 0.08003 test_loss: 0.08949 \n",
      "[198/500] train_loss: 0.06803 valid_loss: 0.08383 test_loss: 0.08901 \n",
      "[199/500] train_loss: 0.06762 valid_loss: 0.08002 test_loss: 0.08903 \n",
      "[200/500] train_loss: 0.06493 valid_loss: 0.07950 test_loss: 0.08810 \n",
      "[201/500] train_loss: 0.06761 valid_loss: 0.07799 test_loss: 0.08874 \n",
      "验证损失减少 (0.078895 --> 0.077991). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202/500] train_loss: 0.06621 valid_loss: 0.07857 test_loss: 0.09021 \n",
      "[203/500] train_loss: 0.06641 valid_loss: 0.08042 test_loss: 0.08981 \n",
      "[204/500] train_loss: 0.06358 valid_loss: 0.08478 test_loss: 0.08997 \n",
      "[205/500] train_loss: 0.06443 valid_loss: 0.08573 test_loss: 0.08845 \n",
      "[206/500] train_loss: 0.06480 valid_loss: 0.07895 test_loss: 0.08935 \n",
      "[207/500] train_loss: 0.06475 valid_loss: 0.07908 test_loss: 0.08835 \n",
      "[208/500] train_loss: 0.06621 valid_loss: 0.08133 test_loss: 0.08981 \n",
      "[209/500] train_loss: 0.06507 valid_loss: 0.07982 test_loss: 0.08801 \n",
      "[210/500] train_loss: 0.06505 valid_loss: 0.08161 test_loss: 0.08765 \n",
      "[211/500] train_loss: 0.06682 valid_loss: 0.07903 test_loss: 0.09001 \n",
      "[212/500] train_loss: 0.06458 valid_loss: 0.08538 test_loss: 0.08768 \n",
      "[213/500] train_loss: 0.06506 valid_loss: 0.08106 test_loss: 0.08856 \n",
      "[214/500] train_loss: 0.06555 valid_loss: 0.07984 test_loss: 0.08795 \n",
      "[215/500] train_loss: 0.06475 valid_loss: 0.08045 test_loss: 0.08800 \n",
      "[216/500] train_loss: 0.06682 valid_loss: 0.07868 test_loss: 0.08827 \n",
      "[217/500] train_loss: 0.06630 valid_loss: 0.07805 test_loss: 0.08877 \n",
      "[218/500] train_loss: 0.06500 valid_loss: 0.07833 test_loss: 0.08852 \n",
      "[219/500] train_loss: 0.06456 valid_loss: 0.07891 test_loss: 0.08891 \n",
      "[220/500] train_loss: 0.06324 valid_loss: 0.07926 test_loss: 0.08831 \n",
      "[221/500] train_loss: 0.06296 valid_loss: 0.07902 test_loss: 0.08831 \n",
      "[222/500] train_loss: 0.06402 valid_loss: 0.07875 test_loss: 0.08748 \n",
      "[223/500] train_loss: 0.06384 valid_loss: 0.07889 test_loss: 0.08820 \n",
      "[224/500] train_loss: 0.06399 valid_loss: 0.07759 test_loss: 0.08860 \n",
      "验证损失减少 (0.077991 --> 0.077592). 正在保存模型...\n",
      "[225/500] train_loss: 0.06390 valid_loss: 0.07857 test_loss: 0.08767 \n",
      "[226/500] train_loss: 0.06483 valid_loss: 0.08207 test_loss: 0.08798 \n",
      "[227/500] train_loss: 0.06344 valid_loss: 0.07778 test_loss: 0.08856 \n",
      "[228/500] train_loss: 0.06567 valid_loss: 0.07824 test_loss: 0.08743 \n",
      "[229/500] train_loss: 0.06332 valid_loss: 0.07979 test_loss: 0.08773 \n",
      "[230/500] train_loss: 0.06709 valid_loss: 0.07861 test_loss: 0.08830 \n",
      "[231/500] train_loss: 0.06138 valid_loss: 0.08114 test_loss: 0.09017 \n",
      "[232/500] train_loss: 0.06385 valid_loss: 0.07822 test_loss: 0.08785 \n",
      "[233/500] train_loss: 0.06593 valid_loss: 0.07906 test_loss: 0.08783 \n",
      "[234/500] train_loss: 0.06381 valid_loss: 0.07996 test_loss: 0.08761 \n",
      "[235/500] train_loss: 0.06458 valid_loss: 0.07818 test_loss: 0.08836 \n",
      "[236/500] train_loss: 0.06353 valid_loss: 0.07700 test_loss: 0.08750 \n",
      "验证损失减少 (0.077592 --> 0.076999). 正在保存模型...\n",
      "[237/500] train_loss: 0.06327 valid_loss: 0.07905 test_loss: 0.08876 \n",
      "[238/500] train_loss: 0.06144 valid_loss: 0.07792 test_loss: 0.08872 \n",
      "[239/500] train_loss: 0.06557 valid_loss: 0.07726 test_loss: 0.08891 \n",
      "[240/500] train_loss: 0.06268 valid_loss: 0.07945 test_loss: 0.08952 \n",
      "[241/500] train_loss: 0.06192 valid_loss: 0.07986 test_loss: 0.08882 \n",
      "[242/500] train_loss: 0.06145 valid_loss: 0.07961 test_loss: 0.08956 \n",
      "[243/500] train_loss: 0.06281 valid_loss: 0.07989 test_loss: 0.08924 \n",
      "[244/500] train_loss: 0.06477 valid_loss: 0.07781 test_loss: 0.08847 \n",
      "[245/500] train_loss: 0.06313 valid_loss: 0.07850 test_loss: 0.08946 \n",
      "[246/500] train_loss: 0.06317 valid_loss: 0.07798 test_loss: 0.08894 \n",
      "[247/500] train_loss: 0.06230 valid_loss: 0.08063 test_loss: 0.08850 \n",
      "[248/500] train_loss: 0.06186 valid_loss: 0.07933 test_loss: 0.09052 \n",
      "[249/500] train_loss: 0.06120 valid_loss: 0.07953 test_loss: 0.08904 \n",
      "[250/500] train_loss: 0.06211 valid_loss: 0.07733 test_loss: 0.08876 \n",
      "[251/500] train_loss: 0.06251 valid_loss: 0.07936 test_loss: 0.09022 \n",
      "[252/500] train_loss: 0.06063 valid_loss: 0.07864 test_loss: 0.08883 \n",
      "[253/500] train_loss: 0.06457 valid_loss: 0.08006 test_loss: 0.08801 \n",
      "[254/500] train_loss: 0.06278 valid_loss: 0.07813 test_loss: 0.08872 \n",
      "[255/500] train_loss: 0.06124 valid_loss: 0.07852 test_loss: 0.08807 \n",
      "[256/500] train_loss: 0.06111 valid_loss: 0.07730 test_loss: 0.08787 \n",
      "[257/500] train_loss: 0.06120 valid_loss: 0.07972 test_loss: 0.08864 \n",
      "[258/500] train_loss: 0.06102 valid_loss: 0.07833 test_loss: 0.08872 \n",
      "[259/500] train_loss: 0.06309 valid_loss: 0.07951 test_loss: 0.09036 \n",
      "[260/500] train_loss: 0.06107 valid_loss: 0.07815 test_loss: 0.08873 \n",
      "[261/500] train_loss: 0.06093 valid_loss: 0.07865 test_loss: 0.08808 \n",
      "[262/500] train_loss: 0.06299 valid_loss: 0.07723 test_loss: 0.08924 \n",
      "[263/500] train_loss: 0.06246 valid_loss: 0.07748 test_loss: 0.08968 \n",
      "[264/500] train_loss: 0.06279 valid_loss: 0.07610 test_loss: 0.08819 \n",
      "验证损失减少 (0.076999 --> 0.076100). 正在保存模型...\n",
      "[265/500] train_loss: 0.06004 valid_loss: 0.07579 test_loss: 0.08722 \n",
      "验证损失减少 (0.076100 --> 0.075794). 正在保存模型...\n",
      "[266/500] train_loss: 0.06075 valid_loss: 0.07728 test_loss: 0.08756 \n",
      "[267/500] train_loss: 0.06195 valid_loss: 0.07679 test_loss: 0.08864 \n",
      "[268/500] train_loss: 0.06238 valid_loss: 0.07630 test_loss: 0.08717 \n",
      "[269/500] train_loss: 0.05987 valid_loss: 0.07844 test_loss: 0.08854 \n",
      "[270/500] train_loss: 0.06193 valid_loss: 0.07657 test_loss: 0.08674 \n",
      "[271/500] train_loss: 0.06177 valid_loss: 0.07805 test_loss: 0.08897 \n",
      "[272/500] train_loss: 0.06184 valid_loss: 0.07681 test_loss: 0.08717 \n",
      "[273/500] train_loss: 0.06077 valid_loss: 0.07702 test_loss: 0.08770 \n",
      "[274/500] train_loss: 0.06165 valid_loss: 0.07574 test_loss: 0.08711 \n",
      "验证损失减少 (0.075794 --> 0.075743). 正在保存模型...\n",
      "[275/500] train_loss: 0.06150 valid_loss: 0.07927 test_loss: 0.08850 \n",
      "[276/500] train_loss: 0.05961 valid_loss: 0.07849 test_loss: 0.08817 \n",
      "[277/500] train_loss: 0.06085 valid_loss: 0.07745 test_loss: 0.08797 \n",
      "[278/500] train_loss: 0.06080 valid_loss: 0.07980 test_loss: 0.08842 \n",
      "[279/500] train_loss: 0.06101 valid_loss: 0.07669 test_loss: 0.08919 \n",
      "[280/500] train_loss: 0.06040 valid_loss: 0.07899 test_loss: 0.08687 \n",
      "[281/500] train_loss: 0.06008 valid_loss: 0.07737 test_loss: 0.08728 \n",
      "[282/500] train_loss: 0.06206 valid_loss: 0.08025 test_loss: 0.08721 \n",
      "[283/500] train_loss: 0.06024 valid_loss: 0.07736 test_loss: 0.08804 \n",
      "[284/500] train_loss: 0.05978 valid_loss: 0.07820 test_loss: 0.08794 \n",
      "[285/500] train_loss: 0.06239 valid_loss: 0.07756 test_loss: 0.08693 \n",
      "[286/500] train_loss: 0.06085 valid_loss: 0.07600 test_loss: 0.08688 \n",
      "[287/500] train_loss: 0.06090 valid_loss: 0.07819 test_loss: 0.08702 \n",
      "[288/500] train_loss: 0.06096 valid_loss: 0.07666 test_loss: 0.08734 \n",
      "[289/500] train_loss: 0.05999 valid_loss: 0.08027 test_loss: 0.08573 \n",
      "[290/500] train_loss: 0.05935 valid_loss: 0.08105 test_loss: 0.08675 \n",
      "[291/500] train_loss: 0.06076 valid_loss: 0.08349 test_loss: 0.08587 \n",
      "[292/500] train_loss: 0.05973 valid_loss: 0.07849 test_loss: 0.08897 \n",
      "[293/500] train_loss: 0.06134 valid_loss: 0.08382 test_loss: 0.08779 \n",
      "[294/500] train_loss: 0.05939 valid_loss: 0.08302 test_loss: 0.08905 \n",
      "[295/500] train_loss: 0.05939 valid_loss: 0.09336 test_loss: 0.08780 \n",
      "[296/500] train_loss: 0.05930 valid_loss: 0.08751 test_loss: 0.08754 \n",
      "[297/500] train_loss: 0.06008 valid_loss: 0.08782 test_loss: 0.08831 \n",
      "[298/500] train_loss: 0.05823 valid_loss: 0.07669 test_loss: 0.08768 \n",
      "[299/500] train_loss: 0.05872 valid_loss: 0.08632 test_loss: 0.08786 \n",
      "[300/500] train_loss: 0.06008 valid_loss: 0.09160 test_loss: 0.08638 \n",
      "[301/500] train_loss: 0.06121 valid_loss: 0.08012 test_loss: 0.08783 \n",
      "[302/500] train_loss: 0.06091 valid_loss: 0.07757 test_loss: 0.08799 \n",
      "[303/500] train_loss: 0.05925 valid_loss: 0.07583 test_loss: 0.08673 \n",
      "[304/500] train_loss: 0.06012 valid_loss: 0.07467 test_loss: 0.08609 \n",
      "验证损失减少 (0.075743 --> 0.074669). 正在保存模型...\n",
      "[305/500] train_loss: 0.05874 valid_loss: 0.07625 test_loss: 0.08733 \n",
      "[306/500] train_loss: 0.05988 valid_loss: 0.07581 test_loss: 0.08500 \n",
      "[307/500] train_loss: 0.05894 valid_loss: 0.07575 test_loss: 0.08517 \n",
      "[308/500] train_loss: 0.05885 valid_loss: 0.07943 test_loss: 0.08630 \n",
      "[309/500] train_loss: 0.05922 valid_loss: 0.07807 test_loss: 0.08711 \n",
      "[310/500] train_loss: 0.05786 valid_loss: 0.07562 test_loss: 0.08668 \n",
      "[311/500] train_loss: 0.06011 valid_loss: 0.07649 test_loss: 0.08601 \n",
      "[312/500] train_loss: 0.05992 valid_loss: 0.07653 test_loss: 0.08566 \n",
      "[313/500] train_loss: 0.05855 valid_loss: 0.08469 test_loss: 0.08811 \n",
      "[314/500] train_loss: 0.06005 valid_loss: 0.07965 test_loss: 0.08796 \n",
      "[315/500] train_loss: 0.05979 valid_loss: 0.07925 test_loss: 0.08865 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[316/500] train_loss: 0.06045 valid_loss: 0.08133 test_loss: 0.08737 \n",
      "[317/500] train_loss: 0.05967 valid_loss: 0.07750 test_loss: 0.08635 \n",
      "[318/500] train_loss: 0.05680 valid_loss: 0.07669 test_loss: 0.08737 \n",
      "[319/500] train_loss: 0.05958 valid_loss: 0.07584 test_loss: 0.08788 \n",
      "[320/500] train_loss: 0.05782 valid_loss: 0.07587 test_loss: 0.08775 \n",
      "[321/500] train_loss: 0.05820 valid_loss: 0.07640 test_loss: 0.08577 \n",
      "[322/500] train_loss: 0.05767 valid_loss: 0.07706 test_loss: 0.08653 \n",
      "[323/500] train_loss: 0.05778 valid_loss: 0.07626 test_loss: 0.08596 \n",
      "[324/500] train_loss: 0.05635 valid_loss: 0.07657 test_loss: 0.08608 \n",
      "[325/500] train_loss: 0.05817 valid_loss: 0.07638 test_loss: 0.08750 \n",
      "[326/500] train_loss: 0.05734 valid_loss: 0.07628 test_loss: 0.08885 \n",
      "[327/500] train_loss: 0.05904 valid_loss: 0.07611 test_loss: 0.08843 \n",
      "[328/500] train_loss: 0.05791 valid_loss: 0.07581 test_loss: 0.08843 \n",
      "[329/500] train_loss: 0.05881 valid_loss: 0.07652 test_loss: 0.08825 \n",
      "[330/500] train_loss: 0.05759 valid_loss: 0.07603 test_loss: 0.08687 \n",
      "[331/500] train_loss: 0.05707 valid_loss: 0.07518 test_loss: 0.08719 \n",
      "[332/500] train_loss: 0.05858 valid_loss: 0.07846 test_loss: 0.08922 \n",
      "[333/500] train_loss: 0.05680 valid_loss: 0.07513 test_loss: 0.08720 \n",
      "[334/500] train_loss: 0.05707 valid_loss: 0.07498 test_loss: 0.08803 \n",
      "[335/500] train_loss: 0.05923 valid_loss: 0.07600 test_loss: 0.08753 \n",
      "[336/500] train_loss: 0.05675 valid_loss: 0.07603 test_loss: 0.08653 \n",
      "[337/500] train_loss: 0.05688 valid_loss: 0.07960 test_loss: 0.08776 \n",
      "[338/500] train_loss: 0.05776 valid_loss: 0.07525 test_loss: 0.08584 \n",
      "[339/500] train_loss: 0.05686 valid_loss: 0.07603 test_loss: 0.08673 \n",
      "[340/500] train_loss: 0.05712 valid_loss: 0.07615 test_loss: 0.08553 \n",
      "[341/500] train_loss: 0.05780 valid_loss: 0.07672 test_loss: 0.08763 \n",
      "[342/500] train_loss: 0.06109 valid_loss: 0.07689 test_loss: 0.08813 \n",
      "[343/500] train_loss: 0.05705 valid_loss: 0.07558 test_loss: 0.08682 \n",
      "[344/500] train_loss: 0.05766 valid_loss: 0.07485 test_loss: 0.08658 \n",
      "[345/500] train_loss: 0.05672 valid_loss: 0.07579 test_loss: 0.08747 \n",
      "[346/500] train_loss: 0.05638 valid_loss: 0.07517 test_loss: 0.08622 \n",
      "[347/500] train_loss: 0.05733 valid_loss: 0.08009 test_loss: 0.08672 \n",
      "[348/500] train_loss: 0.05656 valid_loss: 0.07671 test_loss: 0.08764 \n",
      "[349/500] train_loss: 0.05772 valid_loss: 0.07566 test_loss: 0.08612 \n",
      "[350/500] train_loss: 0.05708 valid_loss: 0.08043 test_loss: 0.08613 \n",
      "[351/500] train_loss: 0.05623 valid_loss: 0.07693 test_loss: 0.08725 \n",
      "[352/500] train_loss: 0.05611 valid_loss: 0.07618 test_loss: 0.08700 \n",
      "[353/500] train_loss: 0.05623 valid_loss: 0.07665 test_loss: 0.08703 \n",
      "[354/500] train_loss: 0.05689 valid_loss: 0.07489 test_loss: 0.08545 \n",
      "[355/500] train_loss: 0.05643 valid_loss: 0.07715 test_loss: 0.08720 \n",
      "[356/500] train_loss: 0.05684 valid_loss: 0.07612 test_loss: 0.08593 \n",
      "[357/500] train_loss: 0.05634 valid_loss: 0.07603 test_loss: 0.08790 \n",
      "[358/500] train_loss: 0.05726 valid_loss: 0.07944 test_loss: 0.08880 \n",
      "[359/500] train_loss: 0.05781 valid_loss: 0.07658 test_loss: 0.08631 \n",
      "[360/500] train_loss: 0.05621 valid_loss: 0.07618 test_loss: 0.08625 \n",
      "[361/500] train_loss: 0.05525 valid_loss: 0.07732 test_loss: 0.08758 \n",
      "[362/500] train_loss: 0.05505 valid_loss: 0.07608 test_loss: 0.08649 \n",
      "[363/500] train_loss: 0.05538 valid_loss: 0.07830 test_loss: 0.08851 \n",
      "[364/500] train_loss: 0.05616 valid_loss: 0.07734 test_loss: 0.08664 \n",
      "[365/500] train_loss: 0.05671 valid_loss: 0.08100 test_loss: 0.08663 \n",
      "[366/500] train_loss: 0.05551 valid_loss: 0.07615 test_loss: 0.08667 \n",
      "[367/500] train_loss: 0.05488 valid_loss: 0.07510 test_loss: 0.08553 \n",
      "[368/500] train_loss: 0.05584 valid_loss: 0.07605 test_loss: 0.08584 \n",
      "[369/500] train_loss: 0.05512 valid_loss: 0.07362 test_loss: 0.08592 \n",
      "验证损失减少 (0.074669 --> 0.073622). 正在保存模型...\n",
      "[370/500] train_loss: 0.05538 valid_loss: 0.07635 test_loss: 0.08609 \n",
      "[371/500] train_loss: 0.05630 valid_loss: 0.07519 test_loss: 0.08726 \n",
      "[372/500] train_loss: 0.05586 valid_loss: 0.07512 test_loss: 0.08819 \n",
      "[373/500] train_loss: 0.05505 valid_loss: 0.07593 test_loss: 0.08672 \n",
      "[374/500] train_loss: 0.05578 valid_loss: 0.07644 test_loss: 0.08707 \n",
      "[375/500] train_loss: 0.05620 valid_loss: 0.07617 test_loss: 0.08755 \n",
      "[376/500] train_loss: 0.05543 valid_loss: 0.07600 test_loss: 0.08764 \n",
      "[377/500] train_loss: 0.05608 valid_loss: 0.07548 test_loss: 0.08634 \n",
      "[378/500] train_loss: 0.05675 valid_loss: 0.07535 test_loss: 0.08625 \n",
      "[379/500] train_loss: 0.05506 valid_loss: 0.07482 test_loss: 0.08534 \n",
      "[380/500] train_loss: 0.05566 valid_loss: 0.07637 test_loss: 0.08644 \n",
      "[381/500] train_loss: 0.05488 valid_loss: 0.07627 test_loss: 0.08812 \n",
      "[382/500] train_loss: 0.05551 valid_loss: 0.07610 test_loss: 0.08808 \n",
      "[383/500] train_loss: 0.05562 valid_loss: 0.07629 test_loss: 0.08582 \n",
      "[384/500] train_loss: 0.05579 valid_loss: 0.07638 test_loss: 0.08683 \n",
      "[385/500] train_loss: 0.05533 valid_loss: 0.07878 test_loss: 0.08857 \n",
      "[386/500] train_loss: 0.05609 valid_loss: 0.07593 test_loss: 0.08608 \n",
      "[387/500] train_loss: 0.05505 valid_loss: 0.07590 test_loss: 0.08694 \n",
      "[388/500] train_loss: 0.05703 valid_loss: 0.07481 test_loss: 0.08627 \n",
      "[389/500] train_loss: 0.05576 valid_loss: 0.07394 test_loss: 0.08698 \n",
      "[390/500] train_loss: 0.05560 valid_loss: 0.07380 test_loss: 0.08644 \n",
      "[391/500] train_loss: 0.05258 valid_loss: 0.07452 test_loss: 0.08591 \n",
      "[392/500] train_loss: 0.05549 valid_loss: 0.07535 test_loss: 0.08614 \n",
      "[393/500] train_loss: 0.05660 valid_loss: 0.07378 test_loss: 0.08528 \n",
      "[394/500] train_loss: 0.05588 valid_loss: 0.07495 test_loss: 0.08845 \n",
      "[395/500] train_loss: 0.05361 valid_loss: 0.07517 test_loss: 0.08675 \n",
      "[396/500] train_loss: 0.05508 valid_loss: 0.07450 test_loss: 0.08728 \n",
      "[397/500] train_loss: 0.05340 valid_loss: 0.07478 test_loss: 0.08627 \n",
      "[398/500] train_loss: 0.05387 valid_loss: 0.07637 test_loss: 0.08671 \n",
      "[399/500] train_loss: 0.05518 valid_loss: 0.07778 test_loss: 0.08731 \n",
      "[400/500] train_loss: 0.05416 valid_loss: 0.07514 test_loss: 0.08658 \n",
      "[401/500] train_loss: 0.05421 valid_loss: 0.07510 test_loss: 0.08661 \n",
      "[402/500] train_loss: 0.05419 valid_loss: 0.07545 test_loss: 0.08688 \n",
      "[403/500] train_loss: 0.05671 valid_loss: 0.07539 test_loss: 0.08725 \n",
      "[404/500] train_loss: 0.05433 valid_loss: 0.07537 test_loss: 0.08655 \n",
      "[405/500] train_loss: 0.05531 valid_loss: 0.07541 test_loss: 0.08535 \n",
      "[406/500] train_loss: 0.05564 valid_loss: 0.07468 test_loss: 0.08629 \n",
      "[407/500] train_loss: 0.05398 valid_loss: 0.07557 test_loss: 0.08648 \n",
      "[408/500] train_loss: 0.05500 valid_loss: 0.07420 test_loss: 0.08731 \n",
      "[409/500] train_loss: 0.05499 valid_loss: 0.07642 test_loss: 0.08778 \n",
      "[410/500] train_loss: 0.05182 valid_loss: 0.07592 test_loss: 0.08905 \n",
      "[411/500] train_loss: 0.05466 valid_loss: 0.07594 test_loss: 0.08684 \n",
      "[412/500] train_loss: 0.05352 valid_loss: 0.07757 test_loss: 0.08693 \n",
      "[413/500] train_loss: 0.05475 valid_loss: 0.07631 test_loss: 0.08778 \n",
      "[414/500] train_loss: 0.05227 valid_loss: 0.07561 test_loss: 0.08663 \n",
      "[415/500] train_loss: 0.05330 valid_loss: 0.07598 test_loss: 0.08705 \n",
      "[416/500] train_loss: 0.05329 valid_loss: 0.07477 test_loss: 0.08627 \n",
      "[417/500] train_loss: 0.05518 valid_loss: 0.07611 test_loss: 0.08763 \n",
      "[418/500] train_loss: 0.05451 valid_loss: 0.07958 test_loss: 0.08782 \n",
      "[419/500] train_loss: 0.05431 valid_loss: 0.07542 test_loss: 0.08607 \n",
      "[420/500] train_loss: 0.05479 valid_loss: 0.07560 test_loss: 0.08637 \n",
      "[421/500] train_loss: 0.05346 valid_loss: 0.07547 test_loss: 0.08663 \n",
      "[422/500] train_loss: 0.05358 valid_loss: 0.07648 test_loss: 0.08820 \n",
      "[423/500] train_loss: 0.05515 valid_loss: 0.07799 test_loss: 0.08728 \n",
      "[424/500] train_loss: 0.05437 valid_loss: 0.07607 test_loss: 0.08783 \n",
      "[425/500] train_loss: 0.05235 valid_loss: 0.07466 test_loss: 0.08587 \n",
      "[426/500] train_loss: 0.05291 valid_loss: 0.07560 test_loss: 0.08701 \n",
      "[427/500] train_loss: 0.05337 valid_loss: 0.07550 test_loss: 0.08630 \n",
      "[428/500] train_loss: 0.05256 valid_loss: 0.07736 test_loss: 0.08526 \n",
      "[429/500] train_loss: 0.05380 valid_loss: 0.07655 test_loss: 0.08638 \n",
      "[430/500] train_loss: 0.05341 valid_loss: 0.07544 test_loss: 0.08787 \n",
      "[431/500] train_loss: 0.05403 valid_loss: 0.07764 test_loss: 0.08625 \n",
      "[432/500] train_loss: 0.05301 valid_loss: 0.07514 test_loss: 0.08777 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[433/500] train_loss: 0.05279 valid_loss: 0.07373 test_loss: 0.08599 \n",
      "[434/500] train_loss: 0.05329 valid_loss: 0.07746 test_loss: 0.08659 \n",
      "[435/500] train_loss: 0.05414 valid_loss: 0.07458 test_loss: 0.08554 \n",
      "[436/500] train_loss: 0.05268 valid_loss: 0.07611 test_loss: 0.08648 \n",
      "[437/500] train_loss: 0.05293 valid_loss: 0.07367 test_loss: 0.08623 \n",
      "[438/500] train_loss: 0.05230 valid_loss: 0.07671 test_loss: 0.08640 \n",
      "[439/500] train_loss: 0.05261 valid_loss: 0.07513 test_loss: 0.08649 \n",
      "[440/500] train_loss: 0.05127 valid_loss: 0.07590 test_loss: 0.08640 \n",
      "[441/500] train_loss: 0.05323 valid_loss: 0.07419 test_loss: 0.08596 \n",
      "[442/500] train_loss: 0.05286 valid_loss: 0.07410 test_loss: 0.08579 \n",
      "[443/500] train_loss: 0.05288 valid_loss: 0.07619 test_loss: 0.08735 \n",
      "[444/500] train_loss: 0.05280 valid_loss: 0.07471 test_loss: 0.08573 \n",
      "[445/500] train_loss: 0.05356 valid_loss: 0.07556 test_loss: 0.08694 \n",
      "[446/500] train_loss: 0.05149 valid_loss: 0.07630 test_loss: 0.08628 \n",
      "[447/500] train_loss: 0.05218 valid_loss: 0.07808 test_loss: 0.08724 \n",
      "[448/500] train_loss: 0.05300 valid_loss: 0.07509 test_loss: 0.08701 \n",
      "[449/500] train_loss: 0.05202 valid_loss: 0.07576 test_loss: 0.08660 \n",
      "[450/500] train_loss: 0.05143 valid_loss: 0.07382 test_loss: 0.08604 \n",
      "[451/500] train_loss: 0.05345 valid_loss: 0.07770 test_loss: 0.08666 \n",
      "[452/500] train_loss: 0.04980 valid_loss: 0.07974 test_loss: 0.08699 \n",
      "[453/500] train_loss: 0.05289 valid_loss: 0.07509 test_loss: 0.08591 \n",
      "[454/500] train_loss: 0.05211 valid_loss: 0.07663 test_loss: 0.08675 \n",
      "[455/500] train_loss: 0.05366 valid_loss: 0.08224 test_loss: 0.08618 \n",
      "[456/500] train_loss: 0.05237 valid_loss: 0.07557 test_loss: 0.08559 \n",
      "[457/500] train_loss: 0.05347 valid_loss: 0.07906 test_loss: 0.08554 \n",
      "[458/500] train_loss: 0.05247 valid_loss: 0.07702 test_loss: 0.08986 \n",
      "[459/500] train_loss: 0.05308 valid_loss: 0.07548 test_loss: 0.08680 \n",
      "[460/500] train_loss: 0.05418 valid_loss: 0.07795 test_loss: 0.08697 \n",
      "[461/500] train_loss: 0.05206 valid_loss: 0.07567 test_loss: 0.08662 \n",
      "[462/500] train_loss: 0.05285 valid_loss: 0.07550 test_loss: 0.08617 \n",
      "[463/500] train_loss: 0.05185 valid_loss: 0.07890 test_loss: 0.08649 \n",
      "[464/500] train_loss: 0.05248 valid_loss: 0.08730 test_loss: 0.08639 \n",
      "[465/500] train_loss: 0.05267 valid_loss: 0.07673 test_loss: 0.08786 \n",
      "[466/500] train_loss: 0.05116 valid_loss: 0.07737 test_loss: 0.08855 \n",
      "[467/500] train_loss: 0.05009 valid_loss: 0.07559 test_loss: 0.08700 \n",
      "[468/500] train_loss: 0.05127 valid_loss: 0.08229 test_loss: 0.08615 \n",
      "[469/500] train_loss: 0.05115 valid_loss: 0.07947 test_loss: 0.08716 \n",
      "[470/500] train_loss: 0.05230 valid_loss: 0.07647 test_loss: 0.08759 \n",
      "[471/500] train_loss: 0.05257 valid_loss: 0.07582 test_loss: 0.08778 \n",
      "[472/500] train_loss: 0.05109 valid_loss: 0.07472 test_loss: 0.08698 \n",
      "[473/500] train_loss: 0.05257 valid_loss: 0.07826 test_loss: 0.08722 \n",
      "[474/500] train_loss: 0.05359 valid_loss: 0.07609 test_loss: 0.08716 \n",
      "[475/500] train_loss: 0.05086 valid_loss: 0.07697 test_loss: 0.08753 \n",
      "[476/500] train_loss: 0.05187 valid_loss: 0.07889 test_loss: 0.08689 \n",
      "[477/500] train_loss: 0.05302 valid_loss: 0.07497 test_loss: 0.08760 \n",
      "[478/500] train_loss: 0.04980 valid_loss: 0.07502 test_loss: 0.08594 \n",
      "[479/500] train_loss: 0.05218 valid_loss: 0.07540 test_loss: 0.08808 \n",
      "[480/500] train_loss: 0.05035 valid_loss: 0.07640 test_loss: 0.08641 \n",
      "[481/500] train_loss: 0.05173 valid_loss: 0.07476 test_loss: 0.08764 \n",
      "[482/500] train_loss: 0.05115 valid_loss: 0.07725 test_loss: 0.08645 \n",
      "[483/500] train_loss: 0.05013 valid_loss: 0.07485 test_loss: 0.08677 \n",
      "[484/500] train_loss: 0.05015 valid_loss: 0.07472 test_loss: 0.08647 \n",
      "[485/500] train_loss: 0.05151 valid_loss: 0.07600 test_loss: 0.08771 \n",
      "[486/500] train_loss: 0.05371 valid_loss: 0.07512 test_loss: 0.08703 \n",
      "[487/500] train_loss: 0.05309 valid_loss: 0.07534 test_loss: 0.08761 \n",
      "[488/500] train_loss: 0.05114 valid_loss: 0.07614 test_loss: 0.08698 \n",
      "[489/500] train_loss: 0.05157 valid_loss: 0.07582 test_loss: 0.08567 \n",
      "[490/500] train_loss: 0.05189 valid_loss: 0.07713 test_loss: 0.08641 \n",
      "[491/500] train_loss: 0.05457 valid_loss: 0.07607 test_loss: 0.08593 \n",
      "[492/500] train_loss: 0.05208 valid_loss: 0.07601 test_loss: 0.08604 \n",
      "[493/500] train_loss: 0.05056 valid_loss: 0.07692 test_loss: 0.08587 \n",
      "[494/500] train_loss: 0.05259 valid_loss: 0.07554 test_loss: 0.08562 \n",
      "[495/500] train_loss: 0.05154 valid_loss: 0.07571 test_loss: 0.08578 \n",
      "[496/500] train_loss: 0.05181 valid_loss: 0.07643 test_loss: 0.08670 \n",
      "[497/500] train_loss: 0.05100 valid_loss: 0.07587 test_loss: 0.08533 \n",
      "[498/500] train_loss: 0.05212 valid_loss: 0.07639 test_loss: 0.08670 \n",
      "[499/500] train_loss: 0.05134 valid_loss: 0.07475 test_loss: 0.08545 \n",
      "[500/500] train_loss: 0.05021 valid_loss: 0.07725 test_loss: 0.08640 \n",
      "TRAINING MODEL 6\n",
      "[  1/500] train_loss: 0.38388 valid_loss: 0.26971 test_loss: 0.27623 \n",
      "验证损失减少 (inf --> 0.269711). 正在保存模型...\n",
      "[  2/500] train_loss: 0.21907 valid_loss: 0.20533 test_loss: 0.21428 \n",
      "验证损失减少 (0.269711 --> 0.205332). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17598 valid_loss: 0.17333 test_loss: 0.18470 \n",
      "验证损失减少 (0.205332 --> 0.173327). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15791 valid_loss: 0.15830 test_loss: 0.17191 \n",
      "验证损失减少 (0.173327 --> 0.158305). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14824 valid_loss: 0.14859 test_loss: 0.16343 \n",
      "验证损失减少 (0.158305 --> 0.148585). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14269 valid_loss: 0.14527 test_loss: 0.16073 \n",
      "验证损失减少 (0.148585 --> 0.145267). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13923 valid_loss: 0.13949 test_loss: 0.15503 \n",
      "验证损失减少 (0.145267 --> 0.139488). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13466 valid_loss: 0.13613 test_loss: 0.15052 \n",
      "验证损失减少 (0.139488 --> 0.136131). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13116 valid_loss: 0.13852 test_loss: 0.15131 \n",
      "[ 10/500] train_loss: 0.12475 valid_loss: 0.13325 test_loss: 0.14756 \n",
      "验证损失减少 (0.136131 --> 0.133251). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12527 valid_loss: 0.13068 test_loss: 0.14584 \n",
      "验证损失减少 (0.133251 --> 0.130682). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12089 valid_loss: 0.12657 test_loss: 0.14251 \n",
      "验证损失减少 (0.130682 --> 0.126565). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12207 valid_loss: 0.12760 test_loss: 0.14203 \n",
      "[ 14/500] train_loss: 0.12073 valid_loss: 0.12294 test_loss: 0.13899 \n",
      "验证损失减少 (0.126565 --> 0.122937). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.12208 valid_loss: 0.12094 test_loss: 0.13591 \n",
      "验证损失减少 (0.122937 --> 0.120942). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11723 valid_loss: 0.11951 test_loss: 0.13631 \n",
      "验证损失减少 (0.120942 --> 0.119507). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11536 valid_loss: 0.12039 test_loss: 0.13855 \n",
      "[ 18/500] train_loss: 0.11444 valid_loss: 0.11990 test_loss: 0.13503 \n",
      "[ 19/500] train_loss: 0.11557 valid_loss: 0.11588 test_loss: 0.13182 \n",
      "验证损失减少 (0.119507 --> 0.115876). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11392 valid_loss: 0.11746 test_loss: 0.13321 \n",
      "[ 21/500] train_loss: 0.11473 valid_loss: 0.11659 test_loss: 0.13321 \n",
      "[ 22/500] train_loss: 0.11104 valid_loss: 0.11470 test_loss: 0.13037 \n",
      "验证损失减少 (0.115876 --> 0.114702). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.11254 valid_loss: 0.11687 test_loss: 0.13112 \n",
      "[ 24/500] train_loss: 0.10595 valid_loss: 0.11468 test_loss: 0.13027 \n",
      "验证损失减少 (0.114702 --> 0.114677). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10967 valid_loss: 0.11649 test_loss: 0.13250 \n",
      "[ 26/500] train_loss: 0.10841 valid_loss: 0.11227 test_loss: 0.12928 \n",
      "验证损失减少 (0.114677 --> 0.112273). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.10712 valid_loss: 0.11545 test_loss: 0.13041 \n",
      "[ 28/500] train_loss: 0.10360 valid_loss: 0.11067 test_loss: 0.12524 \n",
      "验证损失减少 (0.112273 --> 0.110670). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10350 valid_loss: 0.10948 test_loss: 0.12438 \n",
      "验证损失减少 (0.110670 --> 0.109483). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10109 valid_loss: 0.11029 test_loss: 0.12457 \n",
      "[ 31/500] train_loss: 0.10382 valid_loss: 0.11018 test_loss: 0.12460 \n",
      "[ 32/500] train_loss: 0.10421 valid_loss: 0.10818 test_loss: 0.12364 \n",
      "验证损失减少 (0.109483 --> 0.108175). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.09861 valid_loss: 0.11094 test_loss: 0.12344 \n",
      "[ 34/500] train_loss: 0.10372 valid_loss: 0.10731 test_loss: 0.12180 \n",
      "验证损失减少 (0.108175 --> 0.107311). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.10260 valid_loss: 0.10518 test_loss: 0.12115 \n",
      "验证损失减少 (0.107311 --> 0.105185). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.10238 valid_loss: 0.10715 test_loss: 0.12100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 37/500] train_loss: 0.10115 valid_loss: 0.10602 test_loss: 0.12082 \n",
      "[ 38/500] train_loss: 0.09942 valid_loss: 0.10479 test_loss: 0.11993 \n",
      "验证损失减少 (0.105185 --> 0.104788). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.09827 valid_loss: 0.10431 test_loss: 0.11839 \n",
      "验证损失减少 (0.104788 --> 0.104312). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09904 valid_loss: 0.10268 test_loss: 0.11936 \n",
      "验证损失减少 (0.104312 --> 0.102683). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.09774 valid_loss: 0.10518 test_loss: 0.11773 \n",
      "[ 42/500] train_loss: 0.09661 valid_loss: 0.10313 test_loss: 0.11835 \n",
      "[ 43/500] train_loss: 0.09991 valid_loss: 0.10394 test_loss: 0.11866 \n",
      "[ 44/500] train_loss: 0.09707 valid_loss: 0.10828 test_loss: 0.11731 \n",
      "[ 45/500] train_loss: 0.09859 valid_loss: 0.10156 test_loss: 0.11570 \n",
      "验证损失减少 (0.102683 --> 0.101557). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.09470 valid_loss: 0.10278 test_loss: 0.11607 \n",
      "[ 47/500] train_loss: 0.09885 valid_loss: 0.10131 test_loss: 0.11567 \n",
      "验证损失减少 (0.101557 --> 0.101309). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.09557 valid_loss: 0.10258 test_loss: 0.11507 \n",
      "[ 49/500] train_loss: 0.09570 valid_loss: 0.09947 test_loss: 0.11441 \n",
      "验证损失减少 (0.101309 --> 0.099469). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.09431 valid_loss: 0.09979 test_loss: 0.11408 \n",
      "[ 51/500] train_loss: 0.09412 valid_loss: 0.10003 test_loss: 0.11489 \n",
      "[ 52/500] train_loss: 0.09441 valid_loss: 0.09906 test_loss: 0.11222 \n",
      "验证损失减少 (0.099469 --> 0.099062). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.09075 valid_loss: 0.09858 test_loss: 0.11276 \n",
      "验证损失减少 (0.099062 --> 0.098575). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.09425 valid_loss: 0.09768 test_loss: 0.11073 \n",
      "验证损失减少 (0.098575 --> 0.097679). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.08894 valid_loss: 0.10132 test_loss: 0.11406 \n",
      "[ 56/500] train_loss: 0.09283 valid_loss: 0.10090 test_loss: 0.11364 \n",
      "[ 57/500] train_loss: 0.09168 valid_loss: 0.10048 test_loss: 0.11219 \n",
      "[ 58/500] train_loss: 0.09138 valid_loss: 0.09912 test_loss: 0.11237 \n",
      "[ 59/500] train_loss: 0.09160 valid_loss: 0.09527 test_loss: 0.10913 \n",
      "验证损失减少 (0.097679 --> 0.095267). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.09347 valid_loss: 0.09719 test_loss: 0.11022 \n",
      "[ 61/500] train_loss: 0.08952 valid_loss: 0.09700 test_loss: 0.11123 \n",
      "[ 62/500] train_loss: 0.08953 valid_loss: 0.09772 test_loss: 0.11070 \n",
      "[ 63/500] train_loss: 0.08920 valid_loss: 0.09650 test_loss: 0.10981 \n",
      "[ 64/500] train_loss: 0.08838 valid_loss: 0.09840 test_loss: 0.10917 \n",
      "[ 65/500] train_loss: 0.08842 valid_loss: 0.09410 test_loss: 0.10912 \n",
      "验证损失减少 (0.095267 --> 0.094097). 正在保存模型...\n",
      "[ 66/500] train_loss: 0.08999 valid_loss: 0.09493 test_loss: 0.10968 \n",
      "[ 67/500] train_loss: 0.08879 valid_loss: 0.09575 test_loss: 0.10868 \n",
      "[ 68/500] train_loss: 0.08741 valid_loss: 0.09672 test_loss: 0.10878 \n",
      "[ 69/500] train_loss: 0.09126 valid_loss: 0.09501 test_loss: 0.10836 \n",
      "[ 70/500] train_loss: 0.08821 valid_loss: 0.09369 test_loss: 0.10757 \n",
      "验证损失减少 (0.094097 --> 0.093688). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.08779 valid_loss: 0.09232 test_loss: 0.10649 \n",
      "验证损失减少 (0.093688 --> 0.092321). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.08758 valid_loss: 0.09513 test_loss: 0.10707 \n",
      "[ 73/500] train_loss: 0.08536 valid_loss: 0.09406 test_loss: 0.10631 \n",
      "[ 74/500] train_loss: 0.08598 valid_loss: 0.09358 test_loss: 0.10616 \n",
      "[ 75/500] train_loss: 0.08722 valid_loss: 0.09600 test_loss: 0.10607 \n",
      "[ 76/500] train_loss: 0.08567 valid_loss: 0.09500 test_loss: 0.10887 \n",
      "[ 77/500] train_loss: 0.08736 valid_loss: 0.09571 test_loss: 0.10984 \n",
      "[ 78/500] train_loss: 0.08682 valid_loss: 0.09147 test_loss: 0.10506 \n",
      "验证损失减少 (0.092321 --> 0.091472). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08231 valid_loss: 0.09158 test_loss: 0.10673 \n",
      "[ 80/500] train_loss: 0.08394 valid_loss: 0.09437 test_loss: 0.10834 \n",
      "[ 81/500] train_loss: 0.08631 valid_loss: 0.09105 test_loss: 0.10539 \n",
      "验证损失减少 (0.091472 --> 0.091046). 正在保存模型...\n",
      "[ 82/500] train_loss: 0.08522 valid_loss: 0.09461 test_loss: 0.10450 \n",
      "[ 83/500] train_loss: 0.08503 valid_loss: 0.09143 test_loss: 0.10482 \n",
      "[ 84/500] train_loss: 0.08460 valid_loss: 0.09193 test_loss: 0.10571 \n",
      "[ 85/500] train_loss: 0.08337 valid_loss: 0.09128 test_loss: 0.10559 \n",
      "[ 86/500] train_loss: 0.08562 valid_loss: 0.09431 test_loss: 0.10757 \n",
      "[ 87/500] train_loss: 0.08550 valid_loss: 0.09046 test_loss: 0.10495 \n",
      "验证损失减少 (0.091046 --> 0.090455). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.08337 valid_loss: 0.09009 test_loss: 0.10293 \n",
      "验证损失减少 (0.090455 --> 0.090093). 正在保存模型...\n",
      "[ 89/500] train_loss: 0.08069 valid_loss: 0.08926 test_loss: 0.10362 \n",
      "验证损失减少 (0.090093 --> 0.089264). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.08059 valid_loss: 0.08815 test_loss: 0.10237 \n",
      "验证损失减少 (0.089264 --> 0.088152). 正在保存模型...\n",
      "[ 91/500] train_loss: 0.08446 valid_loss: 0.09035 test_loss: 0.10288 \n",
      "[ 92/500] train_loss: 0.08368 valid_loss: 0.09405 test_loss: 0.10370 \n",
      "[ 93/500] train_loss: 0.08542 valid_loss: 0.09311 test_loss: 0.10383 \n",
      "[ 94/500] train_loss: 0.08358 valid_loss: 0.09079 test_loss: 0.10378 \n",
      "[ 95/500] train_loss: 0.08194 valid_loss: 0.09134 test_loss: 0.10433 \n",
      "[ 96/500] train_loss: 0.08298 valid_loss: 0.09138 test_loss: 0.10465 \n",
      "[ 97/500] train_loss: 0.08170 valid_loss: 0.09171 test_loss: 0.10521 \n",
      "[ 98/500] train_loss: 0.08210 valid_loss: 0.08959 test_loss: 0.10279 \n",
      "[ 99/500] train_loss: 0.08148 valid_loss: 0.08835 test_loss: 0.10200 \n",
      "[100/500] train_loss: 0.08175 valid_loss: 0.08899 test_loss: 0.10372 \n",
      "[101/500] train_loss: 0.08309 valid_loss: 0.08898 test_loss: 0.10203 \n",
      "[102/500] train_loss: 0.08125 valid_loss: 0.08968 test_loss: 0.10146 \n",
      "[103/500] train_loss: 0.08291 valid_loss: 0.08799 test_loss: 0.10138 \n",
      "验证损失减少 (0.088152 --> 0.087994). 正在保存模型...\n",
      "[104/500] train_loss: 0.07951 valid_loss: 0.08929 test_loss: 0.10092 \n",
      "[105/500] train_loss: 0.07879 valid_loss: 0.09069 test_loss: 0.10188 \n",
      "[106/500] train_loss: 0.07960 valid_loss: 0.08766 test_loss: 0.10073 \n",
      "验证损失减少 (0.087994 --> 0.087659). 正在保存模型...\n",
      "[107/500] train_loss: 0.07866 valid_loss: 0.09105 test_loss: 0.10180 \n",
      "[108/500] train_loss: 0.07872 valid_loss: 0.08811 test_loss: 0.10151 \n",
      "[109/500] train_loss: 0.08029 valid_loss: 0.08738 test_loss: 0.10108 \n",
      "验证损失减少 (0.087659 --> 0.087376). 正在保存模型...\n",
      "[110/500] train_loss: 0.07759 valid_loss: 0.08810 test_loss: 0.10292 \n",
      "[111/500] train_loss: 0.07888 valid_loss: 0.08583 test_loss: 0.10097 \n",
      "验证损失减少 (0.087376 --> 0.085834). 正在保存模型...\n",
      "[112/500] train_loss: 0.07755 valid_loss: 0.08748 test_loss: 0.10252 \n",
      "[113/500] train_loss: 0.07889 valid_loss: 0.08822 test_loss: 0.09963 \n",
      "[114/500] train_loss: 0.07830 valid_loss: 0.08497 test_loss: 0.09989 \n",
      "验证损失减少 (0.085834 --> 0.084965). 正在保存模型...\n",
      "[115/500] train_loss: 0.07864 valid_loss: 0.08914 test_loss: 0.10372 \n",
      "[116/500] train_loss: 0.07792 valid_loss: 0.08546 test_loss: 0.09891 \n",
      "[117/500] train_loss: 0.07609 valid_loss: 0.08600 test_loss: 0.09910 \n",
      "[118/500] train_loss: 0.07755 valid_loss: 0.08518 test_loss: 0.09973 \n",
      "[119/500] train_loss: 0.07733 valid_loss: 0.08896 test_loss: 0.10102 \n",
      "[120/500] train_loss: 0.07608 valid_loss: 0.08807 test_loss: 0.09836 \n",
      "[121/500] train_loss: 0.07953 valid_loss: 0.08382 test_loss: 0.09845 \n",
      "验证损失减少 (0.084965 --> 0.083819). 正在保存模型...\n",
      "[122/500] train_loss: 0.07639 valid_loss: 0.08478 test_loss: 0.09918 \n",
      "[123/500] train_loss: 0.07804 valid_loss: 0.09076 test_loss: 0.09833 \n",
      "[124/500] train_loss: 0.07706 valid_loss: 0.08737 test_loss: 0.09881 \n",
      "[125/500] train_loss: 0.07487 valid_loss: 0.08512 test_loss: 0.09803 \n",
      "[126/500] train_loss: 0.07626 valid_loss: 0.08502 test_loss: 0.09885 \n",
      "[127/500] train_loss: 0.07477 valid_loss: 0.08951 test_loss: 0.09955 \n",
      "[128/500] train_loss: 0.07643 valid_loss: 0.08620 test_loss: 0.09745 \n",
      "[129/500] train_loss: 0.07675 valid_loss: 0.08559 test_loss: 0.09651 \n",
      "[130/500] train_loss: 0.07514 valid_loss: 0.08452 test_loss: 0.09781 \n",
      "[131/500] train_loss: 0.07689 valid_loss: 0.08356 test_loss: 0.09716 \n",
      "验证损失减少 (0.083819 --> 0.083557). 正在保存模型...\n",
      "[132/500] train_loss: 0.07748 valid_loss: 0.08538 test_loss: 0.09808 \n",
      "[133/500] train_loss: 0.07844 valid_loss: 0.08390 test_loss: 0.09856 \n",
      "[134/500] train_loss: 0.07500 valid_loss: 0.08700 test_loss: 0.09799 \n",
      "[135/500] train_loss: 0.07570 valid_loss: 0.08925 test_loss: 0.09597 \n",
      "[136/500] train_loss: 0.07402 valid_loss: 0.08252 test_loss: 0.09637 \n",
      "验证损失减少 (0.083557 --> 0.082520). 正在保存模型...\n",
      "[137/500] train_loss: 0.07327 valid_loss: 0.08873 test_loss: 0.09671 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138/500] train_loss: 0.07534 valid_loss: 0.08460 test_loss: 0.09597 \n",
      "[139/500] train_loss: 0.07641 valid_loss: 0.08889 test_loss: 0.09819 \n",
      "[140/500] train_loss: 0.07743 valid_loss: 0.08283 test_loss: 0.09680 \n",
      "[141/500] train_loss: 0.07422 valid_loss: 0.08450 test_loss: 0.09609 \n",
      "[142/500] train_loss: 0.07655 valid_loss: 0.08480 test_loss: 0.09593 \n",
      "[143/500] train_loss: 0.07646 valid_loss: 0.08209 test_loss: 0.09661 \n",
      "验证损失减少 (0.082520 --> 0.082093). 正在保存模型...\n",
      "[144/500] train_loss: 0.07528 valid_loss: 0.08467 test_loss: 0.09608 \n",
      "[145/500] train_loss: 0.07384 valid_loss: 0.08390 test_loss: 0.09780 \n",
      "[146/500] train_loss: 0.07358 valid_loss: 0.08604 test_loss: 0.09540 \n",
      "[147/500] train_loss: 0.07355 valid_loss: 0.08382 test_loss: 0.09635 \n",
      "[148/500] train_loss: 0.07388 valid_loss: 0.08145 test_loss: 0.09448 \n",
      "验证损失减少 (0.082093 --> 0.081451). 正在保存模型...\n",
      "[149/500] train_loss: 0.07330 valid_loss: 0.08138 test_loss: 0.09474 \n",
      "验证损失减少 (0.081451 --> 0.081376). 正在保存模型...\n",
      "[150/500] train_loss: 0.07329 valid_loss: 0.08281 test_loss: 0.09508 \n",
      "[151/500] train_loss: 0.07261 valid_loss: 0.08524 test_loss: 0.09531 \n",
      "[152/500] train_loss: 0.07415 valid_loss: 0.08127 test_loss: 0.09469 \n",
      "验证损失减少 (0.081376 --> 0.081273). 正在保存模型...\n",
      "[153/500] train_loss: 0.07258 valid_loss: 0.08194 test_loss: 0.09565 \n",
      "[154/500] train_loss: 0.07274 valid_loss: 0.08204 test_loss: 0.09550 \n",
      "[155/500] train_loss: 0.07317 valid_loss: 0.09088 test_loss: 0.09735 \n",
      "[156/500] train_loss: 0.07415 valid_loss: 0.08609 test_loss: 0.09496 \n",
      "[157/500] train_loss: 0.07053 valid_loss: 0.08666 test_loss: 0.09583 \n",
      "[158/500] train_loss: 0.07266 valid_loss: 0.08110 test_loss: 0.09391 \n",
      "验证损失减少 (0.081273 --> 0.081102). 正在保存模型...\n",
      "[159/500] train_loss: 0.07531 valid_loss: 0.08229 test_loss: 0.09522 \n",
      "[160/500] train_loss: 0.07197 valid_loss: 0.08221 test_loss: 0.09574 \n",
      "[161/500] train_loss: 0.07131 valid_loss: 0.08354 test_loss: 0.09388 \n",
      "[162/500] train_loss: 0.07199 valid_loss: 0.08464 test_loss: 0.09553 \n",
      "[163/500] train_loss: 0.07164 valid_loss: 0.08168 test_loss: 0.09625 \n",
      "[164/500] train_loss: 0.07225 valid_loss: 0.08341 test_loss: 0.09583 \n",
      "[165/500] train_loss: 0.07205 valid_loss: 0.07971 test_loss: 0.09341 \n",
      "验证损失减少 (0.081102 --> 0.079706). 正在保存模型...\n",
      "[166/500] train_loss: 0.07203 valid_loss: 0.08435 test_loss: 0.09543 \n",
      "[167/500] train_loss: 0.07228 valid_loss: 0.08393 test_loss: 0.09744 \n",
      "[168/500] train_loss: 0.07172 valid_loss: 0.07966 test_loss: 0.09426 \n",
      "验证损失减少 (0.079706 --> 0.079655). 正在保存模型...\n",
      "[169/500] train_loss: 0.07083 valid_loss: 0.08107 test_loss: 0.09507 \n",
      "[170/500] train_loss: 0.07071 valid_loss: 0.08189 test_loss: 0.09556 \n",
      "[171/500] train_loss: 0.07263 valid_loss: 0.08080 test_loss: 0.09432 \n",
      "[172/500] train_loss: 0.07040 valid_loss: 0.08241 test_loss: 0.09383 \n",
      "[173/500] train_loss: 0.07060 valid_loss: 0.08513 test_loss: 0.09500 \n",
      "[174/500] train_loss: 0.07180 valid_loss: 0.08098 test_loss: 0.09443 \n",
      "[175/500] train_loss: 0.07160 valid_loss: 0.08344 test_loss: 0.09484 \n",
      "[176/500] train_loss: 0.07232 valid_loss: 0.08592 test_loss: 0.09377 \n",
      "[177/500] train_loss: 0.07103 valid_loss: 0.08127 test_loss: 0.09516 \n",
      "[178/500] train_loss: 0.06859 valid_loss: 0.08098 test_loss: 0.09420 \n",
      "[179/500] train_loss: 0.06822 valid_loss: 0.08066 test_loss: 0.09353 \n",
      "[180/500] train_loss: 0.07064 valid_loss: 0.08053 test_loss: 0.09321 \n",
      "[181/500] train_loss: 0.06963 valid_loss: 0.08373 test_loss: 0.09382 \n",
      "[182/500] train_loss: 0.06931 valid_loss: 0.08225 test_loss: 0.09225 \n",
      "[183/500] train_loss: 0.06931 valid_loss: 0.07977 test_loss: 0.09301 \n",
      "[184/500] train_loss: 0.07077 valid_loss: 0.08196 test_loss: 0.09571 \n",
      "[185/500] train_loss: 0.07043 valid_loss: 0.07892 test_loss: 0.09311 \n",
      "验证损失减少 (0.079655 --> 0.078917). 正在保存模型...\n",
      "[186/500] train_loss: 0.06998 valid_loss: 0.07884 test_loss: 0.09316 \n",
      "验证损失减少 (0.078917 --> 0.078836). 正在保存模型...\n",
      "[187/500] train_loss: 0.07053 valid_loss: 0.08353 test_loss: 0.09537 \n",
      "[188/500] train_loss: 0.07013 valid_loss: 0.08438 test_loss: 0.09267 \n",
      "[189/500] train_loss: 0.07071 valid_loss: 0.07955 test_loss: 0.09244 \n",
      "[190/500] train_loss: 0.06925 valid_loss: 0.08066 test_loss: 0.09238 \n",
      "[191/500] train_loss: 0.06981 valid_loss: 0.07943 test_loss: 0.09255 \n",
      "[192/500] train_loss: 0.07044 valid_loss: 0.07867 test_loss: 0.09143 \n",
      "验证损失减少 (0.078836 --> 0.078671). 正在保存模型...\n",
      "[193/500] train_loss: 0.06778 valid_loss: 0.08089 test_loss: 0.09265 \n",
      "[194/500] train_loss: 0.06961 valid_loss: 0.07910 test_loss: 0.09111 \n",
      "[195/500] train_loss: 0.06990 valid_loss: 0.08768 test_loss: 0.09347 \n",
      "[196/500] train_loss: 0.06830 valid_loss: 0.08065 test_loss: 0.09180 \n",
      "[197/500] train_loss: 0.06707 valid_loss: 0.08176 test_loss: 0.09223 \n",
      "[198/500] train_loss: 0.06784 valid_loss: 0.08089 test_loss: 0.09146 \n",
      "[199/500] train_loss: 0.06887 valid_loss: 0.08402 test_loss: 0.09203 \n",
      "[200/500] train_loss: 0.06670 valid_loss: 0.08236 test_loss: 0.09358 \n",
      "[201/500] train_loss: 0.06903 valid_loss: 0.07958 test_loss: 0.09341 \n",
      "[202/500] train_loss: 0.06707 valid_loss: 0.07853 test_loss: 0.09259 \n",
      "验证损失减少 (0.078671 --> 0.078525). 正在保存模型...\n",
      "[203/500] train_loss: 0.06894 valid_loss: 0.08076 test_loss: 0.09359 \n",
      "[204/500] train_loss: 0.06601 valid_loss: 0.07938 test_loss: 0.09362 \n",
      "[205/500] train_loss: 0.06782 valid_loss: 0.08514 test_loss: 0.09216 \n",
      "[206/500] train_loss: 0.06836 valid_loss: 0.08464 test_loss: 0.09157 \n",
      "[207/500] train_loss: 0.06881 valid_loss: 0.08253 test_loss: 0.09433 \n",
      "[208/500] train_loss: 0.06793 valid_loss: 0.08662 test_loss: 0.09232 \n",
      "[209/500] train_loss: 0.06767 valid_loss: 0.08097 test_loss: 0.09277 \n",
      "[210/500] train_loss: 0.06675 valid_loss: 0.08183 test_loss: 0.09186 \n",
      "[211/500] train_loss: 0.06753 valid_loss: 0.08350 test_loss: 0.09402 \n",
      "[212/500] train_loss: 0.06714 valid_loss: 0.08415 test_loss: 0.09225 \n",
      "[213/500] train_loss: 0.06540 valid_loss: 0.07922 test_loss: 0.09190 \n",
      "[214/500] train_loss: 0.06699 valid_loss: 0.08132 test_loss: 0.09253 \n",
      "[215/500] train_loss: 0.06747 valid_loss: 0.08113 test_loss: 0.09351 \n",
      "[216/500] train_loss: 0.06639 valid_loss: 0.08056 test_loss: 0.09405 \n",
      "[217/500] train_loss: 0.06839 valid_loss: 0.08044 test_loss: 0.09286 \n",
      "[218/500] train_loss: 0.06543 valid_loss: 0.07847 test_loss: 0.09092 \n",
      "验证损失减少 (0.078525 --> 0.078468). 正在保存模型...\n",
      "[219/500] train_loss: 0.06635 valid_loss: 0.08228 test_loss: 0.09239 \n",
      "[220/500] train_loss: 0.06554 valid_loss: 0.08087 test_loss: 0.09177 \n",
      "[221/500] train_loss: 0.06769 valid_loss: 0.07901 test_loss: 0.09155 \n",
      "[222/500] train_loss: 0.06666 valid_loss: 0.07960 test_loss: 0.09274 \n",
      "[223/500] train_loss: 0.06613 valid_loss: 0.09486 test_loss: 0.09321 \n",
      "[224/500] train_loss: 0.06605 valid_loss: 0.08479 test_loss: 0.09360 \n",
      "[225/500] train_loss: 0.06792 valid_loss: 0.08292 test_loss: 0.09307 \n",
      "[226/500] train_loss: 0.06550 valid_loss: 0.08071 test_loss: 0.09361 \n",
      "[227/500] train_loss: 0.06643 valid_loss: 0.08326 test_loss: 0.09328 \n",
      "[228/500] train_loss: 0.06515 valid_loss: 0.08125 test_loss: 0.09445 \n",
      "[229/500] train_loss: 0.06530 valid_loss: 0.07995 test_loss: 0.09297 \n",
      "[230/500] train_loss: 0.06617 valid_loss: 0.07983 test_loss: 0.09155 \n",
      "[231/500] train_loss: 0.06505 valid_loss: 0.08189 test_loss: 0.09201 \n",
      "[232/500] train_loss: 0.06410 valid_loss: 0.08018 test_loss: 0.09483 \n",
      "[233/500] train_loss: 0.06568 valid_loss: 0.07938 test_loss: 0.09147 \n",
      "[234/500] train_loss: 0.06602 valid_loss: 0.07818 test_loss: 0.09197 \n",
      "验证损失减少 (0.078468 --> 0.078183). 正在保存模型...\n",
      "[235/500] train_loss: 0.06637 valid_loss: 0.07851 test_loss: 0.09248 \n",
      "[236/500] train_loss: 0.06695 valid_loss: 0.07779 test_loss: 0.09145 \n",
      "验证损失减少 (0.078183 --> 0.077785). 正在保存模型...\n",
      "[237/500] train_loss: 0.06494 valid_loss: 0.07941 test_loss: 0.09235 \n",
      "[238/500] train_loss: 0.06541 valid_loss: 0.07805 test_loss: 0.09178 \n",
      "[239/500] train_loss: 0.06615 valid_loss: 0.08814 test_loss: 0.09158 \n",
      "[240/500] train_loss: 0.06649 valid_loss: 0.08163 test_loss: 0.09149 \n",
      "[241/500] train_loss: 0.06527 valid_loss: 0.08445 test_loss: 0.09200 \n",
      "[242/500] train_loss: 0.06530 valid_loss: 0.07911 test_loss: 0.09028 \n",
      "[243/500] train_loss: 0.06444 valid_loss: 0.08047 test_loss: 0.09151 \n",
      "[244/500] train_loss: 0.06469 valid_loss: 0.08099 test_loss: 0.09282 \n",
      "[245/500] train_loss: 0.06579 valid_loss: 0.07957 test_loss: 0.09198 \n",
      "[246/500] train_loss: 0.06419 valid_loss: 0.08123 test_loss: 0.09159 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247/500] train_loss: 0.06373 valid_loss: 0.08368 test_loss: 0.09262 \n",
      "[248/500] train_loss: 0.06636 valid_loss: 0.08559 test_loss: 0.09240 \n",
      "[249/500] train_loss: 0.06356 valid_loss: 0.08602 test_loss: 0.09199 \n",
      "[250/500] train_loss: 0.06397 valid_loss: 0.07966 test_loss: 0.09137 \n",
      "[251/500] train_loss: 0.06203 valid_loss: 0.07978 test_loss: 0.09464 \n",
      "[252/500] train_loss: 0.06542 valid_loss: 0.08226 test_loss: 0.09149 \n",
      "[253/500] train_loss: 0.06337 valid_loss: 0.07764 test_loss: 0.09078 \n",
      "验证损失减少 (0.077785 --> 0.077643). 正在保存模型...\n",
      "[254/500] train_loss: 0.06439 valid_loss: 0.07772 test_loss: 0.09241 \n",
      "[255/500] train_loss: 0.06491 valid_loss: 0.08028 test_loss: 0.09129 \n",
      "[256/500] train_loss: 0.06394 valid_loss: 0.07911 test_loss: 0.09213 \n",
      "[257/500] train_loss: 0.06428 valid_loss: 0.07968 test_loss: 0.09261 \n",
      "[258/500] train_loss: 0.06345 valid_loss: 0.08182 test_loss: 0.09247 \n",
      "[259/500] train_loss: 0.06349 valid_loss: 0.08262 test_loss: 0.09112 \n",
      "[260/500] train_loss: 0.06240 valid_loss: 0.08536 test_loss: 0.09207 \n",
      "[261/500] train_loss: 0.06253 valid_loss: 0.07955 test_loss: 0.09144 \n",
      "[262/500] train_loss: 0.06167 valid_loss: 0.08377 test_loss: 0.09248 \n",
      "[263/500] train_loss: 0.06525 valid_loss: 0.08117 test_loss: 0.09140 \n",
      "[264/500] train_loss: 0.06630 valid_loss: 0.07944 test_loss: 0.09214 \n",
      "[265/500] train_loss: 0.06343 valid_loss: 0.07762 test_loss: 0.09071 \n",
      "验证损失减少 (0.077643 --> 0.077623). 正在保存模型...\n",
      "[266/500] train_loss: 0.06347 valid_loss: 0.08059 test_loss: 0.09225 \n",
      "[267/500] train_loss: 0.06471 valid_loss: 0.09235 test_loss: 0.09327 \n",
      "[268/500] train_loss: 0.06232 valid_loss: 0.08393 test_loss: 0.09108 \n",
      "[269/500] train_loss: 0.06444 valid_loss: 0.08251 test_loss: 0.09091 \n",
      "[270/500] train_loss: 0.06378 valid_loss: 0.07963 test_loss: 0.09161 \n",
      "[271/500] train_loss: 0.06291 valid_loss: 0.08427 test_loss: 0.09126 \n",
      "[272/500] train_loss: 0.06367 valid_loss: 0.07766 test_loss: 0.08963 \n",
      "[273/500] train_loss: 0.06276 valid_loss: 0.08146 test_loss: 0.09231 \n",
      "[274/500] train_loss: 0.06076 valid_loss: 0.07985 test_loss: 0.09127 \n",
      "[275/500] train_loss: 0.06132 valid_loss: 0.08083 test_loss: 0.09043 \n",
      "[276/500] train_loss: 0.06330 valid_loss: 0.08524 test_loss: 0.09109 \n",
      "[277/500] train_loss: 0.06082 valid_loss: 0.08151 test_loss: 0.09283 \n",
      "[278/500] train_loss: 0.06311 valid_loss: 0.08195 test_loss: 0.09133 \n",
      "[279/500] train_loss: 0.06341 valid_loss: 0.08116 test_loss: 0.09019 \n",
      "[280/500] train_loss: 0.06412 valid_loss: 0.08411 test_loss: 0.09042 \n",
      "[281/500] train_loss: 0.06287 valid_loss: 0.07851 test_loss: 0.09013 \n",
      "[282/500] train_loss: 0.06099 valid_loss: 0.08019 test_loss: 0.09124 \n",
      "[283/500] train_loss: 0.06203 valid_loss: 0.08246 test_loss: 0.09023 \n",
      "[284/500] train_loss: 0.06350 valid_loss: 0.09372 test_loss: 0.09108 \n",
      "[285/500] train_loss: 0.06351 valid_loss: 0.08373 test_loss: 0.09108 \n",
      "[286/500] train_loss: 0.06119 valid_loss: 0.09588 test_loss: 0.09142 \n",
      "[287/500] train_loss: 0.06153 valid_loss: 0.08182 test_loss: 0.09147 \n",
      "[288/500] train_loss: 0.06195 valid_loss: 0.08650 test_loss: 0.09095 \n",
      "[289/500] train_loss: 0.06009 valid_loss: 0.08387 test_loss: 0.09076 \n",
      "[290/500] train_loss: 0.06246 valid_loss: 0.08937 test_loss: 0.09085 \n",
      "[291/500] train_loss: 0.06148 valid_loss: 0.09270 test_loss: 0.09035 \n",
      "[292/500] train_loss: 0.06294 valid_loss: 0.08278 test_loss: 0.09301 \n",
      "[293/500] train_loss: 0.06257 valid_loss: 0.08111 test_loss: 0.09117 \n",
      "[294/500] train_loss: 0.06209 valid_loss: 0.09447 test_loss: 0.09149 \n",
      "[295/500] train_loss: 0.06200 valid_loss: 0.08763 test_loss: 0.09002 \n",
      "[296/500] train_loss: 0.06034 valid_loss: 0.08012 test_loss: 0.09143 \n",
      "[297/500] train_loss: 0.06071 valid_loss: 0.08004 test_loss: 0.09208 \n",
      "[298/500] train_loss: 0.05991 valid_loss: 0.09499 test_loss: 0.09144 \n",
      "[299/500] train_loss: 0.06099 valid_loss: 0.09985 test_loss: 0.09197 \n",
      "[300/500] train_loss: 0.06013 valid_loss: 0.09112 test_loss: 0.09009 \n",
      "[301/500] train_loss: 0.06164 valid_loss: 0.08936 test_loss: 0.09085 \n",
      "[302/500] train_loss: 0.06309 valid_loss: 0.08092 test_loss: 0.09032 \n",
      "[303/500] train_loss: 0.06080 valid_loss: 0.07846 test_loss: 0.09052 \n",
      "[304/500] train_loss: 0.06060 valid_loss: 0.09221 test_loss: 0.09065 \n",
      "[305/500] train_loss: 0.06298 valid_loss: 0.08374 test_loss: 0.09069 \n",
      "[306/500] train_loss: 0.06146 valid_loss: 0.09672 test_loss: 0.09249 \n",
      "[307/500] train_loss: 0.06128 valid_loss: 0.08034 test_loss: 0.08920 \n",
      "[308/500] train_loss: 0.06160 valid_loss: 0.09507 test_loss: 0.09106 \n",
      "[309/500] train_loss: 0.05979 valid_loss: 0.10350 test_loss: 0.08978 \n",
      "[310/500] train_loss: 0.05974 valid_loss: 0.09232 test_loss: 0.09193 \n",
      "[311/500] train_loss: 0.06069 valid_loss: 0.08786 test_loss: 0.09105 \n",
      "[312/500] train_loss: 0.05989 valid_loss: 0.08295 test_loss: 0.09035 \n",
      "[313/500] train_loss: 0.05996 valid_loss: 0.09952 test_loss: 0.08960 \n",
      "[314/500] train_loss: 0.06092 valid_loss: 0.09114 test_loss: 0.09122 \n",
      "[315/500] train_loss: 0.05851 valid_loss: 0.10344 test_loss: 0.09109 \n",
      "[316/500] train_loss: 0.05864 valid_loss: 0.09869 test_loss: 0.09015 \n",
      "[317/500] train_loss: 0.05970 valid_loss: 0.10293 test_loss: 0.08986 \n",
      "[318/500] train_loss: 0.06039 valid_loss: 0.09285 test_loss: 0.09118 \n",
      "[319/500] train_loss: 0.05817 valid_loss: 0.08101 test_loss: 0.08965 \n",
      "[320/500] train_loss: 0.05841 valid_loss: 0.08910 test_loss: 0.08901 \n",
      "[321/500] train_loss: 0.05986 valid_loss: 0.08092 test_loss: 0.09073 \n",
      "[322/500] train_loss: 0.05848 valid_loss: 0.09648 test_loss: 0.09047 \n",
      "[323/500] train_loss: 0.05776 valid_loss: 0.09977 test_loss: 0.09024 \n",
      "[324/500] train_loss: 0.05799 valid_loss: 0.09040 test_loss: 0.08968 \n",
      "[325/500] train_loss: 0.05937 valid_loss: 0.08191 test_loss: 0.08963 \n",
      "[326/500] train_loss: 0.06015 valid_loss: 0.09634 test_loss: 0.08998 \n",
      "[327/500] train_loss: 0.06019 valid_loss: 0.07978 test_loss: 0.08969 \n",
      "[328/500] train_loss: 0.06081 valid_loss: 0.08909 test_loss: 0.08941 \n",
      "[329/500] train_loss: 0.05924 valid_loss: 0.08234 test_loss: 0.08928 \n",
      "[330/500] train_loss: 0.05806 valid_loss: 0.08612 test_loss: 0.09107 \n",
      "[331/500] train_loss: 0.05945 valid_loss: 0.09358 test_loss: 0.09171 \n",
      "[332/500] train_loss: 0.06013 valid_loss: 0.09148 test_loss: 0.09163 \n",
      "[333/500] train_loss: 0.05997 valid_loss: 0.09294 test_loss: 0.09041 \n",
      "[334/500] train_loss: 0.06164 valid_loss: 0.07640 test_loss: 0.08850 \n",
      "验证损失减少 (0.077623 --> 0.076401). 正在保存模型...\n",
      "[335/500] train_loss: 0.05899 valid_loss: 0.08710 test_loss: 0.09040 \n",
      "[336/500] train_loss: 0.05942 valid_loss: 0.09824 test_loss: 0.08968 \n",
      "[337/500] train_loss: 0.05969 valid_loss: 0.10863 test_loss: 0.08894 \n",
      "[338/500] train_loss: 0.05853 valid_loss: 0.08122 test_loss: 0.08945 \n",
      "[339/500] train_loss: 0.05964 valid_loss: 0.07950 test_loss: 0.08886 \n",
      "[340/500] train_loss: 0.05843 valid_loss: 0.09172 test_loss: 0.09181 \n",
      "[341/500] train_loss: 0.05943 valid_loss: 0.08182 test_loss: 0.09029 \n",
      "[342/500] train_loss: 0.05812 valid_loss: 0.08133 test_loss: 0.08975 \n",
      "[343/500] train_loss: 0.05819 valid_loss: 0.09246 test_loss: 0.08922 \n",
      "[344/500] train_loss: 0.05758 valid_loss: 0.07833 test_loss: 0.09126 \n",
      "[345/500] train_loss: 0.05801 valid_loss: 0.07593 test_loss: 0.09032 \n",
      "验证损失减少 (0.076401 --> 0.075934). 正在保存模型...\n",
      "[346/500] train_loss: 0.05931 valid_loss: 0.08409 test_loss: 0.08984 \n",
      "[347/500] train_loss: 0.05972 valid_loss: 0.07928 test_loss: 0.08924 \n",
      "[348/500] train_loss: 0.05956 valid_loss: 0.07862 test_loss: 0.08996 \n",
      "[349/500] train_loss: 0.05988 valid_loss: 0.07682 test_loss: 0.08918 \n",
      "[350/500] train_loss: 0.05762 valid_loss: 0.07622 test_loss: 0.09004 \n",
      "[351/500] train_loss: 0.05969 valid_loss: 0.07627 test_loss: 0.08909 \n",
      "[352/500] train_loss: 0.05909 valid_loss: 0.08102 test_loss: 0.08987 \n",
      "[353/500] train_loss: 0.05914 valid_loss: 0.08815 test_loss: 0.08822 \n",
      "[354/500] train_loss: 0.05927 valid_loss: 0.08104 test_loss: 0.08990 \n",
      "[355/500] train_loss: 0.05686 valid_loss: 0.07668 test_loss: 0.08930 \n",
      "[356/500] train_loss: 0.05740 valid_loss: 0.07716 test_loss: 0.08976 \n",
      "[357/500] train_loss: 0.05763 valid_loss: 0.08808 test_loss: 0.09047 \n",
      "[358/500] train_loss: 0.05807 valid_loss: 0.08438 test_loss: 0.08868 \n",
      "[359/500] train_loss: 0.05826 valid_loss: 0.09107 test_loss: 0.08916 \n",
      "[360/500] train_loss: 0.05893 valid_loss: 0.08920 test_loss: 0.08998 \n",
      "[361/500] train_loss: 0.05740 valid_loss: 0.08835 test_loss: 0.09070 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362/500] train_loss: 0.05829 valid_loss: 0.08552 test_loss: 0.09078 \n",
      "[363/500] train_loss: 0.05779 valid_loss: 0.07698 test_loss: 0.08873 \n",
      "[364/500] train_loss: 0.05778 valid_loss: 0.08373 test_loss: 0.09043 \n",
      "[365/500] train_loss: 0.05815 valid_loss: 0.09399 test_loss: 0.08932 \n",
      "[366/500] train_loss: 0.05812 valid_loss: 0.09707 test_loss: 0.08883 \n",
      "[367/500] train_loss: 0.05663 valid_loss: 0.07760 test_loss: 0.08934 \n",
      "[368/500] train_loss: 0.05855 valid_loss: 0.09712 test_loss: 0.09101 \n",
      "[369/500] train_loss: 0.05710 valid_loss: 0.09692 test_loss: 0.09018 \n",
      "[370/500] train_loss: 0.05792 valid_loss: 0.08169 test_loss: 0.09001 \n",
      "[371/500] train_loss: 0.05839 valid_loss: 0.07640 test_loss: 0.08952 \n",
      "[372/500] train_loss: 0.05786 valid_loss: 0.07914 test_loss: 0.09004 \n",
      "[373/500] train_loss: 0.05581 valid_loss: 0.07987 test_loss: 0.09014 \n",
      "[374/500] train_loss: 0.05813 valid_loss: 0.09071 test_loss: 0.08858 \n",
      "[375/500] train_loss: 0.05585 valid_loss: 0.08798 test_loss: 0.08834 \n",
      "[376/500] train_loss: 0.05765 valid_loss: 0.10570 test_loss: 0.08996 \n",
      "[377/500] train_loss: 0.05652 valid_loss: 0.07950 test_loss: 0.08844 \n",
      "[378/500] train_loss: 0.05654 valid_loss: 0.08145 test_loss: 0.09102 \n",
      "[379/500] train_loss: 0.05717 valid_loss: 0.08678 test_loss: 0.09115 \n",
      "[380/500] train_loss: 0.05608 valid_loss: 0.08244 test_loss: 0.08952 \n",
      "[381/500] train_loss: 0.05653 valid_loss: 0.08172 test_loss: 0.09084 \n",
      "[382/500] train_loss: 0.05794 valid_loss: 0.08542 test_loss: 0.08978 \n",
      "[383/500] train_loss: 0.05740 valid_loss: 0.08912 test_loss: 0.08909 \n",
      "[384/500] train_loss: 0.05643 valid_loss: 0.11246 test_loss: 0.08969 \n",
      "[385/500] train_loss: 0.05623 valid_loss: 0.09578 test_loss: 0.08945 \n",
      "[386/500] train_loss: 0.05521 valid_loss: 0.08404 test_loss: 0.08867 \n",
      "[387/500] train_loss: 0.05677 valid_loss: 0.07787 test_loss: 0.09112 \n",
      "[388/500] train_loss: 0.05556 valid_loss: 0.08090 test_loss: 0.08994 \n",
      "[389/500] train_loss: 0.05618 valid_loss: 0.07773 test_loss: 0.09007 \n",
      "[390/500] train_loss: 0.05761 valid_loss: 0.07858 test_loss: 0.08896 \n",
      "[391/500] train_loss: 0.05755 valid_loss: 0.08903 test_loss: 0.09003 \n",
      "[392/500] train_loss: 0.05746 valid_loss: 0.09072 test_loss: 0.08949 \n",
      "[393/500] train_loss: 0.05647 valid_loss: 0.08970 test_loss: 0.08998 \n",
      "[394/500] train_loss: 0.05759 valid_loss: 0.08887 test_loss: 0.08739 \n",
      "[395/500] train_loss: 0.05697 valid_loss: 0.11255 test_loss: 0.08930 \n",
      "[396/500] train_loss: 0.05536 valid_loss: 0.07717 test_loss: 0.09006 \n",
      "[397/500] train_loss: 0.05347 valid_loss: 0.08464 test_loss: 0.09042 \n",
      "[398/500] train_loss: 0.05618 valid_loss: 0.08609 test_loss: 0.08823 \n",
      "[399/500] train_loss: 0.05410 valid_loss: 0.10606 test_loss: 0.08899 \n",
      "[400/500] train_loss: 0.05552 valid_loss: 0.09164 test_loss: 0.08915 \n",
      "[401/500] train_loss: 0.05530 valid_loss: 0.07929 test_loss: 0.08845 \n",
      "[402/500] train_loss: 0.05670 valid_loss: 0.10328 test_loss: 0.08939 \n",
      "[403/500] train_loss: 0.05553 valid_loss: 0.10306 test_loss: 0.08988 \n",
      "[404/500] train_loss: 0.05492 valid_loss: 0.08649 test_loss: 0.09018 \n",
      "[405/500] train_loss: 0.05577 valid_loss: 0.08649 test_loss: 0.08844 \n",
      "[406/500] train_loss: 0.05445 valid_loss: 0.07788 test_loss: 0.08934 \n",
      "[407/500] train_loss: 0.05623 valid_loss: 0.09398 test_loss: 0.08993 \n",
      "[408/500] train_loss: 0.05651 valid_loss: 0.08478 test_loss: 0.09114 \n",
      "[409/500] train_loss: 0.05570 valid_loss: 0.08207 test_loss: 0.08853 \n",
      "[410/500] train_loss: 0.05550 valid_loss: 0.08261 test_loss: 0.09176 \n",
      "[411/500] train_loss: 0.05567 valid_loss: 0.08807 test_loss: 0.08874 \n",
      "[412/500] train_loss: 0.05654 valid_loss: 0.08178 test_loss: 0.08940 \n",
      "[413/500] train_loss: 0.05446 valid_loss: 0.07651 test_loss: 0.08906 \n",
      "[414/500] train_loss: 0.05467 valid_loss: 0.08135 test_loss: 0.08795 \n",
      "[415/500] train_loss: 0.05571 valid_loss: 0.07756 test_loss: 0.08847 \n",
      "[416/500] train_loss: 0.05622 valid_loss: 0.07827 test_loss: 0.09036 \n",
      "[417/500] train_loss: 0.05580 valid_loss: 0.08200 test_loss: 0.08888 \n",
      "[418/500] train_loss: 0.05439 valid_loss: 0.07540 test_loss: 0.08818 \n",
      "验证损失减少 (0.075934 --> 0.075397). 正在保存模型...\n",
      "[419/500] train_loss: 0.05516 valid_loss: 0.07526 test_loss: 0.08954 \n",
      "验证损失减少 (0.075397 --> 0.075263). 正在保存模型...\n",
      "[420/500] train_loss: 0.05376 valid_loss: 0.07494 test_loss: 0.08955 \n",
      "验证损失减少 (0.075263 --> 0.074940). 正在保存模型...\n",
      "[421/500] train_loss: 0.05361 valid_loss: 0.07812 test_loss: 0.08989 \n",
      "[422/500] train_loss: 0.05462 valid_loss: 0.09423 test_loss: 0.08807 \n",
      "[423/500] train_loss: 0.05459 valid_loss: 0.08469 test_loss: 0.08897 \n",
      "[424/500] train_loss: 0.05554 valid_loss: 0.08162 test_loss: 0.08812 \n",
      "[425/500] train_loss: 0.05463 valid_loss: 0.07747 test_loss: 0.08818 \n",
      "[426/500] train_loss: 0.05437 valid_loss: 0.07702 test_loss: 0.08947 \n",
      "[427/500] train_loss: 0.05466 valid_loss: 0.07517 test_loss: 0.08797 \n",
      "[428/500] train_loss: 0.05514 valid_loss: 0.07576 test_loss: 0.08874 \n",
      "[429/500] train_loss: 0.05344 valid_loss: 0.07617 test_loss: 0.08968 \n",
      "[430/500] train_loss: 0.05460 valid_loss: 0.07505 test_loss: 0.08838 \n",
      "[431/500] train_loss: 0.05640 valid_loss: 0.07790 test_loss: 0.09044 \n",
      "[432/500] train_loss: 0.05421 valid_loss: 0.07615 test_loss: 0.08906 \n",
      "[433/500] train_loss: 0.05469 valid_loss: 0.07607 test_loss: 0.08892 \n",
      "[434/500] train_loss: 0.05588 valid_loss: 0.07811 test_loss: 0.08918 \n",
      "[435/500] train_loss: 0.05274 valid_loss: 0.07479 test_loss: 0.08892 \n",
      "验证损失减少 (0.074940 --> 0.074789). 正在保存模型...\n",
      "[436/500] train_loss: 0.05251 valid_loss: 0.07754 test_loss: 0.08987 \n",
      "[437/500] train_loss: 0.05282 valid_loss: 0.08067 test_loss: 0.08801 \n",
      "[438/500] train_loss: 0.05297 valid_loss: 0.08050 test_loss: 0.08922 \n",
      "[439/500] train_loss: 0.05485 valid_loss: 0.07487 test_loss: 0.08826 \n",
      "[440/500] train_loss: 0.05430 valid_loss: 0.07506 test_loss: 0.08838 \n",
      "[441/500] train_loss: 0.05499 valid_loss: 0.08378 test_loss: 0.08959 \n",
      "[442/500] train_loss: 0.05517 valid_loss: 0.08109 test_loss: 0.08977 \n",
      "[443/500] train_loss: 0.05392 valid_loss: 0.07759 test_loss: 0.08912 \n",
      "[444/500] train_loss: 0.05379 valid_loss: 0.08598 test_loss: 0.08830 \n",
      "[445/500] train_loss: 0.05468 valid_loss: 0.08167 test_loss: 0.08925 \n",
      "[446/500] train_loss: 0.05523 valid_loss: 0.07652 test_loss: 0.09001 \n",
      "[447/500] train_loss: 0.05254 valid_loss: 0.07737 test_loss: 0.08898 \n",
      "[448/500] train_loss: 0.05435 valid_loss: 0.08909 test_loss: 0.08874 \n",
      "[449/500] train_loss: 0.05217 valid_loss: 0.08105 test_loss: 0.09082 \n",
      "[450/500] train_loss: 0.05423 valid_loss: 0.07558 test_loss: 0.08891 \n",
      "[451/500] train_loss: 0.05403 valid_loss: 0.07720 test_loss: 0.08750 \n",
      "[452/500] train_loss: 0.05304 valid_loss: 0.09789 test_loss: 0.08869 \n",
      "[453/500] train_loss: 0.05448 valid_loss: 0.08054 test_loss: 0.08813 \n",
      "[454/500] train_loss: 0.05341 valid_loss: 0.07574 test_loss: 0.08942 \n",
      "[455/500] train_loss: 0.05279 valid_loss: 0.09485 test_loss: 0.08788 \n",
      "[456/500] train_loss: 0.05312 valid_loss: 0.07807 test_loss: 0.08911 \n",
      "[457/500] train_loss: 0.05269 valid_loss: 0.07954 test_loss: 0.08778 \n",
      "[458/500] train_loss: 0.05332 valid_loss: 0.07631 test_loss: 0.08918 \n",
      "[459/500] train_loss: 0.05418 valid_loss: 0.07661 test_loss: 0.08990 \n",
      "[460/500] train_loss: 0.05443 valid_loss: 0.07479 test_loss: 0.08724 \n",
      "验证损失减少 (0.074789 --> 0.074788). 正在保存模型...\n",
      "[461/500] train_loss: 0.05417 valid_loss: 0.08285 test_loss: 0.08977 \n",
      "[462/500] train_loss: 0.05255 valid_loss: 0.07511 test_loss: 0.08782 \n",
      "[463/500] train_loss: 0.05383 valid_loss: 0.07542 test_loss: 0.08864 \n",
      "[464/500] train_loss: 0.05360 valid_loss: 0.07446 test_loss: 0.08937 \n",
      "验证损失减少 (0.074788 --> 0.074457). 正在保存模型...\n",
      "[465/500] train_loss: 0.05310 valid_loss: 0.07641 test_loss: 0.08860 \n",
      "[466/500] train_loss: 0.05317 valid_loss: 0.07567 test_loss: 0.08974 \n",
      "[467/500] train_loss: 0.05188 valid_loss: 0.07589 test_loss: 0.08840 \n",
      "[468/500] train_loss: 0.05351 valid_loss: 0.07654 test_loss: 0.08935 \n",
      "[469/500] train_loss: 0.05321 valid_loss: 0.09247 test_loss: 0.08875 \n",
      "[470/500] train_loss: 0.05247 valid_loss: 0.07843 test_loss: 0.08648 \n",
      "[471/500] train_loss: 0.05419 valid_loss: 0.07511 test_loss: 0.08809 \n",
      "[472/500] train_loss: 0.05288 valid_loss: 0.07753 test_loss: 0.08759 \n",
      "[473/500] train_loss: 0.05307 valid_loss: 0.07496 test_loss: 0.08855 \n",
      "[474/500] train_loss: 0.05248 valid_loss: 0.07698 test_loss: 0.09050 \n",
      "[475/500] train_loss: 0.05140 valid_loss: 0.07686 test_loss: 0.08817 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[476/500] train_loss: 0.05270 valid_loss: 0.07501 test_loss: 0.08659 \n",
      "[477/500] train_loss: 0.05280 valid_loss: 0.07557 test_loss: 0.08744 \n",
      "[478/500] train_loss: 0.05124 valid_loss: 0.07452 test_loss: 0.08817 \n",
      "[479/500] train_loss: 0.05247 valid_loss: 0.07610 test_loss: 0.08820 \n",
      "[480/500] train_loss: 0.05312 valid_loss: 0.07451 test_loss: 0.08729 \n",
      "[481/500] train_loss: 0.05303 valid_loss: 0.07509 test_loss: 0.08745 \n",
      "[482/500] train_loss: 0.05372 valid_loss: 0.07477 test_loss: 0.08812 \n",
      "[483/500] train_loss: 0.05220 valid_loss: 0.07771 test_loss: 0.09042 \n",
      "[484/500] train_loss: 0.05213 valid_loss: 0.07674 test_loss: 0.08901 \n",
      "[485/500] train_loss: 0.05292 valid_loss: 0.07376 test_loss: 0.08736 \n",
      "验证损失减少 (0.074457 --> 0.073756). 正在保存模型...\n",
      "[486/500] train_loss: 0.05223 valid_loss: 0.07657 test_loss: 0.08841 \n",
      "[487/500] train_loss: 0.05313 valid_loss: 0.07636 test_loss: 0.08793 \n",
      "[488/500] train_loss: 0.05200 valid_loss: 0.07506 test_loss: 0.08801 \n",
      "[489/500] train_loss: 0.05175 valid_loss: 0.07635 test_loss: 0.08792 \n",
      "[490/500] train_loss: 0.05238 valid_loss: 0.07503 test_loss: 0.08680 \n",
      "[491/500] train_loss: 0.05285 valid_loss: 0.07643 test_loss: 0.08829 \n",
      "[492/500] train_loss: 0.05251 valid_loss: 0.07626 test_loss: 0.08828 \n",
      "[493/500] train_loss: 0.05254 valid_loss: 0.07475 test_loss: 0.08784 \n",
      "[494/500] train_loss: 0.05284 valid_loss: 0.08093 test_loss: 0.08703 \n",
      "[495/500] train_loss: 0.05153 valid_loss: 0.07793 test_loss: 0.08784 \n",
      "[496/500] train_loss: 0.05153 valid_loss: 0.08680 test_loss: 0.08878 \n",
      "[497/500] train_loss: 0.05063 valid_loss: 0.08172 test_loss: 0.08895 \n",
      "[498/500] train_loss: 0.05234 valid_loss: 0.07688 test_loss: 0.08851 \n",
      "[499/500] train_loss: 0.05269 valid_loss: 0.07678 test_loss: 0.09128 \n",
      "[500/500] train_loss: 0.05088 valid_loss: 0.07651 test_loss: 0.08881 \n",
      "TRAINING MODEL 7\n",
      "[  1/500] train_loss: 0.45057 valid_loss: 0.30719 test_loss: 0.31183 \n",
      "验证损失减少 (inf --> 0.307188). 正在保存模型...\n",
      "[  2/500] train_loss: 0.23899 valid_loss: 0.21923 test_loss: 0.22541 \n",
      "验证损失减少 (0.307188 --> 0.219226). 正在保存模型...\n",
      "[  3/500] train_loss: 0.18490 valid_loss: 0.17959 test_loss: 0.18916 \n",
      "验证损失减少 (0.219226 --> 0.179593). 正在保存模型...\n",
      "[  4/500] train_loss: 0.16484 valid_loss: 0.16637 test_loss: 0.17718 \n",
      "验证损失减少 (0.179593 --> 0.166367). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15399 valid_loss: 0.15625 test_loss: 0.16661 \n",
      "验证损失减少 (0.166367 --> 0.156252). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14732 valid_loss: 0.15027 test_loss: 0.16065 \n",
      "验证损失减少 (0.156252 --> 0.150273). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13886 valid_loss: 0.14237 test_loss: 0.15431 \n",
      "验证损失减少 (0.150273 --> 0.142366). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13460 valid_loss: 0.14037 test_loss: 0.15261 \n",
      "验证损失减少 (0.142366 --> 0.140372). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13073 valid_loss: 0.13763 test_loss: 0.14870 \n",
      "验证损失减少 (0.140372 --> 0.137625). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12885 valid_loss: 0.13120 test_loss: 0.14320 \n",
      "验证损失减少 (0.137625 --> 0.131202). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12510 valid_loss: 0.12914 test_loss: 0.14219 \n",
      "验证损失减少 (0.131202 --> 0.129139). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12405 valid_loss: 0.12833 test_loss: 0.14236 \n",
      "验证损失减少 (0.129139 --> 0.128327). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12081 valid_loss: 0.12502 test_loss: 0.13830 \n",
      "验证损失减少 (0.128327 --> 0.125021). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.12044 valid_loss: 0.12459 test_loss: 0.13988 \n",
      "验证损失减少 (0.125021 --> 0.124589). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.12012 valid_loss: 0.12204 test_loss: 0.13658 \n",
      "验证损失减少 (0.124589 --> 0.122037). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11782 valid_loss: 0.12763 test_loss: 0.13991 \n",
      "[ 17/500] train_loss: 0.11664 valid_loss: 0.11957 test_loss: 0.13495 \n",
      "验证损失减少 (0.122037 --> 0.119569). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11267 valid_loss: 0.11826 test_loss: 0.13375 \n",
      "验证损失减少 (0.119569 --> 0.118264). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11316 valid_loss: 0.12205 test_loss: 0.13660 \n",
      "[ 20/500] train_loss: 0.11122 valid_loss: 0.11780 test_loss: 0.13003 \n",
      "验证损失减少 (0.118264 --> 0.117802). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.11173 valid_loss: 0.11639 test_loss: 0.12886 \n",
      "验证损失减少 (0.117802 --> 0.116387). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.10741 valid_loss: 0.11304 test_loss: 0.12737 \n",
      "验证损失减少 (0.116387 --> 0.113037). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10913 valid_loss: 0.11387 test_loss: 0.12771 \n",
      "[ 24/500] train_loss: 0.10923 valid_loss: 0.11432 test_loss: 0.12609 \n",
      "[ 25/500] train_loss: 0.10448 valid_loss: 0.10841 test_loss: 0.12415 \n",
      "验证损失减少 (0.113037 --> 0.108411). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10542 valid_loss: 0.11149 test_loss: 0.12415 \n",
      "[ 27/500] train_loss: 0.10609 valid_loss: 0.11283 test_loss: 0.12466 \n",
      "[ 28/500] train_loss: 0.10506 valid_loss: 0.10911 test_loss: 0.12417 \n",
      "[ 29/500] train_loss: 0.10366 valid_loss: 0.10919 test_loss: 0.12303 \n",
      "[ 30/500] train_loss: 0.10491 valid_loss: 0.10793 test_loss: 0.12091 \n",
      "验证损失减少 (0.108411 --> 0.107934). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.10237 valid_loss: 0.10814 test_loss: 0.12163 \n",
      "[ 32/500] train_loss: 0.10139 valid_loss: 0.10676 test_loss: 0.12196 \n",
      "验证损失减少 (0.107934 --> 0.106765). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10425 valid_loss: 0.10424 test_loss: 0.11791 \n",
      "验证损失减少 (0.106765 --> 0.104238). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.10134 valid_loss: 0.10475 test_loss: 0.11816 \n",
      "[ 35/500] train_loss: 0.09959 valid_loss: 0.10315 test_loss: 0.11686 \n",
      "验证损失减少 (0.104238 --> 0.103152). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09871 valid_loss: 0.10771 test_loss: 0.12005 \n",
      "[ 37/500] train_loss: 0.09894 valid_loss: 0.10419 test_loss: 0.11624 \n",
      "[ 38/500] train_loss: 0.09828 valid_loss: 0.10451 test_loss: 0.11787 \n",
      "[ 39/500] train_loss: 0.09658 valid_loss: 0.10172 test_loss: 0.11532 \n",
      "验证损失减少 (0.103152 --> 0.101720). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09654 valid_loss: 0.10234 test_loss: 0.11525 \n",
      "[ 41/500] train_loss: 0.09662 valid_loss: 0.10378 test_loss: 0.11593 \n",
      "[ 42/500] train_loss: 0.09374 valid_loss: 0.10019 test_loss: 0.11239 \n",
      "验证损失减少 (0.101720 --> 0.100189). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.09410 valid_loss: 0.10092 test_loss: 0.11431 \n",
      "[ 44/500] train_loss: 0.09396 valid_loss: 0.10155 test_loss: 0.11259 \n",
      "[ 45/500] train_loss: 0.09460 valid_loss: 0.09881 test_loss: 0.11300 \n",
      "验证损失减少 (0.100189 --> 0.098810). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.09198 valid_loss: 0.09920 test_loss: 0.11341 \n",
      "[ 47/500] train_loss: 0.09075 valid_loss: 0.09851 test_loss: 0.11229 \n",
      "验证损失减少 (0.098810 --> 0.098505). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.09277 valid_loss: 0.09731 test_loss: 0.10975 \n",
      "验证损失减少 (0.098505 --> 0.097312). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09089 valid_loss: 0.09882 test_loss: 0.11256 \n",
      "[ 50/500] train_loss: 0.09323 valid_loss: 0.09680 test_loss: 0.11331 \n",
      "验证损失减少 (0.097312 --> 0.096802). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.09237 valid_loss: 0.09840 test_loss: 0.11130 \n",
      "[ 52/500] train_loss: 0.09116 valid_loss: 0.09824 test_loss: 0.11092 \n",
      "[ 53/500] train_loss: 0.08834 valid_loss: 0.09739 test_loss: 0.11101 \n",
      "[ 54/500] train_loss: 0.09297 valid_loss: 0.09541 test_loss: 0.10861 \n",
      "验证损失减少 (0.096802 --> 0.095411). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.08938 valid_loss: 0.09605 test_loss: 0.10804 \n",
      "[ 56/500] train_loss: 0.08920 valid_loss: 0.09644 test_loss: 0.10795 \n",
      "[ 57/500] train_loss: 0.09135 valid_loss: 0.09784 test_loss: 0.10924 \n",
      "[ 58/500] train_loss: 0.08920 valid_loss: 0.09942 test_loss: 0.10966 \n",
      "[ 59/500] train_loss: 0.08655 valid_loss: 0.09778 test_loss: 0.10813 \n",
      "[ 60/500] train_loss: 0.08910 valid_loss: 0.09647 test_loss: 0.10837 \n",
      "[ 61/500] train_loss: 0.08957 valid_loss: 0.09833 test_loss: 0.10985 \n",
      "[ 62/500] train_loss: 0.08865 valid_loss: 0.09432 test_loss: 0.10680 \n",
      "验证损失减少 (0.095411 --> 0.094316). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.08531 valid_loss: 0.09295 test_loss: 0.10660 \n",
      "验证损失减少 (0.094316 --> 0.092951). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.08530 valid_loss: 0.09343 test_loss: 0.10745 \n",
      "[ 65/500] train_loss: 0.08891 valid_loss: 0.09829 test_loss: 0.10770 \n",
      "[ 66/500] train_loss: 0.08332 valid_loss: 0.09457 test_loss: 0.10733 \n",
      "[ 67/500] train_loss: 0.08491 valid_loss: 0.09494 test_loss: 0.10615 \n",
      "[ 68/500] train_loss: 0.08559 valid_loss: 0.09344 test_loss: 0.10330 \n",
      "[ 69/500] train_loss: 0.08648 valid_loss: 0.09396 test_loss: 0.10481 \n",
      "[ 70/500] train_loss: 0.08672 valid_loss: 0.09812 test_loss: 0.10825 \n",
      "[ 71/500] train_loss: 0.08377 valid_loss: 0.09131 test_loss: 0.10451 \n",
      "验证损失减少 (0.092951 --> 0.091314). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 72/500] train_loss: 0.08494 valid_loss: 0.09397 test_loss: 0.10348 \n",
      "[ 73/500] train_loss: 0.08457 valid_loss: 0.09115 test_loss: 0.10313 \n",
      "验证损失减少 (0.091314 --> 0.091146). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.08446 valid_loss: 0.09368 test_loss: 0.10311 \n",
      "[ 75/500] train_loss: 0.08393 valid_loss: 0.09275 test_loss: 0.10477 \n",
      "[ 76/500] train_loss: 0.08421 valid_loss: 0.09374 test_loss: 0.10399 \n",
      "[ 77/500] train_loss: 0.08532 valid_loss: 0.09299 test_loss: 0.10250 \n",
      "[ 78/500] train_loss: 0.08510 valid_loss: 0.08988 test_loss: 0.10074 \n",
      "验证损失减少 (0.091146 --> 0.089884). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08410 valid_loss: 0.09208 test_loss: 0.10364 \n",
      "[ 80/500] train_loss: 0.08124 valid_loss: 0.09015 test_loss: 0.10301 \n",
      "[ 81/500] train_loss: 0.08199 valid_loss: 0.08965 test_loss: 0.10216 \n",
      "验证损失减少 (0.089884 --> 0.089649). 正在保存模型...\n",
      "[ 82/500] train_loss: 0.08269 valid_loss: 0.09163 test_loss: 0.10174 \n",
      "[ 83/500] train_loss: 0.08248 valid_loss: 0.08944 test_loss: 0.10122 \n",
      "验证损失减少 (0.089649 --> 0.089435). 正在保存模型...\n",
      "[ 84/500] train_loss: 0.08335 valid_loss: 0.09062 test_loss: 0.10337 \n",
      "[ 85/500] train_loss: 0.08223 valid_loss: 0.08910 test_loss: 0.10099 \n",
      "验证损失减少 (0.089435 --> 0.089100). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08158 valid_loss: 0.08826 test_loss: 0.10004 \n",
      "验证损失减少 (0.089100 --> 0.088262). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.08247 valid_loss: 0.08755 test_loss: 0.09859 \n",
      "验证损失减少 (0.088262 --> 0.087549). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.08121 valid_loss: 0.09137 test_loss: 0.10125 \n",
      "[ 89/500] train_loss: 0.08052 valid_loss: 0.09146 test_loss: 0.09826 \n",
      "[ 90/500] train_loss: 0.08049 valid_loss: 0.09040 test_loss: 0.10074 \n",
      "[ 91/500] train_loss: 0.08296 valid_loss: 0.08883 test_loss: 0.09920 \n",
      "[ 92/500] train_loss: 0.08047 valid_loss: 0.08859 test_loss: 0.09956 \n",
      "[ 93/500] train_loss: 0.08063 valid_loss: 0.08670 test_loss: 0.09973 \n",
      "验证损失减少 (0.087549 --> 0.086704). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.08042 valid_loss: 0.08920 test_loss: 0.09863 \n",
      "[ 95/500] train_loss: 0.07983 valid_loss: 0.08833 test_loss: 0.09902 \n",
      "[ 96/500] train_loss: 0.07807 valid_loss: 0.08731 test_loss: 0.09961 \n",
      "[ 97/500] train_loss: 0.08022 valid_loss: 0.08937 test_loss: 0.09878 \n",
      "[ 98/500] train_loss: 0.07891 valid_loss: 0.09016 test_loss: 0.10081 \n",
      "[ 99/500] train_loss: 0.07900 valid_loss: 0.08874 test_loss: 0.09956 \n",
      "[100/500] train_loss: 0.08029 valid_loss: 0.08867 test_loss: 0.10058 \n",
      "[101/500] train_loss: 0.08034 valid_loss: 0.08796 test_loss: 0.10121 \n",
      "[102/500] train_loss: 0.07857 valid_loss: 0.08693 test_loss: 0.09946 \n",
      "[103/500] train_loss: 0.07848 valid_loss: 0.09027 test_loss: 0.09685 \n",
      "[104/500] train_loss: 0.07801 valid_loss: 0.08844 test_loss: 0.09876 \n",
      "[105/500] train_loss: 0.07769 valid_loss: 0.08622 test_loss: 0.09942 \n",
      "验证损失减少 (0.086704 --> 0.086218). 正在保存模型...\n",
      "[106/500] train_loss: 0.08028 valid_loss: 0.08641 test_loss: 0.09860 \n",
      "[107/500] train_loss: 0.07999 valid_loss: 0.08514 test_loss: 0.09654 \n",
      "验证损失减少 (0.086218 --> 0.085140). 正在保存模型...\n",
      "[108/500] train_loss: 0.07699 valid_loss: 0.08579 test_loss: 0.09817 \n",
      "[109/500] train_loss: 0.08000 valid_loss: 0.08857 test_loss: 0.09785 \n",
      "[110/500] train_loss: 0.07682 valid_loss: 0.08605 test_loss: 0.09901 \n",
      "[111/500] train_loss: 0.07780 valid_loss: 0.08703 test_loss: 0.09832 \n",
      "[112/500] train_loss: 0.07814 valid_loss: 0.08611 test_loss: 0.09763 \n",
      "[113/500] train_loss: 0.07726 valid_loss: 0.08640 test_loss: 0.09801 \n",
      "[114/500] train_loss: 0.07466 valid_loss: 0.08633 test_loss: 0.09638 \n",
      "[115/500] train_loss: 0.07468 valid_loss: 0.08660 test_loss: 0.09681 \n",
      "[116/500] train_loss: 0.07566 valid_loss: 0.08624 test_loss: 0.09706 \n",
      "[117/500] train_loss: 0.07690 valid_loss: 0.08753 test_loss: 0.09827 \n",
      "[118/500] train_loss: 0.07519 valid_loss: 0.08575 test_loss: 0.09702 \n",
      "[119/500] train_loss: 0.07835 valid_loss: 0.08303 test_loss: 0.09528 \n",
      "验证损失减少 (0.085140 --> 0.083030). 正在保存模型...\n",
      "[120/500] train_loss: 0.07600 valid_loss: 0.08466 test_loss: 0.09600 \n",
      "[121/500] train_loss: 0.07377 valid_loss: 0.08533 test_loss: 0.09594 \n",
      "[122/500] train_loss: 0.07477 valid_loss: 0.08342 test_loss: 0.09511 \n",
      "[123/500] train_loss: 0.07363 valid_loss: 0.08253 test_loss: 0.09369 \n",
      "验证损失减少 (0.083030 --> 0.082535). 正在保存模型...\n",
      "[124/500] train_loss: 0.07738 valid_loss: 0.08360 test_loss: 0.09711 \n",
      "[125/500] train_loss: 0.07386 valid_loss: 0.08280 test_loss: 0.09521 \n",
      "[126/500] train_loss: 0.07465 valid_loss: 0.08294 test_loss: 0.09541 \n",
      "[127/500] train_loss: 0.07603 valid_loss: 0.08280 test_loss: 0.09496 \n",
      "[128/500] train_loss: 0.07378 valid_loss: 0.08531 test_loss: 0.09715 \n",
      "[129/500] train_loss: 0.07354 valid_loss: 0.08343 test_loss: 0.09555 \n",
      "[130/500] train_loss: 0.07420 valid_loss: 0.08339 test_loss: 0.09513 \n",
      "[131/500] train_loss: 0.07385 valid_loss: 0.08371 test_loss: 0.09442 \n",
      "[132/500] train_loss: 0.07353 valid_loss: 0.08260 test_loss: 0.09661 \n",
      "[133/500] train_loss: 0.07340 valid_loss: 0.08173 test_loss: 0.09538 \n",
      "验证损失减少 (0.082535 --> 0.081727). 正在保存模型...\n",
      "[134/500] train_loss: 0.07319 valid_loss: 0.08326 test_loss: 0.09608 \n",
      "[135/500] train_loss: 0.07401 valid_loss: 0.08217 test_loss: 0.09520 \n",
      "[136/500] train_loss: 0.07416 valid_loss: 0.08335 test_loss: 0.09482 \n",
      "[137/500] train_loss: 0.07258 valid_loss: 0.08324 test_loss: 0.09404 \n",
      "[138/500] train_loss: 0.07326 valid_loss: 0.08385 test_loss: 0.09644 \n",
      "[139/500] train_loss: 0.07293 valid_loss: 0.08303 test_loss: 0.09393 \n",
      "[140/500] train_loss: 0.07121 valid_loss: 0.08208 test_loss: 0.09360 \n",
      "[141/500] train_loss: 0.07062 valid_loss: 0.08275 test_loss: 0.09432 \n",
      "[142/500] train_loss: 0.07568 valid_loss: 0.08433 test_loss: 0.09354 \n",
      "[143/500] train_loss: 0.07191 valid_loss: 0.08165 test_loss: 0.09481 \n",
      "验证损失减少 (0.081727 --> 0.081651). 正在保存模型...\n",
      "[144/500] train_loss: 0.07242 valid_loss: 0.08342 test_loss: 0.09406 \n",
      "[145/500] train_loss: 0.07310 valid_loss: 0.08186 test_loss: 0.09424 \n",
      "[146/500] train_loss: 0.06935 valid_loss: 0.08622 test_loss: 0.09347 \n",
      "[147/500] train_loss: 0.07243 valid_loss: 0.08304 test_loss: 0.09470 \n",
      "[148/500] train_loss: 0.07251 valid_loss: 0.08116 test_loss: 0.09405 \n",
      "验证损失减少 (0.081651 --> 0.081157). 正在保存模型...\n",
      "[149/500] train_loss: 0.07291 valid_loss: 0.08139 test_loss: 0.09469 \n",
      "[150/500] train_loss: 0.07067 valid_loss: 0.08272 test_loss: 0.09364 \n",
      "[151/500] train_loss: 0.07162 valid_loss: 0.08077 test_loss: 0.09305 \n",
      "验证损失减少 (0.081157 --> 0.080770). 正在保存模型...\n",
      "[152/500] train_loss: 0.07252 valid_loss: 0.08120 test_loss: 0.09386 \n",
      "[153/500] train_loss: 0.07254 valid_loss: 0.08465 test_loss: 0.09376 \n",
      "[154/500] train_loss: 0.07168 valid_loss: 0.08156 test_loss: 0.09256 \n",
      "[155/500] train_loss: 0.07468 valid_loss: 0.08325 test_loss: 0.09277 \n",
      "[156/500] train_loss: 0.07043 valid_loss: 0.08239 test_loss: 0.09368 \n",
      "[157/500] train_loss: 0.06960 valid_loss: 0.08244 test_loss: 0.09269 \n",
      "[158/500] train_loss: 0.06982 valid_loss: 0.08056 test_loss: 0.09339 \n",
      "验证损失减少 (0.080770 --> 0.080563). 正在保存模型...\n",
      "[159/500] train_loss: 0.06984 valid_loss: 0.08112 test_loss: 0.09260 \n",
      "[160/500] train_loss: 0.07013 valid_loss: 0.08355 test_loss: 0.09360 \n",
      "[161/500] train_loss: 0.07263 valid_loss: 0.08160 test_loss: 0.09134 \n",
      "[162/500] train_loss: 0.06927 valid_loss: 0.08102 test_loss: 0.09258 \n",
      "[163/500] train_loss: 0.06930 valid_loss: 0.08017 test_loss: 0.09229 \n",
      "验证损失减少 (0.080563 --> 0.080167). 正在保存模型...\n",
      "[164/500] train_loss: 0.07349 valid_loss: 0.08185 test_loss: 0.09439 \n",
      "[165/500] train_loss: 0.06954 valid_loss: 0.08224 test_loss: 0.09320 \n",
      "[166/500] train_loss: 0.06782 valid_loss: 0.08464 test_loss: 0.09255 \n",
      "[167/500] train_loss: 0.06813 valid_loss: 0.08040 test_loss: 0.09281 \n",
      "[168/500] train_loss: 0.06920 valid_loss: 0.07988 test_loss: 0.09392 \n",
      "验证损失减少 (0.080167 --> 0.079883). 正在保存模型...\n",
      "[169/500] train_loss: 0.06799 valid_loss: 0.08248 test_loss: 0.09362 \n",
      "[170/500] train_loss: 0.07008 valid_loss: 0.08061 test_loss: 0.09296 \n",
      "[171/500] train_loss: 0.06945 valid_loss: 0.08028 test_loss: 0.09105 \n",
      "[172/500] train_loss: 0.06807 valid_loss: 0.08472 test_loss: 0.09261 \n",
      "[173/500] train_loss: 0.06992 valid_loss: 0.08348 test_loss: 0.09250 \n",
      "[174/500] train_loss: 0.06902 valid_loss: 0.08353 test_loss: 0.09169 \n",
      "[175/500] train_loss: 0.06932 valid_loss: 0.08592 test_loss: 0.09180 \n",
      "[176/500] train_loss: 0.07245 valid_loss: 0.08540 test_loss: 0.09379 \n",
      "[177/500] train_loss: 0.07022 valid_loss: 0.08308 test_loss: 0.09067 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178/500] train_loss: 0.06888 valid_loss: 0.08401 test_loss: 0.09096 \n",
      "[179/500] train_loss: 0.07041 valid_loss: 0.08733 test_loss: 0.09274 \n",
      "[180/500] train_loss: 0.06898 valid_loss: 0.08417 test_loss: 0.09137 \n",
      "[181/500] train_loss: 0.06551 valid_loss: 0.08267 test_loss: 0.09273 \n",
      "[182/500] train_loss: 0.06692 valid_loss: 0.08160 test_loss: 0.09167 \n",
      "[183/500] train_loss: 0.06854 valid_loss: 0.08617 test_loss: 0.09218 \n",
      "[184/500] train_loss: 0.06721 valid_loss: 0.08074 test_loss: 0.09122 \n",
      "[185/500] train_loss: 0.07002 valid_loss: 0.08242 test_loss: 0.09195 \n",
      "[186/500] train_loss: 0.06593 valid_loss: 0.07857 test_loss: 0.09013 \n",
      "验证损失减少 (0.079883 --> 0.078572). 正在保存模型...\n",
      "[187/500] train_loss: 0.06817 valid_loss: 0.07903 test_loss: 0.09319 \n",
      "[188/500] train_loss: 0.06718 valid_loss: 0.07838 test_loss: 0.09077 \n",
      "验证损失减少 (0.078572 --> 0.078383). 正在保存模型...\n",
      "[189/500] train_loss: 0.06752 valid_loss: 0.08509 test_loss: 0.09100 \n",
      "[190/500] train_loss: 0.06849 valid_loss: 0.08538 test_loss: 0.09150 \n",
      "[191/500] train_loss: 0.06922 valid_loss: 0.08215 test_loss: 0.08995 \n",
      "[192/500] train_loss: 0.06777 valid_loss: 0.08297 test_loss: 0.09010 \n",
      "[193/500] train_loss: 0.06821 valid_loss: 0.08100 test_loss: 0.08965 \n",
      "[194/500] train_loss: 0.06818 valid_loss: 0.07850 test_loss: 0.09101 \n",
      "[195/500] train_loss: 0.06711 valid_loss: 0.08073 test_loss: 0.09118 \n",
      "[196/500] train_loss: 0.06810 valid_loss: 0.08226 test_loss: 0.09025 \n",
      "[197/500] train_loss: 0.06779 valid_loss: 0.08808 test_loss: 0.09238 \n",
      "[198/500] train_loss: 0.06577 valid_loss: 0.08165 test_loss: 0.09171 \n",
      "[199/500] train_loss: 0.06956 valid_loss: 0.07813 test_loss: 0.09111 \n",
      "验证损失减少 (0.078383 --> 0.078133). 正在保存模型...\n",
      "[200/500] train_loss: 0.06584 valid_loss: 0.08075 test_loss: 0.08987 \n",
      "[201/500] train_loss: 0.06722 valid_loss: 0.08348 test_loss: 0.09154 \n",
      "[202/500] train_loss: 0.06669 valid_loss: 0.07861 test_loss: 0.08933 \n",
      "[203/500] train_loss: 0.06358 valid_loss: 0.08826 test_loss: 0.09055 \n",
      "[204/500] train_loss: 0.06679 valid_loss: 0.07904 test_loss: 0.08988 \n",
      "[205/500] train_loss: 0.06654 valid_loss: 0.08523 test_loss: 0.09170 \n",
      "[206/500] train_loss: 0.06532 valid_loss: 0.07975 test_loss: 0.09023 \n",
      "[207/500] train_loss: 0.06777 valid_loss: 0.08161 test_loss: 0.09018 \n",
      "[208/500] train_loss: 0.06686 valid_loss: 0.08208 test_loss: 0.09221 \n",
      "[209/500] train_loss: 0.06517 valid_loss: 0.07863 test_loss: 0.08998 \n",
      "[210/500] train_loss: 0.06544 valid_loss: 0.07935 test_loss: 0.09086 \n",
      "[211/500] train_loss: 0.06817 valid_loss: 0.07932 test_loss: 0.08997 \n",
      "[212/500] train_loss: 0.06394 valid_loss: 0.07951 test_loss: 0.09243 \n",
      "[213/500] train_loss: 0.06411 valid_loss: 0.08039 test_loss: 0.08993 \n",
      "[214/500] train_loss: 0.06674 valid_loss: 0.08404 test_loss: 0.09321 \n",
      "[215/500] train_loss: 0.06611 valid_loss: 0.07730 test_loss: 0.08877 \n",
      "验证损失减少 (0.078133 --> 0.077304). 正在保存模型...\n",
      "[216/500] train_loss: 0.06579 valid_loss: 0.07717 test_loss: 0.08870 \n",
      "验证损失减少 (0.077304 --> 0.077170). 正在保存模型...\n",
      "[217/500] train_loss: 0.06578 valid_loss: 0.07662 test_loss: 0.08905 \n",
      "验证损失减少 (0.077170 --> 0.076625). 正在保存模型...\n",
      "[218/500] train_loss: 0.06521 valid_loss: 0.07937 test_loss: 0.09083 \n",
      "[219/500] train_loss: 0.06497 valid_loss: 0.08043 test_loss: 0.08868 \n",
      "[220/500] train_loss: 0.06526 valid_loss: 0.07750 test_loss: 0.08808 \n",
      "[221/500] train_loss: 0.06606 valid_loss: 0.07867 test_loss: 0.09038 \n",
      "[222/500] train_loss: 0.06606 valid_loss: 0.07717 test_loss: 0.09123 \n",
      "[223/500] train_loss: 0.06643 valid_loss: 0.07826 test_loss: 0.09117 \n",
      "[224/500] train_loss: 0.06464 valid_loss: 0.08106 test_loss: 0.09208 \n",
      "[225/500] train_loss: 0.06791 valid_loss: 0.07856 test_loss: 0.09075 \n",
      "[226/500] train_loss: 0.06348 valid_loss: 0.07845 test_loss: 0.09029 \n",
      "[227/500] train_loss: 0.06458 valid_loss: 0.08037 test_loss: 0.09126 \n",
      "[228/500] train_loss: 0.06383 valid_loss: 0.07890 test_loss: 0.09047 \n",
      "[229/500] train_loss: 0.06320 valid_loss: 0.07849 test_loss: 0.09041 \n",
      "[230/500] train_loss: 0.06523 valid_loss: 0.07850 test_loss: 0.08890 \n",
      "[231/500] train_loss: 0.06444 valid_loss: 0.07867 test_loss: 0.08969 \n",
      "[232/500] train_loss: 0.06462 valid_loss: 0.07728 test_loss: 0.08951 \n",
      "[233/500] train_loss: 0.06417 valid_loss: 0.08762 test_loss: 0.09025 \n",
      "[234/500] train_loss: 0.06286 valid_loss: 0.08377 test_loss: 0.09188 \n",
      "[235/500] train_loss: 0.06287 valid_loss: 0.08265 test_loss: 0.08959 \n",
      "[236/500] train_loss: 0.06482 valid_loss: 0.08186 test_loss: 0.08868 \n",
      "[237/500] train_loss: 0.06454 valid_loss: 0.08462 test_loss: 0.08958 \n",
      "[238/500] train_loss: 0.06354 valid_loss: 0.07993 test_loss: 0.08916 \n",
      "[239/500] train_loss: 0.06251 valid_loss: 0.08064 test_loss: 0.08894 \n",
      "[240/500] train_loss: 0.06469 valid_loss: 0.07991 test_loss: 0.08910 \n",
      "[241/500] train_loss: 0.06312 valid_loss: 0.07898 test_loss: 0.09016 \n",
      "[242/500] train_loss: 0.06480 valid_loss: 0.07868 test_loss: 0.08927 \n",
      "[243/500] train_loss: 0.06302 valid_loss: 0.08046 test_loss: 0.08855 \n",
      "[244/500] train_loss: 0.06376 valid_loss: 0.07756 test_loss: 0.08839 \n",
      "[245/500] train_loss: 0.06293 valid_loss: 0.07779 test_loss: 0.08884 \n",
      "[246/500] train_loss: 0.06529 valid_loss: 0.08055 test_loss: 0.09047 \n",
      "[247/500] train_loss: 0.06384 valid_loss: 0.07815 test_loss: 0.09027 \n",
      "[248/500] train_loss: 0.06236 valid_loss: 0.07707 test_loss: 0.08927 \n",
      "[249/500] train_loss: 0.06375 valid_loss: 0.07709 test_loss: 0.08939 \n",
      "[250/500] train_loss: 0.06242 valid_loss: 0.07949 test_loss: 0.08880 \n",
      "[251/500] train_loss: 0.06174 valid_loss: 0.07900 test_loss: 0.09013 \n",
      "[252/500] train_loss: 0.06351 valid_loss: 0.07967 test_loss: 0.08930 \n",
      "[253/500] train_loss: 0.06189 valid_loss: 0.08127 test_loss: 0.08920 \n",
      "[254/500] train_loss: 0.06277 valid_loss: 0.07897 test_loss: 0.09045 \n",
      "[255/500] train_loss: 0.06248 valid_loss: 0.07962 test_loss: 0.08994 \n",
      "[256/500] train_loss: 0.06178 valid_loss: 0.07994 test_loss: 0.08880 \n",
      "[257/500] train_loss: 0.06336 valid_loss: 0.08490 test_loss: 0.09033 \n",
      "[258/500] train_loss: 0.06364 valid_loss: 0.08097 test_loss: 0.08883 \n",
      "[259/500] train_loss: 0.06365 valid_loss: 0.07990 test_loss: 0.08980 \n",
      "[260/500] train_loss: 0.06167 valid_loss: 0.08212 test_loss: 0.09032 \n",
      "[261/500] train_loss: 0.06362 valid_loss: 0.08706 test_loss: 0.08918 \n",
      "[262/500] train_loss: 0.06350 valid_loss: 0.08088 test_loss: 0.08850 \n",
      "[263/500] train_loss: 0.06279 valid_loss: 0.08239 test_loss: 0.08934 \n",
      "[264/500] train_loss: 0.06168 valid_loss: 0.07944 test_loss: 0.08918 \n",
      "[265/500] train_loss: 0.06300 valid_loss: 0.08102 test_loss: 0.08862 \n",
      "[266/500] train_loss: 0.06205 valid_loss: 0.07684 test_loss: 0.08885 \n",
      "[267/500] train_loss: 0.06356 valid_loss: 0.07675 test_loss: 0.08804 \n",
      "[268/500] train_loss: 0.06072 valid_loss: 0.07699 test_loss: 0.09008 \n",
      "[269/500] train_loss: 0.06126 valid_loss: 0.07607 test_loss: 0.08797 \n",
      "验证损失减少 (0.076625 --> 0.076068). 正在保存模型...\n",
      "[270/500] train_loss: 0.06080 valid_loss: 0.07686 test_loss: 0.08950 \n",
      "[271/500] train_loss: 0.06099 valid_loss: 0.07864 test_loss: 0.08837 \n",
      "[272/500] train_loss: 0.06258 valid_loss: 0.07900 test_loss: 0.08936 \n",
      "[273/500] train_loss: 0.06070 valid_loss: 0.07661 test_loss: 0.08888 \n",
      "[274/500] train_loss: 0.06018 valid_loss: 0.07818 test_loss: 0.08871 \n",
      "[275/500] train_loss: 0.06262 valid_loss: 0.07812 test_loss: 0.08708 \n",
      "[276/500] train_loss: 0.06107 valid_loss: 0.07639 test_loss: 0.08957 \n",
      "[277/500] train_loss: 0.06084 valid_loss: 0.07828 test_loss: 0.08897 \n",
      "[278/500] train_loss: 0.06239 valid_loss: 0.07936 test_loss: 0.08971 \n",
      "[279/500] train_loss: 0.06227 valid_loss: 0.08065 test_loss: 0.08839 \n",
      "[280/500] train_loss: 0.06056 valid_loss: 0.07693 test_loss: 0.08910 \n",
      "[281/500] train_loss: 0.06204 valid_loss: 0.07684 test_loss: 0.08933 \n",
      "[282/500] train_loss: 0.05969 valid_loss: 0.07760 test_loss: 0.09010 \n",
      "[283/500] train_loss: 0.06330 valid_loss: 0.07725 test_loss: 0.08844 \n",
      "[284/500] train_loss: 0.06328 valid_loss: 0.07781 test_loss: 0.08928 \n",
      "[285/500] train_loss: 0.06161 valid_loss: 0.07592 test_loss: 0.08882 \n",
      "验证损失减少 (0.076068 --> 0.075921). 正在保存模型...\n",
      "[286/500] train_loss: 0.06053 valid_loss: 0.07548 test_loss: 0.08781 \n",
      "验证损失减少 (0.075921 --> 0.075478). 正在保存模型...\n",
      "[287/500] train_loss: 0.06113 valid_loss: 0.07498 test_loss: 0.08970 \n",
      "验证损失减少 (0.075478 --> 0.074984). 正在保存模型...\n",
      "[288/500] train_loss: 0.06071 valid_loss: 0.07544 test_loss: 0.08820 \n",
      "[289/500] train_loss: 0.06120 valid_loss: 0.07882 test_loss: 0.08973 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290/500] train_loss: 0.06054 valid_loss: 0.07655 test_loss: 0.08917 \n",
      "[291/500] train_loss: 0.05974 valid_loss: 0.07825 test_loss: 0.08968 \n",
      "[292/500] train_loss: 0.05817 valid_loss: 0.07768 test_loss: 0.09145 \n",
      "[293/500] train_loss: 0.06031 valid_loss: 0.07610 test_loss: 0.08900 \n",
      "[294/500] train_loss: 0.05965 valid_loss: 0.07627 test_loss: 0.09021 \n",
      "[295/500] train_loss: 0.06168 valid_loss: 0.07633 test_loss: 0.08877 \n",
      "[296/500] train_loss: 0.06209 valid_loss: 0.08408 test_loss: 0.09041 \n",
      "[297/500] train_loss: 0.05996 valid_loss: 0.07993 test_loss: 0.08943 \n",
      "[298/500] train_loss: 0.06114 valid_loss: 0.08255 test_loss: 0.08961 \n",
      "[299/500] train_loss: 0.06212 valid_loss: 0.07616 test_loss: 0.08729 \n",
      "[300/500] train_loss: 0.05856 valid_loss: 0.07773 test_loss: 0.08867 \n",
      "[301/500] train_loss: 0.06076 valid_loss: 0.07750 test_loss: 0.08773 \n",
      "[302/500] train_loss: 0.06125 valid_loss: 0.07777 test_loss: 0.08826 \n",
      "[303/500] train_loss: 0.06013 valid_loss: 0.07626 test_loss: 0.08874 \n",
      "[304/500] train_loss: 0.05986 valid_loss: 0.07689 test_loss: 0.08885 \n",
      "[305/500] train_loss: 0.05823 valid_loss: 0.07861 test_loss: 0.08791 \n",
      "[306/500] train_loss: 0.06468 valid_loss: 0.07775 test_loss: 0.08722 \n",
      "[307/500] train_loss: 0.06071 valid_loss: 0.07715 test_loss: 0.08947 \n",
      "[308/500] train_loss: 0.05944 valid_loss: 0.07555 test_loss: 0.08885 \n",
      "[309/500] train_loss: 0.06035 valid_loss: 0.07639 test_loss: 0.08786 \n",
      "[310/500] train_loss: 0.06012 valid_loss: 0.07599 test_loss: 0.08714 \n",
      "[311/500] train_loss: 0.05930 valid_loss: 0.07518 test_loss: 0.08757 \n",
      "[312/500] train_loss: 0.05901 valid_loss: 0.07580 test_loss: 0.08869 \n",
      "[313/500] train_loss: 0.05939 valid_loss: 0.07639 test_loss: 0.08788 \n",
      "[314/500] train_loss: 0.06027 valid_loss: 0.07542 test_loss: 0.08583 \n",
      "[315/500] train_loss: 0.05963 valid_loss: 0.07525 test_loss: 0.08622 \n",
      "[316/500] train_loss: 0.06031 valid_loss: 0.07590 test_loss: 0.08780 \n",
      "[317/500] train_loss: 0.05878 valid_loss: 0.08723 test_loss: 0.08739 \n",
      "[318/500] train_loss: 0.05954 valid_loss: 0.07654 test_loss: 0.08611 \n",
      "[319/500] train_loss: 0.06049 valid_loss: 0.07696 test_loss: 0.08699 \n",
      "[320/500] train_loss: 0.06120 valid_loss: 0.07489 test_loss: 0.08632 \n",
      "验证损失减少 (0.074984 --> 0.074888). 正在保存模型...\n",
      "[321/500] train_loss: 0.05995 valid_loss: 0.08031 test_loss: 0.08712 \n",
      "[322/500] train_loss: 0.05820 valid_loss: 0.07597 test_loss: 0.08754 \n",
      "[323/500] train_loss: 0.05737 valid_loss: 0.08549 test_loss: 0.08821 \n",
      "[324/500] train_loss: 0.05760 valid_loss: 0.07760 test_loss: 0.08953 \n",
      "[325/500] train_loss: 0.05863 valid_loss: 0.07707 test_loss: 0.08834 \n",
      "[326/500] train_loss: 0.05921 valid_loss: 0.07642 test_loss: 0.08828 \n",
      "[327/500] train_loss: 0.05858 valid_loss: 0.07568 test_loss: 0.08704 \n",
      "[328/500] train_loss: 0.05888 valid_loss: 0.07910 test_loss: 0.08794 \n",
      "[329/500] train_loss: 0.05833 valid_loss: 0.07667 test_loss: 0.08748 \n",
      "[330/500] train_loss: 0.06000 valid_loss: 0.07799 test_loss: 0.08725 \n",
      "[331/500] train_loss: 0.05798 valid_loss: 0.07499 test_loss: 0.08677 \n",
      "[332/500] train_loss: 0.05828 valid_loss: 0.07503 test_loss: 0.08709 \n",
      "[333/500] train_loss: 0.05810 valid_loss: 0.07512 test_loss: 0.08640 \n",
      "[334/500] train_loss: 0.05912 valid_loss: 0.07667 test_loss: 0.08699 \n",
      "[335/500] train_loss: 0.05862 valid_loss: 0.07600 test_loss: 0.08817 \n",
      "[336/500] train_loss: 0.05878 valid_loss: 0.07502 test_loss: 0.08659 \n",
      "[337/500] train_loss: 0.05720 valid_loss: 0.07548 test_loss: 0.08638 \n",
      "[338/500] train_loss: 0.05731 valid_loss: 0.07457 test_loss: 0.08681 \n",
      "验证损失减少 (0.074888 --> 0.074569). 正在保存模型...\n",
      "[339/500] train_loss: 0.05971 valid_loss: 0.07471 test_loss: 0.08680 \n",
      "[340/500] train_loss: 0.05859 valid_loss: 0.07510 test_loss: 0.08708 \n",
      "[341/500] train_loss: 0.05791 valid_loss: 0.07754 test_loss: 0.08805 \n",
      "[342/500] train_loss: 0.05830 valid_loss: 0.07559 test_loss: 0.08779 \n",
      "[343/500] train_loss: 0.05745 valid_loss: 0.07632 test_loss: 0.08836 \n",
      "[344/500] train_loss: 0.05680 valid_loss: 0.07612 test_loss: 0.08801 \n",
      "[345/500] train_loss: 0.05771 valid_loss: 0.07752 test_loss: 0.08696 \n",
      "[346/500] train_loss: 0.05734 valid_loss: 0.07701 test_loss: 0.08748 \n",
      "[347/500] train_loss: 0.05626 valid_loss: 0.07461 test_loss: 0.08695 \n",
      "[348/500] train_loss: 0.05942 valid_loss: 0.07348 test_loss: 0.08654 \n",
      "验证损失减少 (0.074569 --> 0.073477). 正在保存模型...\n",
      "[349/500] train_loss: 0.05709 valid_loss: 0.07657 test_loss: 0.08694 \n",
      "[350/500] train_loss: 0.05711 valid_loss: 0.07516 test_loss: 0.08715 \n",
      "[351/500] train_loss: 0.05768 valid_loss: 0.07533 test_loss: 0.08738 \n",
      "[352/500] train_loss: 0.05600 valid_loss: 0.08130 test_loss: 0.08608 \n",
      "[353/500] train_loss: 0.05651 valid_loss: 0.07653 test_loss: 0.08836 \n",
      "[354/500] train_loss: 0.05786 valid_loss: 0.07499 test_loss: 0.08627 \n",
      "[355/500] train_loss: 0.05605 valid_loss: 0.07488 test_loss: 0.08756 \n",
      "[356/500] train_loss: 0.05590 valid_loss: 0.07477 test_loss: 0.08702 \n",
      "[357/500] train_loss: 0.05687 valid_loss: 0.07528 test_loss: 0.08698 \n",
      "[358/500] train_loss: 0.05614 valid_loss: 0.07535 test_loss: 0.08734 \n",
      "[359/500] train_loss: 0.05744 valid_loss: 0.07436 test_loss: 0.08645 \n",
      "[360/500] train_loss: 0.05564 valid_loss: 0.07450 test_loss: 0.08788 \n",
      "[361/500] train_loss: 0.05752 valid_loss: 0.07545 test_loss: 0.08681 \n",
      "[362/500] train_loss: 0.05765 valid_loss: 0.07638 test_loss: 0.08865 \n",
      "[363/500] train_loss: 0.05856 valid_loss: 0.07446 test_loss: 0.08619 \n",
      "[364/500] train_loss: 0.05684 valid_loss: 0.07454 test_loss: 0.08782 \n",
      "[365/500] train_loss: 0.05633 valid_loss: 0.07433 test_loss: 0.08693 \n",
      "[366/500] train_loss: 0.05629 valid_loss: 0.07490 test_loss: 0.08628 \n",
      "[367/500] train_loss: 0.05583 valid_loss: 0.07608 test_loss: 0.08719 \n",
      "[368/500] train_loss: 0.05649 valid_loss: 0.07677 test_loss: 0.08743 \n",
      "[369/500] train_loss: 0.05676 valid_loss: 0.07529 test_loss: 0.08806 \n",
      "[370/500] train_loss: 0.05652 valid_loss: 0.07484 test_loss: 0.08527 \n",
      "[371/500] train_loss: 0.05697 valid_loss: 0.07550 test_loss: 0.08658 \n",
      "[372/500] train_loss: 0.05636 valid_loss: 0.07550 test_loss: 0.08706 \n",
      "[373/500] train_loss: 0.05543 valid_loss: 0.07642 test_loss: 0.08787 \n",
      "[374/500] train_loss: 0.05666 valid_loss: 0.07731 test_loss: 0.08572 \n",
      "[375/500] train_loss: 0.05571 valid_loss: 0.07810 test_loss: 0.08772 \n",
      "[376/500] train_loss: 0.05622 valid_loss: 0.07855 test_loss: 0.08756 \n",
      "[377/500] train_loss: 0.05548 valid_loss: 0.07555 test_loss: 0.08784 \n",
      "[378/500] train_loss: 0.05485 valid_loss: 0.07587 test_loss: 0.08663 \n",
      "[379/500] train_loss: 0.05602 valid_loss: 0.07820 test_loss: 0.08683 \n",
      "[380/500] train_loss: 0.05661 valid_loss: 0.07997 test_loss: 0.08493 \n",
      "[381/500] train_loss: 0.05741 valid_loss: 0.07877 test_loss: 0.08659 \n",
      "[382/500] train_loss: 0.05604 valid_loss: 0.07474 test_loss: 0.08708 \n",
      "[383/500] train_loss: 0.05780 valid_loss: 0.09267 test_loss: 0.08611 \n",
      "[384/500] train_loss: 0.05722 valid_loss: 0.08772 test_loss: 0.08695 \n",
      "[385/500] train_loss: 0.05405 valid_loss: 0.07914 test_loss: 0.08777 \n",
      "[386/500] train_loss: 0.05553 valid_loss: 0.07434 test_loss: 0.08734 \n",
      "[387/500] train_loss: 0.05590 valid_loss: 0.08008 test_loss: 0.08753 \n",
      "[388/500] train_loss: 0.05495 valid_loss: 0.08911 test_loss: 0.08614 \n",
      "[389/500] train_loss: 0.05545 valid_loss: 0.08325 test_loss: 0.08664 \n",
      "[390/500] train_loss: 0.05619 valid_loss: 0.08002 test_loss: 0.08811 \n",
      "[391/500] train_loss: 0.05593 valid_loss: 0.09165 test_loss: 0.08664 \n",
      "[392/500] train_loss: 0.05654 valid_loss: 0.07702 test_loss: 0.09018 \n",
      "[393/500] train_loss: 0.05693 valid_loss: 0.07704 test_loss: 0.08623 \n",
      "[394/500] train_loss: 0.05486 valid_loss: 0.08100 test_loss: 0.08840 \n",
      "[395/500] train_loss: 0.05536 valid_loss: 0.08480 test_loss: 0.08798 \n",
      "[396/500] train_loss: 0.05484 valid_loss: 0.07838 test_loss: 0.08540 \n",
      "[397/500] train_loss: 0.05479 valid_loss: 0.07521 test_loss: 0.08695 \n",
      "[398/500] train_loss: 0.05521 valid_loss: 0.07395 test_loss: 0.08748 \n",
      "[399/500] train_loss: 0.05410 valid_loss: 0.07431 test_loss: 0.08722 \n",
      "[400/500] train_loss: 0.05393 valid_loss: 0.07442 test_loss: 0.08653 \n",
      "[401/500] train_loss: 0.05545 valid_loss: 0.07617 test_loss: 0.08767 \n",
      "[402/500] train_loss: 0.05635 valid_loss: 0.08673 test_loss: 0.08632 \n",
      "[403/500] train_loss: 0.05316 valid_loss: 0.07530 test_loss: 0.08750 \n",
      "[404/500] train_loss: 0.05492 valid_loss: 0.07543 test_loss: 0.08701 \n",
      "[405/500] train_loss: 0.05590 valid_loss: 0.07542 test_loss: 0.08650 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[406/500] train_loss: 0.05441 valid_loss: 0.07643 test_loss: 0.08696 \n",
      "[407/500] train_loss: 0.05468 valid_loss: 0.07530 test_loss: 0.08542 \n",
      "[408/500] train_loss: 0.05620 valid_loss: 0.07338 test_loss: 0.08511 \n",
      "验证损失减少 (0.073477 --> 0.073383). 正在保存模型...\n",
      "[409/500] train_loss: 0.05515 valid_loss: 0.07762 test_loss: 0.08644 \n",
      "[410/500] train_loss: 0.05599 valid_loss: 0.08338 test_loss: 0.08662 \n",
      "[411/500] train_loss: 0.05415 valid_loss: 0.07789 test_loss: 0.08758 \n",
      "[412/500] train_loss: 0.05439 valid_loss: 0.08817 test_loss: 0.08665 \n",
      "[413/500] train_loss: 0.05524 valid_loss: 0.07738 test_loss: 0.08744 \n",
      "[414/500] train_loss: 0.05445 valid_loss: 0.07786 test_loss: 0.08838 \n",
      "[415/500] train_loss: 0.05442 valid_loss: 0.07707 test_loss: 0.08670 \n",
      "[416/500] train_loss: 0.05332 valid_loss: 0.07946 test_loss: 0.08877 \n",
      "[417/500] train_loss: 0.05459 valid_loss: 0.07949 test_loss: 0.08759 \n",
      "[418/500] train_loss: 0.05420 valid_loss: 0.08377 test_loss: 0.08629 \n",
      "[419/500] train_loss: 0.05613 valid_loss: 0.07721 test_loss: 0.08607 \n",
      "[420/500] train_loss: 0.05584 valid_loss: 0.07583 test_loss: 0.08809 \n",
      "[421/500] train_loss: 0.05458 valid_loss: 0.07929 test_loss: 0.08593 \n",
      "[422/500] train_loss: 0.05382 valid_loss: 0.08714 test_loss: 0.08524 \n",
      "[423/500] train_loss: 0.05435 valid_loss: 0.07531 test_loss: 0.08601 \n",
      "[424/500] train_loss: 0.05539 valid_loss: 0.09982 test_loss: 0.08523 \n",
      "[425/500] train_loss: 0.05694 valid_loss: 0.07497 test_loss: 0.08772 \n",
      "[426/500] train_loss: 0.05443 valid_loss: 0.07681 test_loss: 0.08522 \n",
      "[427/500] train_loss: 0.05706 valid_loss: 0.07946 test_loss: 0.08663 \n",
      "[428/500] train_loss: 0.05389 valid_loss: 0.07509 test_loss: 0.08693 \n",
      "[429/500] train_loss: 0.05303 valid_loss: 0.07478 test_loss: 0.08686 \n",
      "[430/500] train_loss: 0.05457 valid_loss: 0.08203 test_loss: 0.08555 \n",
      "[431/500] train_loss: 0.05545 valid_loss: 0.07737 test_loss: 0.08704 \n",
      "[432/500] train_loss: 0.05421 valid_loss: 0.07450 test_loss: 0.08701 \n",
      "[433/500] train_loss: 0.05410 valid_loss: 0.07379 test_loss: 0.08622 \n",
      "[434/500] train_loss: 0.05358 valid_loss: 0.08164 test_loss: 0.08637 \n",
      "[435/500] train_loss: 0.05533 valid_loss: 0.07375 test_loss: 0.08433 \n",
      "[436/500] train_loss: 0.05468 valid_loss: 0.08204 test_loss: 0.08548 \n",
      "[437/500] train_loss: 0.05322 valid_loss: 0.07992 test_loss: 0.08426 \n",
      "[438/500] train_loss: 0.05366 valid_loss: 0.07485 test_loss: 0.08489 \n",
      "[439/500] train_loss: 0.05314 valid_loss: 0.07487 test_loss: 0.08502 \n",
      "[440/500] train_loss: 0.05444 valid_loss: 0.07487 test_loss: 0.08532 \n",
      "[441/500] train_loss: 0.05478 valid_loss: 0.07683 test_loss: 0.09025 \n",
      "[442/500] train_loss: 0.05488 valid_loss: 0.07479 test_loss: 0.08671 \n",
      "[443/500] train_loss: 0.05235 valid_loss: 0.07495 test_loss: 0.08599 \n",
      "[444/500] train_loss: 0.05409 valid_loss: 0.07577 test_loss: 0.08516 \n",
      "[445/500] train_loss: 0.05348 valid_loss: 0.07472 test_loss: 0.08704 \n",
      "[446/500] train_loss: 0.05370 valid_loss: 0.07485 test_loss: 0.08497 \n",
      "[447/500] train_loss: 0.05248 valid_loss: 0.07678 test_loss: 0.08710 \n",
      "[448/500] train_loss: 0.05460 valid_loss: 0.07522 test_loss: 0.08575 \n",
      "[449/500] train_loss: 0.05475 valid_loss: 0.07789 test_loss: 0.08737 \n",
      "[450/500] train_loss: 0.05220 valid_loss: 0.07598 test_loss: 0.08694 \n",
      "[451/500] train_loss: 0.05303 valid_loss: 0.07620 test_loss: 0.08708 \n",
      "[452/500] train_loss: 0.05400 valid_loss: 0.07550 test_loss: 0.08753 \n",
      "[453/500] train_loss: 0.05294 valid_loss: 0.07579 test_loss: 0.08841 \n",
      "[454/500] train_loss: 0.05203 valid_loss: 0.07466 test_loss: 0.08696 \n",
      "[455/500] train_loss: 0.05263 valid_loss: 0.07495 test_loss: 0.08603 \n",
      "[456/500] train_loss: 0.05461 valid_loss: 0.07397 test_loss: 0.08607 \n",
      "[457/500] train_loss: 0.05184 valid_loss: 0.07331 test_loss: 0.08565 \n",
      "验证损失减少 (0.073383 --> 0.073309). 正在保存模型...\n",
      "[458/500] train_loss: 0.05338 valid_loss: 0.07492 test_loss: 0.08586 \n",
      "[459/500] train_loss: 0.05366 valid_loss: 0.07552 test_loss: 0.08528 \n",
      "[460/500] train_loss: 0.05364 valid_loss: 0.07631 test_loss: 0.08690 \n",
      "[461/500] train_loss: 0.05264 valid_loss: 0.07450 test_loss: 0.08450 \n",
      "[462/500] train_loss: 0.05458 valid_loss: 0.07806 test_loss: 0.08532 \n",
      "[463/500] train_loss: 0.05187 valid_loss: 0.07519 test_loss: 0.08734 \n",
      "[464/500] train_loss: 0.05387 valid_loss: 0.07526 test_loss: 0.08623 \n",
      "[465/500] train_loss: 0.05185 valid_loss: 0.07484 test_loss: 0.08510 \n",
      "[466/500] train_loss: 0.05232 valid_loss: 0.07649 test_loss: 0.08649 \n",
      "[467/500] train_loss: 0.05230 valid_loss: 0.07537 test_loss: 0.08637 \n",
      "[468/500] train_loss: 0.05265 valid_loss: 0.07546 test_loss: 0.08652 \n",
      "[469/500] train_loss: 0.05277 valid_loss: 0.07545 test_loss: 0.08612 \n",
      "[470/500] train_loss: 0.05363 valid_loss: 0.07613 test_loss: 0.08652 \n",
      "[471/500] train_loss: 0.05358 valid_loss: 0.07455 test_loss: 0.08679 \n",
      "[472/500] train_loss: 0.05200 valid_loss: 0.07826 test_loss: 0.08590 \n",
      "[473/500] train_loss: 0.05125 valid_loss: 0.08454 test_loss: 0.08847 \n",
      "[474/500] train_loss: 0.05129 valid_loss: 0.07498 test_loss: 0.08683 \n",
      "[475/500] train_loss: 0.05244 valid_loss: 0.07453 test_loss: 0.08665 \n",
      "[476/500] train_loss: 0.05279 valid_loss: 0.07439 test_loss: 0.08689 \n",
      "[477/500] train_loss: 0.05208 valid_loss: 0.07544 test_loss: 0.08629 \n",
      "[478/500] train_loss: 0.05219 valid_loss: 0.07544 test_loss: 0.08567 \n",
      "[479/500] train_loss: 0.05204 valid_loss: 0.07412 test_loss: 0.08546 \n",
      "[480/500] train_loss: 0.05345 valid_loss: 0.07763 test_loss: 0.08597 \n",
      "[481/500] train_loss: 0.05502 valid_loss: 0.07602 test_loss: 0.08536 \n",
      "[482/500] train_loss: 0.05246 valid_loss: 0.07479 test_loss: 0.08634 \n",
      "[483/500] train_loss: 0.05250 valid_loss: 0.07390 test_loss: 0.08450 \n",
      "[484/500] train_loss: 0.05252 valid_loss: 0.07575 test_loss: 0.08850 \n",
      "[485/500] train_loss: 0.05194 valid_loss: 0.07391 test_loss: 0.08754 \n",
      "[486/500] train_loss: 0.05222 valid_loss: 0.07482 test_loss: 0.08834 \n",
      "[487/500] train_loss: 0.05209 valid_loss: 0.07555 test_loss: 0.08715 \n",
      "[488/500] train_loss: 0.05160 valid_loss: 0.07456 test_loss: 0.08633 \n",
      "[489/500] train_loss: 0.05057 valid_loss: 0.07486 test_loss: 0.08587 \n",
      "[490/500] train_loss: 0.05196 valid_loss: 0.07568 test_loss: 0.08574 \n",
      "[491/500] train_loss: 0.05336 valid_loss: 0.07507 test_loss: 0.08578 \n",
      "[492/500] train_loss: 0.05213 valid_loss: 0.07551 test_loss: 0.08468 \n",
      "[493/500] train_loss: 0.05229 valid_loss: 0.07578 test_loss: 0.08797 \n",
      "[494/500] train_loss: 0.05284 valid_loss: 0.07469 test_loss: 0.08500 \n",
      "[495/500] train_loss: 0.05108 valid_loss: 0.07442 test_loss: 0.08502 \n",
      "[496/500] train_loss: 0.05238 valid_loss: 0.07530 test_loss: 0.08618 \n",
      "[497/500] train_loss: 0.05075 valid_loss: 0.08897 test_loss: 0.08657 \n",
      "[498/500] train_loss: 0.05027 valid_loss: 0.08252 test_loss: 0.08578 \n",
      "[499/500] train_loss: 0.05246 valid_loss: 0.07547 test_loss: 0.08721 \n",
      "[500/500] train_loss: 0.05178 valid_loss: 0.08261 test_loss: 0.08543 \n",
      "TRAINING MODEL 8\n",
      "[  1/500] train_loss: 0.44267 valid_loss: 0.30492 test_loss: 0.30864 \n",
      "验证损失减少 (inf --> 0.304917). 正在保存模型...\n",
      "[  2/500] train_loss: 0.24043 valid_loss: 0.21618 test_loss: 0.22402 \n",
      "验证损失减少 (0.304917 --> 0.216179). 正在保存模型...\n",
      "[  3/500] train_loss: 0.18706 valid_loss: 0.18147 test_loss: 0.19257 \n",
      "验证损失减少 (0.216179 --> 0.181472). 正在保存模型...\n",
      "[  4/500] train_loss: 0.16372 valid_loss: 0.16372 test_loss: 0.17477 \n",
      "验证损失减少 (0.181472 --> 0.163718). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15290 valid_loss: 0.15517 test_loss: 0.16787 \n",
      "验证损失减少 (0.163718 --> 0.155168). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14318 valid_loss: 0.14975 test_loss: 0.16220 \n",
      "验证损失减少 (0.155168 --> 0.149755). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13789 valid_loss: 0.14245 test_loss: 0.15326 \n",
      "验证损失减少 (0.149755 --> 0.142447). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13676 valid_loss: 0.13892 test_loss: 0.15045 \n",
      "验证损失减少 (0.142447 --> 0.138916). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13267 valid_loss: 0.13538 test_loss: 0.14751 \n",
      "验证损失减少 (0.138916 --> 0.135382). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12888 valid_loss: 0.13310 test_loss: 0.14545 \n",
      "验证损失减少 (0.135382 --> 0.133104). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12693 valid_loss: 0.12954 test_loss: 0.14409 \n",
      "验证损失减少 (0.133104 --> 0.129545). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12646 valid_loss: 0.12929 test_loss: 0.14209 \n",
      "验证损失减少 (0.129545 --> 0.129293). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12216 valid_loss: 0.12774 test_loss: 0.14182 \n",
      "验证损失减少 (0.129293 --> 0.127737). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14/500] train_loss: 0.11963 valid_loss: 0.13012 test_loss: 0.14398 \n",
      "[ 15/500] train_loss: 0.12054 valid_loss: 0.12426 test_loss: 0.13717 \n",
      "验证损失减少 (0.127737 --> 0.124260). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11813 valid_loss: 0.12505 test_loss: 0.13838 \n",
      "[ 17/500] train_loss: 0.11591 valid_loss: 0.12038 test_loss: 0.13390 \n",
      "验证损失减少 (0.124260 --> 0.120377). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11360 valid_loss: 0.11950 test_loss: 0.13239 \n",
      "验证损失减少 (0.120377 --> 0.119498). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11681 valid_loss: 0.11714 test_loss: 0.13288 \n",
      "验证损失减少 (0.119498 --> 0.117138). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11346 valid_loss: 0.11765 test_loss: 0.13226 \n",
      "[ 21/500] train_loss: 0.11019 valid_loss: 0.11800 test_loss: 0.13121 \n",
      "[ 22/500] train_loss: 0.11057 valid_loss: 0.11888 test_loss: 0.13159 \n",
      "[ 23/500] train_loss: 0.11048 valid_loss: 0.11619 test_loss: 0.13054 \n",
      "验证损失减少 (0.117138 --> 0.116189). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10896 valid_loss: 0.11464 test_loss: 0.12740 \n",
      "验证损失减少 (0.116189 --> 0.114645). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10853 valid_loss: 0.11362 test_loss: 0.12989 \n",
      "验证损失减少 (0.114645 --> 0.113624). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10665 valid_loss: 0.11744 test_loss: 0.12834 \n",
      "[ 27/500] train_loss: 0.10701 valid_loss: 0.11301 test_loss: 0.12702 \n",
      "验证损失减少 (0.113624 --> 0.113007). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10505 valid_loss: 0.11280 test_loss: 0.12618 \n",
      "验证损失减少 (0.113007 --> 0.112802). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10473 valid_loss: 0.10930 test_loss: 0.12419 \n",
      "验证损失减少 (0.112802 --> 0.109302). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10578 valid_loss: 0.11183 test_loss: 0.12551 \n",
      "[ 31/500] train_loss: 0.10465 valid_loss: 0.10929 test_loss: 0.12401 \n",
      "验证损失减少 (0.109302 --> 0.109291). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.10371 valid_loss: 0.10921 test_loss: 0.12321 \n",
      "验证损失减少 (0.109291 --> 0.109212). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10296 valid_loss: 0.11194 test_loss: 0.12409 \n",
      "[ 34/500] train_loss: 0.10059 valid_loss: 0.10880 test_loss: 0.12265 \n",
      "验证损失减少 (0.109212 --> 0.108798). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.10389 valid_loss: 0.10608 test_loss: 0.12077 \n",
      "验证损失减少 (0.108798 --> 0.106083). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.10338 valid_loss: 0.10943 test_loss: 0.12343 \n",
      "[ 37/500] train_loss: 0.10195 valid_loss: 0.10817 test_loss: 0.12067 \n",
      "[ 38/500] train_loss: 0.09871 valid_loss: 0.10599 test_loss: 0.12028 \n",
      "验证损失减少 (0.106083 --> 0.105989). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.10073 valid_loss: 0.10412 test_loss: 0.11897 \n",
      "验证损失减少 (0.105989 --> 0.104123). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.10035 valid_loss: 0.10472 test_loss: 0.11908 \n",
      "[ 41/500] train_loss: 0.09802 valid_loss: 0.10793 test_loss: 0.12067 \n",
      "[ 42/500] train_loss: 0.09778 valid_loss: 0.10551 test_loss: 0.11754 \n",
      "[ 43/500] train_loss: 0.09721 valid_loss: 0.10479 test_loss: 0.11930 \n",
      "[ 44/500] train_loss: 0.09704 valid_loss: 0.10908 test_loss: 0.11900 \n",
      "[ 45/500] train_loss: 0.09725 valid_loss: 0.10259 test_loss: 0.11633 \n",
      "验证损失减少 (0.104123 --> 0.102593). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.09648 valid_loss: 0.10355 test_loss: 0.11805 \n",
      "[ 47/500] train_loss: 0.09500 valid_loss: 0.10478 test_loss: 0.11532 \n",
      "[ 48/500] train_loss: 0.09542 valid_loss: 0.10295 test_loss: 0.11640 \n",
      "[ 49/500] train_loss: 0.09465 valid_loss: 0.10086 test_loss: 0.11571 \n",
      "验证损失减少 (0.102593 --> 0.100859). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.09291 valid_loss: 0.10078 test_loss: 0.11629 \n",
      "验证损失减少 (0.100859 --> 0.100781). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.09501 valid_loss: 0.10145 test_loss: 0.11273 \n",
      "[ 52/500] train_loss: 0.09436 valid_loss: 0.09928 test_loss: 0.11467 \n",
      "验证损失减少 (0.100781 --> 0.099282). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.09434 valid_loss: 0.09812 test_loss: 0.11378 \n",
      "验证损失减少 (0.099282 --> 0.098123). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.09255 valid_loss: 0.09889 test_loss: 0.11337 \n",
      "[ 55/500] train_loss: 0.09344 valid_loss: 0.09882 test_loss: 0.11095 \n",
      "[ 56/500] train_loss: 0.09144 valid_loss: 0.09743 test_loss: 0.11061 \n",
      "验证损失减少 (0.098123 --> 0.097435). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.09363 valid_loss: 0.09927 test_loss: 0.11131 \n",
      "[ 58/500] train_loss: 0.09249 valid_loss: 0.09697 test_loss: 0.10975 \n",
      "验证损失减少 (0.097435 --> 0.096970). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.09171 valid_loss: 0.10318 test_loss: 0.11338 \n",
      "[ 60/500] train_loss: 0.09081 valid_loss: 0.09945 test_loss: 0.11339 \n",
      "[ 61/500] train_loss: 0.09105 valid_loss: 0.09786 test_loss: 0.10947 \n",
      "[ 62/500] train_loss: 0.09064 valid_loss: 0.09693 test_loss: 0.11073 \n",
      "验证损失减少 (0.096970 --> 0.096929). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.08995 valid_loss: 0.09771 test_loss: 0.11013 \n",
      "[ 64/500] train_loss: 0.09134 valid_loss: 0.09487 test_loss: 0.10816 \n",
      "验证损失减少 (0.096929 --> 0.094868). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.09027 valid_loss: 0.09967 test_loss: 0.10957 \n",
      "[ 66/500] train_loss: 0.08764 valid_loss: 0.09638 test_loss: 0.10716 \n",
      "[ 67/500] train_loss: 0.09026 valid_loss: 0.09604 test_loss: 0.10691 \n",
      "[ 68/500] train_loss: 0.08791 valid_loss: 0.09450 test_loss: 0.11014 \n",
      "验证损失减少 (0.094868 --> 0.094497). 正在保存模型...\n",
      "[ 69/500] train_loss: 0.08826 valid_loss: 0.09498 test_loss: 0.10837 \n",
      "[ 70/500] train_loss: 0.08761 valid_loss: 0.09305 test_loss: 0.10621 \n",
      "验证损失减少 (0.094497 --> 0.093053). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.08791 valid_loss: 0.09583 test_loss: 0.10686 \n",
      "[ 72/500] train_loss: 0.08756 valid_loss: 0.09480 test_loss: 0.10700 \n",
      "[ 73/500] train_loss: 0.08831 valid_loss: 0.09295 test_loss: 0.10804 \n",
      "验证损失减少 (0.093053 --> 0.092948). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.08415 valid_loss: 0.09492 test_loss: 0.10593 \n",
      "[ 75/500] train_loss: 0.08750 valid_loss: 0.09311 test_loss: 0.10640 \n",
      "[ 76/500] train_loss: 0.08482 valid_loss: 0.09347 test_loss: 0.10487 \n",
      "[ 77/500] train_loss: 0.08411 valid_loss: 0.09356 test_loss: 0.10500 \n",
      "[ 78/500] train_loss: 0.08555 valid_loss: 0.09215 test_loss: 0.10501 \n",
      "验证损失减少 (0.092948 --> 0.092153). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08633 valid_loss: 0.09307 test_loss: 0.10456 \n",
      "[ 80/500] train_loss: 0.08237 valid_loss: 0.09225 test_loss: 0.10447 \n",
      "[ 81/500] train_loss: 0.08687 valid_loss: 0.09469 test_loss: 0.10612 \n",
      "[ 82/500] train_loss: 0.08667 valid_loss: 0.09273 test_loss: 0.10513 \n",
      "[ 83/500] train_loss: 0.08710 valid_loss: 0.09249 test_loss: 0.10435 \n",
      "[ 84/500] train_loss: 0.08285 valid_loss: 0.09153 test_loss: 0.10308 \n",
      "验证损失减少 (0.092153 --> 0.091529). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.08507 valid_loss: 0.09606 test_loss: 0.10553 \n",
      "[ 86/500] train_loss: 0.08619 valid_loss: 0.09349 test_loss: 0.10331 \n",
      "[ 87/500] train_loss: 0.08298 valid_loss: 0.09299 test_loss: 0.10319 \n",
      "[ 88/500] train_loss: 0.08194 valid_loss: 0.09160 test_loss: 0.10358 \n",
      "[ 89/500] train_loss: 0.08499 valid_loss: 0.09000 test_loss: 0.10313 \n",
      "验证损失减少 (0.091529 --> 0.089998). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.08303 valid_loss: 0.09095 test_loss: 0.10166 \n",
      "[ 91/500] train_loss: 0.08276 valid_loss: 0.08988 test_loss: 0.10228 \n",
      "验证损失减少 (0.089998 --> 0.089882). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.08154 valid_loss: 0.09528 test_loss: 0.10357 \n",
      "[ 93/500] train_loss: 0.08231 valid_loss: 0.09591 test_loss: 0.10197 \n",
      "[ 94/500] train_loss: 0.08055 valid_loss: 0.08942 test_loss: 0.10117 \n",
      "验证损失减少 (0.089882 --> 0.089425). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.07996 valid_loss: 0.08923 test_loss: 0.10121 \n",
      "验证损失减少 (0.089425 --> 0.089227). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.08460 valid_loss: 0.08980 test_loss: 0.10269 \n",
      "[ 97/500] train_loss: 0.08156 valid_loss: 0.09025 test_loss: 0.10062 \n",
      "[ 98/500] train_loss: 0.08096 valid_loss: 0.09203 test_loss: 0.10041 \n",
      "[ 99/500] train_loss: 0.07984 valid_loss: 0.08933 test_loss: 0.10170 \n",
      "[100/500] train_loss: 0.08238 valid_loss: 0.08727 test_loss: 0.09971 \n",
      "验证损失减少 (0.089227 --> 0.087271). 正在保存模型...\n",
      "[101/500] train_loss: 0.08098 valid_loss: 0.09148 test_loss: 0.10144 \n",
      "[102/500] train_loss: 0.08199 valid_loss: 0.08825 test_loss: 0.10000 \n",
      "[103/500] train_loss: 0.08167 valid_loss: 0.09075 test_loss: 0.09982 \n",
      "[104/500] train_loss: 0.07666 valid_loss: 0.08700 test_loss: 0.09921 \n",
      "验证损失减少 (0.087271 --> 0.087000). 正在保存模型...\n",
      "[105/500] train_loss: 0.07723 valid_loss: 0.09362 test_loss: 0.10241 \n",
      "[106/500] train_loss: 0.08110 valid_loss: 0.08808 test_loss: 0.09923 \n",
      "[107/500] train_loss: 0.07890 valid_loss: 0.09227 test_loss: 0.09925 \n",
      "[108/500] train_loss: 0.07809 valid_loss: 0.08763 test_loss: 0.09817 \n",
      "[109/500] train_loss: 0.07923 valid_loss: 0.09086 test_loss: 0.09915 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110/500] train_loss: 0.08127 valid_loss: 0.09045 test_loss: 0.09853 \n",
      "[111/500] train_loss: 0.07796 valid_loss: 0.08592 test_loss: 0.09845 \n",
      "验证损失减少 (0.087000 --> 0.085917). 正在保存模型...\n",
      "[112/500] train_loss: 0.07915 valid_loss: 0.08947 test_loss: 0.10057 \n",
      "[113/500] train_loss: 0.07693 valid_loss: 0.08788 test_loss: 0.09815 \n",
      "[114/500] train_loss: 0.07732 valid_loss: 0.08837 test_loss: 0.09849 \n",
      "[115/500] train_loss: 0.07850 valid_loss: 0.08961 test_loss: 0.09891 \n",
      "[116/500] train_loss: 0.07749 valid_loss: 0.08666 test_loss: 0.09795 \n",
      "[117/500] train_loss: 0.07518 valid_loss: 0.08853 test_loss: 0.09855 \n",
      "[118/500] train_loss: 0.07720 valid_loss: 0.08803 test_loss: 0.09888 \n",
      "[119/500] train_loss: 0.07857 valid_loss: 0.08754 test_loss: 0.10011 \n",
      "[120/500] train_loss: 0.07806 valid_loss: 0.09281 test_loss: 0.09703 \n",
      "[121/500] train_loss: 0.07643 valid_loss: 0.08534 test_loss: 0.09576 \n",
      "验证损失减少 (0.085917 --> 0.085342). 正在保存模型...\n",
      "[122/500] train_loss: 0.07607 valid_loss: 0.09031 test_loss: 0.09813 \n",
      "[123/500] train_loss: 0.07539 valid_loss: 0.09005 test_loss: 0.09883 \n",
      "[124/500] train_loss: 0.07824 valid_loss: 0.08599 test_loss: 0.09623 \n",
      "[125/500] train_loss: 0.07561 valid_loss: 0.08787 test_loss: 0.09868 \n",
      "[126/500] train_loss: 0.07806 valid_loss: 0.09289 test_loss: 0.09752 \n",
      "[127/500] train_loss: 0.07843 valid_loss: 0.08680 test_loss: 0.09919 \n",
      "[128/500] train_loss: 0.07558 valid_loss: 0.08538 test_loss: 0.09538 \n",
      "[129/500] train_loss: 0.07509 valid_loss: 0.08548 test_loss: 0.09850 \n",
      "[130/500] train_loss: 0.07468 valid_loss: 0.08800 test_loss: 0.09693 \n",
      "[131/500] train_loss: 0.07438 valid_loss: 0.08414 test_loss: 0.09513 \n",
      "验证损失减少 (0.085342 --> 0.084137). 正在保存模型...\n",
      "[132/500] train_loss: 0.07506 valid_loss: 0.08675 test_loss: 0.09706 \n",
      "[133/500] train_loss: 0.07497 valid_loss: 0.08474 test_loss: 0.09730 \n",
      "[134/500] train_loss: 0.07420 valid_loss: 0.08884 test_loss: 0.09561 \n",
      "[135/500] train_loss: 0.07552 valid_loss: 0.08554 test_loss: 0.09686 \n",
      "[136/500] train_loss: 0.07587 valid_loss: 0.08469 test_loss: 0.09569 \n",
      "[137/500] train_loss: 0.07463 valid_loss: 0.08500 test_loss: 0.09729 \n",
      "[138/500] train_loss: 0.07447 valid_loss: 0.08444 test_loss: 0.09494 \n",
      "[139/500] train_loss: 0.07408 valid_loss: 0.08430 test_loss: 0.09445 \n",
      "[140/500] train_loss: 0.07363 valid_loss: 0.09094 test_loss: 0.09698 \n",
      "[141/500] train_loss: 0.07558 valid_loss: 0.08743 test_loss: 0.09483 \n",
      "[142/500] train_loss: 0.07246 valid_loss: 0.08643 test_loss: 0.09395 \n",
      "[143/500] train_loss: 0.07335 valid_loss: 0.08473 test_loss: 0.09388 \n",
      "[144/500] train_loss: 0.07351 valid_loss: 0.08739 test_loss: 0.09500 \n",
      "[145/500] train_loss: 0.07415 valid_loss: 0.08389 test_loss: 0.09495 \n",
      "验证损失减少 (0.084137 --> 0.083886). 正在保存模型...\n",
      "[146/500] train_loss: 0.07212 valid_loss: 0.08536 test_loss: 0.09550 \n",
      "[147/500] train_loss: 0.07319 valid_loss: 0.08676 test_loss: 0.09382 \n",
      "[148/500] train_loss: 0.07294 valid_loss: 0.09220 test_loss: 0.09389 \n",
      "[149/500] train_loss: 0.07179 valid_loss: 0.08787 test_loss: 0.09437 \n",
      "[150/500] train_loss: 0.07318 valid_loss: 0.08554 test_loss: 0.09602 \n",
      "[151/500] train_loss: 0.07216 valid_loss: 0.08589 test_loss: 0.09543 \n",
      "[152/500] train_loss: 0.07423 valid_loss: 0.08432 test_loss: 0.09572 \n",
      "[153/500] train_loss: 0.07308 valid_loss: 0.08313 test_loss: 0.09391 \n",
      "验证损失减少 (0.083886 --> 0.083125). 正在保存模型...\n",
      "[154/500] train_loss: 0.07407 valid_loss: 0.08368 test_loss: 0.09291 \n",
      "[155/500] train_loss: 0.07103 valid_loss: 0.08437 test_loss: 0.09438 \n",
      "[156/500] train_loss: 0.07118 valid_loss: 0.08367 test_loss: 0.09394 \n",
      "[157/500] train_loss: 0.07045 valid_loss: 0.08215 test_loss: 0.09330 \n",
      "验证损失减少 (0.083125 --> 0.082145). 正在保存模型...\n",
      "[158/500] train_loss: 0.07367 valid_loss: 0.08426 test_loss: 0.09369 \n",
      "[159/500] train_loss: 0.07245 valid_loss: 0.08501 test_loss: 0.09499 \n",
      "[160/500] train_loss: 0.07185 valid_loss: 0.08542 test_loss: 0.09530 \n",
      "[161/500] train_loss: 0.07152 valid_loss: 0.08200 test_loss: 0.09404 \n",
      "验证损失减少 (0.082145 --> 0.081995). 正在保存模型...\n",
      "[162/500] train_loss: 0.07248 valid_loss: 0.08284 test_loss: 0.09410 \n",
      "[163/500] train_loss: 0.06831 valid_loss: 0.08519 test_loss: 0.09604 \n",
      "[164/500] train_loss: 0.07027 valid_loss: 0.08503 test_loss: 0.09455 \n",
      "[165/500] train_loss: 0.06811 valid_loss: 0.08080 test_loss: 0.09418 \n",
      "验证损失减少 (0.081995 --> 0.080796). 正在保存模型...\n",
      "[166/500] train_loss: 0.07195 valid_loss: 0.08162 test_loss: 0.09354 \n",
      "[167/500] train_loss: 0.07111 valid_loss: 0.08373 test_loss: 0.09507 \n",
      "[168/500] train_loss: 0.07201 valid_loss: 0.08531 test_loss: 0.09305 \n",
      "[169/500] train_loss: 0.06867 valid_loss: 0.08341 test_loss: 0.09411 \n",
      "[170/500] train_loss: 0.07041 valid_loss: 0.07974 test_loss: 0.09443 \n",
      "验证损失减少 (0.080796 --> 0.079739). 正在保存模型...\n",
      "[171/500] train_loss: 0.07033 valid_loss: 0.08128 test_loss: 0.09487 \n",
      "[172/500] train_loss: 0.06922 valid_loss: 0.08139 test_loss: 0.09461 \n",
      "[173/500] train_loss: 0.07006 valid_loss: 0.08389 test_loss: 0.09111 \n",
      "[174/500] train_loss: 0.06822 valid_loss: 0.08345 test_loss: 0.09172 \n",
      "[175/500] train_loss: 0.06999 valid_loss: 0.08597 test_loss: 0.09723 \n",
      "[176/500] train_loss: 0.07185 valid_loss: 0.08379 test_loss: 0.09176 \n",
      "[177/500] train_loss: 0.07268 valid_loss: 0.08189 test_loss: 0.09332 \n",
      "[178/500] train_loss: 0.07025 valid_loss: 0.08100 test_loss: 0.09332 \n",
      "[179/500] train_loss: 0.06954 valid_loss: 0.08354 test_loss: 0.09188 \n",
      "[180/500] train_loss: 0.06901 valid_loss: 0.08170 test_loss: 0.09252 \n",
      "[181/500] train_loss: 0.07023 valid_loss: 0.08135 test_loss: 0.09286 \n",
      "[182/500] train_loss: 0.06901 valid_loss: 0.08111 test_loss: 0.09271 \n",
      "[183/500] train_loss: 0.06933 valid_loss: 0.08316 test_loss: 0.09210 \n",
      "[184/500] train_loss: 0.06700 valid_loss: 0.08247 test_loss: 0.09127 \n",
      "[185/500] train_loss: 0.06862 valid_loss: 0.08008 test_loss: 0.09380 \n",
      "[186/500] train_loss: 0.06835 valid_loss: 0.08337 test_loss: 0.09171 \n",
      "[187/500] train_loss: 0.06794 valid_loss: 0.08182 test_loss: 0.09390 \n",
      "[188/500] train_loss: 0.06925 valid_loss: 0.08830 test_loss: 0.09333 \n",
      "[189/500] train_loss: 0.06927 valid_loss: 0.08561 test_loss: 0.09264 \n",
      "[190/500] train_loss: 0.06787 valid_loss: 0.08419 test_loss: 0.09017 \n",
      "[191/500] train_loss: 0.06808 valid_loss: 0.08876 test_loss: 0.09282 \n",
      "[192/500] train_loss: 0.06655 valid_loss: 0.07840 test_loss: 0.09091 \n",
      "验证损失减少 (0.079739 --> 0.078400). 正在保存模型...\n",
      "[193/500] train_loss: 0.06937 valid_loss: 0.08433 test_loss: 0.09254 \n",
      "[194/500] train_loss: 0.06671 valid_loss: 0.08468 test_loss: 0.09200 \n",
      "[195/500] train_loss: 0.06783 valid_loss: 0.08231 test_loss: 0.09190 \n",
      "[196/500] train_loss: 0.06870 valid_loss: 0.08584 test_loss: 0.09007 \n",
      "[197/500] train_loss: 0.06850 valid_loss: 0.08250 test_loss: 0.09240 \n",
      "[198/500] train_loss: 0.06840 valid_loss: 0.08304 test_loss: 0.09120 \n",
      "[199/500] train_loss: 0.06748 valid_loss: 0.08297 test_loss: 0.09018 \n",
      "[200/500] train_loss: 0.06714 valid_loss: 0.08601 test_loss: 0.09178 \n",
      "[201/500] train_loss: 0.06883 valid_loss: 0.08299 test_loss: 0.09306 \n",
      "[202/500] train_loss: 0.06705 valid_loss: 0.08015 test_loss: 0.09006 \n",
      "[203/500] train_loss: 0.06660 valid_loss: 0.08087 test_loss: 0.09024 \n",
      "[204/500] train_loss: 0.06710 valid_loss: 0.08743 test_loss: 0.09138 \n",
      "[205/500] train_loss: 0.06697 valid_loss: 0.08216 test_loss: 0.09163 \n",
      "[206/500] train_loss: 0.06559 valid_loss: 0.08794 test_loss: 0.09390 \n",
      "[207/500] train_loss: 0.06735 valid_loss: 0.08254 test_loss: 0.09110 \n",
      "[208/500] train_loss: 0.06645 valid_loss: 0.08119 test_loss: 0.09008 \n",
      "[209/500] train_loss: 0.06807 valid_loss: 0.07991 test_loss: 0.09097 \n",
      "[210/500] train_loss: 0.06641 valid_loss: 0.07904 test_loss: 0.08992 \n",
      "[211/500] train_loss: 0.06572 valid_loss: 0.07996 test_loss: 0.09030 \n",
      "[212/500] train_loss: 0.06635 valid_loss: 0.07881 test_loss: 0.09028 \n",
      "[213/500] train_loss: 0.06475 valid_loss: 0.08005 test_loss: 0.08907 \n",
      "[214/500] train_loss: 0.06615 valid_loss: 0.08149 test_loss: 0.08948 \n",
      "[215/500] train_loss: 0.06536 valid_loss: 0.08071 test_loss: 0.08973 \n",
      "[216/500] train_loss: 0.06642 valid_loss: 0.08070 test_loss: 0.09062 \n",
      "[217/500] train_loss: 0.06534 valid_loss: 0.08323 test_loss: 0.09005 \n",
      "[218/500] train_loss: 0.06517 valid_loss: 0.08048 test_loss: 0.08958 \n",
      "[219/500] train_loss: 0.06647 valid_loss: 0.07986 test_loss: 0.08984 \n",
      "[220/500] train_loss: 0.06434 valid_loss: 0.08077 test_loss: 0.09137 \n",
      "[221/500] train_loss: 0.06573 valid_loss: 0.07946 test_loss: 0.09173 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[222/500] train_loss: 0.06656 valid_loss: 0.08135 test_loss: 0.09003 \n",
      "[223/500] train_loss: 0.06660 valid_loss: 0.07835 test_loss: 0.09078 \n",
      "验证损失减少 (0.078400 --> 0.078353). 正在保存模型...\n",
      "[224/500] train_loss: 0.06537 valid_loss: 0.08212 test_loss: 0.08934 \n",
      "[225/500] train_loss: 0.06458 valid_loss: 0.07882 test_loss: 0.08995 \n",
      "[226/500] train_loss: 0.06591 valid_loss: 0.07892 test_loss: 0.08951 \n",
      "[227/500] train_loss: 0.06341 valid_loss: 0.08568 test_loss: 0.08996 \n",
      "[228/500] train_loss: 0.06482 valid_loss: 0.07908 test_loss: 0.08963 \n",
      "[229/500] train_loss: 0.06503 valid_loss: 0.08000 test_loss: 0.08995 \n",
      "[230/500] train_loss: 0.06388 valid_loss: 0.07809 test_loss: 0.09064 \n",
      "验证损失减少 (0.078353 --> 0.078089). 正在保存模型...\n",
      "[231/500] train_loss: 0.06405 valid_loss: 0.08919 test_loss: 0.09046 \n",
      "[232/500] train_loss: 0.06427 valid_loss: 0.08884 test_loss: 0.09004 \n",
      "[233/500] train_loss: 0.06726 valid_loss: 0.08916 test_loss: 0.08900 \n",
      "[234/500] train_loss: 0.06344 valid_loss: 0.09301 test_loss: 0.08984 \n",
      "[235/500] train_loss: 0.06452 valid_loss: 0.08823 test_loss: 0.08960 \n",
      "[236/500] train_loss: 0.06502 valid_loss: 0.08007 test_loss: 0.08973 \n",
      "[237/500] train_loss: 0.06659 valid_loss: 0.07794 test_loss: 0.08910 \n",
      "验证损失减少 (0.078089 --> 0.077944). 正在保存模型...\n",
      "[238/500] train_loss: 0.06407 valid_loss: 0.08284 test_loss: 0.08886 \n",
      "[239/500] train_loss: 0.06586 valid_loss: 0.08866 test_loss: 0.08791 \n",
      "[240/500] train_loss: 0.06459 valid_loss: 0.08616 test_loss: 0.08925 \n",
      "[241/500] train_loss: 0.06428 valid_loss: 0.08947 test_loss: 0.09011 \n",
      "[242/500] train_loss: 0.06503 valid_loss: 0.08876 test_loss: 0.08843 \n",
      "[243/500] train_loss: 0.06504 valid_loss: 0.08576 test_loss: 0.08951 \n",
      "[244/500] train_loss: 0.06434 valid_loss: 0.08715 test_loss: 0.08959 \n",
      "[245/500] train_loss: 0.06307 valid_loss: 0.08444 test_loss: 0.08853 \n",
      "[246/500] train_loss: 0.06410 valid_loss: 0.08354 test_loss: 0.08921 \n",
      "[247/500] train_loss: 0.06292 valid_loss: 0.08605 test_loss: 0.08985 \n",
      "[248/500] train_loss: 0.06185 valid_loss: 0.08572 test_loss: 0.09017 \n",
      "[249/500] train_loss: 0.06409 valid_loss: 0.08154 test_loss: 0.09058 \n",
      "[250/500] train_loss: 0.06461 valid_loss: 0.08132 test_loss: 0.08783 \n",
      "[251/500] train_loss: 0.06282 valid_loss: 0.07666 test_loss: 0.08830 \n",
      "验证损失减少 (0.077944 --> 0.076657). 正在保存模型...\n",
      "[252/500] train_loss: 0.06276 valid_loss: 0.07879 test_loss: 0.09132 \n",
      "[253/500] train_loss: 0.06270 valid_loss: 0.07827 test_loss: 0.09112 \n",
      "[254/500] train_loss: 0.06429 valid_loss: 0.07720 test_loss: 0.08848 \n",
      "[255/500] train_loss: 0.06361 valid_loss: 0.07932 test_loss: 0.08962 \n",
      "[256/500] train_loss: 0.06299 valid_loss: 0.08189 test_loss: 0.08828 \n",
      "[257/500] train_loss: 0.06402 valid_loss: 0.08192 test_loss: 0.08955 \n",
      "[258/500] train_loss: 0.06325 valid_loss: 0.07741 test_loss: 0.08844 \n",
      "[259/500] train_loss: 0.06471 valid_loss: 0.07734 test_loss: 0.08858 \n",
      "[260/500] train_loss: 0.06242 valid_loss: 0.07607 test_loss: 0.08884 \n",
      "验证损失减少 (0.076657 --> 0.076071). 正在保存模型...\n",
      "[261/500] train_loss: 0.06222 valid_loss: 0.07695 test_loss: 0.08774 \n",
      "[262/500] train_loss: 0.06096 valid_loss: 0.07763 test_loss: 0.08831 \n",
      "[263/500] train_loss: 0.06274 valid_loss: 0.07655 test_loss: 0.08805 \n",
      "[264/500] train_loss: 0.06157 valid_loss: 0.07908 test_loss: 0.08789 \n",
      "[265/500] train_loss: 0.06264 valid_loss: 0.07752 test_loss: 0.08950 \n",
      "[266/500] train_loss: 0.06105 valid_loss: 0.07713 test_loss: 0.08696 \n",
      "[267/500] train_loss: 0.06058 valid_loss: 0.07869 test_loss: 0.08857 \n",
      "[268/500] train_loss: 0.06223 valid_loss: 0.08540 test_loss: 0.08851 \n",
      "[269/500] train_loss: 0.06113 valid_loss: 0.07746 test_loss: 0.08973 \n",
      "[270/500] train_loss: 0.06074 valid_loss: 0.07737 test_loss: 0.09011 \n",
      "[271/500] train_loss: 0.06216 valid_loss: 0.07943 test_loss: 0.08871 \n",
      "[272/500] train_loss: 0.06255 valid_loss: 0.07611 test_loss: 0.08746 \n",
      "[273/500] train_loss: 0.06235 valid_loss: 0.07624 test_loss: 0.08762 \n",
      "[274/500] train_loss: 0.06283 valid_loss: 0.07730 test_loss: 0.08805 \n",
      "[275/500] train_loss: 0.06435 valid_loss: 0.07755 test_loss: 0.08922 \n",
      "[276/500] train_loss: 0.06181 valid_loss: 0.07852 test_loss: 0.08760 \n",
      "[277/500] train_loss: 0.06076 valid_loss: 0.07649 test_loss: 0.08865 \n",
      "[278/500] train_loss: 0.06498 valid_loss: 0.07801 test_loss: 0.08766 \n",
      "[279/500] train_loss: 0.06204 valid_loss: 0.07820 test_loss: 0.08864 \n",
      "[280/500] train_loss: 0.06289 valid_loss: 0.07805 test_loss: 0.08871 \n",
      "[281/500] train_loss: 0.06215 valid_loss: 0.07848 test_loss: 0.08770 \n",
      "[282/500] train_loss: 0.06126 valid_loss: 0.08018 test_loss: 0.08814 \n",
      "[283/500] train_loss: 0.06144 valid_loss: 0.07971 test_loss: 0.08766 \n",
      "[284/500] train_loss: 0.06234 valid_loss: 0.07642 test_loss: 0.08662 \n",
      "[285/500] train_loss: 0.06160 valid_loss: 0.07946 test_loss: 0.08795 \n",
      "[286/500] train_loss: 0.06225 valid_loss: 0.07701 test_loss: 0.08706 \n",
      "[287/500] train_loss: 0.06266 valid_loss: 0.07792 test_loss: 0.08775 \n",
      "[288/500] train_loss: 0.05896 valid_loss: 0.07932 test_loss: 0.08959 \n",
      "[289/500] train_loss: 0.06198 valid_loss: 0.07970 test_loss: 0.08724 \n",
      "[290/500] train_loss: 0.06113 valid_loss: 0.08134 test_loss: 0.08750 \n",
      "[291/500] train_loss: 0.06147 valid_loss: 0.08077 test_loss: 0.08763 \n",
      "[292/500] train_loss: 0.06087 valid_loss: 0.08220 test_loss: 0.08872 \n",
      "[293/500] train_loss: 0.06184 valid_loss: 0.07639 test_loss: 0.08823 \n",
      "[294/500] train_loss: 0.06177 valid_loss: 0.08504 test_loss: 0.08791 \n",
      "[295/500] train_loss: 0.05960 valid_loss: 0.07996 test_loss: 0.08677 \n",
      "[296/500] train_loss: 0.06028 valid_loss: 0.07629 test_loss: 0.08770 \n",
      "[297/500] train_loss: 0.05920 valid_loss: 0.07946 test_loss: 0.08878 \n",
      "[298/500] train_loss: 0.06021 valid_loss: 0.08131 test_loss: 0.08806 \n",
      "[299/500] train_loss: 0.05930 valid_loss: 0.08672 test_loss: 0.08722 \n",
      "[300/500] train_loss: 0.05991 valid_loss: 0.07591 test_loss: 0.08767 \n",
      "验证损失减少 (0.076071 --> 0.075906). 正在保存模型...\n",
      "[301/500] train_loss: 0.06119 valid_loss: 0.08614 test_loss: 0.08860 \n",
      "[302/500] train_loss: 0.06089 valid_loss: 0.08495 test_loss: 0.08682 \n",
      "[303/500] train_loss: 0.05965 valid_loss: 0.09086 test_loss: 0.08808 \n",
      "[304/500] train_loss: 0.06056 valid_loss: 0.08733 test_loss: 0.08664 \n",
      "[305/500] train_loss: 0.06031 valid_loss: 0.08511 test_loss: 0.08722 \n",
      "[306/500] train_loss: 0.05890 valid_loss: 0.08484 test_loss: 0.08691 \n",
      "[307/500] train_loss: 0.06032 valid_loss: 0.08848 test_loss: 0.08722 \n",
      "[308/500] train_loss: 0.05875 valid_loss: 0.10130 test_loss: 0.08622 \n",
      "[309/500] train_loss: 0.05993 valid_loss: 0.09213 test_loss: 0.08688 \n",
      "[310/500] train_loss: 0.05969 valid_loss: 0.08224 test_loss: 0.08820 \n",
      "[311/500] train_loss: 0.06051 valid_loss: 0.07984 test_loss: 0.08696 \n",
      "[312/500] train_loss: 0.05964 valid_loss: 0.08214 test_loss: 0.08735 \n",
      "[313/500] train_loss: 0.06022 valid_loss: 0.08966 test_loss: 0.08813 \n",
      "[314/500] train_loss: 0.05809 valid_loss: 0.09402 test_loss: 0.08730 \n",
      "[315/500] train_loss: 0.06100 valid_loss: 0.08740 test_loss: 0.08711 \n",
      "[316/500] train_loss: 0.06016 valid_loss: 0.07701 test_loss: 0.08899 \n",
      "[317/500] train_loss: 0.05972 valid_loss: 0.08698 test_loss: 0.08598 \n",
      "[318/500] train_loss: 0.05965 valid_loss: 0.08032 test_loss: 0.08748 \n",
      "[319/500] train_loss: 0.05915 valid_loss: 0.08956 test_loss: 0.08837 \n",
      "[320/500] train_loss: 0.05881 valid_loss: 0.08798 test_loss: 0.08803 \n",
      "[321/500] train_loss: 0.05953 valid_loss: 0.07959 test_loss: 0.08835 \n",
      "[322/500] train_loss: 0.05954 valid_loss: 0.08400 test_loss: 0.08826 \n",
      "[323/500] train_loss: 0.05824 valid_loss: 0.08078 test_loss: 0.08679 \n",
      "[324/500] train_loss: 0.05898 valid_loss: 0.08411 test_loss: 0.08710 \n",
      "[325/500] train_loss: 0.06027 valid_loss: 0.08399 test_loss: 0.08817 \n",
      "[326/500] train_loss: 0.05698 valid_loss: 0.08221 test_loss: 0.08902 \n",
      "[327/500] train_loss: 0.05816 valid_loss: 0.09047 test_loss: 0.08785 \n",
      "[328/500] train_loss: 0.05888 valid_loss: 0.09059 test_loss: 0.08664 \n",
      "[329/500] train_loss: 0.05974 valid_loss: 0.08600 test_loss: 0.08627 \n",
      "[330/500] train_loss: 0.05752 valid_loss: 0.08082 test_loss: 0.08693 \n",
      "[331/500] train_loss: 0.05937 valid_loss: 0.08335 test_loss: 0.08873 \n",
      "[332/500] train_loss: 0.05757 valid_loss: 0.08033 test_loss: 0.08829 \n",
      "[333/500] train_loss: 0.05844 valid_loss: 0.09507 test_loss: 0.08786 \n",
      "[334/500] train_loss: 0.05914 valid_loss: 0.08852 test_loss: 0.08715 \n",
      "[335/500] train_loss: 0.05943 valid_loss: 0.08846 test_loss: 0.08773 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[336/500] train_loss: 0.05787 valid_loss: 0.08277 test_loss: 0.08708 \n",
      "[337/500] train_loss: 0.05714 valid_loss: 0.09769 test_loss: 0.08890 \n",
      "[338/500] train_loss: 0.05786 valid_loss: 0.08539 test_loss: 0.08761 \n",
      "[339/500] train_loss: 0.05889 valid_loss: 0.08020 test_loss: 0.08696 \n",
      "[340/500] train_loss: 0.05782 valid_loss: 0.09132 test_loss: 0.08713 \n",
      "[341/500] train_loss: 0.05650 valid_loss: 0.08276 test_loss: 0.08653 \n",
      "[342/500] train_loss: 0.06018 valid_loss: 0.08023 test_loss: 0.08674 \n",
      "[343/500] train_loss: 0.05997 valid_loss: 0.07961 test_loss: 0.08768 \n",
      "[344/500] train_loss: 0.05767 valid_loss: 0.08059 test_loss: 0.08784 \n",
      "[345/500] train_loss: 0.06016 valid_loss: 0.09232 test_loss: 0.08991 \n",
      "[346/500] train_loss: 0.05960 valid_loss: 0.08401 test_loss: 0.08727 \n",
      "[347/500] train_loss: 0.05676 valid_loss: 0.08343 test_loss: 0.08801 \n",
      "[348/500] train_loss: 0.05673 valid_loss: 0.08283 test_loss: 0.08673 \n",
      "[349/500] train_loss: 0.05848 valid_loss: 0.08155 test_loss: 0.08607 \n",
      "[350/500] train_loss: 0.05626 valid_loss: 0.08246 test_loss: 0.08678 \n",
      "[351/500] train_loss: 0.05588 valid_loss: 0.08018 test_loss: 0.08790 \n",
      "[352/500] train_loss: 0.05717 valid_loss: 0.08340 test_loss: 0.08768 \n",
      "[353/500] train_loss: 0.05581 valid_loss: 0.07853 test_loss: 0.08775 \n",
      "[354/500] train_loss: 0.05504 valid_loss: 0.07871 test_loss: 0.08786 \n",
      "[355/500] train_loss: 0.05717 valid_loss: 0.08456 test_loss: 0.08776 \n",
      "[356/500] train_loss: 0.05823 valid_loss: 0.07594 test_loss: 0.08766 \n",
      "[357/500] train_loss: 0.05862 valid_loss: 0.07403 test_loss: 0.08603 \n",
      "验证损失减少 (0.075906 --> 0.074033). 正在保存模型...\n",
      "[358/500] train_loss: 0.05843 valid_loss: 0.07659 test_loss: 0.08660 \n",
      "[359/500] train_loss: 0.05696 valid_loss: 0.08493 test_loss: 0.08858 \n",
      "[360/500] train_loss: 0.05606 valid_loss: 0.07918 test_loss: 0.08734 \n",
      "[361/500] train_loss: 0.05628 valid_loss: 0.08319 test_loss: 0.08770 \n",
      "[362/500] train_loss: 0.05563 valid_loss: 0.07735 test_loss: 0.08735 \n",
      "[363/500] train_loss: 0.05625 valid_loss: 0.08015 test_loss: 0.08708 \n",
      "[364/500] train_loss: 0.05467 valid_loss: 0.07986 test_loss: 0.08743 \n",
      "[365/500] train_loss: 0.05520 valid_loss: 0.08001 test_loss: 0.08966 \n",
      "[366/500] train_loss: 0.05745 valid_loss: 0.08670 test_loss: 0.08599 \n",
      "[367/500] train_loss: 0.05701 valid_loss: 0.08276 test_loss: 0.08633 \n",
      "[368/500] train_loss: 0.05770 valid_loss: 0.07735 test_loss: 0.08751 \n",
      "[369/500] train_loss: 0.05641 valid_loss: 0.08775 test_loss: 0.08691 \n",
      "[370/500] train_loss: 0.05687 valid_loss: 0.08668 test_loss: 0.08799 \n",
      "[371/500] train_loss: 0.05644 valid_loss: 0.08930 test_loss: 0.08636 \n",
      "[372/500] train_loss: 0.05542 valid_loss: 0.08456 test_loss: 0.08897 \n",
      "[373/500] train_loss: 0.05569 valid_loss: 0.07783 test_loss: 0.08610 \n",
      "[374/500] train_loss: 0.05548 valid_loss: 0.08092 test_loss: 0.08666 \n",
      "[375/500] train_loss: 0.05735 valid_loss: 0.07880 test_loss: 0.08779 \n",
      "[376/500] train_loss: 0.05600 valid_loss: 0.08318 test_loss: 0.08614 \n",
      "[377/500] train_loss: 0.05527 valid_loss: 0.07939 test_loss: 0.08805 \n",
      "[378/500] train_loss: 0.05547 valid_loss: 0.08088 test_loss: 0.08689 \n",
      "[379/500] train_loss: 0.05553 valid_loss: 0.08316 test_loss: 0.08666 \n",
      "[380/500] train_loss: 0.05562 valid_loss: 0.08051 test_loss: 0.08498 \n",
      "[381/500] train_loss: 0.05714 valid_loss: 0.09655 test_loss: 0.08560 \n",
      "[382/500] train_loss: 0.05646 valid_loss: 0.09793 test_loss: 0.08712 \n",
      "[383/500] train_loss: 0.05645 valid_loss: 0.07932 test_loss: 0.08682 \n",
      "[384/500] train_loss: 0.05586 valid_loss: 0.09286 test_loss: 0.08778 \n",
      "[385/500] train_loss: 0.05578 valid_loss: 0.09243 test_loss: 0.08693 \n",
      "[386/500] train_loss: 0.05643 valid_loss: 0.08556 test_loss: 0.08669 \n",
      "[387/500] train_loss: 0.05788 valid_loss: 0.07820 test_loss: 0.08628 \n",
      "[388/500] train_loss: 0.05501 valid_loss: 0.08547 test_loss: 0.08962 \n",
      "[389/500] train_loss: 0.05548 valid_loss: 0.08236 test_loss: 0.08824 \n",
      "[390/500] train_loss: 0.05570 valid_loss: 0.09427 test_loss: 0.08725 \n",
      "[391/500] train_loss: 0.05762 valid_loss: 0.09048 test_loss: 0.08612 \n",
      "[392/500] train_loss: 0.05522 valid_loss: 0.09887 test_loss: 0.08680 \n",
      "[393/500] train_loss: 0.05566 valid_loss: 0.07571 test_loss: 0.08710 \n",
      "[394/500] train_loss: 0.05606 valid_loss: 0.08180 test_loss: 0.08669 \n",
      "[395/500] train_loss: 0.05614 valid_loss: 0.09413 test_loss: 0.08814 \n",
      "[396/500] train_loss: 0.05674 valid_loss: 0.09875 test_loss: 0.08739 \n",
      "[397/500] train_loss: 0.05518 valid_loss: 0.08584 test_loss: 0.08683 \n",
      "[398/500] train_loss: 0.05541 valid_loss: 0.08286 test_loss: 0.08682 \n",
      "[399/500] train_loss: 0.05492 valid_loss: 0.08269 test_loss: 0.08647 \n",
      "[400/500] train_loss: 0.05513 valid_loss: 0.09314 test_loss: 0.08812 \n",
      "[401/500] train_loss: 0.05473 valid_loss: 0.09146 test_loss: 0.08700 \n",
      "[402/500] train_loss: 0.05466 valid_loss: 0.08769 test_loss: 0.08796 \n",
      "[403/500] train_loss: 0.05656 valid_loss: 0.09048 test_loss: 0.08631 \n",
      "[404/500] train_loss: 0.05683 valid_loss: 0.08684 test_loss: 0.08642 \n",
      "[405/500] train_loss: 0.05512 valid_loss: 0.08343 test_loss: 0.08823 \n",
      "[406/500] train_loss: 0.05376 valid_loss: 0.08533 test_loss: 0.08742 \n",
      "[407/500] train_loss: 0.05579 valid_loss: 0.07545 test_loss: 0.08546 \n",
      "[408/500] train_loss: 0.05415 valid_loss: 0.08357 test_loss: 0.08669 \n",
      "[409/500] train_loss: 0.05581 valid_loss: 0.07808 test_loss: 0.08841 \n",
      "[410/500] train_loss: 0.05509 valid_loss: 0.07754 test_loss: 0.08624 \n",
      "[411/500] train_loss: 0.05466 valid_loss: 0.08092 test_loss: 0.08804 \n",
      "[412/500] train_loss: 0.05527 valid_loss: 0.07776 test_loss: 0.08794 \n",
      "[413/500] train_loss: 0.05502 valid_loss: 0.07744 test_loss: 0.08802 \n",
      "[414/500] train_loss: 0.05613 valid_loss: 0.07631 test_loss: 0.08792 \n",
      "[415/500] train_loss: 0.05397 valid_loss: 0.08416 test_loss: 0.08713 \n",
      "[416/500] train_loss: 0.05752 valid_loss: 0.08176 test_loss: 0.08867 \n",
      "[417/500] train_loss: 0.05651 valid_loss: 0.07634 test_loss: 0.08709 \n",
      "[418/500] train_loss: 0.05330 valid_loss: 0.08279 test_loss: 0.08890 \n",
      "[419/500] train_loss: 0.05460 valid_loss: 0.08282 test_loss: 0.09102 \n",
      "[420/500] train_loss: 0.05606 valid_loss: 0.08726 test_loss: 0.08693 \n",
      "[421/500] train_loss: 0.05585 valid_loss: 0.08963 test_loss: 0.08725 \n",
      "[422/500] train_loss: 0.05463 valid_loss: 0.08648 test_loss: 0.08677 \n",
      "[423/500] train_loss: 0.05347 valid_loss: 0.07853 test_loss: 0.08745 \n",
      "[424/500] train_loss: 0.05394 valid_loss: 0.07629 test_loss: 0.08836 \n",
      "[425/500] train_loss: 0.05372 valid_loss: 0.07628 test_loss: 0.08792 \n",
      "[426/500] train_loss: 0.05646 valid_loss: 0.07923 test_loss: 0.08808 \n",
      "[427/500] train_loss: 0.05653 valid_loss: 0.08612 test_loss: 0.08851 \n",
      "[428/500] train_loss: 0.05402 valid_loss: 0.07745 test_loss: 0.08882 \n",
      "[429/500] train_loss: 0.05322 valid_loss: 0.08049 test_loss: 0.08775 \n",
      "[430/500] train_loss: 0.05320 valid_loss: 0.07889 test_loss: 0.08716 \n",
      "[431/500] train_loss: 0.05529 valid_loss: 0.08883 test_loss: 0.08829 \n",
      "[432/500] train_loss: 0.05548 valid_loss: 0.07681 test_loss: 0.08674 \n",
      "[433/500] train_loss: 0.05391 valid_loss: 0.07664 test_loss: 0.08963 \n",
      "[434/500] train_loss: 0.05405 valid_loss: 0.07600 test_loss: 0.08834 \n",
      "[435/500] train_loss: 0.05421 valid_loss: 0.07938 test_loss: 0.08711 \n",
      "[436/500] train_loss: 0.05409 valid_loss: 0.08354 test_loss: 0.08981 \n",
      "[437/500] train_loss: 0.05475 valid_loss: 0.07988 test_loss: 0.08844 \n",
      "[438/500] train_loss: 0.05304 valid_loss: 0.08568 test_loss: 0.08869 \n",
      "[439/500] train_loss: 0.05506 valid_loss: 0.09188 test_loss: 0.09088 \n",
      "[440/500] train_loss: 0.05402 valid_loss: 0.07719 test_loss: 0.08915 \n",
      "[441/500] train_loss: 0.05308 valid_loss: 0.08310 test_loss: 0.08806 \n",
      "[442/500] train_loss: 0.05440 valid_loss: 0.07558 test_loss: 0.08966 \n",
      "[443/500] train_loss: 0.05336 valid_loss: 0.08538 test_loss: 0.09061 \n",
      "[444/500] train_loss: 0.05472 valid_loss: 0.08190 test_loss: 0.08860 \n",
      "[445/500] train_loss: 0.05504 valid_loss: 0.08428 test_loss: 0.08775 \n",
      "[446/500] train_loss: 0.05455 valid_loss: 0.08649 test_loss: 0.09048 \n",
      "[447/500] train_loss: 0.05430 valid_loss: 0.07937 test_loss: 0.08843 \n",
      "[448/500] train_loss: 0.05385 valid_loss: 0.09500 test_loss: 0.08986 \n",
      "[449/500] train_loss: 0.05336 valid_loss: 0.07652 test_loss: 0.08754 \n",
      "[450/500] train_loss: 0.05402 valid_loss: 0.08219 test_loss: 0.08853 \n",
      "[451/500] train_loss: 0.05273 valid_loss: 0.08193 test_loss: 0.08712 \n",
      "[452/500] train_loss: 0.05415 valid_loss: 0.07844 test_loss: 0.08951 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[453/500] train_loss: 0.05297 valid_loss: 0.07693 test_loss: 0.08775 \n",
      "[454/500] train_loss: 0.05399 valid_loss: 0.08123 test_loss: 0.08824 \n",
      "[455/500] train_loss: 0.05208 valid_loss: 0.08322 test_loss: 0.08777 \n",
      "[456/500] train_loss: 0.05474 valid_loss: 0.08794 test_loss: 0.08756 \n",
      "[457/500] train_loss: 0.05176 valid_loss: 0.08047 test_loss: 0.08875 \n",
      "[458/500] train_loss: 0.05236 valid_loss: 0.07922 test_loss: 0.08645 \n",
      "[459/500] train_loss: 0.05213 valid_loss: 0.08282 test_loss: 0.08707 \n",
      "[460/500] train_loss: 0.05388 valid_loss: 0.09554 test_loss: 0.08712 \n",
      "[461/500] train_loss: 0.05307 valid_loss: 0.08229 test_loss: 0.08796 \n",
      "[462/500] train_loss: 0.05177 valid_loss: 0.07971 test_loss: 0.08945 \n",
      "[463/500] train_loss: 0.05356 valid_loss: 0.07696 test_loss: 0.08721 \n",
      "[464/500] train_loss: 0.05131 valid_loss: 0.09213 test_loss: 0.08962 \n",
      "[465/500] train_loss: 0.05300 valid_loss: 0.07862 test_loss: 0.08755 \n",
      "[466/500] train_loss: 0.05249 valid_loss: 0.08462 test_loss: 0.08885 \n",
      "[467/500] train_loss: 0.05276 valid_loss: 0.07778 test_loss: 0.08775 \n",
      "[468/500] train_loss: 0.05235 valid_loss: 0.08761 test_loss: 0.08663 \n",
      "[469/500] train_loss: 0.05413 valid_loss: 0.07663 test_loss: 0.08726 \n",
      "[470/500] train_loss: 0.05363 valid_loss: 0.07916 test_loss: 0.08605 \n",
      "[471/500] train_loss: 0.05302 valid_loss: 0.08812 test_loss: 0.08555 \n",
      "[472/500] train_loss: 0.05177 valid_loss: 0.08993 test_loss: 0.08815 \n",
      "[473/500] train_loss: 0.05247 valid_loss: 0.09651 test_loss: 0.08695 \n",
      "[474/500] train_loss: 0.05265 valid_loss: 0.08670 test_loss: 0.08689 \n",
      "[475/500] train_loss: 0.05192 valid_loss: 0.07933 test_loss: 0.08756 \n",
      "[476/500] train_loss: 0.05330 valid_loss: 0.08096 test_loss: 0.08718 \n",
      "[477/500] train_loss: 0.05235 valid_loss: 0.08152 test_loss: 0.08758 \n",
      "[478/500] train_loss: 0.05099 valid_loss: 0.07832 test_loss: 0.08713 \n",
      "[479/500] train_loss: 0.05349 valid_loss: 0.09416 test_loss: 0.08730 \n",
      "[480/500] train_loss: 0.05209 valid_loss: 0.08278 test_loss: 0.08775 \n",
      "[481/500] train_loss: 0.05140 valid_loss: 0.08247 test_loss: 0.08807 \n",
      "[482/500] train_loss: 0.05371 valid_loss: 0.07698 test_loss: 0.08691 \n",
      "[483/500] train_loss: 0.05229 valid_loss: 0.08885 test_loss: 0.08750 \n",
      "[484/500] train_loss: 0.05210 valid_loss: 0.08709 test_loss: 0.08680 \n",
      "[485/500] train_loss: 0.05112 valid_loss: 0.09557 test_loss: 0.08923 \n",
      "[486/500] train_loss: 0.05438 valid_loss: 0.09233 test_loss: 0.08707 \n",
      "[487/500] train_loss: 0.05092 valid_loss: 0.08484 test_loss: 0.08811 \n",
      "[488/500] train_loss: 0.05207 valid_loss: 0.08724 test_loss: 0.08706 \n",
      "[489/500] train_loss: 0.05295 valid_loss: 0.09395 test_loss: 0.08839 \n",
      "[490/500] train_loss: 0.05205 valid_loss: 0.08273 test_loss: 0.08738 \n",
      "[491/500] train_loss: 0.05197 valid_loss: 0.08423 test_loss: 0.08924 \n",
      "[492/500] train_loss: 0.05078 valid_loss: 0.09782 test_loss: 0.08799 \n",
      "[493/500] train_loss: 0.05168 valid_loss: 0.08387 test_loss: 0.08769 \n",
      "[494/500] train_loss: 0.05266 valid_loss: 0.08278 test_loss: 0.08640 \n",
      "[495/500] train_loss: 0.05157 valid_loss: 0.07972 test_loss: 0.08679 \n",
      "[496/500] train_loss: 0.05209 valid_loss: 0.07882 test_loss: 0.08686 \n",
      "[497/500] train_loss: 0.05200 valid_loss: 0.08578 test_loss: 0.08573 \n",
      "[498/500] train_loss: 0.05212 valid_loss: 0.08194 test_loss: 0.08573 \n",
      "[499/500] train_loss: 0.05158 valid_loss: 0.07831 test_loss: 0.08758 \n",
      "[500/500] train_loss: 0.05319 valid_loss: 0.09102 test_loss: 0.08782 \n",
      "TRAINING MODEL 9\n",
      "[  1/500] train_loss: 0.52166 valid_loss: 0.36552 test_loss: 0.36991 \n",
      "验证损失减少 (inf --> 0.365522). 正在保存模型...\n",
      "[  2/500] train_loss: 0.27998 valid_loss: 0.24530 test_loss: 0.25109 \n",
      "验证损失减少 (0.365522 --> 0.245297). 正在保存模型...\n",
      "[  3/500] train_loss: 0.20737 valid_loss: 0.20020 test_loss: 0.20995 \n",
      "验证损失减少 (0.245297 --> 0.200199). 正在保存模型...\n",
      "[  4/500] train_loss: 0.17419 valid_loss: 0.17282 test_loss: 0.18206 \n",
      "验证损失减少 (0.200199 --> 0.172822). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15980 valid_loss: 0.15833 test_loss: 0.16991 \n",
      "验证损失减少 (0.172822 --> 0.158329). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14808 valid_loss: 0.15034 test_loss: 0.16358 \n",
      "验证损失减少 (0.158329 --> 0.150342). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13959 valid_loss: 0.14588 test_loss: 0.15946 \n",
      "验证损失减少 (0.150342 --> 0.145880). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13413 valid_loss: 0.13960 test_loss: 0.15294 \n",
      "验证损失减少 (0.145880 --> 0.139596). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13115 valid_loss: 0.13727 test_loss: 0.15149 \n",
      "验证损失减少 (0.139596 --> 0.137274). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12962 valid_loss: 0.13204 test_loss: 0.14603 \n",
      "验证损失减少 (0.137274 --> 0.132040). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12966 valid_loss: 0.13248 test_loss: 0.14492 \n",
      "[ 12/500] train_loss: 0.12305 valid_loss: 0.12543 test_loss: 0.13906 \n",
      "验证损失减少 (0.132040 --> 0.125431). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12027 valid_loss: 0.12828 test_loss: 0.14096 \n",
      "[ 14/500] train_loss: 0.12032 valid_loss: 0.12341 test_loss: 0.13686 \n",
      "验证损失减少 (0.125431 --> 0.123406). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11704 valid_loss: 0.12486 test_loss: 0.14008 \n",
      "[ 16/500] train_loss: 0.11371 valid_loss: 0.12186 test_loss: 0.13434 \n",
      "验证损失减少 (0.123406 --> 0.121856). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11109 valid_loss: 0.11986 test_loss: 0.13227 \n",
      "验证损失减少 (0.121856 --> 0.119857). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11657 valid_loss: 0.12080 test_loss: 0.13440 \n",
      "[ 19/500] train_loss: 0.11634 valid_loss: 0.11633 test_loss: 0.13095 \n",
      "验证损失减少 (0.119857 --> 0.116327). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11172 valid_loss: 0.11857 test_loss: 0.13140 \n",
      "[ 21/500] train_loss: 0.11056 valid_loss: 0.11550 test_loss: 0.13027 \n",
      "验证损失减少 (0.116327 --> 0.115497). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.10895 valid_loss: 0.11738 test_loss: 0.13185 \n",
      "[ 23/500] train_loss: 0.11096 valid_loss: 0.11404 test_loss: 0.12894 \n",
      "验证损失减少 (0.115497 --> 0.114044). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10863 valid_loss: 0.11190 test_loss: 0.12501 \n",
      "验证损失减少 (0.114044 --> 0.111896). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10596 valid_loss: 0.11166 test_loss: 0.12667 \n",
      "验证损失减少 (0.111896 --> 0.111657). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10481 valid_loss: 0.11006 test_loss: 0.12257 \n",
      "验证损失减少 (0.111657 --> 0.110059). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.10667 valid_loss: 0.11197 test_loss: 0.12577 \n",
      "[ 28/500] train_loss: 0.10589 valid_loss: 0.10776 test_loss: 0.12212 \n",
      "验证损失减少 (0.110059 --> 0.107765). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10232 valid_loss: 0.10874 test_loss: 0.12355 \n",
      "[ 30/500] train_loss: 0.10440 valid_loss: 0.10815 test_loss: 0.12221 \n",
      "[ 31/500] train_loss: 0.10244 valid_loss: 0.10892 test_loss: 0.12061 \n",
      "[ 32/500] train_loss: 0.10103 valid_loss: 0.10650 test_loss: 0.11969 \n",
      "验证损失减少 (0.107765 --> 0.106502). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10005 valid_loss: 0.10523 test_loss: 0.11901 \n",
      "验证损失减少 (0.106502 --> 0.105231). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.09809 valid_loss: 0.10637 test_loss: 0.11922 \n",
      "[ 35/500] train_loss: 0.09716 valid_loss: 0.10690 test_loss: 0.11837 \n",
      "[ 36/500] train_loss: 0.09972 valid_loss: 0.10510 test_loss: 0.11882 \n",
      "验证损失减少 (0.105231 --> 0.105103). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.09842 valid_loss: 0.10649 test_loss: 0.11841 \n",
      "[ 38/500] train_loss: 0.09762 valid_loss: 0.10691 test_loss: 0.11818 \n",
      "[ 39/500] train_loss: 0.09951 valid_loss: 0.10790 test_loss: 0.11835 \n",
      "[ 40/500] train_loss: 0.09821 valid_loss: 0.10224 test_loss: 0.11487 \n",
      "验证损失减少 (0.105103 --> 0.102237). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.09662 valid_loss: 0.10235 test_loss: 0.11516 \n",
      "[ 42/500] train_loss: 0.09787 valid_loss: 0.10111 test_loss: 0.11490 \n",
      "验证损失减少 (0.102237 --> 0.101115). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.09279 valid_loss: 0.10010 test_loss: 0.11326 \n",
      "验证损失减少 (0.101115 --> 0.100102). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.09504 valid_loss: 0.09813 test_loss: 0.11308 \n",
      "验证损失减少 (0.100102 --> 0.098132). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.09416 valid_loss: 0.10073 test_loss: 0.11473 \n",
      "[ 46/500] train_loss: 0.09220 valid_loss: 0.10147 test_loss: 0.11447 \n",
      "[ 47/500] train_loss: 0.09504 valid_loss: 0.10434 test_loss: 0.11570 \n",
      "[ 48/500] train_loss: 0.09218 valid_loss: 0.10215 test_loss: 0.11267 \n",
      "[ 49/500] train_loss: 0.09288 valid_loss: 0.10204 test_loss: 0.11234 \n",
      "[ 50/500] train_loss: 0.08986 valid_loss: 0.10277 test_loss: 0.11135 \n",
      "[ 51/500] train_loss: 0.09155 valid_loss: 0.09809 test_loss: 0.11086 \n",
      "验证损失减少 (0.098132 --> 0.098093). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.09115 valid_loss: 0.09877 test_loss: 0.10992 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 53/500] train_loss: 0.09126 valid_loss: 0.09944 test_loss: 0.11184 \n",
      "[ 54/500] train_loss: 0.08981 valid_loss: 0.09712 test_loss: 0.10925 \n",
      "验证损失减少 (0.098093 --> 0.097119). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.09278 valid_loss: 0.09622 test_loss: 0.11002 \n",
      "验证损失减少 (0.097119 --> 0.096221). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.08864 valid_loss: 0.10126 test_loss: 0.11021 \n",
      "[ 57/500] train_loss: 0.08994 valid_loss: 0.09932 test_loss: 0.11113 \n",
      "[ 58/500] train_loss: 0.08852 valid_loss: 0.09484 test_loss: 0.10724 \n",
      "验证损失减少 (0.096221 --> 0.094839). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.08858 valid_loss: 0.09870 test_loss: 0.11154 \n",
      "[ 60/500] train_loss: 0.08915 valid_loss: 0.09589 test_loss: 0.10874 \n",
      "[ 61/500] train_loss: 0.08826 valid_loss: 0.09500 test_loss: 0.10756 \n",
      "[ 62/500] train_loss: 0.08958 valid_loss: 0.10026 test_loss: 0.10928 \n",
      "[ 63/500] train_loss: 0.08611 valid_loss: 0.09689 test_loss: 0.10655 \n",
      "[ 64/500] train_loss: 0.08615 valid_loss: 0.09559 test_loss: 0.10738 \n",
      "[ 65/500] train_loss: 0.08837 valid_loss: 0.09475 test_loss: 0.10682 \n",
      "验证损失减少 (0.094839 --> 0.094748). 正在保存模型...\n",
      "[ 66/500] train_loss: 0.08626 valid_loss: 0.09399 test_loss: 0.10725 \n",
      "验证损失减少 (0.094748 --> 0.093995). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.08555 valid_loss: 0.09807 test_loss: 0.10640 \n",
      "[ 68/500] train_loss: 0.08647 valid_loss: 0.09548 test_loss: 0.10668 \n",
      "[ 69/500] train_loss: 0.08629 valid_loss: 0.09256 test_loss: 0.10526 \n",
      "验证损失减少 (0.093995 --> 0.092563). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.08594 valid_loss: 0.09434 test_loss: 0.10526 \n",
      "[ 71/500] train_loss: 0.08689 valid_loss: 0.09330 test_loss: 0.10773 \n",
      "[ 72/500] train_loss: 0.08380 valid_loss: 0.09169 test_loss: 0.10385 \n",
      "验证损失减少 (0.092563 --> 0.091693). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.08450 valid_loss: 0.09299 test_loss: 0.10456 \n",
      "[ 74/500] train_loss: 0.08572 valid_loss: 0.09402 test_loss: 0.10520 \n",
      "[ 75/500] train_loss: 0.08438 valid_loss: 0.09187 test_loss: 0.10270 \n",
      "[ 76/500] train_loss: 0.08530 valid_loss: 0.09399 test_loss: 0.10428 \n",
      "[ 77/500] train_loss: 0.08303 valid_loss: 0.09136 test_loss: 0.10439 \n",
      "验证损失减少 (0.091693 --> 0.091356). 正在保存模型...\n",
      "[ 78/500] train_loss: 0.08312 valid_loss: 0.09452 test_loss: 0.10524 \n",
      "[ 79/500] train_loss: 0.08340 valid_loss: 0.08936 test_loss: 0.10192 \n",
      "验证损失减少 (0.091356 --> 0.089365). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.08343 valid_loss: 0.09043 test_loss: 0.10242 \n",
      "[ 81/500] train_loss: 0.08413 valid_loss: 0.09026 test_loss: 0.10224 \n",
      "[ 82/500] train_loss: 0.08280 valid_loss: 0.09181 test_loss: 0.10263 \n",
      "[ 83/500] train_loss: 0.08182 valid_loss: 0.08989 test_loss: 0.10191 \n",
      "[ 84/500] train_loss: 0.08189 valid_loss: 0.09014 test_loss: 0.10189 \n",
      "[ 85/500] train_loss: 0.08383 valid_loss: 0.08852 test_loss: 0.10218 \n",
      "验证损失减少 (0.089365 --> 0.088524). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08293 valid_loss: 0.08819 test_loss: 0.10077 \n",
      "验证损失减少 (0.088524 --> 0.088187). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.08203 valid_loss: 0.08820 test_loss: 0.10044 \n",
      "[ 88/500] train_loss: 0.08081 valid_loss: 0.08981 test_loss: 0.10084 \n",
      "[ 89/500] train_loss: 0.08237 valid_loss: 0.08893 test_loss: 0.10208 \n",
      "[ 90/500] train_loss: 0.08135 valid_loss: 0.08788 test_loss: 0.10047 \n",
      "验证损失减少 (0.088187 --> 0.087883). 正在保存模型...\n",
      "[ 91/500] train_loss: 0.08141 valid_loss: 0.08991 test_loss: 0.10051 \n",
      "[ 92/500] train_loss: 0.08163 valid_loss: 0.09087 test_loss: 0.10013 \n",
      "[ 93/500] train_loss: 0.08179 valid_loss: 0.08847 test_loss: 0.09915 \n",
      "[ 94/500] train_loss: 0.08060 valid_loss: 0.08832 test_loss: 0.10015 \n",
      "[ 95/500] train_loss: 0.07802 valid_loss: 0.08719 test_loss: 0.09900 \n",
      "验证损失减少 (0.087883 --> 0.087190). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.08124 valid_loss: 0.08774 test_loss: 0.09931 \n",
      "[ 97/500] train_loss: 0.07816 valid_loss: 0.08900 test_loss: 0.09984 \n",
      "[ 98/500] train_loss: 0.08061 valid_loss: 0.08640 test_loss: 0.09889 \n",
      "验证损失减少 (0.087190 --> 0.086396). 正在保存模型...\n",
      "[ 99/500] train_loss: 0.08010 valid_loss: 0.08637 test_loss: 0.09941 \n",
      "验证损失减少 (0.086396 --> 0.086375). 正在保存模型...\n",
      "[100/500] train_loss: 0.07821 valid_loss: 0.08859 test_loss: 0.09873 \n",
      "[101/500] train_loss: 0.07930 valid_loss: 0.08658 test_loss: 0.09830 \n",
      "[102/500] train_loss: 0.07672 valid_loss: 0.08826 test_loss: 0.09865 \n",
      "[103/500] train_loss: 0.07873 valid_loss: 0.08880 test_loss: 0.09809 \n",
      "[104/500] train_loss: 0.07816 valid_loss: 0.08749 test_loss: 0.09791 \n",
      "[105/500] train_loss: 0.07753 valid_loss: 0.08727 test_loss: 0.09768 \n",
      "[106/500] train_loss: 0.08007 valid_loss: 0.08761 test_loss: 0.09835 \n",
      "[107/500] train_loss: 0.07753 valid_loss: 0.08615 test_loss: 0.09661 \n",
      "验证损失减少 (0.086375 --> 0.086147). 正在保存模型...\n",
      "[108/500] train_loss: 0.07807 valid_loss: 0.08681 test_loss: 0.10012 \n",
      "[109/500] train_loss: 0.07840 valid_loss: 0.08894 test_loss: 0.09809 \n",
      "[110/500] train_loss: 0.07756 valid_loss: 0.08766 test_loss: 0.09779 \n",
      "[111/500] train_loss: 0.07933 valid_loss: 0.08639 test_loss: 0.09723 \n",
      "[112/500] train_loss: 0.07557 valid_loss: 0.08522 test_loss: 0.09752 \n",
      "验证损失减少 (0.086147 --> 0.085217). 正在保存模型...\n",
      "[113/500] train_loss: 0.07740 valid_loss: 0.08597 test_loss: 0.09749 \n",
      "[114/500] train_loss: 0.07342 valid_loss: 0.08589 test_loss: 0.09795 \n",
      "[115/500] train_loss: 0.07351 valid_loss: 0.08542 test_loss: 0.09549 \n",
      "[116/500] train_loss: 0.07535 valid_loss: 0.08451 test_loss: 0.09639 \n",
      "验证损失减少 (0.085217 --> 0.084506). 正在保存模型...\n",
      "[117/500] train_loss: 0.07661 valid_loss: 0.08499 test_loss: 0.09653 \n",
      "[118/500] train_loss: 0.07842 valid_loss: 0.08417 test_loss: 0.09695 \n",
      "验证损失减少 (0.084506 --> 0.084170). 正在保存模型...\n",
      "[119/500] train_loss: 0.07590 valid_loss: 0.08607 test_loss: 0.09637 \n",
      "[120/500] train_loss: 0.07657 valid_loss: 0.08507 test_loss: 0.09611 \n",
      "[121/500] train_loss: 0.07490 valid_loss: 0.08497 test_loss: 0.09709 \n",
      "[122/500] train_loss: 0.07479 valid_loss: 0.08584 test_loss: 0.09606 \n",
      "[123/500] train_loss: 0.07442 valid_loss: 0.08581 test_loss: 0.09511 \n",
      "[124/500] train_loss: 0.07631 valid_loss: 0.08538 test_loss: 0.09610 \n",
      "[125/500] train_loss: 0.07513 valid_loss: 0.08917 test_loss: 0.09711 \n",
      "[126/500] train_loss: 0.07491 valid_loss: 0.08488 test_loss: 0.09552 \n",
      "[127/500] train_loss: 0.07445 valid_loss: 0.08514 test_loss: 0.09497 \n",
      "[128/500] train_loss: 0.07332 valid_loss: 0.08653 test_loss: 0.09766 \n",
      "[129/500] train_loss: 0.07301 valid_loss: 0.08550 test_loss: 0.09607 \n",
      "[130/500] train_loss: 0.07442 valid_loss: 0.08598 test_loss: 0.09630 \n",
      "[131/500] train_loss: 0.07500 valid_loss: 0.08393 test_loss: 0.09367 \n",
      "验证损失减少 (0.084170 --> 0.083929). 正在保存模型...\n",
      "[132/500] train_loss: 0.07529 valid_loss: 0.08725 test_loss: 0.09565 \n",
      "[133/500] train_loss: 0.07260 valid_loss: 0.08447 test_loss: 0.09590 \n",
      "[134/500] train_loss: 0.07297 valid_loss: 0.08340 test_loss: 0.09596 \n",
      "验证损失减少 (0.083929 --> 0.083404). 正在保存模型...\n",
      "[135/500] train_loss: 0.07428 valid_loss: 0.08183 test_loss: 0.09398 \n",
      "验证损失减少 (0.083404 --> 0.081831). 正在保存模型...\n",
      "[136/500] train_loss: 0.06969 valid_loss: 0.08362 test_loss: 0.09425 \n",
      "[137/500] train_loss: 0.07471 valid_loss: 0.08544 test_loss: 0.09762 \n",
      "[138/500] train_loss: 0.07090 valid_loss: 0.08390 test_loss: 0.09546 \n",
      "[139/500] train_loss: 0.07486 valid_loss: 0.08686 test_loss: 0.09691 \n",
      "[140/500] train_loss: 0.07240 valid_loss: 0.08200 test_loss: 0.09397 \n",
      "[141/500] train_loss: 0.07150 valid_loss: 0.08376 test_loss: 0.09468 \n",
      "[142/500] train_loss: 0.07246 valid_loss: 0.08144 test_loss: 0.09453 \n",
      "验证损失减少 (0.081831 --> 0.081440). 正在保存模型...\n",
      "[143/500] train_loss: 0.07155 valid_loss: 0.08497 test_loss: 0.09416 \n",
      "[144/500] train_loss: 0.07295 valid_loss: 0.08256 test_loss: 0.09292 \n",
      "[145/500] train_loss: 0.07303 valid_loss: 0.08141 test_loss: 0.09354 \n",
      "验证损失减少 (0.081440 --> 0.081409). 正在保存模型...\n",
      "[146/500] train_loss: 0.07372 valid_loss: 0.08162 test_loss: 0.09342 \n",
      "[147/500] train_loss: 0.07225 valid_loss: 0.08199 test_loss: 0.09308 \n",
      "[148/500] train_loss: 0.07378 valid_loss: 0.08204 test_loss: 0.09422 \n",
      "[149/500] train_loss: 0.07148 valid_loss: 0.08551 test_loss: 0.09447 \n",
      "[150/500] train_loss: 0.07300 valid_loss: 0.08647 test_loss: 0.09270 \n",
      "[151/500] train_loss: 0.07183 valid_loss: 0.08306 test_loss: 0.09302 \n",
      "[152/500] train_loss: 0.07163 valid_loss: 0.08156 test_loss: 0.09402 \n",
      "[153/500] train_loss: 0.07206 valid_loss: 0.08246 test_loss: 0.09372 \n",
      "[154/500] train_loss: 0.07324 valid_loss: 0.08317 test_loss: 0.09278 \n",
      "[155/500] train_loss: 0.07117 valid_loss: 0.08318 test_loss: 0.09354 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156/500] train_loss: 0.07127 valid_loss: 0.08074 test_loss: 0.09379 \n",
      "验证损失减少 (0.081409 --> 0.080737). 正在保存模型...\n",
      "[157/500] train_loss: 0.07193 valid_loss: 0.08303 test_loss: 0.09306 \n",
      "[158/500] train_loss: 0.06939 valid_loss: 0.08106 test_loss: 0.09407 \n",
      "[159/500] train_loss: 0.07247 valid_loss: 0.08152 test_loss: 0.09363 \n",
      "[160/500] train_loss: 0.06818 valid_loss: 0.08280 test_loss: 0.09342 \n",
      "[161/500] train_loss: 0.07109 valid_loss: 0.08108 test_loss: 0.09243 \n",
      "[162/500] train_loss: 0.07174 valid_loss: 0.08016 test_loss: 0.09203 \n",
      "验证损失减少 (0.080737 --> 0.080157). 正在保存模型...\n",
      "[163/500] train_loss: 0.07076 valid_loss: 0.08533 test_loss: 0.09465 \n",
      "[164/500] train_loss: 0.06933 valid_loss: 0.08154 test_loss: 0.09213 \n",
      "[165/500] train_loss: 0.07039 valid_loss: 0.08710 test_loss: 0.09201 \n",
      "[166/500] train_loss: 0.06850 valid_loss: 0.08238 test_loss: 0.09214 \n",
      "[167/500] train_loss: 0.07205 valid_loss: 0.08305 test_loss: 0.09304 \n",
      "[168/500] train_loss: 0.06978 valid_loss: 0.08193 test_loss: 0.09376 \n",
      "[169/500] train_loss: 0.06989 valid_loss: 0.08060 test_loss: 0.09039 \n",
      "[170/500] train_loss: 0.07031 valid_loss: 0.08232 test_loss: 0.09266 \n",
      "[171/500] train_loss: 0.06875 valid_loss: 0.08081 test_loss: 0.09238 \n",
      "[172/500] train_loss: 0.06837 valid_loss: 0.08105 test_loss: 0.09212 \n",
      "[173/500] train_loss: 0.07018 valid_loss: 0.08233 test_loss: 0.09467 \n",
      "[174/500] train_loss: 0.06868 valid_loss: 0.07985 test_loss: 0.09273 \n",
      "验证损失减少 (0.080157 --> 0.079849). 正在保存模型...\n",
      "[175/500] train_loss: 0.06815 valid_loss: 0.08321 test_loss: 0.09499 \n",
      "[176/500] train_loss: 0.06703 valid_loss: 0.08187 test_loss: 0.09197 \n",
      "[177/500] train_loss: 0.06924 valid_loss: 0.08639 test_loss: 0.09283 \n",
      "[178/500] train_loss: 0.06979 valid_loss: 0.08031 test_loss: 0.09176 \n",
      "[179/500] train_loss: 0.06735 valid_loss: 0.07964 test_loss: 0.09163 \n",
      "验证损失减少 (0.079849 --> 0.079638). 正在保存模型...\n",
      "[180/500] train_loss: 0.06780 valid_loss: 0.08069 test_loss: 0.09307 \n",
      "[181/500] train_loss: 0.06990 valid_loss: 0.07961 test_loss: 0.09265 \n",
      "验证损失减少 (0.079638 --> 0.079606). 正在保存模型...\n",
      "[182/500] train_loss: 0.06797 valid_loss: 0.07953 test_loss: 0.09250 \n",
      "验证损失减少 (0.079606 --> 0.079526). 正在保存模型...\n",
      "[183/500] train_loss: 0.06690 valid_loss: 0.07969 test_loss: 0.09138 \n",
      "[184/500] train_loss: 0.07117 valid_loss: 0.08005 test_loss: 0.09225 \n",
      "[185/500] train_loss: 0.06719 valid_loss: 0.07967 test_loss: 0.09214 \n",
      "[186/500] train_loss: 0.06867 valid_loss: 0.08010 test_loss: 0.09342 \n",
      "[187/500] train_loss: 0.06906 valid_loss: 0.08125 test_loss: 0.09348 \n",
      "[188/500] train_loss: 0.06717 valid_loss: 0.08031 test_loss: 0.09364 \n",
      "[189/500] train_loss: 0.06615 valid_loss: 0.07957 test_loss: 0.09212 \n",
      "[190/500] train_loss: 0.06907 valid_loss: 0.07908 test_loss: 0.09125 \n",
      "验证损失减少 (0.079526 --> 0.079079). 正在保存模型...\n",
      "[191/500] train_loss: 0.06657 valid_loss: 0.07861 test_loss: 0.09263 \n",
      "验证损失减少 (0.079079 --> 0.078611). 正在保存模型...\n",
      "[192/500] train_loss: 0.06819 valid_loss: 0.08015 test_loss: 0.09251 \n",
      "[193/500] train_loss: 0.07038 valid_loss: 0.07964 test_loss: 0.09230 \n",
      "[194/500] train_loss: 0.06805 valid_loss: 0.08007 test_loss: 0.09333 \n",
      "[195/500] train_loss: 0.06780 valid_loss: 0.07993 test_loss: 0.09344 \n",
      "[196/500] train_loss: 0.06738 valid_loss: 0.08200 test_loss: 0.09189 \n",
      "[197/500] train_loss: 0.06689 valid_loss: 0.08034 test_loss: 0.09230 \n",
      "[198/500] train_loss: 0.06959 valid_loss: 0.08093 test_loss: 0.09163 \n",
      "[199/500] train_loss: 0.06727 valid_loss: 0.07873 test_loss: 0.09027 \n",
      "[200/500] train_loss: 0.06712 valid_loss: 0.07850 test_loss: 0.09007 \n",
      "验证损失减少 (0.078611 --> 0.078501). 正在保存模型...\n",
      "[201/500] train_loss: 0.06936 valid_loss: 0.08018 test_loss: 0.09173 \n",
      "[202/500] train_loss: 0.06760 valid_loss: 0.08293 test_loss: 0.09176 \n",
      "[203/500] train_loss: 0.06679 valid_loss: 0.07897 test_loss: 0.09036 \n",
      "[204/500] train_loss: 0.06683 valid_loss: 0.08065 test_loss: 0.09132 \n",
      "[205/500] train_loss: 0.06794 valid_loss: 0.07877 test_loss: 0.09010 \n",
      "[206/500] train_loss: 0.06482 valid_loss: 0.07962 test_loss: 0.09189 \n",
      "[207/500] train_loss: 0.06813 valid_loss: 0.08244 test_loss: 0.09121 \n",
      "[208/500] train_loss: 0.06657 valid_loss: 0.07763 test_loss: 0.09087 \n",
      "验证损失减少 (0.078501 --> 0.077632). 正在保存模型...\n",
      "[209/500] train_loss: 0.06638 valid_loss: 0.08071 test_loss: 0.09055 \n",
      "[210/500] train_loss: 0.06740 valid_loss: 0.07770 test_loss: 0.09019 \n",
      "[211/500] train_loss: 0.06765 valid_loss: 0.08160 test_loss: 0.09206 \n",
      "[212/500] train_loss: 0.06708 valid_loss: 0.08039 test_loss: 0.09228 \n",
      "[213/500] train_loss: 0.06705 valid_loss: 0.07944 test_loss: 0.09170 \n",
      "[214/500] train_loss: 0.06472 valid_loss: 0.07829 test_loss: 0.09025 \n",
      "[215/500] train_loss: 0.06667 valid_loss: 0.07968 test_loss: 0.09297 \n",
      "[216/500] train_loss: 0.06335 valid_loss: 0.07859 test_loss: 0.09152 \n",
      "[217/500] train_loss: 0.06393 valid_loss: 0.08108 test_loss: 0.09208 \n",
      "[218/500] train_loss: 0.06496 valid_loss: 0.08331 test_loss: 0.09192 \n",
      "[219/500] train_loss: 0.06370 valid_loss: 0.07915 test_loss: 0.09115 \n",
      "[220/500] train_loss: 0.06415 valid_loss: 0.07991 test_loss: 0.09122 \n",
      "[221/500] train_loss: 0.06496 valid_loss: 0.08379 test_loss: 0.09054 \n",
      "[222/500] train_loss: 0.06462 valid_loss: 0.08058 test_loss: 0.09075 \n",
      "[223/500] train_loss: 0.06524 valid_loss: 0.07856 test_loss: 0.09077 \n",
      "[224/500] train_loss: 0.06528 valid_loss: 0.08003 test_loss: 0.09138 \n",
      "[225/500] train_loss: 0.06570 valid_loss: 0.07998 test_loss: 0.09118 \n",
      "[226/500] train_loss: 0.06571 valid_loss: 0.07956 test_loss: 0.09131 \n",
      "[227/500] train_loss: 0.06447 valid_loss: 0.07924 test_loss: 0.08983 \n",
      "[228/500] train_loss: 0.06442 valid_loss: 0.07968 test_loss: 0.09118 \n",
      "[229/500] train_loss: 0.06399 valid_loss: 0.07973 test_loss: 0.09174 \n",
      "[230/500] train_loss: 0.06475 valid_loss: 0.08035 test_loss: 0.08971 \n",
      "[231/500] train_loss: 0.06491 valid_loss: 0.07837 test_loss: 0.09069 \n",
      "[232/500] train_loss: 0.06263 valid_loss: 0.07915 test_loss: 0.09129 \n",
      "[233/500] train_loss: 0.06239 valid_loss: 0.07952 test_loss: 0.09200 \n",
      "[234/500] train_loss: 0.06455 valid_loss: 0.08199 test_loss: 0.09254 \n",
      "[235/500] train_loss: 0.06327 valid_loss: 0.07722 test_loss: 0.09078 \n",
      "验证损失减少 (0.077632 --> 0.077217). 正在保存模型...\n",
      "[236/500] train_loss: 0.06225 valid_loss: 0.07941 test_loss: 0.09221 \n",
      "[237/500] train_loss: 0.06391 valid_loss: 0.08216 test_loss: 0.09100 \n",
      "[238/500] train_loss: 0.06251 valid_loss: 0.07936 test_loss: 0.09061 \n",
      "[239/500] train_loss: 0.06414 valid_loss: 0.07891 test_loss: 0.09089 \n",
      "[240/500] train_loss: 0.06447 valid_loss: 0.07886 test_loss: 0.08980 \n",
      "[241/500] train_loss: 0.06377 valid_loss: 0.07902 test_loss: 0.08954 \n",
      "[242/500] train_loss: 0.06486 valid_loss: 0.08113 test_loss: 0.08970 \n",
      "[243/500] train_loss: 0.06273 valid_loss: 0.07844 test_loss: 0.09174 \n",
      "[244/500] train_loss: 0.06353 valid_loss: 0.07913 test_loss: 0.09134 \n",
      "[245/500] train_loss: 0.06337 valid_loss: 0.07962 test_loss: 0.09081 \n",
      "[246/500] train_loss: 0.06256 valid_loss: 0.07889 test_loss: 0.09095 \n",
      "[247/500] train_loss: 0.06240 valid_loss: 0.08204 test_loss: 0.09037 \n",
      "[248/500] train_loss: 0.06248 valid_loss: 0.07868 test_loss: 0.09082 \n",
      "[249/500] train_loss: 0.06375 valid_loss: 0.08321 test_loss: 0.09194 \n",
      "[250/500] train_loss: 0.06263 valid_loss: 0.08289 test_loss: 0.09230 \n",
      "[251/500] train_loss: 0.06278 valid_loss: 0.08124 test_loss: 0.09189 \n",
      "[252/500] train_loss: 0.06129 valid_loss: 0.08547 test_loss: 0.09119 \n",
      "[253/500] train_loss: 0.06186 valid_loss: 0.07836 test_loss: 0.08959 \n",
      "[254/500] train_loss: 0.06239 valid_loss: 0.08544 test_loss: 0.09041 \n",
      "[255/500] train_loss: 0.06221 valid_loss: 0.07813 test_loss: 0.09033 \n",
      "[256/500] train_loss: 0.06358 valid_loss: 0.07848 test_loss: 0.09056 \n",
      "[257/500] train_loss: 0.06290 valid_loss: 0.07919 test_loss: 0.09228 \n",
      "[258/500] train_loss: 0.06202 valid_loss: 0.07898 test_loss: 0.08973 \n",
      "[259/500] train_loss: 0.06062 valid_loss: 0.07997 test_loss: 0.08986 \n",
      "[260/500] train_loss: 0.06237 valid_loss: 0.07853 test_loss: 0.09033 \n",
      "[261/500] train_loss: 0.06345 valid_loss: 0.08155 test_loss: 0.09086 \n",
      "[262/500] train_loss: 0.06285 valid_loss: 0.07678 test_loss: 0.08906 \n",
      "验证损失减少 (0.077217 --> 0.076780). 正在保存模型...\n",
      "[263/500] train_loss: 0.06057 valid_loss: 0.07823 test_loss: 0.09033 \n",
      "[264/500] train_loss: 0.06164 valid_loss: 0.08195 test_loss: 0.09094 \n",
      "[265/500] train_loss: 0.06142 valid_loss: 0.07991 test_loss: 0.08893 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266/500] train_loss: 0.06125 valid_loss: 0.08401 test_loss: 0.09129 \n",
      "[267/500] train_loss: 0.06186 valid_loss: 0.08845 test_loss: 0.09080 \n",
      "[268/500] train_loss: 0.06191 valid_loss: 0.07723 test_loss: 0.08879 \n",
      "[269/500] train_loss: 0.06127 valid_loss: 0.07667 test_loss: 0.08931 \n",
      "验证损失减少 (0.076780 --> 0.076667). 正在保存模型...\n",
      "[270/500] train_loss: 0.06099 valid_loss: 0.07912 test_loss: 0.08854 \n",
      "[271/500] train_loss: 0.06205 valid_loss: 0.07745 test_loss: 0.08849 \n",
      "[272/500] train_loss: 0.06147 valid_loss: 0.07693 test_loss: 0.08976 \n",
      "[273/500] train_loss: 0.06079 valid_loss: 0.09276 test_loss: 0.08973 \n",
      "[274/500] train_loss: 0.06156 valid_loss: 0.07872 test_loss: 0.08936 \n",
      "[275/500] train_loss: 0.06134 valid_loss: 0.07782 test_loss: 0.09112 \n",
      "[276/500] train_loss: 0.05926 valid_loss: 0.07600 test_loss: 0.08979 \n",
      "验证损失减少 (0.076667 --> 0.076004). 正在保存模型...\n",
      "[277/500] train_loss: 0.06240 valid_loss: 0.07775 test_loss: 0.08860 \n",
      "[278/500] train_loss: 0.06162 valid_loss: 0.07666 test_loss: 0.08898 \n",
      "[279/500] train_loss: 0.05947 valid_loss: 0.07725 test_loss: 0.09146 \n",
      "[280/500] train_loss: 0.06136 valid_loss: 0.07701 test_loss: 0.08973 \n",
      "[281/500] train_loss: 0.06008 valid_loss: 0.08034 test_loss: 0.09060 \n",
      "[282/500] train_loss: 0.06079 valid_loss: 0.07827 test_loss: 0.09131 \n",
      "[283/500] train_loss: 0.06004 valid_loss: 0.07926 test_loss: 0.09126 \n",
      "[284/500] train_loss: 0.06012 valid_loss: 0.07849 test_loss: 0.08969 \n",
      "[285/500] train_loss: 0.06121 valid_loss: 0.07880 test_loss: 0.08847 \n",
      "[286/500] train_loss: 0.06120 valid_loss: 0.08762 test_loss: 0.09092 \n",
      "[287/500] train_loss: 0.05966 valid_loss: 0.08107 test_loss: 0.08965 \n",
      "[288/500] train_loss: 0.06211 valid_loss: 0.07929 test_loss: 0.09018 \n",
      "[289/500] train_loss: 0.06115 valid_loss: 0.08210 test_loss: 0.09140 \n",
      "[290/500] train_loss: 0.06154 valid_loss: 0.08729 test_loss: 0.09082 \n",
      "[291/500] train_loss: 0.06031 valid_loss: 0.08663 test_loss: 0.09022 \n",
      "[292/500] train_loss: 0.05918 valid_loss: 0.08404 test_loss: 0.09165 \n",
      "[293/500] train_loss: 0.05961 valid_loss: 0.07930 test_loss: 0.09131 \n",
      "[294/500] train_loss: 0.06209 valid_loss: 0.07807 test_loss: 0.09031 \n",
      "[295/500] train_loss: 0.05994 valid_loss: 0.07748 test_loss: 0.09090 \n",
      "[296/500] train_loss: 0.06074 valid_loss: 0.07948 test_loss: 0.09192 \n",
      "[297/500] train_loss: 0.05970 valid_loss: 0.08243 test_loss: 0.09170 \n",
      "[298/500] train_loss: 0.06042 valid_loss: 0.08600 test_loss: 0.09074 \n",
      "[299/500] train_loss: 0.05856 valid_loss: 0.08712 test_loss: 0.09095 \n",
      "[300/500] train_loss: 0.05828 valid_loss: 0.08120 test_loss: 0.09001 \n",
      "[301/500] train_loss: 0.05874 valid_loss: 0.08485 test_loss: 0.09152 \n",
      "[302/500] train_loss: 0.05993 valid_loss: 0.08665 test_loss: 0.08993 \n",
      "[303/500] train_loss: 0.05933 valid_loss: 0.07812 test_loss: 0.08921 \n",
      "[304/500] train_loss: 0.05833 valid_loss: 0.08060 test_loss: 0.08977 \n",
      "[305/500] train_loss: 0.05984 valid_loss: 0.09125 test_loss: 0.09003 \n",
      "[306/500] train_loss: 0.06044 valid_loss: 0.07875 test_loss: 0.08819 \n",
      "[307/500] train_loss: 0.05918 valid_loss: 0.07579 test_loss: 0.08837 \n",
      "验证损失减少 (0.076004 --> 0.075794). 正在保存模型...\n",
      "[308/500] train_loss: 0.05635 valid_loss: 0.07849 test_loss: 0.09017 \n",
      "[309/500] train_loss: 0.06170 valid_loss: 0.07755 test_loss: 0.08910 \n",
      "[310/500] train_loss: 0.05922 valid_loss: 0.07627 test_loss: 0.08926 \n",
      "[311/500] train_loss: 0.05910 valid_loss: 0.07609 test_loss: 0.09055 \n",
      "[312/500] train_loss: 0.05844 valid_loss: 0.07646 test_loss: 0.09050 \n",
      "[313/500] train_loss: 0.05829 valid_loss: 0.07758 test_loss: 0.08884 \n",
      "[314/500] train_loss: 0.05820 valid_loss: 0.07699 test_loss: 0.08949 \n",
      "[315/500] train_loss: 0.05936 valid_loss: 0.07750 test_loss: 0.08892 \n",
      "[316/500] train_loss: 0.05896 valid_loss: 0.07694 test_loss: 0.08878 \n",
      "[317/500] train_loss: 0.05755 valid_loss: 0.08096 test_loss: 0.08908 \n",
      "[318/500] train_loss: 0.05894 valid_loss: 0.08207 test_loss: 0.09018 \n",
      "[319/500] train_loss: 0.05994 valid_loss: 0.07793 test_loss: 0.08976 \n",
      "[320/500] train_loss: 0.05868 valid_loss: 0.07632 test_loss: 0.09031 \n",
      "[321/500] train_loss: 0.05814 valid_loss: 0.07833 test_loss: 0.08984 \n",
      "[322/500] train_loss: 0.05866 valid_loss: 0.08699 test_loss: 0.09117 \n",
      "[323/500] train_loss: 0.05928 valid_loss: 0.07657 test_loss: 0.09092 \n",
      "[324/500] train_loss: 0.05690 valid_loss: 0.07627 test_loss: 0.09012 \n",
      "[325/500] train_loss: 0.06079 valid_loss: 0.07699 test_loss: 0.09096 \n",
      "[326/500] train_loss: 0.05714 valid_loss: 0.07716 test_loss: 0.09045 \n",
      "[327/500] train_loss: 0.05847 valid_loss: 0.08025 test_loss: 0.08984 \n",
      "[328/500] train_loss: 0.05873 valid_loss: 0.07756 test_loss: 0.09067 \n",
      "[329/500] train_loss: 0.05713 valid_loss: 0.08228 test_loss: 0.08953 \n",
      "[330/500] train_loss: 0.05880 valid_loss: 0.07906 test_loss: 0.08882 \n",
      "[331/500] train_loss: 0.05903 valid_loss: 0.08212 test_loss: 0.08930 \n",
      "[332/500] train_loss: 0.05732 valid_loss: 0.07739 test_loss: 0.09029 \n",
      "[333/500] train_loss: 0.05869 valid_loss: 0.07658 test_loss: 0.08995 \n",
      "[334/500] train_loss: 0.05741 valid_loss: 0.07872 test_loss: 0.08847 \n",
      "[335/500] train_loss: 0.05638 valid_loss: 0.07719 test_loss: 0.09021 \n",
      "[336/500] train_loss: 0.05682 valid_loss: 0.07847 test_loss: 0.08969 \n",
      "[337/500] train_loss: 0.05862 valid_loss: 0.07753 test_loss: 0.08939 \n",
      "[338/500] train_loss: 0.05694 valid_loss: 0.07763 test_loss: 0.09173 \n",
      "[339/500] train_loss: 0.05740 valid_loss: 0.07677 test_loss: 0.08868 \n",
      "[340/500] train_loss: 0.05684 valid_loss: 0.07685 test_loss: 0.08917 \n",
      "[341/500] train_loss: 0.05795 valid_loss: 0.08877 test_loss: 0.09174 \n",
      "[342/500] train_loss: 0.05515 valid_loss: 0.08257 test_loss: 0.09008 \n",
      "[343/500] train_loss: 0.05837 valid_loss: 0.07715 test_loss: 0.08765 \n",
      "[344/500] train_loss: 0.05885 valid_loss: 0.07946 test_loss: 0.08872 \n",
      "[345/500] train_loss: 0.05897 valid_loss: 0.07741 test_loss: 0.08816 \n",
      "[346/500] train_loss: 0.05808 valid_loss: 0.07603 test_loss: 0.08780 \n",
      "[347/500] train_loss: 0.05588 valid_loss: 0.07689 test_loss: 0.08924 \n",
      "[348/500] train_loss: 0.05699 valid_loss: 0.07764 test_loss: 0.08976 \n",
      "[349/500] train_loss: 0.05767 valid_loss: 0.07710 test_loss: 0.08913 \n",
      "[350/500] train_loss: 0.05875 valid_loss: 0.07586 test_loss: 0.08847 \n",
      "[351/500] train_loss: 0.05771 valid_loss: 0.07579 test_loss: 0.08916 \n",
      "[352/500] train_loss: 0.05682 valid_loss: 0.07810 test_loss: 0.08884 \n",
      "[353/500] train_loss: 0.05696 valid_loss: 0.08174 test_loss: 0.08649 \n",
      "[354/500] train_loss: 0.05634 valid_loss: 0.08491 test_loss: 0.09048 \n",
      "[355/500] train_loss: 0.05746 valid_loss: 0.07934 test_loss: 0.08872 \n",
      "[356/500] train_loss: 0.05529 valid_loss: 0.07725 test_loss: 0.09213 \n",
      "[357/500] train_loss: 0.05642 valid_loss: 0.07587 test_loss: 0.08853 \n",
      "[358/500] train_loss: 0.05747 valid_loss: 0.08001 test_loss: 0.09040 \n",
      "[359/500] train_loss: 0.05687 valid_loss: 0.08179 test_loss: 0.08913 \n",
      "[360/500] train_loss: 0.05869 valid_loss: 0.08183 test_loss: 0.09025 \n",
      "[361/500] train_loss: 0.05599 valid_loss: 0.07730 test_loss: 0.08914 \n",
      "[362/500] train_loss: 0.05616 valid_loss: 0.08316 test_loss: 0.08731 \n",
      "[363/500] train_loss: 0.05628 valid_loss: 0.08043 test_loss: 0.08970 \n",
      "[364/500] train_loss: 0.05706 valid_loss: 0.07892 test_loss: 0.08748 \n",
      "[365/500] train_loss: 0.05670 valid_loss: 0.07765 test_loss: 0.08965 \n",
      "[366/500] train_loss: 0.05720 valid_loss: 0.07897 test_loss: 0.08822 \n",
      "[367/500] train_loss: 0.05620 valid_loss: 0.08045 test_loss: 0.08946 \n",
      "[368/500] train_loss: 0.05484 valid_loss: 0.08617 test_loss: 0.08894 \n",
      "[369/500] train_loss: 0.05834 valid_loss: 0.07710 test_loss: 0.09037 \n",
      "[370/500] train_loss: 0.05736 valid_loss: 0.08321 test_loss: 0.09038 \n",
      "[371/500] train_loss: 0.05725 valid_loss: 0.07700 test_loss: 0.08996 \n",
      "[372/500] train_loss: 0.05727 valid_loss: 0.07460 test_loss: 0.09009 \n",
      "验证损失减少 (0.075794 --> 0.074599). 正在保存模型...\n",
      "[373/500] train_loss: 0.05674 valid_loss: 0.07671 test_loss: 0.09078 \n",
      "[374/500] train_loss: 0.05539 valid_loss: 0.07976 test_loss: 0.09012 \n",
      "[375/500] train_loss: 0.05724 valid_loss: 0.08004 test_loss: 0.08882 \n",
      "[376/500] train_loss: 0.05631 valid_loss: 0.07712 test_loss: 0.08885 \n",
      "[377/500] train_loss: 0.05530 valid_loss: 0.07698 test_loss: 0.08958 \n",
      "[378/500] train_loss: 0.05565 valid_loss: 0.07819 test_loss: 0.09138 \n",
      "[379/500] train_loss: 0.05521 valid_loss: 0.07658 test_loss: 0.08834 \n",
      "[380/500] train_loss: 0.05664 valid_loss: 0.07778 test_loss: 0.08859 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[381/500] train_loss: 0.05561 valid_loss: 0.07569 test_loss: 0.08892 \n",
      "[382/500] train_loss: 0.05479 valid_loss: 0.07589 test_loss: 0.09002 \n",
      "[383/500] train_loss: 0.05573 valid_loss: 0.07731 test_loss: 0.09234 \n",
      "[384/500] train_loss: 0.05528 valid_loss: 0.08084 test_loss: 0.08949 \n",
      "[385/500] train_loss: 0.05624 valid_loss: 0.08224 test_loss: 0.08949 \n",
      "[386/500] train_loss: 0.05408 valid_loss: 0.07779 test_loss: 0.08978 \n",
      "[387/500] train_loss: 0.05602 valid_loss: 0.08232 test_loss: 0.08899 \n",
      "[388/500] train_loss: 0.05445 valid_loss: 0.08061 test_loss: 0.09035 \n",
      "[389/500] train_loss: 0.05460 valid_loss: 0.07675 test_loss: 0.08828 \n",
      "[390/500] train_loss: 0.05559 valid_loss: 0.07718 test_loss: 0.08944 \n",
      "[391/500] train_loss: 0.05493 valid_loss: 0.07671 test_loss: 0.08857 \n",
      "[392/500] train_loss: 0.05508 valid_loss: 0.07737 test_loss: 0.08868 \n",
      "[393/500] train_loss: 0.05457 valid_loss: 0.07728 test_loss: 0.08930 \n",
      "[394/500] train_loss: 0.05415 valid_loss: 0.07878 test_loss: 0.09033 \n",
      "[395/500] train_loss: 0.05514 valid_loss: 0.07588 test_loss: 0.09043 \n",
      "[396/500] train_loss: 0.05597 valid_loss: 0.07648 test_loss: 0.08903 \n",
      "[397/500] train_loss: 0.05457 valid_loss: 0.07722 test_loss: 0.09056 \n",
      "[398/500] train_loss: 0.05410 valid_loss: 0.07737 test_loss: 0.08975 \n",
      "[399/500] train_loss: 0.05488 valid_loss: 0.08115 test_loss: 0.08902 \n",
      "[400/500] train_loss: 0.05634 valid_loss: 0.07621 test_loss: 0.08952 \n",
      "[401/500] train_loss: 0.05527 valid_loss: 0.07898 test_loss: 0.09056 \n",
      "[402/500] train_loss: 0.05582 valid_loss: 0.07589 test_loss: 0.08921 \n",
      "[403/500] train_loss: 0.05401 valid_loss: 0.07532 test_loss: 0.08889 \n",
      "[404/500] train_loss: 0.05544 valid_loss: 0.07720 test_loss: 0.09029 \n",
      "[405/500] train_loss: 0.05417 valid_loss: 0.07681 test_loss: 0.08966 \n",
      "[406/500] train_loss: 0.05458 valid_loss: 0.07647 test_loss: 0.08980 \n",
      "[407/500] train_loss: 0.05346 valid_loss: 0.07762 test_loss: 0.08807 \n",
      "[408/500] train_loss: 0.05511 valid_loss: 0.07829 test_loss: 0.08867 \n",
      "[409/500] train_loss: 0.05351 valid_loss: 0.07806 test_loss: 0.08962 \n",
      "[410/500] train_loss: 0.05407 valid_loss: 0.07696 test_loss: 0.08970 \n",
      "[411/500] train_loss: 0.05518 valid_loss: 0.07733 test_loss: 0.08911 \n",
      "[412/500] train_loss: 0.05436 valid_loss: 0.07661 test_loss: 0.08910 \n",
      "[413/500] train_loss: 0.05433 valid_loss: 0.07902 test_loss: 0.09010 \n",
      "[414/500] train_loss: 0.05707 valid_loss: 0.07617 test_loss: 0.08716 \n",
      "[415/500] train_loss: 0.05562 valid_loss: 0.07634 test_loss: 0.08939 \n",
      "[416/500] train_loss: 0.05281 valid_loss: 0.07747 test_loss: 0.08970 \n",
      "[417/500] train_loss: 0.05495 valid_loss: 0.07653 test_loss: 0.08938 \n",
      "[418/500] train_loss: 0.05477 valid_loss: 0.07940 test_loss: 0.09021 \n",
      "[419/500] train_loss: 0.05465 valid_loss: 0.07900 test_loss: 0.09095 \n",
      "[420/500] train_loss: 0.05223 valid_loss: 0.07550 test_loss: 0.08928 \n",
      "[421/500] train_loss: 0.05427 valid_loss: 0.07656 test_loss: 0.09009 \n",
      "[422/500] train_loss: 0.05268 valid_loss: 0.07854 test_loss: 0.09013 \n",
      "[423/500] train_loss: 0.05490 valid_loss: 0.07816 test_loss: 0.08843 \n",
      "[424/500] train_loss: 0.05324 valid_loss: 0.07811 test_loss: 0.09151 \n",
      "[425/500] train_loss: 0.05290 valid_loss: 0.07809 test_loss: 0.08977 \n",
      "[426/500] train_loss: 0.05447 valid_loss: 0.07764 test_loss: 0.08944 \n",
      "[427/500] train_loss: 0.05475 valid_loss: 0.07871 test_loss: 0.09073 \n",
      "[428/500] train_loss: 0.05355 valid_loss: 0.07649 test_loss: 0.08816 \n",
      "[429/500] train_loss: 0.05348 valid_loss: 0.07735 test_loss: 0.08952 \n",
      "[430/500] train_loss: 0.05327 valid_loss: 0.07868 test_loss: 0.08909 \n",
      "[431/500] train_loss: 0.05531 valid_loss: 0.07839 test_loss: 0.08943 \n",
      "[432/500] train_loss: 0.05424 valid_loss: 0.07701 test_loss: 0.08895 \n",
      "[433/500] train_loss: 0.05427 valid_loss: 0.07693 test_loss: 0.08929 \n",
      "[434/500] train_loss: 0.05321 valid_loss: 0.07683 test_loss: 0.08905 \n",
      "[435/500] train_loss: 0.05319 valid_loss: 0.07560 test_loss: 0.08904 \n",
      "[436/500] train_loss: 0.05382 valid_loss: 0.07680 test_loss: 0.09034 \n",
      "[437/500] train_loss: 0.05348 valid_loss: 0.07829 test_loss: 0.09112 \n",
      "[438/500] train_loss: 0.05364 valid_loss: 0.08243 test_loss: 0.09042 \n",
      "[439/500] train_loss: 0.05257 valid_loss: 0.07816 test_loss: 0.09094 \n",
      "[440/500] train_loss: 0.05359 valid_loss: 0.08048 test_loss: 0.09114 \n",
      "[441/500] train_loss: 0.05364 valid_loss: 0.07770 test_loss: 0.08908 \n",
      "[442/500] train_loss: 0.05279 valid_loss: 0.07692 test_loss: 0.08890 \n",
      "[443/500] train_loss: 0.05263 valid_loss: 0.07771 test_loss: 0.09097 \n",
      "[444/500] train_loss: 0.05275 valid_loss: 0.07895 test_loss: 0.08813 \n",
      "[445/500] train_loss: 0.05554 valid_loss: 0.07693 test_loss: 0.08838 \n",
      "[446/500] train_loss: 0.05255 valid_loss: 0.07911 test_loss: 0.08776 \n",
      "[447/500] train_loss: 0.05318 valid_loss: 0.07792 test_loss: 0.08908 \n",
      "[448/500] train_loss: 0.05235 valid_loss: 0.07792 test_loss: 0.09001 \n",
      "[449/500] train_loss: 0.05273 valid_loss: 0.08265 test_loss: 0.08969 \n",
      "[450/500] train_loss: 0.05459 valid_loss: 0.07759 test_loss: 0.08836 \n",
      "[451/500] train_loss: 0.05415 valid_loss: 0.07892 test_loss: 0.08813 \n",
      "[452/500] train_loss: 0.05377 valid_loss: 0.08650 test_loss: 0.08778 \n",
      "[453/500] train_loss: 0.05299 valid_loss: 0.07830 test_loss: 0.08752 \n",
      "[454/500] train_loss: 0.05305 valid_loss: 0.07748 test_loss: 0.08804 \n",
      "[455/500] train_loss: 0.05205 valid_loss: 0.07862 test_loss: 0.08976 \n",
      "[456/500] train_loss: 0.05379 valid_loss: 0.07788 test_loss: 0.08762 \n",
      "[457/500] train_loss: 0.05467 valid_loss: 0.07705 test_loss: 0.08884 \n",
      "[458/500] train_loss: 0.05391 valid_loss: 0.08434 test_loss: 0.08744 \n",
      "[459/500] train_loss: 0.05168 valid_loss: 0.07572 test_loss: 0.08836 \n",
      "[460/500] train_loss: 0.05174 valid_loss: 0.08053 test_loss: 0.08823 \n",
      "[461/500] train_loss: 0.05221 valid_loss: 0.07726 test_loss: 0.08968 \n",
      "[462/500] train_loss: 0.05432 valid_loss: 0.07735 test_loss: 0.09017 \n",
      "[463/500] train_loss: 0.05382 valid_loss: 0.07580 test_loss: 0.08992 \n",
      "[464/500] train_loss: 0.05095 valid_loss: 0.07705 test_loss: 0.09034 \n",
      "[465/500] train_loss: 0.05275 valid_loss: 0.07747 test_loss: 0.09138 \n",
      "[466/500] train_loss: 0.05299 valid_loss: 0.08095 test_loss: 0.09194 \n",
      "[467/500] train_loss: 0.05252 valid_loss: 0.07638 test_loss: 0.08854 \n",
      "[468/500] train_loss: 0.05274 valid_loss: 0.08361 test_loss: 0.08799 \n",
      "[469/500] train_loss: 0.05415 valid_loss: 0.08152 test_loss: 0.08799 \n",
      "[470/500] train_loss: 0.05122 valid_loss: 0.09030 test_loss: 0.08850 \n",
      "[471/500] train_loss: 0.05126 valid_loss: 0.08166 test_loss: 0.08923 \n",
      "[472/500] train_loss: 0.05305 valid_loss: 0.08257 test_loss: 0.08916 \n",
      "[473/500] train_loss: 0.05335 valid_loss: 0.07824 test_loss: 0.08939 \n",
      "[474/500] train_loss: 0.05341 valid_loss: 0.07963 test_loss: 0.08880 \n",
      "[475/500] train_loss: 0.05286 valid_loss: 0.08706 test_loss: 0.08868 \n",
      "[476/500] train_loss: 0.05094 valid_loss: 0.08494 test_loss: 0.08914 \n",
      "[477/500] train_loss: 0.05274 valid_loss: 0.08416 test_loss: 0.08927 \n",
      "[478/500] train_loss: 0.05329 valid_loss: 0.08300 test_loss: 0.08922 \n",
      "[479/500] train_loss: 0.05203 valid_loss: 0.07668 test_loss: 0.08846 \n",
      "[480/500] train_loss: 0.05305 valid_loss: 0.07791 test_loss: 0.08941 \n",
      "[481/500] train_loss: 0.05202 valid_loss: 0.08590 test_loss: 0.08959 \n",
      "[482/500] train_loss: 0.05213 valid_loss: 0.08050 test_loss: 0.09108 \n",
      "[483/500] train_loss: 0.05172 valid_loss: 0.09245 test_loss: 0.09046 \n",
      "[484/500] train_loss: 0.05189 valid_loss: 0.08833 test_loss: 0.08985 \n",
      "[485/500] train_loss: 0.05174 valid_loss: 0.09050 test_loss: 0.08902 \n",
      "[486/500] train_loss: 0.05066 valid_loss: 0.09128 test_loss: 0.09075 \n",
      "[487/500] train_loss: 0.05078 valid_loss: 0.08287 test_loss: 0.08993 \n",
      "[488/500] train_loss: 0.05261 valid_loss: 0.07649 test_loss: 0.08773 \n",
      "[489/500] train_loss: 0.05226 valid_loss: 0.10112 test_loss: 0.09060 \n",
      "[490/500] train_loss: 0.05022 valid_loss: 0.08178 test_loss: 0.08914 \n",
      "[491/500] train_loss: 0.05188 valid_loss: 0.07657 test_loss: 0.08916 \n",
      "[492/500] train_loss: 0.05136 valid_loss: 0.07784 test_loss: 0.08884 \n",
      "[493/500] train_loss: 0.05115 valid_loss: 0.07724 test_loss: 0.08907 \n",
      "[494/500] train_loss: 0.05059 valid_loss: 0.07799 test_loss: 0.08954 \n",
      "[495/500] train_loss: 0.05077 valid_loss: 0.07536 test_loss: 0.08790 \n",
      "[496/500] train_loss: 0.05094 valid_loss: 0.07559 test_loss: 0.08834 \n",
      "[497/500] train_loss: 0.05054 valid_loss: 0.07631 test_loss: 0.08820 \n",
      "[498/500] train_loss: 0.05157 valid_loss: 0.07751 test_loss: 0.08804 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/500] train_loss: 0.05029 valid_loss: 0.07893 test_loss: 0.09078 \n",
      "[500/500] train_loss: 0.05053 valid_loss: 0.07571 test_loss: 0.08964 \n",
      "TRAINING MODEL 10\n",
      "[  1/500] train_loss: 0.36269 valid_loss: 0.26740 test_loss: 0.27168 \n",
      "验证损失减少 (inf --> 0.267398). 正在保存模型...\n",
      "[  2/500] train_loss: 0.21289 valid_loss: 0.19987 test_loss: 0.20847 \n",
      "验证损失减少 (0.267398 --> 0.199873). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17296 valid_loss: 0.17506 test_loss: 0.18597 \n",
      "验证损失减少 (0.199873 --> 0.175063). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15654 valid_loss: 0.15982 test_loss: 0.17008 \n",
      "验证损失减少 (0.175063 --> 0.159816). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14856 valid_loss: 0.15021 test_loss: 0.16073 \n",
      "验证损失减少 (0.159816 --> 0.150213). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14115 valid_loss: 0.14411 test_loss: 0.15796 \n",
      "验证损失减少 (0.150213 --> 0.144109). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13652 valid_loss: 0.14050 test_loss: 0.15481 \n",
      "验证损失减少 (0.144109 --> 0.140500). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13389 valid_loss: 0.13852 test_loss: 0.15204 \n",
      "验证损失减少 (0.140500 --> 0.138516). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13079 valid_loss: 0.13621 test_loss: 0.14925 \n",
      "验证损失减少 (0.138516 --> 0.136206). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.13019 valid_loss: 0.13452 test_loss: 0.14594 \n",
      "验证损失减少 (0.136206 --> 0.134516). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12256 valid_loss: 0.13295 test_loss: 0.14382 \n",
      "验证损失减少 (0.134516 --> 0.132950). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12049 valid_loss: 0.12619 test_loss: 0.14215 \n",
      "验证损失减少 (0.132950 --> 0.126186). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12455 valid_loss: 0.12663 test_loss: 0.14233 \n",
      "[ 14/500] train_loss: 0.12038 valid_loss: 0.12366 test_loss: 0.13768 \n",
      "验证损失减少 (0.126186 --> 0.123659). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11797 valid_loss: 0.12305 test_loss: 0.13694 \n",
      "验证损失减少 (0.123659 --> 0.123050). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11808 valid_loss: 0.12091 test_loss: 0.13493 \n",
      "验证损失减少 (0.123050 --> 0.120910). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11249 valid_loss: 0.12067 test_loss: 0.13417 \n",
      "验证损失减少 (0.120910 --> 0.120673). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11490 valid_loss: 0.12065 test_loss: 0.13384 \n",
      "验证损失减少 (0.120673 --> 0.120649). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11136 valid_loss: 0.11828 test_loss: 0.13126 \n",
      "验证损失减少 (0.120649 --> 0.118279). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11018 valid_loss: 0.11668 test_loss: 0.13243 \n",
      "验证损失减少 (0.118279 --> 0.116685). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.11029 valid_loss: 0.11650 test_loss: 0.13072 \n",
      "验证损失减少 (0.116685 --> 0.116496). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.11198 valid_loss: 0.11450 test_loss: 0.12731 \n",
      "验证损失减少 (0.116496 --> 0.114502). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10887 valid_loss: 0.11586 test_loss: 0.12804 \n",
      "[ 24/500] train_loss: 0.10856 valid_loss: 0.11093 test_loss: 0.12484 \n",
      "验证损失减少 (0.114502 --> 0.110934). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10753 valid_loss: 0.11300 test_loss: 0.12460 \n",
      "[ 26/500] train_loss: 0.10834 valid_loss: 0.11170 test_loss: 0.12501 \n",
      "[ 27/500] train_loss: 0.10278 valid_loss: 0.10844 test_loss: 0.12319 \n",
      "验证损失减少 (0.110934 --> 0.108436). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10538 valid_loss: 0.10778 test_loss: 0.12254 \n",
      "验证损失减少 (0.108436 --> 0.107775). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10053 valid_loss: 0.10796 test_loss: 0.12257 \n",
      "[ 30/500] train_loss: 0.10043 valid_loss: 0.11138 test_loss: 0.12263 \n",
      "[ 31/500] train_loss: 0.10105 valid_loss: 0.11176 test_loss: 0.12386 \n",
      "[ 32/500] train_loss: 0.10202 valid_loss: 0.10835 test_loss: 0.12257 \n",
      "[ 33/500] train_loss: 0.10155 valid_loss: 0.10609 test_loss: 0.12122 \n",
      "验证损失减少 (0.107775 --> 0.106090). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.10045 valid_loss: 0.10832 test_loss: 0.11993 \n",
      "[ 35/500] train_loss: 0.10105 valid_loss: 0.10952 test_loss: 0.12104 \n",
      "[ 36/500] train_loss: 0.09844 valid_loss: 0.10616 test_loss: 0.11860 \n",
      "[ 37/500] train_loss: 0.09629 valid_loss: 0.10532 test_loss: 0.11853 \n",
      "验证损失减少 (0.106090 --> 0.105320). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.09752 valid_loss: 0.10534 test_loss: 0.12049 \n",
      "[ 39/500] train_loss: 0.09602 valid_loss: 0.10289 test_loss: 0.11733 \n",
      "验证损失减少 (0.105320 --> 0.102887). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09729 valid_loss: 0.10381 test_loss: 0.11651 \n",
      "[ 41/500] train_loss: 0.09459 valid_loss: 0.10265 test_loss: 0.11454 \n",
      "验证损失减少 (0.102887 --> 0.102649). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.09667 valid_loss: 0.10335 test_loss: 0.11632 \n",
      "[ 43/500] train_loss: 0.09402 valid_loss: 0.10138 test_loss: 0.11556 \n",
      "验证损失减少 (0.102649 --> 0.101384). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.09541 valid_loss: 0.10545 test_loss: 0.11717 \n",
      "[ 45/500] train_loss: 0.09501 valid_loss: 0.10194 test_loss: 0.11291 \n",
      "[ 46/500] train_loss: 0.09348 valid_loss: 0.09871 test_loss: 0.11322 \n",
      "验证损失减少 (0.101384 --> 0.098706). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.09450 valid_loss: 0.10096 test_loss: 0.11456 \n",
      "[ 48/500] train_loss: 0.09373 valid_loss: 0.09865 test_loss: 0.11164 \n",
      "验证损失减少 (0.098706 --> 0.098651). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09084 valid_loss: 0.10208 test_loss: 0.11393 \n",
      "[ 50/500] train_loss: 0.09379 valid_loss: 0.10007 test_loss: 0.11094 \n",
      "[ 51/500] train_loss: 0.09015 valid_loss: 0.10009 test_loss: 0.11134 \n",
      "[ 52/500] train_loss: 0.09192 valid_loss: 0.10007 test_loss: 0.11168 \n",
      "[ 53/500] train_loss: 0.09038 valid_loss: 0.10228 test_loss: 0.10925 \n",
      "[ 54/500] train_loss: 0.09003 valid_loss: 0.09845 test_loss: 0.11055 \n",
      "验证损失减少 (0.098651 --> 0.098445). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.08916 valid_loss: 0.10244 test_loss: 0.11168 \n",
      "[ 56/500] train_loss: 0.09042 valid_loss: 0.09826 test_loss: 0.10976 \n",
      "验证损失减少 (0.098445 --> 0.098263). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.08961 valid_loss: 0.09935 test_loss: 0.10877 \n",
      "[ 58/500] train_loss: 0.09067 valid_loss: 0.09518 test_loss: 0.10879 \n",
      "验证损失减少 (0.098263 --> 0.095183). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.08970 valid_loss: 0.09614 test_loss: 0.10793 \n",
      "[ 60/500] train_loss: 0.09089 valid_loss: 0.10002 test_loss: 0.10746 \n",
      "[ 61/500] train_loss: 0.08838 valid_loss: 0.09831 test_loss: 0.10862 \n",
      "[ 62/500] train_loss: 0.08925 valid_loss: 0.09571 test_loss: 0.10761 \n",
      "[ 63/500] train_loss: 0.08825 valid_loss: 0.09597 test_loss: 0.10763 \n",
      "[ 64/500] train_loss: 0.08638 valid_loss: 0.09558 test_loss: 0.10853 \n",
      "[ 65/500] train_loss: 0.08620 valid_loss: 0.09585 test_loss: 0.10906 \n",
      "[ 66/500] train_loss: 0.08459 valid_loss: 0.09509 test_loss: 0.10634 \n",
      "验证损失减少 (0.095183 --> 0.095088). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.08497 valid_loss: 0.09402 test_loss: 0.10541 \n",
      "验证损失减少 (0.095088 --> 0.094022). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.08726 valid_loss: 0.09571 test_loss: 0.10487 \n",
      "[ 69/500] train_loss: 0.08596 valid_loss: 0.09341 test_loss: 0.10569 \n",
      "验证损失减少 (0.094022 --> 0.093414). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.08515 valid_loss: 0.09220 test_loss: 0.10427 \n",
      "验证损失减少 (0.093414 --> 0.092204). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.08314 valid_loss: 0.09373 test_loss: 0.10514 \n",
      "[ 72/500] train_loss: 0.08404 valid_loss: 0.09570 test_loss: 0.10501 \n",
      "[ 73/500] train_loss: 0.08505 valid_loss: 0.09735 test_loss: 0.10424 \n",
      "[ 74/500] train_loss: 0.08642 valid_loss: 0.09581 test_loss: 0.10512 \n",
      "[ 75/500] train_loss: 0.08350 valid_loss: 0.09322 test_loss: 0.10254 \n",
      "[ 76/500] train_loss: 0.08495 valid_loss: 0.09375 test_loss: 0.10449 \n",
      "[ 77/500] train_loss: 0.08267 valid_loss: 0.09710 test_loss: 0.10678 \n",
      "[ 78/500] train_loss: 0.08245 valid_loss: 0.09186 test_loss: 0.10472 \n",
      "验证损失减少 (0.092204 --> 0.091860). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08204 valid_loss: 0.09122 test_loss: 0.10338 \n",
      "验证损失减少 (0.091860 --> 0.091219). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.08268 valid_loss: 0.09105 test_loss: 0.10328 \n",
      "验证损失减少 (0.091219 --> 0.091050). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.08240 valid_loss: 0.09377 test_loss: 0.10396 \n",
      "[ 82/500] train_loss: 0.08217 valid_loss: 0.09355 test_loss: 0.10312 \n",
      "[ 83/500] train_loss: 0.08322 valid_loss: 0.09162 test_loss: 0.10429 \n",
      "[ 84/500] train_loss: 0.08402 valid_loss: 0.09233 test_loss: 0.10305 \n",
      "[ 85/500] train_loss: 0.08079 valid_loss: 0.09309 test_loss: 0.10151 \n",
      "[ 86/500] train_loss: 0.08203 valid_loss: 0.09014 test_loss: 0.10244 \n",
      "验证损失减少 (0.091050 --> 0.090138). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.08281 valid_loss: 0.08890 test_loss: 0.10141 \n",
      "验证损失减少 (0.090138 --> 0.088899). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.08414 valid_loss: 0.08987 test_loss: 0.10298 \n",
      "[ 89/500] train_loss: 0.08075 valid_loss: 0.09250 test_loss: 0.10224 \n",
      "[ 90/500] train_loss: 0.07910 valid_loss: 0.08954 test_loss: 0.10122 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 91/500] train_loss: 0.08090 valid_loss: 0.08851 test_loss: 0.10104 \n",
      "验证损失减少 (0.088899 --> 0.088514). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.08023 valid_loss: 0.09192 test_loss: 0.10187 \n",
      "[ 93/500] train_loss: 0.07815 valid_loss: 0.09134 test_loss: 0.10250 \n",
      "[ 94/500] train_loss: 0.08111 valid_loss: 0.09108 test_loss: 0.10092 \n",
      "[ 95/500] train_loss: 0.07859 valid_loss: 0.09386 test_loss: 0.10085 \n",
      "[ 96/500] train_loss: 0.07769 valid_loss: 0.08941 test_loss: 0.10024 \n",
      "[ 97/500] train_loss: 0.08072 valid_loss: 0.09189 test_loss: 0.10018 \n",
      "[ 98/500] train_loss: 0.08093 valid_loss: 0.08814 test_loss: 0.10018 \n",
      "验证损失减少 (0.088514 --> 0.088144). 正在保存模型...\n",
      "[ 99/500] train_loss: 0.07975 valid_loss: 0.09391 test_loss: 0.10353 \n",
      "[100/500] train_loss: 0.08211 valid_loss: 0.08932 test_loss: 0.09868 \n",
      "[101/500] train_loss: 0.07991 valid_loss: 0.08805 test_loss: 0.09961 \n",
      "验证损失减少 (0.088144 --> 0.088051). 正在保存模型...\n",
      "[102/500] train_loss: 0.07747 valid_loss: 0.08916 test_loss: 0.09922 \n",
      "[103/500] train_loss: 0.07838 valid_loss: 0.08737 test_loss: 0.09888 \n",
      "验证损失减少 (0.088051 --> 0.087367). 正在保存模型...\n",
      "[104/500] train_loss: 0.07917 valid_loss: 0.08953 test_loss: 0.10194 \n",
      "[105/500] train_loss: 0.07949 valid_loss: 0.08831 test_loss: 0.09853 \n",
      "[106/500] train_loss: 0.08018 valid_loss: 0.08652 test_loss: 0.09816 \n",
      "验证损失减少 (0.087367 --> 0.086523). 正在保存模型...\n",
      "[107/500] train_loss: 0.08129 valid_loss: 0.09085 test_loss: 0.09825 \n",
      "[108/500] train_loss: 0.07907 valid_loss: 0.08820 test_loss: 0.09986 \n",
      "[109/500] train_loss: 0.07784 valid_loss: 0.08829 test_loss: 0.09886 \n",
      "[110/500] train_loss: 0.07768 valid_loss: 0.09124 test_loss: 0.10081 \n",
      "[111/500] train_loss: 0.07775 valid_loss: 0.08627 test_loss: 0.09843 \n",
      "验证损失减少 (0.086523 --> 0.086273). 正在保存模型...\n",
      "[112/500] train_loss: 0.07821 valid_loss: 0.08828 test_loss: 0.09769 \n",
      "[113/500] train_loss: 0.07474 valid_loss: 0.08885 test_loss: 0.09934 \n",
      "[114/500] train_loss: 0.07505 valid_loss: 0.08514 test_loss: 0.09646 \n",
      "验证损失减少 (0.086273 --> 0.085135). 正在保存模型...\n",
      "[115/500] train_loss: 0.07518 valid_loss: 0.08770 test_loss: 0.09817 \n",
      "[116/500] train_loss: 0.07520 valid_loss: 0.08571 test_loss: 0.09750 \n",
      "[117/500] train_loss: 0.07566 valid_loss: 0.08860 test_loss: 0.09778 \n",
      "[118/500] train_loss: 0.07542 valid_loss: 0.08572 test_loss: 0.09830 \n",
      "[119/500] train_loss: 0.07630 valid_loss: 0.08600 test_loss: 0.09659 \n",
      "[120/500] train_loss: 0.07712 valid_loss: 0.08492 test_loss: 0.09677 \n",
      "验证损失减少 (0.085135 --> 0.084918). 正在保存模型...\n",
      "[121/500] train_loss: 0.07774 valid_loss: 0.08566 test_loss: 0.09729 \n",
      "[122/500] train_loss: 0.07577 valid_loss: 0.08636 test_loss: 0.09635 \n",
      "[123/500] train_loss: 0.07362 valid_loss: 0.08579 test_loss: 0.09601 \n",
      "[124/500] train_loss: 0.07373 valid_loss: 0.08394 test_loss: 0.09713 \n",
      "验证损失减少 (0.084918 --> 0.083938). 正在保存模型...\n",
      "[125/500] train_loss: 0.07428 valid_loss: 0.08664 test_loss: 0.09611 \n",
      "[126/500] train_loss: 0.07551 valid_loss: 0.08587 test_loss: 0.09640 \n",
      "[127/500] train_loss: 0.07339 valid_loss: 0.08637 test_loss: 0.09558 \n",
      "[128/500] train_loss: 0.07393 valid_loss: 0.08474 test_loss: 0.09654 \n",
      "[129/500] train_loss: 0.07237 valid_loss: 0.08612 test_loss: 0.09693 \n",
      "[130/500] train_loss: 0.07620 valid_loss: 0.09042 test_loss: 0.09672 \n",
      "[131/500] train_loss: 0.07621 valid_loss: 0.08400 test_loss: 0.09530 \n",
      "[132/500] train_loss: 0.07355 valid_loss: 0.08512 test_loss: 0.09478 \n",
      "[133/500] train_loss: 0.07178 valid_loss: 0.08607 test_loss: 0.09684 \n",
      "[134/500] train_loss: 0.07208 valid_loss: 0.08582 test_loss: 0.09465 \n",
      "[135/500] train_loss: 0.07384 valid_loss: 0.08510 test_loss: 0.09544 \n",
      "[136/500] train_loss: 0.07490 valid_loss: 0.08278 test_loss: 0.09555 \n",
      "验证损失减少 (0.083938 --> 0.082782). 正在保存模型...\n",
      "[137/500] train_loss: 0.07413 valid_loss: 0.08431 test_loss: 0.09580 \n",
      "[138/500] train_loss: 0.07265 valid_loss: 0.08487 test_loss: 0.09749 \n",
      "[139/500] train_loss: 0.07380 valid_loss: 0.08486 test_loss: 0.09506 \n",
      "[140/500] train_loss: 0.07288 valid_loss: 0.08264 test_loss: 0.09561 \n",
      "验证损失减少 (0.082782 --> 0.082638). 正在保存模型...\n",
      "[141/500] train_loss: 0.07177 valid_loss: 0.08212 test_loss: 0.09479 \n",
      "验证损失减少 (0.082638 --> 0.082122). 正在保存模型...\n",
      "[142/500] train_loss: 0.07319 valid_loss: 0.08378 test_loss: 0.09500 \n",
      "[143/500] train_loss: 0.07189 valid_loss: 0.08144 test_loss: 0.09328 \n",
      "验证损失减少 (0.082122 --> 0.081439). 正在保存模型...\n",
      "[144/500] train_loss: 0.07208 valid_loss: 0.08539 test_loss: 0.09380 \n",
      "[145/500] train_loss: 0.07278 valid_loss: 0.08562 test_loss: 0.09417 \n",
      "[146/500] train_loss: 0.07323 valid_loss: 0.08392 test_loss: 0.09655 \n",
      "[147/500] train_loss: 0.07179 valid_loss: 0.08326 test_loss: 0.09502 \n",
      "[148/500] train_loss: 0.07211 valid_loss: 0.08496 test_loss: 0.09459 \n",
      "[149/500] train_loss: 0.07278 valid_loss: 0.08445 test_loss: 0.09565 \n",
      "[150/500] train_loss: 0.07247 valid_loss: 0.08086 test_loss: 0.09394 \n",
      "验证损失减少 (0.081439 --> 0.080862). 正在保存模型...\n",
      "[151/500] train_loss: 0.07167 valid_loss: 0.08403 test_loss: 0.09420 \n",
      "[152/500] train_loss: 0.07115 valid_loss: 0.08307 test_loss: 0.09426 \n",
      "[153/500] train_loss: 0.06963 valid_loss: 0.08153 test_loss: 0.09263 \n",
      "[154/500] train_loss: 0.06902 valid_loss: 0.08144 test_loss: 0.09513 \n",
      "[155/500] train_loss: 0.07227 valid_loss: 0.08164 test_loss: 0.09381 \n",
      "[156/500] train_loss: 0.07057 valid_loss: 0.08541 test_loss: 0.09378 \n",
      "[157/500] train_loss: 0.07068 valid_loss: 0.08134 test_loss: 0.09278 \n",
      "[158/500] train_loss: 0.07129 valid_loss: 0.08481 test_loss: 0.09278 \n",
      "[159/500] train_loss: 0.07068 valid_loss: 0.08152 test_loss: 0.09289 \n",
      "[160/500] train_loss: 0.07058 valid_loss: 0.08361 test_loss: 0.09348 \n",
      "[161/500] train_loss: 0.07022 valid_loss: 0.08168 test_loss: 0.09352 \n",
      "[162/500] train_loss: 0.06976 valid_loss: 0.08248 test_loss: 0.09485 \n",
      "[163/500] train_loss: 0.07059 valid_loss: 0.08130 test_loss: 0.09274 \n",
      "[164/500] train_loss: 0.07095 valid_loss: 0.08130 test_loss: 0.09293 \n",
      "[165/500] train_loss: 0.07079 valid_loss: 0.08196 test_loss: 0.09359 \n",
      "[166/500] train_loss: 0.07107 valid_loss: 0.08231 test_loss: 0.09477 \n",
      "[167/500] train_loss: 0.06945 valid_loss: 0.08006 test_loss: 0.09402 \n",
      "验证损失减少 (0.080862 --> 0.080061). 正在保存模型...\n",
      "[168/500] train_loss: 0.06861 valid_loss: 0.08201 test_loss: 0.09273 \n",
      "[169/500] train_loss: 0.06848 valid_loss: 0.08275 test_loss: 0.09322 \n",
      "[170/500] train_loss: 0.06850 valid_loss: 0.08460 test_loss: 0.09499 \n",
      "[171/500] train_loss: 0.07120 valid_loss: 0.08195 test_loss: 0.09375 \n",
      "[172/500] train_loss: 0.07090 valid_loss: 0.07848 test_loss: 0.09226 \n",
      "验证损失减少 (0.080061 --> 0.078483). 正在保存模型...\n",
      "[173/500] train_loss: 0.06962 valid_loss: 0.08299 test_loss: 0.09269 \n",
      "[174/500] train_loss: 0.06690 valid_loss: 0.07975 test_loss: 0.09209 \n",
      "[175/500] train_loss: 0.07002 valid_loss: 0.08055 test_loss: 0.09137 \n",
      "[176/500] train_loss: 0.06853 valid_loss: 0.08020 test_loss: 0.09327 \n",
      "[177/500] train_loss: 0.06875 valid_loss: 0.07949 test_loss: 0.09232 \n",
      "[178/500] train_loss: 0.06936 valid_loss: 0.07931 test_loss: 0.09369 \n",
      "[179/500] train_loss: 0.06970 valid_loss: 0.08147 test_loss: 0.09490 \n",
      "[180/500] train_loss: 0.06972 valid_loss: 0.08340 test_loss: 0.09273 \n",
      "[181/500] train_loss: 0.06617 valid_loss: 0.08405 test_loss: 0.09508 \n",
      "[182/500] train_loss: 0.06817 valid_loss: 0.08075 test_loss: 0.09273 \n",
      "[183/500] train_loss: 0.06792 valid_loss: 0.07984 test_loss: 0.09173 \n",
      "[184/500] train_loss: 0.06707 valid_loss: 0.08339 test_loss: 0.09374 \n",
      "[185/500] train_loss: 0.06785 valid_loss: 0.08025 test_loss: 0.09254 \n",
      "[186/500] train_loss: 0.06620 valid_loss: 0.08392 test_loss: 0.09447 \n",
      "[187/500] train_loss: 0.06845 valid_loss: 0.08402 test_loss: 0.09240 \n",
      "[188/500] train_loss: 0.06560 valid_loss: 0.07982 test_loss: 0.09279 \n",
      "[189/500] train_loss: 0.06932 valid_loss: 0.08021 test_loss: 0.09392 \n",
      "[190/500] train_loss: 0.07036 valid_loss: 0.08053 test_loss: 0.09256 \n",
      "[191/500] train_loss: 0.06680 valid_loss: 0.08392 test_loss: 0.09199 \n",
      "[192/500] train_loss: 0.06747 valid_loss: 0.08028 test_loss: 0.09279 \n",
      "[193/500] train_loss: 0.06670 valid_loss: 0.08337 test_loss: 0.09266 \n",
      "[194/500] train_loss: 0.06636 valid_loss: 0.08040 test_loss: 0.09289 \n",
      "[195/500] train_loss: 0.06717 valid_loss: 0.08243 test_loss: 0.09285 \n",
      "[196/500] train_loss: 0.06556 valid_loss: 0.08175 test_loss: 0.09083 \n",
      "[197/500] train_loss: 0.06456 valid_loss: 0.08159 test_loss: 0.09000 \n",
      "[198/500] train_loss: 0.06537 valid_loss: 0.08236 test_loss: 0.09225 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/500] train_loss: 0.06620 valid_loss: 0.07964 test_loss: 0.09139 \n",
      "[200/500] train_loss: 0.06561 valid_loss: 0.08162 test_loss: 0.09042 \n",
      "[201/500] train_loss: 0.06883 valid_loss: 0.08174 test_loss: 0.09184 \n",
      "[202/500] train_loss: 0.06586 valid_loss: 0.08136 test_loss: 0.09233 \n",
      "[203/500] train_loss: 0.06564 valid_loss: 0.07902 test_loss: 0.09304 \n",
      "[204/500] train_loss: 0.06677 valid_loss: 0.07984 test_loss: 0.09259 \n",
      "[205/500] train_loss: 0.06589 valid_loss: 0.07804 test_loss: 0.09124 \n",
      "验证损失减少 (0.078483 --> 0.078041). 正在保存模型...\n",
      "[206/500] train_loss: 0.06466 valid_loss: 0.08034 test_loss: 0.09309 \n",
      "[207/500] train_loss: 0.06435 valid_loss: 0.08166 test_loss: 0.09369 \n",
      "[208/500] train_loss: 0.06511 valid_loss: 0.08100 test_loss: 0.09479 \n",
      "[209/500] train_loss: 0.06371 valid_loss: 0.08008 test_loss: 0.09088 \n",
      "[210/500] train_loss: 0.06639 valid_loss: 0.08279 test_loss: 0.09002 \n",
      "[211/500] train_loss: 0.06399 valid_loss: 0.07792 test_loss: 0.09127 \n",
      "验证损失减少 (0.078041 --> 0.077922). 正在保存模型...\n",
      "[212/500] train_loss: 0.06627 valid_loss: 0.07991 test_loss: 0.09291 \n",
      "[213/500] train_loss: 0.06561 valid_loss: 0.07983 test_loss: 0.08941 \n",
      "[214/500] train_loss: 0.06459 valid_loss: 0.07818 test_loss: 0.09081 \n",
      "[215/500] train_loss: 0.06328 valid_loss: 0.07998 test_loss: 0.09348 \n",
      "[216/500] train_loss: 0.06509 valid_loss: 0.07822 test_loss: 0.08988 \n",
      "[217/500] train_loss: 0.06461 valid_loss: 0.07994 test_loss: 0.09010 \n",
      "[218/500] train_loss: 0.06539 valid_loss: 0.08191 test_loss: 0.09014 \n",
      "[219/500] train_loss: 0.06481 valid_loss: 0.08271 test_loss: 0.08952 \n",
      "[220/500] train_loss: 0.06389 valid_loss: 0.07836 test_loss: 0.08974 \n",
      "[221/500] train_loss: 0.06615 valid_loss: 0.08427 test_loss: 0.08945 \n",
      "[222/500] train_loss: 0.06502 valid_loss: 0.08195 test_loss: 0.09153 \n",
      "[223/500] train_loss: 0.06620 valid_loss: 0.07833 test_loss: 0.09117 \n",
      "[224/500] train_loss: 0.06475 valid_loss: 0.07926 test_loss: 0.09174 \n",
      "[225/500] train_loss: 0.06429 valid_loss: 0.07907 test_loss: 0.09175 \n",
      "[226/500] train_loss: 0.06390 valid_loss: 0.07779 test_loss: 0.09080 \n",
      "验证损失减少 (0.077922 --> 0.077787). 正在保存模型...\n",
      "[227/500] train_loss: 0.06406 valid_loss: 0.07908 test_loss: 0.09165 \n",
      "[228/500] train_loss: 0.06275 valid_loss: 0.07925 test_loss: 0.09113 \n",
      "[229/500] train_loss: 0.06369 valid_loss: 0.07645 test_loss: 0.08983 \n",
      "验证损失减少 (0.077787 --> 0.076453). 正在保存模型...\n",
      "[230/500] train_loss: 0.06313 valid_loss: 0.08045 test_loss: 0.09285 \n",
      "[231/500] train_loss: 0.06314 valid_loss: 0.07601 test_loss: 0.08913 \n",
      "验证损失减少 (0.076453 --> 0.076010). 正在保存模型...\n",
      "[232/500] train_loss: 0.06212 valid_loss: 0.07761 test_loss: 0.08954 \n",
      "[233/500] train_loss: 0.06400 valid_loss: 0.07843 test_loss: 0.08912 \n",
      "[234/500] train_loss: 0.06340 valid_loss: 0.07778 test_loss: 0.08900 \n",
      "[235/500] train_loss: 0.06269 valid_loss: 0.08086 test_loss: 0.09113 \n",
      "[236/500] train_loss: 0.06645 valid_loss: 0.07928 test_loss: 0.09056 \n",
      "[237/500] train_loss: 0.06313 valid_loss: 0.07982 test_loss: 0.08826 \n",
      "[238/500] train_loss: 0.06225 valid_loss: 0.07982 test_loss: 0.08864 \n",
      "[239/500] train_loss: 0.06350 valid_loss: 0.08080 test_loss: 0.08957 \n",
      "[240/500] train_loss: 0.06355 valid_loss: 0.08032 test_loss: 0.08947 \n",
      "[241/500] train_loss: 0.06182 valid_loss: 0.07880 test_loss: 0.08899 \n",
      "[242/500] train_loss: 0.06277 valid_loss: 0.08073 test_loss: 0.09168 \n",
      "[243/500] train_loss: 0.06090 valid_loss: 0.07934 test_loss: 0.09062 \n",
      "[244/500] train_loss: 0.06182 valid_loss: 0.07970 test_loss: 0.09043 \n",
      "[245/500] train_loss: 0.06220 valid_loss: 0.07873 test_loss: 0.08971 \n",
      "[246/500] train_loss: 0.06395 valid_loss: 0.07980 test_loss: 0.08991 \n",
      "[247/500] train_loss: 0.06132 valid_loss: 0.07810 test_loss: 0.09021 \n",
      "[248/500] train_loss: 0.06222 valid_loss: 0.08030 test_loss: 0.08972 \n",
      "[249/500] train_loss: 0.06317 valid_loss: 0.07754 test_loss: 0.08971 \n",
      "[250/500] train_loss: 0.06424 valid_loss: 0.07926 test_loss: 0.08971 \n",
      "[251/500] train_loss: 0.06404 valid_loss: 0.07791 test_loss: 0.08956 \n",
      "[252/500] train_loss: 0.06233 valid_loss: 0.07928 test_loss: 0.09086 \n",
      "[253/500] train_loss: 0.06296 valid_loss: 0.07858 test_loss: 0.08940 \n",
      "[254/500] train_loss: 0.06264 valid_loss: 0.07731 test_loss: 0.08949 \n",
      "[255/500] train_loss: 0.06310 valid_loss: 0.07806 test_loss: 0.08969 \n",
      "[256/500] train_loss: 0.06183 valid_loss: 0.08345 test_loss: 0.08844 \n",
      "[257/500] train_loss: 0.05987 valid_loss: 0.08595 test_loss: 0.09069 \n",
      "[258/500] train_loss: 0.06381 valid_loss: 0.07689 test_loss: 0.08926 \n",
      "[259/500] train_loss: 0.06221 valid_loss: 0.07767 test_loss: 0.08863 \n",
      "[260/500] train_loss: 0.06247 valid_loss: 0.07774 test_loss: 0.08894 \n",
      "[261/500] train_loss: 0.06103 valid_loss: 0.07717 test_loss: 0.08827 \n",
      "[262/500] train_loss: 0.06135 valid_loss: 0.07898 test_loss: 0.08831 \n",
      "[263/500] train_loss: 0.06364 valid_loss: 0.07694 test_loss: 0.08913 \n",
      "[264/500] train_loss: 0.06111 valid_loss: 0.07870 test_loss: 0.09046 \n",
      "[265/500] train_loss: 0.06118 valid_loss: 0.08078 test_loss: 0.08925 \n",
      "[266/500] train_loss: 0.06011 valid_loss: 0.07973 test_loss: 0.09208 \n",
      "[267/500] train_loss: 0.06141 valid_loss: 0.07744 test_loss: 0.08988 \n",
      "[268/500] train_loss: 0.06074 valid_loss: 0.07643 test_loss: 0.08900 \n",
      "[269/500] train_loss: 0.06195 valid_loss: 0.07719 test_loss: 0.08845 \n",
      "[270/500] train_loss: 0.06106 valid_loss: 0.07756 test_loss: 0.08978 \n",
      "[271/500] train_loss: 0.06231 valid_loss: 0.07821 test_loss: 0.09046 \n",
      "[272/500] train_loss: 0.06274 valid_loss: 0.07775 test_loss: 0.08972 \n",
      "[273/500] train_loss: 0.06030 valid_loss: 0.07644 test_loss: 0.08950 \n",
      "[274/500] train_loss: 0.06041 valid_loss: 0.07889 test_loss: 0.08917 \n",
      "[275/500] train_loss: 0.06220 valid_loss: 0.07744 test_loss: 0.08908 \n",
      "[276/500] train_loss: 0.06046 valid_loss: 0.07729 test_loss: 0.08869 \n",
      "[277/500] train_loss: 0.05953 valid_loss: 0.07699 test_loss: 0.08924 \n",
      "[278/500] train_loss: 0.05956 valid_loss: 0.07796 test_loss: 0.08911 \n",
      "[279/500] train_loss: 0.06250 valid_loss: 0.07816 test_loss: 0.08865 \n",
      "[280/500] train_loss: 0.06073 valid_loss: 0.07741 test_loss: 0.08808 \n",
      "[281/500] train_loss: 0.06085 valid_loss: 0.07772 test_loss: 0.09023 \n",
      "[282/500] train_loss: 0.06056 valid_loss: 0.07691 test_loss: 0.08911 \n",
      "[283/500] train_loss: 0.06148 valid_loss: 0.07892 test_loss: 0.08896 \n",
      "[284/500] train_loss: 0.06191 valid_loss: 0.07787 test_loss: 0.08954 \n",
      "[285/500] train_loss: 0.06171 valid_loss: 0.07957 test_loss: 0.09097 \n",
      "[286/500] train_loss: 0.06217 valid_loss: 0.07946 test_loss: 0.08948 \n",
      "[287/500] train_loss: 0.06101 valid_loss: 0.07807 test_loss: 0.09057 \n",
      "[288/500] train_loss: 0.06111 valid_loss: 0.07677 test_loss: 0.08782 \n",
      "[289/500] train_loss: 0.06073 valid_loss: 0.07741 test_loss: 0.08719 \n",
      "[290/500] train_loss: 0.05972 valid_loss: 0.07941 test_loss: 0.08983 \n",
      "[291/500] train_loss: 0.05978 valid_loss: 0.07808 test_loss: 0.08958 \n",
      "[292/500] train_loss: 0.06019 valid_loss: 0.07791 test_loss: 0.08932 \n",
      "[293/500] train_loss: 0.05956 valid_loss: 0.07825 test_loss: 0.08930 \n",
      "[294/500] train_loss: 0.05979 valid_loss: 0.07801 test_loss: 0.08826 \n",
      "[295/500] train_loss: 0.05975 valid_loss: 0.07675 test_loss: 0.08736 \n",
      "[296/500] train_loss: 0.05954 valid_loss: 0.07939 test_loss: 0.08868 \n",
      "[297/500] train_loss: 0.05986 valid_loss: 0.07735 test_loss: 0.08928 \n",
      "[298/500] train_loss: 0.06025 valid_loss: 0.07786 test_loss: 0.08903 \n",
      "[299/500] train_loss: 0.05863 valid_loss: 0.07924 test_loss: 0.08934 \n",
      "[300/500] train_loss: 0.05897 valid_loss: 0.07741 test_loss: 0.09001 \n",
      "[301/500] train_loss: 0.06085 valid_loss: 0.07582 test_loss: 0.08897 \n",
      "验证损失减少 (0.076010 --> 0.075823). 正在保存模型...\n",
      "[302/500] train_loss: 0.05949 valid_loss: 0.07633 test_loss: 0.08849 \n",
      "[303/500] train_loss: 0.05906 valid_loss: 0.07997 test_loss: 0.08818 \n",
      "[304/500] train_loss: 0.06059 valid_loss: 0.07831 test_loss: 0.08952 \n",
      "[305/500] train_loss: 0.06087 valid_loss: 0.07688 test_loss: 0.08815 \n",
      "[306/500] train_loss: 0.05930 valid_loss: 0.07689 test_loss: 0.08788 \n",
      "[307/500] train_loss: 0.05762 valid_loss: 0.07918 test_loss: 0.09081 \n",
      "[308/500] train_loss: 0.05923 valid_loss: 0.07569 test_loss: 0.09030 \n",
      "验证损失减少 (0.075823 --> 0.075690). 正在保存模型...\n",
      "[309/500] train_loss: 0.05930 valid_loss: 0.07618 test_loss: 0.08804 \n",
      "[310/500] train_loss: 0.05863 valid_loss: 0.07835 test_loss: 0.08776 \n",
      "[311/500] train_loss: 0.05756 valid_loss: 0.07789 test_loss: 0.08980 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312/500] train_loss: 0.05915 valid_loss: 0.07623 test_loss: 0.08926 \n",
      "[313/500] train_loss: 0.05774 valid_loss: 0.07583 test_loss: 0.08834 \n",
      "[314/500] train_loss: 0.05899 valid_loss: 0.08195 test_loss: 0.08878 \n",
      "[315/500] train_loss: 0.05787 valid_loss: 0.08541 test_loss: 0.08812 \n",
      "[316/500] train_loss: 0.05750 valid_loss: 0.07815 test_loss: 0.08956 \n",
      "[317/500] train_loss: 0.05725 valid_loss: 0.07763 test_loss: 0.08890 \n",
      "[318/500] train_loss: 0.05745 valid_loss: 0.07612 test_loss: 0.08788 \n",
      "[319/500] train_loss: 0.05735 valid_loss: 0.09009 test_loss: 0.08787 \n",
      "[320/500] train_loss: 0.05676 valid_loss: 0.08741 test_loss: 0.08884 \n",
      "[321/500] train_loss: 0.05839 valid_loss: 0.08932 test_loss: 0.08889 \n",
      "[322/500] train_loss: 0.05731 valid_loss: 0.07659 test_loss: 0.08866 \n",
      "[323/500] train_loss: 0.05753 valid_loss: 0.07556 test_loss: 0.08840 \n",
      "验证损失减少 (0.075690 --> 0.075559). 正在保存模型...\n",
      "[324/500] train_loss: 0.05626 valid_loss: 0.07706 test_loss: 0.08973 \n",
      "[325/500] train_loss: 0.05780 valid_loss: 0.08363 test_loss: 0.08974 \n",
      "[326/500] train_loss: 0.05794 valid_loss: 0.07867 test_loss: 0.08785 \n",
      "[327/500] train_loss: 0.05708 valid_loss: 0.07718 test_loss: 0.08811 \n",
      "[328/500] train_loss: 0.05691 valid_loss: 0.08077 test_loss: 0.08756 \n",
      "[329/500] train_loss: 0.05811 valid_loss: 0.07806 test_loss: 0.09042 \n",
      "[330/500] train_loss: 0.05860 valid_loss: 0.07729 test_loss: 0.08803 \n",
      "[331/500] train_loss: 0.05714 valid_loss: 0.07668 test_loss: 0.08863 \n",
      "[332/500] train_loss: 0.05843 valid_loss: 0.07572 test_loss: 0.08775 \n",
      "[333/500] train_loss: 0.05637 valid_loss: 0.08038 test_loss: 0.08874 \n",
      "[334/500] train_loss: 0.05997 valid_loss: 0.07778 test_loss: 0.09002 \n",
      "[335/500] train_loss: 0.05616 valid_loss: 0.07621 test_loss: 0.08819 \n",
      "[336/500] train_loss: 0.05651 valid_loss: 0.07624 test_loss: 0.08932 \n",
      "[337/500] train_loss: 0.05788 valid_loss: 0.07632 test_loss: 0.08811 \n",
      "[338/500] train_loss: 0.05790 valid_loss: 0.07635 test_loss: 0.08937 \n",
      "[339/500] train_loss: 0.06021 valid_loss: 0.07804 test_loss: 0.08942 \n",
      "[340/500] train_loss: 0.05676 valid_loss: 0.08186 test_loss: 0.08995 \n",
      "[341/500] train_loss: 0.05780 valid_loss: 0.07742 test_loss: 0.08919 \n",
      "[342/500] train_loss: 0.05857 valid_loss: 0.07791 test_loss: 0.08985 \n",
      "[343/500] train_loss: 0.05594 valid_loss: 0.07808 test_loss: 0.09140 \n",
      "[344/500] train_loss: 0.05667 valid_loss: 0.07701 test_loss: 0.08916 \n",
      "[345/500] train_loss: 0.05721 valid_loss: 0.07636 test_loss: 0.08967 \n",
      "[346/500] train_loss: 0.05801 valid_loss: 0.07713 test_loss: 0.08787 \n",
      "[347/500] train_loss: 0.05598 valid_loss: 0.07689 test_loss: 0.08865 \n",
      "[348/500] train_loss: 0.05498 valid_loss: 0.07658 test_loss: 0.08715 \n",
      "[349/500] train_loss: 0.05784 valid_loss: 0.07643 test_loss: 0.08811 \n",
      "[350/500] train_loss: 0.05657 valid_loss: 0.07655 test_loss: 0.08862 \n",
      "[351/500] train_loss: 0.05742 valid_loss: 0.07496 test_loss: 0.08739 \n",
      "验证损失减少 (0.075559 --> 0.074962). 正在保存模型...\n",
      "[352/500] train_loss: 0.05586 valid_loss: 0.07643 test_loss: 0.08792 \n",
      "[353/500] train_loss: 0.05667 valid_loss: 0.07570 test_loss: 0.08800 \n",
      "[354/500] train_loss: 0.05691 valid_loss: 0.07536 test_loss: 0.08673 \n",
      "[355/500] train_loss: 0.05638 valid_loss: 0.07538 test_loss: 0.08768 \n",
      "[356/500] train_loss: 0.05657 valid_loss: 0.08634 test_loss: 0.09149 \n",
      "[357/500] train_loss: 0.05586 valid_loss: 0.07792 test_loss: 0.08699 \n",
      "[358/500] train_loss: 0.05644 valid_loss: 0.07742 test_loss: 0.09044 \n",
      "[359/500] train_loss: 0.05537 valid_loss: 0.07604 test_loss: 0.08874 \n",
      "[360/500] train_loss: 0.05770 valid_loss: 0.07638 test_loss: 0.08969 \n",
      "[361/500] train_loss: 0.05590 valid_loss: 0.07741 test_loss: 0.08863 \n",
      "[362/500] train_loss: 0.05529 valid_loss: 0.07705 test_loss: 0.08884 \n",
      "[363/500] train_loss: 0.05731 valid_loss: 0.07764 test_loss: 0.08857 \n",
      "[364/500] train_loss: 0.05724 valid_loss: 0.07719 test_loss: 0.08966 \n",
      "[365/500] train_loss: 0.05626 valid_loss: 0.07789 test_loss: 0.08824 \n",
      "[366/500] train_loss: 0.05643 valid_loss: 0.07683 test_loss: 0.08835 \n",
      "[367/500] train_loss: 0.05560 valid_loss: 0.07436 test_loss: 0.08753 \n",
      "验证损失减少 (0.074962 --> 0.074359). 正在保存模型...\n",
      "[368/500] train_loss: 0.05441 valid_loss: 0.07547 test_loss: 0.08814 \n",
      "[369/500] train_loss: 0.05427 valid_loss: 0.07631 test_loss: 0.08975 \n",
      "[370/500] train_loss: 0.05660 valid_loss: 0.07591 test_loss: 0.08780 \n",
      "[371/500] train_loss: 0.05534 valid_loss: 0.07704 test_loss: 0.08750 \n",
      "[372/500] train_loss: 0.05619 valid_loss: 0.07550 test_loss: 0.08901 \n",
      "[373/500] train_loss: 0.05653 valid_loss: 0.07851 test_loss: 0.09074 \n",
      "[374/500] train_loss: 0.05580 valid_loss: 0.07733 test_loss: 0.08954 \n",
      "[375/500] train_loss: 0.05546 valid_loss: 0.07567 test_loss: 0.08786 \n",
      "[376/500] train_loss: 0.05430 valid_loss: 0.07691 test_loss: 0.08875 \n",
      "[377/500] train_loss: 0.05630 valid_loss: 0.07531 test_loss: 0.08753 \n",
      "[378/500] train_loss: 0.05508 valid_loss: 0.07813 test_loss: 0.08915 \n",
      "[379/500] train_loss: 0.05522 valid_loss: 0.07566 test_loss: 0.08786 \n",
      "[380/500] train_loss: 0.05471 valid_loss: 0.07553 test_loss: 0.08731 \n",
      "[381/500] train_loss: 0.05375 valid_loss: 0.07653 test_loss: 0.09109 \n",
      "[382/500] train_loss: 0.05535 valid_loss: 0.07373 test_loss: 0.08727 \n",
      "验证损失减少 (0.074359 --> 0.073734). 正在保存模型...\n",
      "[383/500] train_loss: 0.05611 valid_loss: 0.07520 test_loss: 0.08888 \n",
      "[384/500] train_loss: 0.05673 valid_loss: 0.07654 test_loss: 0.08864 \n",
      "[385/500] train_loss: 0.05618 valid_loss: 0.08168 test_loss: 0.08916 \n",
      "[386/500] train_loss: 0.05633 valid_loss: 0.07607 test_loss: 0.08886 \n",
      "[387/500] train_loss: 0.05517 valid_loss: 0.07583 test_loss: 0.08860 \n",
      "[388/500] train_loss: 0.05451 valid_loss: 0.07626 test_loss: 0.09073 \n",
      "[389/500] train_loss: 0.05372 valid_loss: 0.07789 test_loss: 0.08755 \n",
      "[390/500] train_loss: 0.05472 valid_loss: 0.07966 test_loss: 0.08951 \n",
      "[391/500] train_loss: 0.05424 valid_loss: 0.07654 test_loss: 0.08861 \n",
      "[392/500] train_loss: 0.05468 valid_loss: 0.07610 test_loss: 0.08683 \n",
      "[393/500] train_loss: 0.05554 valid_loss: 0.07825 test_loss: 0.09032 \n",
      "[394/500] train_loss: 0.05563 valid_loss: 0.07686 test_loss: 0.08762 \n",
      "[395/500] train_loss: 0.05526 valid_loss: 0.07584 test_loss: 0.08845 \n",
      "[396/500] train_loss: 0.05478 valid_loss: 0.07656 test_loss: 0.08823 \n",
      "[397/500] train_loss: 0.05479 valid_loss: 0.07660 test_loss: 0.08911 \n",
      "[398/500] train_loss: 0.05382 valid_loss: 0.07504 test_loss: 0.08641 \n",
      "[399/500] train_loss: 0.05482 valid_loss: 0.07538 test_loss: 0.08740 \n",
      "[400/500] train_loss: 0.05365 valid_loss: 0.07619 test_loss: 0.08779 \n",
      "[401/500] train_loss: 0.05374 valid_loss: 0.07629 test_loss: 0.08835 \n",
      "[402/500] train_loss: 0.05597 valid_loss: 0.07619 test_loss: 0.08909 \n",
      "[403/500] train_loss: 0.05476 valid_loss: 0.07591 test_loss: 0.08844 \n",
      "[404/500] train_loss: 0.05385 valid_loss: 0.07622 test_loss: 0.08868 \n",
      "[405/500] train_loss: 0.05535 valid_loss: 0.07704 test_loss: 0.08809 \n",
      "[406/500] train_loss: 0.05412 valid_loss: 0.07609 test_loss: 0.08880 \n",
      "[407/500] train_loss: 0.05307 valid_loss: 0.07525 test_loss: 0.08854 \n",
      "[408/500] train_loss: 0.05460 valid_loss: 0.07723 test_loss: 0.08943 \n",
      "[409/500] train_loss: 0.05412 valid_loss: 0.07670 test_loss: 0.08794 \n",
      "[410/500] train_loss: 0.05422 valid_loss: 0.07468 test_loss: 0.08855 \n",
      "[411/500] train_loss: 0.05349 valid_loss: 0.07511 test_loss: 0.08793 \n",
      "[412/500] train_loss: 0.05308 valid_loss: 0.07673 test_loss: 0.08946 \n",
      "[413/500] train_loss: 0.05283 valid_loss: 0.07715 test_loss: 0.09024 \n",
      "[414/500] train_loss: 0.05211 valid_loss: 0.07765 test_loss: 0.09026 \n",
      "[415/500] train_loss: 0.05349 valid_loss: 0.07505 test_loss: 0.08743 \n",
      "[416/500] train_loss: 0.05276 valid_loss: 0.07591 test_loss: 0.08752 \n",
      "[417/500] train_loss: 0.05403 valid_loss: 0.07555 test_loss: 0.08837 \n",
      "[418/500] train_loss: 0.05354 valid_loss: 0.07487 test_loss: 0.08731 \n",
      "[419/500] train_loss: 0.05331 valid_loss: 0.07636 test_loss: 0.08796 \n",
      "[420/500] train_loss: 0.05431 valid_loss: 0.07764 test_loss: 0.08867 \n",
      "[421/500] train_loss: 0.05356 valid_loss: 0.07708 test_loss: 0.09039 \n",
      "[422/500] train_loss: 0.05301 valid_loss: 0.07652 test_loss: 0.08923 \n",
      "[423/500] train_loss: 0.05408 valid_loss: 0.07515 test_loss: 0.08735 \n",
      "[424/500] train_loss: 0.05292 valid_loss: 0.07631 test_loss: 0.08881 \n",
      "[425/500] train_loss: 0.05311 valid_loss: 0.07530 test_loss: 0.08715 \n",
      "[426/500] train_loss: 0.05359 valid_loss: 0.07649 test_loss: 0.08916 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[427/500] train_loss: 0.05226 valid_loss: 0.07691 test_loss: 0.08839 \n",
      "[428/500] train_loss: 0.05223 valid_loss: 0.07723 test_loss: 0.08825 \n",
      "[429/500] train_loss: 0.05315 valid_loss: 0.07750 test_loss: 0.08885 \n",
      "[430/500] train_loss: 0.05391 valid_loss: 0.07727 test_loss: 0.08887 \n",
      "[431/500] train_loss: 0.05335 valid_loss: 0.07660 test_loss: 0.08807 \n",
      "[432/500] train_loss: 0.05445 valid_loss: 0.07473 test_loss: 0.08793 \n",
      "[433/500] train_loss: 0.05220 valid_loss: 0.07618 test_loss: 0.08880 \n",
      "[434/500] train_loss: 0.05326 valid_loss: 0.07577 test_loss: 0.08709 \n",
      "[435/500] train_loss: 0.05317 valid_loss: 0.07836 test_loss: 0.08925 \n",
      "[436/500] train_loss: 0.05330 valid_loss: 0.07687 test_loss: 0.08853 \n",
      "[437/500] train_loss: 0.05369 valid_loss: 0.07944 test_loss: 0.09178 \n",
      "[438/500] train_loss: 0.05160 valid_loss: 0.07791 test_loss: 0.08832 \n",
      "[439/500] train_loss: 0.05160 valid_loss: 0.07654 test_loss: 0.08968 \n",
      "[440/500] train_loss: 0.05269 valid_loss: 0.07617 test_loss: 0.08773 \n",
      "[441/500] train_loss: 0.05185 valid_loss: 0.07523 test_loss: 0.08622 \n",
      "[442/500] train_loss: 0.05122 valid_loss: 0.07766 test_loss: 0.08834 \n",
      "[443/500] train_loss: 0.05199 valid_loss: 0.07649 test_loss: 0.08783 \n",
      "[444/500] train_loss: 0.05239 valid_loss: 0.07688 test_loss: 0.08817 \n",
      "[445/500] train_loss: 0.05280 valid_loss: 0.07624 test_loss: 0.08840 \n",
      "[446/500] train_loss: 0.05245 valid_loss: 0.07857 test_loss: 0.08851 \n",
      "[447/500] train_loss: 0.05292 valid_loss: 0.07707 test_loss: 0.08860 \n",
      "[448/500] train_loss: 0.05206 valid_loss: 0.07701 test_loss: 0.08843 \n",
      "[449/500] train_loss: 0.05252 valid_loss: 0.07582 test_loss: 0.08881 \n",
      "[450/500] train_loss: 0.05336 valid_loss: 0.07651 test_loss: 0.08905 \n",
      "[451/500] train_loss: 0.05285 valid_loss: 0.07558 test_loss: 0.08796 \n",
      "[452/500] train_loss: 0.05323 valid_loss: 0.07763 test_loss: 0.08928 \n",
      "[453/500] train_loss: 0.05306 valid_loss: 0.07622 test_loss: 0.08817 \n",
      "[454/500] train_loss: 0.05093 valid_loss: 0.07849 test_loss: 0.09010 \n",
      "[455/500] train_loss: 0.05114 valid_loss: 0.08114 test_loss: 0.08882 \n",
      "[456/500] train_loss: 0.05241 valid_loss: 0.07697 test_loss: 0.08997 \n",
      "[457/500] train_loss: 0.05168 valid_loss: 0.07716 test_loss: 0.08854 \n",
      "[458/500] train_loss: 0.05368 valid_loss: 0.07567 test_loss: 0.08783 \n",
      "[459/500] train_loss: 0.05277 valid_loss: 0.07513 test_loss: 0.08701 \n",
      "[460/500] train_loss: 0.05135 valid_loss: 0.07504 test_loss: 0.08816 \n",
      "[461/500] train_loss: 0.05200 valid_loss: 0.07673 test_loss: 0.08758 \n",
      "[462/500] train_loss: 0.05301 valid_loss: 0.07650 test_loss: 0.08685 \n",
      "[463/500] train_loss: 0.05238 valid_loss: 0.07499 test_loss: 0.08651 \n",
      "[464/500] train_loss: 0.05152 valid_loss: 0.07595 test_loss: 0.08758 \n",
      "[465/500] train_loss: 0.05336 valid_loss: 0.07686 test_loss: 0.08752 \n",
      "[466/500] train_loss: 0.05375 valid_loss: 0.07553 test_loss: 0.08741 \n",
      "[467/500] train_loss: 0.05064 valid_loss: 0.07498 test_loss: 0.08783 \n",
      "[468/500] train_loss: 0.05336 valid_loss: 0.07545 test_loss: 0.08757 \n",
      "[469/500] train_loss: 0.05256 valid_loss: 0.07794 test_loss: 0.08862 \n",
      "[470/500] train_loss: 0.05193 valid_loss: 0.07514 test_loss: 0.08753 \n",
      "[471/500] train_loss: 0.05181 valid_loss: 0.08354 test_loss: 0.08886 \n",
      "[472/500] train_loss: 0.05375 valid_loss: 0.07561 test_loss: 0.08858 \n",
      "[473/500] train_loss: 0.05267 valid_loss: 0.07755 test_loss: 0.09033 \n",
      "[474/500] train_loss: 0.05052 valid_loss: 0.07632 test_loss: 0.08934 \n",
      "[475/500] train_loss: 0.05196 valid_loss: 0.07678 test_loss: 0.08916 \n",
      "[476/500] train_loss: 0.05179 valid_loss: 0.07685 test_loss: 0.08964 \n",
      "[477/500] train_loss: 0.05087 valid_loss: 0.07653 test_loss: 0.08860 \n",
      "[478/500] train_loss: 0.05056 valid_loss: 0.07702 test_loss: 0.09032 \n",
      "[479/500] train_loss: 0.04976 valid_loss: 0.07970 test_loss: 0.09078 \n",
      "[480/500] train_loss: 0.05193 valid_loss: 0.08614 test_loss: 0.08882 \n",
      "[481/500] train_loss: 0.05304 valid_loss: 0.08944 test_loss: 0.08868 \n",
      "[482/500] train_loss: 0.05185 valid_loss: 0.07766 test_loss: 0.08934 \n",
      "[483/500] train_loss: 0.05080 valid_loss: 0.07516 test_loss: 0.08932 \n",
      "[484/500] train_loss: 0.05158 valid_loss: 0.07665 test_loss: 0.08882 \n",
      "[485/500] train_loss: 0.05066 valid_loss: 0.07597 test_loss: 0.08883 \n",
      "[486/500] train_loss: 0.05144 valid_loss: 0.07763 test_loss: 0.08880 \n",
      "[487/500] train_loss: 0.05165 valid_loss: 0.07719 test_loss: 0.09111 \n",
      "[488/500] train_loss: 0.05055 valid_loss: 0.07951 test_loss: 0.08926 \n",
      "[489/500] train_loss: 0.05233 valid_loss: 0.08097 test_loss: 0.08913 \n",
      "[490/500] train_loss: 0.05304 valid_loss: 0.07659 test_loss: 0.08786 \n",
      "[491/500] train_loss: 0.05161 valid_loss: 0.07727 test_loss: 0.08770 \n",
      "[492/500] train_loss: 0.05120 valid_loss: 0.08121 test_loss: 0.08758 \n",
      "[493/500] train_loss: 0.05058 valid_loss: 0.07764 test_loss: 0.08881 \n",
      "[494/500] train_loss: 0.05125 valid_loss: 0.08055 test_loss: 0.08828 \n",
      "[495/500] train_loss: 0.05251 valid_loss: 0.07900 test_loss: 0.09042 \n",
      "[496/500] train_loss: 0.05198 valid_loss: 0.07847 test_loss: 0.08769 \n",
      "[497/500] train_loss: 0.05280 valid_loss: 0.07707 test_loss: 0.08766 \n",
      "[498/500] train_loss: 0.05176 valid_loss: 0.08122 test_loss: 0.08716 \n",
      "[499/500] train_loss: 0.05119 valid_loss: 0.07680 test_loss: 0.08848 \n",
      "[500/500] train_loss: 0.05145 valid_loss: 0.07823 test_loss: 0.08739 \n",
      "TRAINING MODEL 11\n",
      "[  1/500] train_loss: 0.38974 valid_loss: 0.27931 test_loss: 0.28491 \n",
      "验证损失减少 (inf --> 0.279306). 正在保存模型...\n",
      "[  2/500] train_loss: 0.21989 valid_loss: 0.20543 test_loss: 0.21330 \n",
      "验证损失减少 (0.279306 --> 0.205427). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17923 valid_loss: 0.17084 test_loss: 0.18225 \n",
      "验证损失减少 (0.205427 --> 0.170839). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15860 valid_loss: 0.15857 test_loss: 0.17081 \n",
      "验证损失减少 (0.170839 --> 0.158572). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15040 valid_loss: 0.15125 test_loss: 0.16296 \n",
      "验证损失减少 (0.158572 --> 0.151255). 正在保存模型...\n",
      "[  6/500] train_loss: 0.13958 valid_loss: 0.14382 test_loss: 0.15654 \n",
      "验证损失减少 (0.151255 --> 0.143818). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13869 valid_loss: 0.14175 test_loss: 0.15497 \n",
      "验证损失减少 (0.143818 --> 0.141749). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13333 valid_loss: 0.13616 test_loss: 0.14830 \n",
      "验证损失减少 (0.141749 --> 0.136162). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13265 valid_loss: 0.14045 test_loss: 0.15414 \n",
      "[ 10/500] train_loss: 0.12857 valid_loss: 0.13009 test_loss: 0.14459 \n",
      "验证损失减少 (0.136162 --> 0.130091). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12312 valid_loss: 0.12944 test_loss: 0.14833 \n",
      "验证损失减少 (0.130091 --> 0.129440). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12327 valid_loss: 0.12454 test_loss: 0.14022 \n",
      "验证损失减少 (0.129440 --> 0.124538). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12029 valid_loss: 0.12701 test_loss: 0.14065 \n",
      "[ 14/500] train_loss: 0.12323 valid_loss: 0.12195 test_loss: 0.13740 \n",
      "验证损失减少 (0.124538 --> 0.121949). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11511 valid_loss: 0.12192 test_loss: 0.13748 \n",
      "验证损失减少 (0.121949 --> 0.121916). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11916 valid_loss: 0.12317 test_loss: 0.13616 \n",
      "[ 17/500] train_loss: 0.11372 valid_loss: 0.12414 test_loss: 0.13835 \n",
      "[ 18/500] train_loss: 0.11484 valid_loss: 0.11830 test_loss: 0.13338 \n",
      "验证损失减少 (0.121916 --> 0.118303). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11369 valid_loss: 0.11586 test_loss: 0.13238 \n",
      "验证损失减少 (0.118303 --> 0.115862). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11059 valid_loss: 0.11750 test_loss: 0.13130 \n",
      "[ 21/500] train_loss: 0.11084 valid_loss: 0.11244 test_loss: 0.12937 \n",
      "验证损失减少 (0.115862 --> 0.112441). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.10643 valid_loss: 0.11415 test_loss: 0.12906 \n",
      "[ 23/500] train_loss: 0.10987 valid_loss: 0.11583 test_loss: 0.12882 \n",
      "[ 24/500] train_loss: 0.10586 valid_loss: 0.11379 test_loss: 0.12959 \n",
      "[ 25/500] train_loss: 0.10761 valid_loss: 0.11085 test_loss: 0.12670 \n",
      "验证损失减少 (0.112441 --> 0.110848). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10597 valid_loss: 0.11005 test_loss: 0.12657 \n",
      "验证损失减少 (0.110848 --> 0.110053). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.10524 valid_loss: 0.10990 test_loss: 0.12522 \n",
      "验证损失减少 (0.110053 --> 0.109896). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10441 valid_loss: 0.11030 test_loss: 0.12414 \n",
      "[ 29/500] train_loss: 0.10206 valid_loss: 0.10863 test_loss: 0.12304 \n",
      "验证损失减少 (0.109896 --> 0.108631). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10391 valid_loss: 0.10703 test_loss: 0.12139 \n",
      "验证损失减少 (0.108631 --> 0.107032). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.10357 valid_loss: 0.10745 test_loss: 0.12173 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 32/500] train_loss: 0.10266 valid_loss: 0.10620 test_loss: 0.12273 \n",
      "验证损失减少 (0.107032 --> 0.106201). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10034 valid_loss: 0.10573 test_loss: 0.12081 \n",
      "验证损失减少 (0.106201 --> 0.105730). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.09732 valid_loss: 0.10495 test_loss: 0.11882 \n",
      "验证损失减少 (0.105730 --> 0.104950). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.09889 valid_loss: 0.10724 test_loss: 0.12135 \n",
      "[ 36/500] train_loss: 0.10026 valid_loss: 0.10479 test_loss: 0.11891 \n",
      "验证损失减少 (0.104950 --> 0.104788). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.09816 valid_loss: 0.10360 test_loss: 0.11961 \n",
      "验证损失减少 (0.104788 --> 0.103601). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.09775 valid_loss: 0.10503 test_loss: 0.11922 \n",
      "[ 39/500] train_loss: 0.09855 valid_loss: 0.10256 test_loss: 0.11584 \n",
      "验证损失减少 (0.103601 --> 0.102562). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09616 valid_loss: 0.10253 test_loss: 0.11539 \n",
      "验证损失减少 (0.102562 --> 0.102534). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.09957 valid_loss: 0.10345 test_loss: 0.11574 \n",
      "[ 42/500] train_loss: 0.09395 valid_loss: 0.10069 test_loss: 0.11465 \n",
      "验证损失减少 (0.102534 --> 0.100685). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.09688 valid_loss: 0.10113 test_loss: 0.11503 \n",
      "[ 44/500] train_loss: 0.09419 valid_loss: 0.10278 test_loss: 0.11402 \n",
      "[ 45/500] train_loss: 0.09432 valid_loss: 0.10276 test_loss: 0.11390 \n",
      "[ 46/500] train_loss: 0.09473 valid_loss: 0.10184 test_loss: 0.11539 \n",
      "[ 47/500] train_loss: 0.09705 valid_loss: 0.09704 test_loss: 0.11095 \n",
      "验证损失减少 (0.100685 --> 0.097039). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.09132 valid_loss: 0.09871 test_loss: 0.11447 \n",
      "[ 49/500] train_loss: 0.09075 valid_loss: 0.09888 test_loss: 0.11147 \n",
      "[ 50/500] train_loss: 0.09366 valid_loss: 0.09743 test_loss: 0.11167 \n",
      "[ 51/500] train_loss: 0.09195 valid_loss: 0.09947 test_loss: 0.11313 \n",
      "[ 52/500] train_loss: 0.08893 valid_loss: 0.09684 test_loss: 0.10957 \n",
      "验证损失减少 (0.097039 --> 0.096838). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.09219 valid_loss: 0.09807 test_loss: 0.11052 \n",
      "[ 54/500] train_loss: 0.09266 valid_loss: 0.09772 test_loss: 0.11226 \n",
      "[ 55/500] train_loss: 0.08891 valid_loss: 0.09527 test_loss: 0.10911 \n",
      "验证损失减少 (0.096838 --> 0.095266). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.08899 valid_loss: 0.09472 test_loss: 0.10990 \n",
      "验证损失减少 (0.095266 --> 0.094724). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.09001 valid_loss: 0.09437 test_loss: 0.10854 \n",
      "验证损失减少 (0.094724 --> 0.094367). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.08987 valid_loss: 0.10177 test_loss: 0.10999 \n",
      "[ 59/500] train_loss: 0.08901 valid_loss: 0.09715 test_loss: 0.10905 \n",
      "[ 60/500] train_loss: 0.09063 valid_loss: 0.09594 test_loss: 0.10868 \n",
      "[ 61/500] train_loss: 0.08863 valid_loss: 0.09596 test_loss: 0.10771 \n",
      "[ 62/500] train_loss: 0.08875 valid_loss: 0.09726 test_loss: 0.10766 \n",
      "[ 63/500] train_loss: 0.08862 valid_loss: 0.09394 test_loss: 0.10593 \n",
      "验证损失减少 (0.094367 --> 0.093940). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.08758 valid_loss: 0.09483 test_loss: 0.10879 \n",
      "[ 65/500] train_loss: 0.08605 valid_loss: 0.09568 test_loss: 0.10728 \n",
      "[ 66/500] train_loss: 0.08436 valid_loss: 0.09319 test_loss: 0.10576 \n",
      "验证损失减少 (0.093940 --> 0.093191). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.08665 valid_loss: 0.09297 test_loss: 0.10452 \n",
      "验证损失减少 (0.093191 --> 0.092968). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.08736 valid_loss: 0.09316 test_loss: 0.10429 \n",
      "[ 69/500] train_loss: 0.08664 valid_loss: 0.09320 test_loss: 0.10555 \n",
      "[ 70/500] train_loss: 0.08667 valid_loss: 0.09183 test_loss: 0.10401 \n",
      "验证损失减少 (0.092968 --> 0.091830). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.08719 valid_loss: 0.09180 test_loss: 0.10495 \n",
      "验证损失减少 (0.091830 --> 0.091798). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.08544 valid_loss: 0.09296 test_loss: 0.10353 \n",
      "[ 73/500] train_loss: 0.08315 valid_loss: 0.09252 test_loss: 0.10310 \n",
      "[ 74/500] train_loss: 0.08411 valid_loss: 0.09268 test_loss: 0.10352 \n",
      "[ 75/500] train_loss: 0.08380 valid_loss: 0.09142 test_loss: 0.10331 \n",
      "验证损失减少 (0.091798 --> 0.091420). 正在保存模型...\n",
      "[ 76/500] train_loss: 0.08449 valid_loss: 0.09376 test_loss: 0.10421 \n",
      "[ 77/500] train_loss: 0.08507 valid_loss: 0.09271 test_loss: 0.10327 \n",
      "[ 78/500] train_loss: 0.08403 valid_loss: 0.09133 test_loss: 0.10427 \n",
      "验证损失减少 (0.091420 --> 0.091327). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08312 valid_loss: 0.09326 test_loss: 0.10360 \n",
      "[ 80/500] train_loss: 0.08559 valid_loss: 0.09036 test_loss: 0.10130 \n",
      "验证损失减少 (0.091327 --> 0.090363). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.08400 valid_loss: 0.09262 test_loss: 0.10371 \n",
      "[ 82/500] train_loss: 0.08089 valid_loss: 0.09106 test_loss: 0.10153 \n",
      "[ 83/500] train_loss: 0.08131 valid_loss: 0.08909 test_loss: 0.10181 \n",
      "验证损失减少 (0.090363 --> 0.089086). 正在保存模型...\n",
      "[ 84/500] train_loss: 0.08291 valid_loss: 0.08955 test_loss: 0.10132 \n",
      "[ 85/500] train_loss: 0.08204 valid_loss: 0.08810 test_loss: 0.09983 \n",
      "验证损失减少 (0.089086 --> 0.088097). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08191 valid_loss: 0.08938 test_loss: 0.10282 \n",
      "[ 87/500] train_loss: 0.08239 valid_loss: 0.08897 test_loss: 0.10184 \n",
      "[ 88/500] train_loss: 0.08077 valid_loss: 0.09028 test_loss: 0.10289 \n",
      "[ 89/500] train_loss: 0.07974 valid_loss: 0.08895 test_loss: 0.10024 \n",
      "[ 90/500] train_loss: 0.08138 valid_loss: 0.08937 test_loss: 0.09949 \n",
      "[ 91/500] train_loss: 0.08154 valid_loss: 0.09015 test_loss: 0.10043 \n",
      "[ 92/500] train_loss: 0.08057 valid_loss: 0.08945 test_loss: 0.10164 \n",
      "[ 93/500] train_loss: 0.07901 valid_loss: 0.08752 test_loss: 0.09953 \n",
      "验证损失减少 (0.088097 --> 0.087520). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.07993 valid_loss: 0.08761 test_loss: 0.10128 \n",
      "[ 95/500] train_loss: 0.07935 valid_loss: 0.09024 test_loss: 0.09981 \n",
      "[ 96/500] train_loss: 0.08116 valid_loss: 0.08903 test_loss: 0.10028 \n",
      "[ 97/500] train_loss: 0.08261 valid_loss: 0.09122 test_loss: 0.10138 \n",
      "[ 98/500] train_loss: 0.07966 valid_loss: 0.08781 test_loss: 0.09991 \n",
      "[ 99/500] train_loss: 0.08067 valid_loss: 0.09060 test_loss: 0.09833 \n",
      "[100/500] train_loss: 0.07853 valid_loss: 0.08795 test_loss: 0.09908 \n",
      "[101/500] train_loss: 0.07772 valid_loss: 0.08886 test_loss: 0.10170 \n",
      "[102/500] train_loss: 0.07757 valid_loss: 0.08891 test_loss: 0.09822 \n",
      "[103/500] train_loss: 0.08023 valid_loss: 0.08704 test_loss: 0.09779 \n",
      "验证损失减少 (0.087520 --> 0.087040). 正在保存模型...\n",
      "[104/500] train_loss: 0.07653 valid_loss: 0.08849 test_loss: 0.09763 \n",
      "[105/500] train_loss: 0.07792 valid_loss: 0.08590 test_loss: 0.09783 \n",
      "验证损失减少 (0.087040 --> 0.085898). 正在保存模型...\n",
      "[106/500] train_loss: 0.07962 valid_loss: 0.08659 test_loss: 0.09754 \n",
      "[107/500] train_loss: 0.07811 valid_loss: 0.08672 test_loss: 0.09779 \n",
      "[108/500] train_loss: 0.07543 valid_loss: 0.08778 test_loss: 0.09812 \n",
      "[109/500] train_loss: 0.07657 valid_loss: 0.08966 test_loss: 0.09888 \n",
      "[110/500] train_loss: 0.07927 valid_loss: 0.08682 test_loss: 0.09757 \n",
      "[111/500] train_loss: 0.07733 valid_loss: 0.09187 test_loss: 0.09946 \n",
      "[112/500] train_loss: 0.07689 valid_loss: 0.08582 test_loss: 0.09869 \n",
      "验证损失减少 (0.085898 --> 0.085816). 正在保存模型...\n",
      "[113/500] train_loss: 0.07689 valid_loss: 0.08791 test_loss: 0.09749 \n",
      "[114/500] train_loss: 0.07544 valid_loss: 0.08492 test_loss: 0.09608 \n",
      "验证损失减少 (0.085816 --> 0.084917). 正在保存模型...\n",
      "[115/500] train_loss: 0.07739 valid_loss: 0.08762 test_loss: 0.09749 \n",
      "[116/500] train_loss: 0.07582 valid_loss: 0.08626 test_loss: 0.09717 \n",
      "[117/500] train_loss: 0.07523 valid_loss: 0.08813 test_loss: 0.09713 \n",
      "[118/500] train_loss: 0.07599 valid_loss: 0.08573 test_loss: 0.09774 \n",
      "[119/500] train_loss: 0.07639 valid_loss: 0.08579 test_loss: 0.09730 \n",
      "[120/500] train_loss: 0.07561 valid_loss: 0.08502 test_loss: 0.09649 \n",
      "[121/500] train_loss: 0.07471 valid_loss: 0.08339 test_loss: 0.09576 \n",
      "验证损失减少 (0.084917 --> 0.083391). 正在保存模型...\n",
      "[122/500] train_loss: 0.07429 valid_loss: 0.08493 test_loss: 0.09705 \n",
      "[123/500] train_loss: 0.07679 valid_loss: 0.08430 test_loss: 0.09631 \n",
      "[124/500] train_loss: 0.07402 valid_loss: 0.08678 test_loss: 0.09788 \n",
      "[125/500] train_loss: 0.07490 valid_loss: 0.08357 test_loss: 0.09643 \n",
      "[126/500] train_loss: 0.07439 valid_loss: 0.08702 test_loss: 0.09686 \n",
      "[127/500] train_loss: 0.07450 valid_loss: 0.08420 test_loss: 0.09691 \n",
      "[128/500] train_loss: 0.07556 valid_loss: 0.08264 test_loss: 0.09407 \n",
      "验证损失减少 (0.083391 --> 0.082636). 正在保存模型...\n",
      "[129/500] train_loss: 0.07365 valid_loss: 0.08223 test_loss: 0.09465 \n",
      "验证损失减少 (0.082636 --> 0.082232). 正在保存模型...\n",
      "[130/500] train_loss: 0.07630 valid_loss: 0.08684 test_loss: 0.09499 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131/500] train_loss: 0.07423 valid_loss: 0.08765 test_loss: 0.09501 \n",
      "[132/500] train_loss: 0.07362 valid_loss: 0.08620 test_loss: 0.09526 \n",
      "[133/500] train_loss: 0.07472 valid_loss: 0.08588 test_loss: 0.09680 \n",
      "[134/500] train_loss: 0.07275 valid_loss: 0.08290 test_loss: 0.09511 \n",
      "[135/500] train_loss: 0.07283 valid_loss: 0.08485 test_loss: 0.09596 \n",
      "[136/500] train_loss: 0.07155 valid_loss: 0.08410 test_loss: 0.09635 \n",
      "[137/500] train_loss: 0.07422 valid_loss: 0.08639 test_loss: 0.09662 \n",
      "[138/500] train_loss: 0.07484 valid_loss: 0.08544 test_loss: 0.09614 \n",
      "[139/500] train_loss: 0.07386 valid_loss: 0.08300 test_loss: 0.09467 \n",
      "[140/500] train_loss: 0.07220 valid_loss: 0.08884 test_loss: 0.09541 \n",
      "[141/500] train_loss: 0.07242 valid_loss: 0.08522 test_loss: 0.09488 \n",
      "[142/500] train_loss: 0.07255 valid_loss: 0.08321 test_loss: 0.09552 \n",
      "[143/500] train_loss: 0.07232 valid_loss: 0.08241 test_loss: 0.09658 \n",
      "[144/500] train_loss: 0.07266 valid_loss: 0.08275 test_loss: 0.09458 \n",
      "[145/500] train_loss: 0.07327 valid_loss: 0.08106 test_loss: 0.09367 \n",
      "验证损失减少 (0.082232 --> 0.081057). 正在保存模型...\n",
      "[146/500] train_loss: 0.07102 valid_loss: 0.08287 test_loss: 0.09472 \n",
      "[147/500] train_loss: 0.07232 valid_loss: 0.08353 test_loss: 0.09542 \n",
      "[148/500] train_loss: 0.07326 valid_loss: 0.08697 test_loss: 0.09552 \n",
      "[149/500] train_loss: 0.07136 valid_loss: 0.08469 test_loss: 0.09447 \n",
      "[150/500] train_loss: 0.07217 valid_loss: 0.08400 test_loss: 0.09558 \n",
      "[151/500] train_loss: 0.07416 valid_loss: 0.08340 test_loss: 0.09741 \n",
      "[152/500] train_loss: 0.07183 valid_loss: 0.08208 test_loss: 0.09447 \n",
      "[153/500] train_loss: 0.07247 valid_loss: 0.08862 test_loss: 0.09431 \n",
      "[154/500] train_loss: 0.07265 valid_loss: 0.08137 test_loss: 0.09404 \n",
      "[155/500] train_loss: 0.06872 valid_loss: 0.08306 test_loss: 0.09421 \n",
      "[156/500] train_loss: 0.07329 valid_loss: 0.08560 test_loss: 0.09328 \n",
      "[157/500] train_loss: 0.07081 valid_loss: 0.08281 test_loss: 0.09291 \n",
      "[158/500] train_loss: 0.06867 valid_loss: 0.08419 test_loss: 0.09245 \n",
      "[159/500] train_loss: 0.07248 valid_loss: 0.08449 test_loss: 0.09241 \n",
      "[160/500] train_loss: 0.07310 valid_loss: 0.08124 test_loss: 0.09344 \n",
      "[161/500] train_loss: 0.07082 valid_loss: 0.08246 test_loss: 0.09425 \n",
      "[162/500] train_loss: 0.07015 valid_loss: 0.08211 test_loss: 0.09289 \n",
      "[163/500] train_loss: 0.06954 valid_loss: 0.08211 test_loss: 0.09333 \n",
      "[164/500] train_loss: 0.06900 valid_loss: 0.08451 test_loss: 0.09658 \n",
      "[165/500] train_loss: 0.07110 valid_loss: 0.08345 test_loss: 0.09286 \n",
      "[166/500] train_loss: 0.06986 valid_loss: 0.08590 test_loss: 0.09339 \n",
      "[167/500] train_loss: 0.07039 valid_loss: 0.08129 test_loss: 0.09370 \n",
      "[168/500] train_loss: 0.07179 valid_loss: 0.08172 test_loss: 0.09387 \n",
      "[169/500] train_loss: 0.07048 valid_loss: 0.08239 test_loss: 0.09261 \n",
      "[170/500] train_loss: 0.06951 valid_loss: 0.08136 test_loss: 0.09324 \n",
      "[171/500] train_loss: 0.06887 valid_loss: 0.08121 test_loss: 0.09375 \n",
      "[172/500] train_loss: 0.06884 valid_loss: 0.08177 test_loss: 0.09269 \n",
      "[173/500] train_loss: 0.06834 valid_loss: 0.08262 test_loss: 0.09424 \n",
      "[174/500] train_loss: 0.06965 valid_loss: 0.08235 test_loss: 0.09302 \n",
      "[175/500] train_loss: 0.07025 valid_loss: 0.08092 test_loss: 0.09233 \n",
      "验证损失减少 (0.081057 --> 0.080922). 正在保存模型...\n",
      "[176/500] train_loss: 0.06874 valid_loss: 0.08166 test_loss: 0.09155 \n",
      "[177/500] train_loss: 0.06950 valid_loss: 0.08175 test_loss: 0.09326 \n",
      "[178/500] train_loss: 0.06934 valid_loss: 0.08042 test_loss: 0.09414 \n",
      "验证损失减少 (0.080922 --> 0.080419). 正在保存模型...\n",
      "[179/500] train_loss: 0.06964 valid_loss: 0.08183 test_loss: 0.09502 \n",
      "[180/500] train_loss: 0.06971 valid_loss: 0.07979 test_loss: 0.09333 \n",
      "验证损失减少 (0.080419 --> 0.079789). 正在保存模型...\n",
      "[181/500] train_loss: 0.07295 valid_loss: 0.08437 test_loss: 0.09385 \n",
      "[182/500] train_loss: 0.06959 valid_loss: 0.08159 test_loss: 0.09101 \n",
      "[183/500] train_loss: 0.06747 valid_loss: 0.07971 test_loss: 0.09393 \n",
      "验证损失减少 (0.079789 --> 0.079706). 正在保存模型...\n",
      "[184/500] train_loss: 0.06829 valid_loss: 0.08039 test_loss: 0.09416 \n",
      "[185/500] train_loss: 0.06650 valid_loss: 0.08116 test_loss: 0.09179 \n",
      "[186/500] train_loss: 0.06804 valid_loss: 0.08731 test_loss: 0.09181 \n",
      "[187/500] train_loss: 0.06845 valid_loss: 0.07970 test_loss: 0.09069 \n",
      "验证损失减少 (0.079706 --> 0.079698). 正在保存模型...\n",
      "[188/500] train_loss: 0.06800 valid_loss: 0.08211 test_loss: 0.09295 \n",
      "[189/500] train_loss: 0.06740 valid_loss: 0.08113 test_loss: 0.09193 \n",
      "[190/500] train_loss: 0.06884 valid_loss: 0.08000 test_loss: 0.09277 \n",
      "[191/500] train_loss: 0.07132 valid_loss: 0.07894 test_loss: 0.09175 \n",
      "验证损失减少 (0.079698 --> 0.078941). 正在保存模型...\n",
      "[192/500] train_loss: 0.06667 valid_loss: 0.07748 test_loss: 0.09008 \n",
      "验证损失减少 (0.078941 --> 0.077485). 正在保存模型...\n",
      "[193/500] train_loss: 0.06834 valid_loss: 0.07936 test_loss: 0.09294 \n",
      "[194/500] train_loss: 0.06689 valid_loss: 0.07907 test_loss: 0.09210 \n",
      "[195/500] train_loss: 0.06543 valid_loss: 0.07962 test_loss: 0.09089 \n",
      "[196/500] train_loss: 0.06782 valid_loss: 0.07852 test_loss: 0.09229 \n",
      "[197/500] train_loss: 0.06725 valid_loss: 0.07926 test_loss: 0.09215 \n",
      "[198/500] train_loss: 0.06558 valid_loss: 0.07789 test_loss: 0.09177 \n",
      "[199/500] train_loss: 0.06765 valid_loss: 0.07835 test_loss: 0.09094 \n",
      "[200/500] train_loss: 0.06710 valid_loss: 0.07813 test_loss: 0.09229 \n",
      "[201/500] train_loss: 0.06710 valid_loss: 0.07992 test_loss: 0.09131 \n",
      "[202/500] train_loss: 0.06623 valid_loss: 0.07830 test_loss: 0.09074 \n",
      "[203/500] train_loss: 0.06514 valid_loss: 0.07957 test_loss: 0.09100 \n",
      "[204/500] train_loss: 0.06490 valid_loss: 0.08172 test_loss: 0.09166 \n",
      "[205/500] train_loss: 0.06587 valid_loss: 0.07925 test_loss: 0.09174 \n",
      "[206/500] train_loss: 0.06564 valid_loss: 0.08141 test_loss: 0.09161 \n",
      "[207/500] train_loss: 0.06726 valid_loss: 0.07774 test_loss: 0.08975 \n",
      "[208/500] train_loss: 0.06527 valid_loss: 0.07960 test_loss: 0.09307 \n",
      "[209/500] train_loss: 0.06601 valid_loss: 0.07809 test_loss: 0.09075 \n",
      "[210/500] train_loss: 0.06621 valid_loss: 0.07860 test_loss: 0.09304 \n",
      "[211/500] train_loss: 0.06594 valid_loss: 0.07834 test_loss: 0.09035 \n",
      "[212/500] train_loss: 0.06760 valid_loss: 0.07957 test_loss: 0.09321 \n",
      "[213/500] train_loss: 0.06651 valid_loss: 0.08034 test_loss: 0.09171 \n",
      "[214/500] train_loss: 0.06570 valid_loss: 0.07962 test_loss: 0.09265 \n",
      "[215/500] train_loss: 0.06650 valid_loss: 0.08195 test_loss: 0.09019 \n",
      "[216/500] train_loss: 0.06598 valid_loss: 0.07945 test_loss: 0.09029 \n",
      "[217/500] train_loss: 0.06650 valid_loss: 0.07804 test_loss: 0.08989 \n",
      "[218/500] train_loss: 0.06592 valid_loss: 0.08289 test_loss: 0.09172 \n",
      "[219/500] train_loss: 0.06652 valid_loss: 0.08115 test_loss: 0.09000 \n",
      "[220/500] train_loss: 0.06566 valid_loss: 0.07959 test_loss: 0.09028 \n",
      "[221/500] train_loss: 0.06603 valid_loss: 0.08225 test_loss: 0.09004 \n",
      "[222/500] train_loss: 0.06793 valid_loss: 0.08006 test_loss: 0.09055 \n",
      "[223/500] train_loss: 0.06536 valid_loss: 0.07880 test_loss: 0.09045 \n",
      "[224/500] train_loss: 0.06502 valid_loss: 0.07674 test_loss: 0.08952 \n",
      "验证损失减少 (0.077485 --> 0.076740). 正在保存模型...\n",
      "[225/500] train_loss: 0.06473 valid_loss: 0.08042 test_loss: 0.08965 \n",
      "[226/500] train_loss: 0.06581 valid_loss: 0.08124 test_loss: 0.08993 \n",
      "[227/500] train_loss: 0.06520 valid_loss: 0.08392 test_loss: 0.09035 \n",
      "[228/500] train_loss: 0.06480 valid_loss: 0.08433 test_loss: 0.08917 \n",
      "[229/500] train_loss: 0.06493 valid_loss: 0.08141 test_loss: 0.08887 \n",
      "[230/500] train_loss: 0.06603 valid_loss: 0.08410 test_loss: 0.09030 \n",
      "[231/500] train_loss: 0.06396 valid_loss: 0.07879 test_loss: 0.08949 \n",
      "[232/500] train_loss: 0.06356 valid_loss: 0.08035 test_loss: 0.08922 \n",
      "[233/500] train_loss: 0.06309 valid_loss: 0.08184 test_loss: 0.08947 \n",
      "[234/500] train_loss: 0.06467 valid_loss: 0.08574 test_loss: 0.08968 \n",
      "[235/500] train_loss: 0.06544 valid_loss: 0.08709 test_loss: 0.08965 \n",
      "[236/500] train_loss: 0.06444 valid_loss: 0.08530 test_loss: 0.09007 \n",
      "[237/500] train_loss: 0.06406 valid_loss: 0.08641 test_loss: 0.09181 \n",
      "[238/500] train_loss: 0.06591 valid_loss: 0.08433 test_loss: 0.09114 \n",
      "[239/500] train_loss: 0.06419 valid_loss: 0.07803 test_loss: 0.08979 \n",
      "[240/500] train_loss: 0.06392 valid_loss: 0.07807 test_loss: 0.09092 \n",
      "[241/500] train_loss: 0.06564 valid_loss: 0.07940 test_loss: 0.09092 \n",
      "[242/500] train_loss: 0.06423 valid_loss: 0.07781 test_loss: 0.08981 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243/500] train_loss: 0.06387 valid_loss: 0.08121 test_loss: 0.09113 \n",
      "[244/500] train_loss: 0.06273 valid_loss: 0.08185 test_loss: 0.09099 \n",
      "[245/500] train_loss: 0.06256 valid_loss: 0.07858 test_loss: 0.09196 \n",
      "[246/500] train_loss: 0.06357 valid_loss: 0.07641 test_loss: 0.08915 \n",
      "验证损失减少 (0.076740 --> 0.076412). 正在保存模型...\n",
      "[247/500] train_loss: 0.06104 valid_loss: 0.07603 test_loss: 0.08903 \n",
      "验证损失减少 (0.076412 --> 0.076032). 正在保存模型...\n",
      "[248/500] train_loss: 0.06382 valid_loss: 0.07896 test_loss: 0.08926 \n",
      "[249/500] train_loss: 0.06432 valid_loss: 0.07873 test_loss: 0.09392 \n",
      "[250/500] train_loss: 0.06239 valid_loss: 0.08031 test_loss: 0.08944 \n",
      "[251/500] train_loss: 0.06404 valid_loss: 0.08027 test_loss: 0.09209 \n",
      "[252/500] train_loss: 0.06265 valid_loss: 0.08460 test_loss: 0.09263 \n",
      "[253/500] train_loss: 0.06268 valid_loss: 0.07969 test_loss: 0.09089 \n",
      "[254/500] train_loss: 0.06271 valid_loss: 0.08398 test_loss: 0.08949 \n",
      "[255/500] train_loss: 0.06387 valid_loss: 0.08551 test_loss: 0.08980 \n",
      "[256/500] train_loss: 0.06423 valid_loss: 0.08552 test_loss: 0.09018 \n",
      "[257/500] train_loss: 0.06110 valid_loss: 0.08028 test_loss: 0.09029 \n",
      "[258/500] train_loss: 0.06529 valid_loss: 0.08057 test_loss: 0.09124 \n",
      "[259/500] train_loss: 0.06378 valid_loss: 0.08139 test_loss: 0.09063 \n",
      "[260/500] train_loss: 0.06237 valid_loss: 0.07695 test_loss: 0.08990 \n",
      "[261/500] train_loss: 0.06101 valid_loss: 0.07939 test_loss: 0.09218 \n",
      "[262/500] train_loss: 0.06356 valid_loss: 0.08193 test_loss: 0.09073 \n",
      "[263/500] train_loss: 0.06352 valid_loss: 0.07905 test_loss: 0.09020 \n",
      "[264/500] train_loss: 0.06279 valid_loss: 0.07936 test_loss: 0.09040 \n",
      "[265/500] train_loss: 0.06213 valid_loss: 0.08423 test_loss: 0.09270 \n",
      "[266/500] train_loss: 0.06208 valid_loss: 0.07974 test_loss: 0.08781 \n",
      "[267/500] train_loss: 0.06428 valid_loss: 0.07839 test_loss: 0.08828 \n",
      "[268/500] train_loss: 0.06173 valid_loss: 0.07954 test_loss: 0.08971 \n",
      "[269/500] train_loss: 0.06440 valid_loss: 0.07822 test_loss: 0.08926 \n",
      "[270/500] train_loss: 0.06334 valid_loss: 0.07810 test_loss: 0.08827 \n",
      "[271/500] train_loss: 0.06287 valid_loss: 0.07777 test_loss: 0.08820 \n",
      "[272/500] train_loss: 0.06113 valid_loss: 0.07712 test_loss: 0.09039 \n",
      "[273/500] train_loss: 0.06301 valid_loss: 0.08172 test_loss: 0.08936 \n",
      "[274/500] train_loss: 0.06034 valid_loss: 0.08388 test_loss: 0.09069 \n",
      "[275/500] train_loss: 0.05976 valid_loss: 0.07906 test_loss: 0.08932 \n",
      "[276/500] train_loss: 0.06036 valid_loss: 0.08261 test_loss: 0.08848 \n",
      "[277/500] train_loss: 0.06175 valid_loss: 0.08244 test_loss: 0.08859 \n",
      "[278/500] train_loss: 0.06083 valid_loss: 0.08038 test_loss: 0.08894 \n",
      "[279/500] train_loss: 0.05961 valid_loss: 0.08009 test_loss: 0.09002 \n",
      "[280/500] train_loss: 0.06142 valid_loss: 0.08336 test_loss: 0.08919 \n",
      "[281/500] train_loss: 0.06123 valid_loss: 0.08310 test_loss: 0.08843 \n",
      "[282/500] train_loss: 0.06262 valid_loss: 0.07778 test_loss: 0.08833 \n",
      "[283/500] train_loss: 0.06192 valid_loss: 0.08219 test_loss: 0.08659 \n",
      "[284/500] train_loss: 0.06131 valid_loss: 0.07943 test_loss: 0.08748 \n",
      "[285/500] train_loss: 0.06020 valid_loss: 0.08247 test_loss: 0.08948 \n",
      "[286/500] train_loss: 0.06170 valid_loss: 0.08221 test_loss: 0.09034 \n",
      "[287/500] train_loss: 0.06208 valid_loss: 0.07733 test_loss: 0.08798 \n",
      "[288/500] train_loss: 0.06006 valid_loss: 0.07592 test_loss: 0.08825 \n",
      "验证损失减少 (0.076032 --> 0.075923). 正在保存模型...\n",
      "[289/500] train_loss: 0.06213 valid_loss: 0.08229 test_loss: 0.09116 \n",
      "[290/500] train_loss: 0.06250 valid_loss: 0.08548 test_loss: 0.08988 \n",
      "[291/500] train_loss: 0.05848 valid_loss: 0.08186 test_loss: 0.09068 \n",
      "[292/500] train_loss: 0.06029 valid_loss: 0.07867 test_loss: 0.08807 \n",
      "[293/500] train_loss: 0.06054 valid_loss: 0.09126 test_loss: 0.08940 \n",
      "[294/500] train_loss: 0.05984 valid_loss: 0.07958 test_loss: 0.08768 \n",
      "[295/500] train_loss: 0.05943 valid_loss: 0.07667 test_loss: 0.08950 \n",
      "[296/500] train_loss: 0.06093 valid_loss: 0.07935 test_loss: 0.08864 \n",
      "[297/500] train_loss: 0.05793 valid_loss: 0.07961 test_loss: 0.08972 \n",
      "[298/500] train_loss: 0.06028 valid_loss: 0.09240 test_loss: 0.08988 \n",
      "[299/500] train_loss: 0.06143 valid_loss: 0.09251 test_loss: 0.08849 \n",
      "[300/500] train_loss: 0.06012 valid_loss: 0.09615 test_loss: 0.08922 \n",
      "[301/500] train_loss: 0.05889 valid_loss: 0.08020 test_loss: 0.08843 \n",
      "[302/500] train_loss: 0.05967 valid_loss: 0.08577 test_loss: 0.08900 \n",
      "[303/500] train_loss: 0.05937 valid_loss: 0.07686 test_loss: 0.08852 \n",
      "[304/500] train_loss: 0.05936 valid_loss: 0.07964 test_loss: 0.09016 \n",
      "[305/500] train_loss: 0.06106 valid_loss: 0.07935 test_loss: 0.09048 \n",
      "[306/500] train_loss: 0.05990 valid_loss: 0.07832 test_loss: 0.08907 \n",
      "[307/500] train_loss: 0.05922 valid_loss: 0.07897 test_loss: 0.08978 \n",
      "[308/500] train_loss: 0.05895 valid_loss: 0.08144 test_loss: 0.08868 \n",
      "[309/500] train_loss: 0.05947 valid_loss: 0.08172 test_loss: 0.08758 \n",
      "[310/500] train_loss: 0.06044 valid_loss: 0.07867 test_loss: 0.08991 \n",
      "[311/500] train_loss: 0.05913 valid_loss: 0.08331 test_loss: 0.08893 \n",
      "[312/500] train_loss: 0.05909 valid_loss: 0.07514 test_loss: 0.08795 \n",
      "验证损失减少 (0.075923 --> 0.075136). 正在保存模型...\n",
      "[313/500] train_loss: 0.06016 valid_loss: 0.07586 test_loss: 0.08953 \n",
      "[314/500] train_loss: 0.05943 valid_loss: 0.07661 test_loss: 0.09014 \n",
      "[315/500] train_loss: 0.06007 valid_loss: 0.07717 test_loss: 0.09056 \n",
      "[316/500] train_loss: 0.05949 valid_loss: 0.07672 test_loss: 0.09180 \n",
      "[317/500] train_loss: 0.06119 valid_loss: 0.07632 test_loss: 0.08827 \n",
      "[318/500] train_loss: 0.05967 valid_loss: 0.07629 test_loss: 0.08973 \n",
      "[319/500] train_loss: 0.06148 valid_loss: 0.07542 test_loss: 0.08856 \n",
      "[320/500] train_loss: 0.05924 valid_loss: 0.07803 test_loss: 0.08912 \n",
      "[321/500] train_loss: 0.05992 valid_loss: 0.07583 test_loss: 0.08838 \n",
      "[322/500] train_loss: 0.05820 valid_loss: 0.07550 test_loss: 0.08857 \n",
      "[323/500] train_loss: 0.05857 valid_loss: 0.07618 test_loss: 0.08904 \n",
      "[324/500] train_loss: 0.06107 valid_loss: 0.07665 test_loss: 0.08733 \n",
      "[325/500] train_loss: 0.05875 valid_loss: 0.07576 test_loss: 0.08889 \n",
      "[326/500] train_loss: 0.05764 valid_loss: 0.07664 test_loss: 0.08846 \n",
      "[327/500] train_loss: 0.05928 valid_loss: 0.07661 test_loss: 0.09003 \n",
      "[328/500] train_loss: 0.05703 valid_loss: 0.07666 test_loss: 0.08802 \n",
      "[329/500] train_loss: 0.05681 valid_loss: 0.07626 test_loss: 0.08766 \n",
      "[330/500] train_loss: 0.05793 valid_loss: 0.07476 test_loss: 0.08912 \n",
      "验证损失减少 (0.075136 --> 0.074755). 正在保存模型...\n",
      "[331/500] train_loss: 0.05765 valid_loss: 0.07947 test_loss: 0.08830 \n",
      "[332/500] train_loss: 0.05813 valid_loss: 0.07835 test_loss: 0.08829 \n",
      "[333/500] train_loss: 0.05829 valid_loss: 0.07940 test_loss: 0.08782 \n",
      "[334/500] train_loss: 0.05953 valid_loss: 0.07932 test_loss: 0.08870 \n",
      "[335/500] train_loss: 0.05992 valid_loss: 0.07741 test_loss: 0.08985 \n",
      "[336/500] train_loss: 0.05812 valid_loss: 0.07792 test_loss: 0.09058 \n",
      "[337/500] train_loss: 0.05862 valid_loss: 0.07822 test_loss: 0.08897 \n",
      "[338/500] train_loss: 0.05852 valid_loss: 0.07692 test_loss: 0.08958 \n",
      "[339/500] train_loss: 0.05791 valid_loss: 0.07738 test_loss: 0.08845 \n",
      "[340/500] train_loss: 0.05865 valid_loss: 0.07930 test_loss: 0.08752 \n",
      "[341/500] train_loss: 0.05833 valid_loss: 0.08355 test_loss: 0.08940 \n",
      "[342/500] train_loss: 0.05795 valid_loss: 0.07672 test_loss: 0.08839 \n",
      "[343/500] train_loss: 0.05843 valid_loss: 0.07881 test_loss: 0.08977 \n",
      "[344/500] train_loss: 0.05959 valid_loss: 0.07991 test_loss: 0.08929 \n",
      "[345/500] train_loss: 0.05918 valid_loss: 0.08182 test_loss: 0.08997 \n",
      "[346/500] train_loss: 0.05872 valid_loss: 0.09665 test_loss: 0.08856 \n",
      "[347/500] train_loss: 0.05790 valid_loss: 0.07889 test_loss: 0.09012 \n",
      "[348/500] train_loss: 0.05771 valid_loss: 0.07592 test_loss: 0.08884 \n",
      "[349/500] train_loss: 0.05804 valid_loss: 0.07731 test_loss: 0.09011 \n",
      "[350/500] train_loss: 0.05663 valid_loss: 0.07749 test_loss: 0.08999 \n",
      "[351/500] train_loss: 0.05792 valid_loss: 0.07594 test_loss: 0.08906 \n",
      "[352/500] train_loss: 0.05668 valid_loss: 0.07720 test_loss: 0.08945 \n",
      "[353/500] train_loss: 0.05646 valid_loss: 0.07857 test_loss: 0.08938 \n",
      "[354/500] train_loss: 0.05672 valid_loss: 0.07631 test_loss: 0.08873 \n",
      "[355/500] train_loss: 0.05960 valid_loss: 0.09138 test_loss: 0.09081 \n",
      "[356/500] train_loss: 0.05774 valid_loss: 0.07815 test_loss: 0.08890 \n",
      "[357/500] train_loss: 0.06002 valid_loss: 0.07414 test_loss: 0.08885 \n",
      "验证损失减少 (0.074755 --> 0.074144). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[358/500] train_loss: 0.05885 valid_loss: 0.07924 test_loss: 0.08927 \n",
      "[359/500] train_loss: 0.05770 valid_loss: 0.07639 test_loss: 0.08969 \n",
      "[360/500] train_loss: 0.05684 valid_loss: 0.07458 test_loss: 0.08841 \n",
      "[361/500] train_loss: 0.05746 valid_loss: 0.07729 test_loss: 0.08842 \n",
      "[362/500] train_loss: 0.05639 valid_loss: 0.07658 test_loss: 0.08789 \n",
      "[363/500] train_loss: 0.05851 valid_loss: 0.07806 test_loss: 0.08743 \n",
      "[364/500] train_loss: 0.05688 valid_loss: 0.07522 test_loss: 0.08898 \n",
      "[365/500] train_loss: 0.05362 valid_loss: 0.07715 test_loss: 0.08892 \n",
      "[366/500] train_loss: 0.05656 valid_loss: 0.08028 test_loss: 0.09035 \n",
      "[367/500] train_loss: 0.05850 valid_loss: 0.08536 test_loss: 0.08804 \n",
      "[368/500] train_loss: 0.05752 valid_loss: 0.07999 test_loss: 0.08914 \n",
      "[369/500] train_loss: 0.05710 valid_loss: 0.07707 test_loss: 0.08857 \n",
      "[370/500] train_loss: 0.05746 valid_loss: 0.07628 test_loss: 0.08934 \n",
      "[371/500] train_loss: 0.05676 valid_loss: 0.07687 test_loss: 0.08924 \n",
      "[372/500] train_loss: 0.05724 valid_loss: 0.07589 test_loss: 0.08944 \n",
      "[373/500] train_loss: 0.05532 valid_loss: 0.07600 test_loss: 0.08878 \n",
      "[374/500] train_loss: 0.05745 valid_loss: 0.07663 test_loss: 0.08982 \n",
      "[375/500] train_loss: 0.05719 valid_loss: 0.07619 test_loss: 0.08897 \n",
      "[376/500] train_loss: 0.05591 valid_loss: 0.07635 test_loss: 0.08999 \n",
      "[377/500] train_loss: 0.05639 valid_loss: 0.07716 test_loss: 0.08875 \n",
      "[378/500] train_loss: 0.05508 valid_loss: 0.07748 test_loss: 0.08871 \n",
      "[379/500] train_loss: 0.05502 valid_loss: 0.07991 test_loss: 0.08997 \n",
      "[380/500] train_loss: 0.05851 valid_loss: 0.07729 test_loss: 0.08750 \n",
      "[381/500] train_loss: 0.05955 valid_loss: 0.08069 test_loss: 0.08966 \n",
      "[382/500] train_loss: 0.05631 valid_loss: 0.07761 test_loss: 0.09030 \n",
      "[383/500] train_loss: 0.05544 valid_loss: 0.07958 test_loss: 0.08979 \n",
      "[384/500] train_loss: 0.05629 valid_loss: 0.07836 test_loss: 0.08799 \n",
      "[385/500] train_loss: 0.05702 valid_loss: 0.07885 test_loss: 0.09033 \n",
      "[386/500] train_loss: 0.05626 valid_loss: 0.07693 test_loss: 0.08737 \n",
      "[387/500] train_loss: 0.05589 valid_loss: 0.07814 test_loss: 0.08862 \n",
      "[388/500] train_loss: 0.05541 valid_loss: 0.07890 test_loss: 0.08913 \n",
      "[389/500] train_loss: 0.05544 valid_loss: 0.07930 test_loss: 0.09038 \n",
      "[390/500] train_loss: 0.05473 valid_loss: 0.07749 test_loss: 0.08900 \n",
      "[391/500] train_loss: 0.05615 valid_loss: 0.07999 test_loss: 0.08711 \n",
      "[392/500] train_loss: 0.05380 valid_loss: 0.08035 test_loss: 0.08734 \n",
      "[393/500] train_loss: 0.05560 valid_loss: 0.07609 test_loss: 0.08726 \n",
      "[394/500] train_loss: 0.05565 valid_loss: 0.08280 test_loss: 0.08873 \n",
      "[395/500] train_loss: 0.05595 valid_loss: 0.07662 test_loss: 0.08824 \n",
      "[396/500] train_loss: 0.05529 valid_loss: 0.07833 test_loss: 0.08855 \n",
      "[397/500] train_loss: 0.05532 valid_loss: 0.07747 test_loss: 0.08684 \n",
      "[398/500] train_loss: 0.05675 valid_loss: 0.07576 test_loss: 0.08835 \n",
      "[399/500] train_loss: 0.05444 valid_loss: 0.07754 test_loss: 0.08928 \n",
      "[400/500] train_loss: 0.05559 valid_loss: 0.07607 test_loss: 0.08881 \n",
      "[401/500] train_loss: 0.05585 valid_loss: 0.07503 test_loss: 0.08727 \n",
      "[402/500] train_loss: 0.05630 valid_loss: 0.07682 test_loss: 0.08891 \n",
      "[403/500] train_loss: 0.05439 valid_loss: 0.07897 test_loss: 0.09096 \n",
      "[404/500] train_loss: 0.05592 valid_loss: 0.07533 test_loss: 0.08932 \n",
      "[405/500] train_loss: 0.05543 valid_loss: 0.07688 test_loss: 0.08973 \n",
      "[406/500] train_loss: 0.05526 valid_loss: 0.07460 test_loss: 0.08709 \n",
      "[407/500] train_loss: 0.05646 valid_loss: 0.08033 test_loss: 0.08828 \n",
      "[408/500] train_loss: 0.05662 valid_loss: 0.07648 test_loss: 0.08839 \n",
      "[409/500] train_loss: 0.05673 valid_loss: 0.07485 test_loss: 0.08698 \n",
      "[410/500] train_loss: 0.05496 valid_loss: 0.07703 test_loss: 0.08868 \n",
      "[411/500] train_loss: 0.05338 valid_loss: 0.07635 test_loss: 0.08733 \n",
      "[412/500] train_loss: 0.05371 valid_loss: 0.08002 test_loss: 0.08866 \n",
      "[413/500] train_loss: 0.05466 valid_loss: 0.07577 test_loss: 0.08841 \n",
      "[414/500] train_loss: 0.05565 valid_loss: 0.07760 test_loss: 0.08867 \n",
      "[415/500] train_loss: 0.05496 valid_loss: 0.07474 test_loss: 0.08761 \n",
      "[416/500] train_loss: 0.05616 valid_loss: 0.07708 test_loss: 0.08890 \n",
      "[417/500] train_loss: 0.05422 valid_loss: 0.07692 test_loss: 0.08996 \n",
      "[418/500] train_loss: 0.05563 valid_loss: 0.07663 test_loss: 0.08833 \n",
      "[419/500] train_loss: 0.05537 valid_loss: 0.07637 test_loss: 0.08923 \n",
      "[420/500] train_loss: 0.05401 valid_loss: 0.07794 test_loss: 0.09005 \n",
      "[421/500] train_loss: 0.05342 valid_loss: 0.07657 test_loss: 0.08864 \n",
      "[422/500] train_loss: 0.05593 valid_loss: 0.07684 test_loss: 0.09037 \n",
      "[423/500] train_loss: 0.05552 valid_loss: 0.08608 test_loss: 0.08825 \n",
      "[424/500] train_loss: 0.05434 valid_loss: 0.07863 test_loss: 0.08949 \n",
      "[425/500] train_loss: 0.05491 valid_loss: 0.07798 test_loss: 0.08905 \n",
      "[426/500] train_loss: 0.05386 valid_loss: 0.07861 test_loss: 0.08903 \n",
      "[427/500] train_loss: 0.05641 valid_loss: 0.07533 test_loss: 0.08891 \n",
      "[428/500] train_loss: 0.05585 valid_loss: 0.07627 test_loss: 0.08908 \n",
      "[429/500] train_loss: 0.05417 valid_loss: 0.09243 test_loss: 0.08795 \n",
      "[430/500] train_loss: 0.05356 valid_loss: 0.07542 test_loss: 0.08898 \n",
      "[431/500] train_loss: 0.05503 valid_loss: 0.08202 test_loss: 0.08812 \n",
      "[432/500] train_loss: 0.05286 valid_loss: 0.07561 test_loss: 0.08975 \n",
      "[433/500] train_loss: 0.05424 valid_loss: 0.09706 test_loss: 0.08918 \n",
      "[434/500] train_loss: 0.05576 valid_loss: 0.08014 test_loss: 0.08855 \n",
      "[435/500] train_loss: 0.05465 valid_loss: 0.09216 test_loss: 0.08790 \n",
      "[436/500] train_loss: 0.05440 valid_loss: 0.09736 test_loss: 0.08894 \n",
      "[437/500] train_loss: 0.05466 valid_loss: 0.08911 test_loss: 0.08856 \n",
      "[438/500] train_loss: 0.05417 valid_loss: 0.08581 test_loss: 0.09035 \n",
      "[439/500] train_loss: 0.05413 valid_loss: 0.09035 test_loss: 0.08821 \n",
      "[440/500] train_loss: 0.05395 valid_loss: 0.09089 test_loss: 0.08743 \n",
      "[441/500] train_loss: 0.05415 valid_loss: 0.07854 test_loss: 0.08963 \n",
      "[442/500] train_loss: 0.05145 valid_loss: 0.07780 test_loss: 0.08864 \n",
      "[443/500] train_loss: 0.05484 valid_loss: 0.07703 test_loss: 0.08843 \n",
      "[444/500] train_loss: 0.05304 valid_loss: 0.07593 test_loss: 0.09087 \n",
      "[445/500] train_loss: 0.05403 valid_loss: 0.07463 test_loss: 0.08891 \n",
      "[446/500] train_loss: 0.05439 valid_loss: 0.07647 test_loss: 0.08960 \n",
      "[447/500] train_loss: 0.05342 valid_loss: 0.07544 test_loss: 0.08856 \n",
      "[448/500] train_loss: 0.05469 valid_loss: 0.07740 test_loss: 0.09032 \n",
      "[449/500] train_loss: 0.05424 valid_loss: 0.07810 test_loss: 0.08899 \n",
      "[450/500] train_loss: 0.05265 valid_loss: 0.07568 test_loss: 0.08822 \n",
      "[451/500] train_loss: 0.05540 valid_loss: 0.07532 test_loss: 0.08703 \n",
      "[452/500] train_loss: 0.05307 valid_loss: 0.07681 test_loss: 0.08803 \n",
      "[453/500] train_loss: 0.05507 valid_loss: 0.07607 test_loss: 0.08914 \n",
      "[454/500] train_loss: 0.05371 valid_loss: 0.08203 test_loss: 0.09013 \n",
      "[455/500] train_loss: 0.05465 valid_loss: 0.08543 test_loss: 0.08879 \n",
      "[456/500] train_loss: 0.05310 valid_loss: 0.07778 test_loss: 0.08777 \n",
      "[457/500] train_loss: 0.05316 valid_loss: 0.07657 test_loss: 0.08962 \n",
      "[458/500] train_loss: 0.05411 valid_loss: 0.08216 test_loss: 0.09087 \n",
      "[459/500] train_loss: 0.05449 valid_loss: 0.07838 test_loss: 0.08795 \n",
      "[460/500] train_loss: 0.05358 valid_loss: 0.07546 test_loss: 0.08869 \n",
      "[461/500] train_loss: 0.05349 valid_loss: 0.07705 test_loss: 0.09165 \n",
      "[462/500] train_loss: 0.05286 valid_loss: 0.07688 test_loss: 0.08919 \n",
      "[463/500] train_loss: 0.05318 valid_loss: 0.07612 test_loss: 0.08996 \n",
      "[464/500] train_loss: 0.05291 valid_loss: 0.07575 test_loss: 0.08930 \n",
      "[465/500] train_loss: 0.05297 valid_loss: 0.07481 test_loss: 0.08780 \n",
      "[466/500] train_loss: 0.05378 valid_loss: 0.07574 test_loss: 0.09010 \n",
      "[467/500] train_loss: 0.05403 valid_loss: 0.07673 test_loss: 0.08942 \n",
      "[468/500] train_loss: 0.05749 valid_loss: 0.07757 test_loss: 0.08719 \n",
      "[469/500] train_loss: 0.05357 valid_loss: 0.08319 test_loss: 0.08963 \n",
      "[470/500] train_loss: 0.05313 valid_loss: 0.07980 test_loss: 0.08669 \n",
      "[471/500] train_loss: 0.05345 valid_loss: 0.07576 test_loss: 0.08770 \n",
      "[472/500] train_loss: 0.05405 valid_loss: 0.07911 test_loss: 0.08948 \n",
      "[473/500] train_loss: 0.05236 valid_loss: 0.07783 test_loss: 0.08958 \n",
      "[474/500] train_loss: 0.05279 valid_loss: 0.07670 test_loss: 0.08799 \n",
      "[475/500] train_loss: 0.05217 valid_loss: 0.07841 test_loss: 0.08845 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[476/500] train_loss: 0.05202 valid_loss: 0.07693 test_loss: 0.08884 \n",
      "[477/500] train_loss: 0.05521 valid_loss: 0.07660 test_loss: 0.08739 \n",
      "[478/500] train_loss: 0.05352 valid_loss: 0.07706 test_loss: 0.08840 \n",
      "[479/500] train_loss: 0.05263 valid_loss: 0.08154 test_loss: 0.09061 \n",
      "[480/500] train_loss: 0.05217 valid_loss: 0.07565 test_loss: 0.08904 \n",
      "[481/500] train_loss: 0.05107 valid_loss: 0.07631 test_loss: 0.08784 \n",
      "[482/500] train_loss: 0.05354 valid_loss: 0.08016 test_loss: 0.08910 \n",
      "[483/500] train_loss: 0.05249 valid_loss: 0.07661 test_loss: 0.08633 \n",
      "[484/500] train_loss: 0.05374 valid_loss: 0.07498 test_loss: 0.08806 \n",
      "[485/500] train_loss: 0.05044 valid_loss: 0.07822 test_loss: 0.08898 \n",
      "[486/500] train_loss: 0.05171 valid_loss: 0.08047 test_loss: 0.09027 \n",
      "[487/500] train_loss: 0.05524 valid_loss: 0.08002 test_loss: 0.09034 \n",
      "[488/500] train_loss: 0.05147 valid_loss: 0.07827 test_loss: 0.09010 \n",
      "[489/500] train_loss: 0.05296 valid_loss: 0.07620 test_loss: 0.08834 \n",
      "[490/500] train_loss: 0.05488 valid_loss: 0.07734 test_loss: 0.08808 \n",
      "[491/500] train_loss: 0.05409 valid_loss: 0.07656 test_loss: 0.08821 \n",
      "[492/500] train_loss: 0.05167 valid_loss: 0.07633 test_loss: 0.08882 \n",
      "[493/500] train_loss: 0.05150 valid_loss: 0.07650 test_loss: 0.08739 \n",
      "[494/500] train_loss: 0.05153 valid_loss: 0.07939 test_loss: 0.08936 \n",
      "[495/500] train_loss: 0.05069 valid_loss: 0.07764 test_loss: 0.08821 \n",
      "[496/500] train_loss: 0.05240 valid_loss: 0.07586 test_loss: 0.08781 \n",
      "[497/500] train_loss: 0.05208 valid_loss: 0.07583 test_loss: 0.08718 \n",
      "[498/500] train_loss: 0.05128 valid_loss: 0.07611 test_loss: 0.08850 \n",
      "[499/500] train_loss: 0.05218 valid_loss: 0.09215 test_loss: 0.09046 \n",
      "[500/500] train_loss: 0.05209 valid_loss: 0.07832 test_loss: 0.08894 \n",
      "TRAINING MODEL 12\n",
      "[  1/500] train_loss: 0.43063 valid_loss: 0.29591 test_loss: 0.30192 \n",
      "验证损失减少 (inf --> 0.295914). 正在保存模型...\n",
      "[  2/500] train_loss: 0.22894 valid_loss: 0.20623 test_loss: 0.21734 \n",
      "验证损失减少 (0.295914 --> 0.206233). 正在保存模型...\n",
      "[  3/500] train_loss: 0.18317 valid_loss: 0.17536 test_loss: 0.18824 \n",
      "验证损失减少 (0.206233 --> 0.175361). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15998 valid_loss: 0.15986 test_loss: 0.17130 \n",
      "验证损失减少 (0.175361 --> 0.159856). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14860 valid_loss: 0.15297 test_loss: 0.16608 \n",
      "验证损失减少 (0.159856 --> 0.152969). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14228 valid_loss: 0.14630 test_loss: 0.15968 \n",
      "验证损失减少 (0.152969 --> 0.146296). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13948 valid_loss: 0.14312 test_loss: 0.15786 \n",
      "验证损失减少 (0.146296 --> 0.143121). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13937 valid_loss: 0.13946 test_loss: 0.15296 \n",
      "验证损失减少 (0.143121 --> 0.139461). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13300 valid_loss: 0.13433 test_loss: 0.14788 \n",
      "验证损失减少 (0.139461 --> 0.134334). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12994 valid_loss: 0.13438 test_loss: 0.14872 \n",
      "[ 11/500] train_loss: 0.12532 valid_loss: 0.12764 test_loss: 0.14178 \n",
      "验证损失减少 (0.134334 --> 0.127637). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12677 valid_loss: 0.12570 test_loss: 0.14026 \n",
      "验证损失减少 (0.127637 --> 0.125703). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12001 valid_loss: 0.12808 test_loss: 0.14023 \n",
      "[ 14/500] train_loss: 0.11784 valid_loss: 0.12699 test_loss: 0.13802 \n",
      "[ 15/500] train_loss: 0.12043 valid_loss: 0.12387 test_loss: 0.13873 \n",
      "验证损失减少 (0.125703 --> 0.123873). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11843 valid_loss: 0.12198 test_loss: 0.13576 \n",
      "验证损失减少 (0.123873 --> 0.121985). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11654 valid_loss: 0.11898 test_loss: 0.13275 \n",
      "验证损失减少 (0.121985 --> 0.118985). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11573 valid_loss: 0.11884 test_loss: 0.13361 \n",
      "验证损失减少 (0.118985 --> 0.118839). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11151 valid_loss: 0.12069 test_loss: 0.13196 \n",
      "[ 20/500] train_loss: 0.11347 valid_loss: 0.11968 test_loss: 0.13125 \n",
      "[ 21/500] train_loss: 0.11112 valid_loss: 0.11478 test_loss: 0.12808 \n",
      "验证损失减少 (0.118839 --> 0.114785). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.10871 valid_loss: 0.11330 test_loss: 0.12947 \n",
      "验证损失减少 (0.114785 --> 0.113301). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10920 valid_loss: 0.11808 test_loss: 0.13203 \n",
      "[ 24/500] train_loss: 0.10449 valid_loss: 0.11418 test_loss: 0.12613 \n",
      "[ 25/500] train_loss: 0.10917 valid_loss: 0.11214 test_loss: 0.12629 \n",
      "验证损失减少 (0.113301 --> 0.112140). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10540 valid_loss: 0.11298 test_loss: 0.12625 \n",
      "[ 27/500] train_loss: 0.10708 valid_loss: 0.11060 test_loss: 0.12394 \n",
      "验证损失减少 (0.112140 --> 0.110605). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10619 valid_loss: 0.11183 test_loss: 0.12539 \n",
      "[ 29/500] train_loss: 0.10510 valid_loss: 0.10816 test_loss: 0.12138 \n",
      "验证损失减少 (0.110605 --> 0.108158). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10350 valid_loss: 0.11217 test_loss: 0.12328 \n",
      "[ 31/500] train_loss: 0.10291 valid_loss: 0.11055 test_loss: 0.12261 \n",
      "[ 32/500] train_loss: 0.10318 valid_loss: 0.10667 test_loss: 0.12023 \n",
      "验证损失减少 (0.108158 --> 0.106669). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10477 valid_loss: 0.10953 test_loss: 0.12016 \n",
      "[ 34/500] train_loss: 0.09993 valid_loss: 0.10554 test_loss: 0.11715 \n",
      "验证损失减少 (0.106669 --> 0.105536). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.09818 valid_loss: 0.10466 test_loss: 0.11737 \n",
      "验证损失减少 (0.105536 --> 0.104657). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09966 valid_loss: 0.10467 test_loss: 0.11706 \n",
      "[ 37/500] train_loss: 0.10243 valid_loss: 0.10397 test_loss: 0.11740 \n",
      "验证损失减少 (0.104657 --> 0.103972). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.09905 valid_loss: 0.10315 test_loss: 0.11643 \n",
      "验证损失减少 (0.103972 --> 0.103147). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.09955 valid_loss: 0.10647 test_loss: 0.11755 \n",
      "[ 40/500] train_loss: 0.09812 valid_loss: 0.10386 test_loss: 0.11640 \n",
      "[ 41/500] train_loss: 0.09802 valid_loss: 0.10361 test_loss: 0.11520 \n",
      "[ 42/500] train_loss: 0.09718 valid_loss: 0.10383 test_loss: 0.11535 \n",
      "[ 43/500] train_loss: 0.09678 valid_loss: 0.10359 test_loss: 0.11567 \n",
      "[ 44/500] train_loss: 0.09716 valid_loss: 0.10107 test_loss: 0.11382 \n",
      "验证损失减少 (0.103147 --> 0.101071). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.09536 valid_loss: 0.10014 test_loss: 0.11307 \n",
      "验证损失减少 (0.101071 --> 0.100140). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.09345 valid_loss: 0.10171 test_loss: 0.11277 \n",
      "[ 47/500] train_loss: 0.09460 valid_loss: 0.10040 test_loss: 0.11297 \n",
      "[ 48/500] train_loss: 0.09367 valid_loss: 0.10085 test_loss: 0.11205 \n",
      "[ 49/500] train_loss: 0.09171 valid_loss: 0.10059 test_loss: 0.11287 \n",
      "[ 50/500] train_loss: 0.09401 valid_loss: 0.09757 test_loss: 0.11150 \n",
      "验证损失减少 (0.100140 --> 0.097571). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.09153 valid_loss: 0.09747 test_loss: 0.11093 \n",
      "验证损失减少 (0.097571 --> 0.097472). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.09351 valid_loss: 0.10072 test_loss: 0.11064 \n",
      "[ 53/500] train_loss: 0.09271 valid_loss: 0.09601 test_loss: 0.10823 \n",
      "验证损失减少 (0.097472 --> 0.096006). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.09240 valid_loss: 0.09766 test_loss: 0.11140 \n",
      "[ 55/500] train_loss: 0.09380 valid_loss: 0.09835 test_loss: 0.10996 \n",
      "[ 56/500] train_loss: 0.08834 valid_loss: 0.10049 test_loss: 0.11009 \n",
      "[ 57/500] train_loss: 0.09078 valid_loss: 0.09683 test_loss: 0.10796 \n",
      "[ 58/500] train_loss: 0.09024 valid_loss: 0.09663 test_loss: 0.10844 \n",
      "[ 59/500] train_loss: 0.09109 valid_loss: 0.09470 test_loss: 0.10701 \n",
      "验证损失减少 (0.096006 --> 0.094697). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.08890 valid_loss: 0.09801 test_loss: 0.10745 \n",
      "[ 61/500] train_loss: 0.08653 valid_loss: 0.09481 test_loss: 0.10554 \n",
      "[ 62/500] train_loss: 0.08884 valid_loss: 0.09469 test_loss: 0.10750 \n",
      "验证损失减少 (0.094697 --> 0.094691). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.09156 valid_loss: 0.09445 test_loss: 0.10576 \n",
      "验证损失减少 (0.094691 --> 0.094448). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.09000 valid_loss: 0.09447 test_loss: 0.10509 \n",
      "[ 65/500] train_loss: 0.08876 valid_loss: 0.09391 test_loss: 0.10647 \n",
      "验证损失减少 (0.094448 --> 0.093910). 正在保存模型...\n",
      "[ 66/500] train_loss: 0.08850 valid_loss: 0.09553 test_loss: 0.10612 \n",
      "[ 67/500] train_loss: 0.08510 valid_loss: 0.09698 test_loss: 0.10725 \n",
      "[ 68/500] train_loss: 0.08506 valid_loss: 0.09749 test_loss: 0.10839 \n",
      "[ 69/500] train_loss: 0.08837 valid_loss: 0.09271 test_loss: 0.10517 \n",
      "验证损失减少 (0.093910 --> 0.092707). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.08634 valid_loss: 0.09383 test_loss: 0.10634 \n",
      "[ 71/500] train_loss: 0.08511 valid_loss: 0.09161 test_loss: 0.10476 \n",
      "验证损失减少 (0.092707 --> 0.091610). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 72/500] train_loss: 0.08692 valid_loss: 0.09207 test_loss: 0.10383 \n",
      "[ 73/500] train_loss: 0.08380 valid_loss: 0.09167 test_loss: 0.10431 \n",
      "[ 74/500] train_loss: 0.08289 valid_loss: 0.09101 test_loss: 0.10303 \n",
      "验证损失减少 (0.091610 --> 0.091011). 正在保存模型...\n",
      "[ 75/500] train_loss: 0.08496 valid_loss: 0.09520 test_loss: 0.10674 \n",
      "[ 76/500] train_loss: 0.08407 valid_loss: 0.09149 test_loss: 0.10214 \n",
      "[ 77/500] train_loss: 0.08265 valid_loss: 0.09204 test_loss: 0.10281 \n",
      "[ 78/500] train_loss: 0.08433 valid_loss: 0.09102 test_loss: 0.10323 \n",
      "[ 79/500] train_loss: 0.08287 valid_loss: 0.09118 test_loss: 0.10309 \n",
      "[ 80/500] train_loss: 0.08371 valid_loss: 0.09238 test_loss: 0.10223 \n",
      "[ 81/500] train_loss: 0.08392 valid_loss: 0.09119 test_loss: 0.10306 \n",
      "[ 82/500] train_loss: 0.08477 valid_loss: 0.09019 test_loss: 0.10084 \n",
      "验证损失减少 (0.091011 --> 0.090195). 正在保存模型...\n",
      "[ 83/500] train_loss: 0.08119 valid_loss: 0.09073 test_loss: 0.10235 \n",
      "[ 84/500] train_loss: 0.08278 valid_loss: 0.08925 test_loss: 0.10071 \n",
      "验证损失减少 (0.090195 --> 0.089247). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.08212 valid_loss: 0.08839 test_loss: 0.10022 \n",
      "验证损失减少 (0.089247 --> 0.088392). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08234 valid_loss: 0.08903 test_loss: 0.10054 \n",
      "[ 87/500] train_loss: 0.08375 valid_loss: 0.09099 test_loss: 0.10034 \n",
      "[ 88/500] train_loss: 0.07987 valid_loss: 0.08806 test_loss: 0.10059 \n",
      "验证损失减少 (0.088392 --> 0.088064). 正在保存模型...\n",
      "[ 89/500] train_loss: 0.08061 valid_loss: 0.08901 test_loss: 0.10206 \n",
      "[ 90/500] train_loss: 0.08327 valid_loss: 0.09089 test_loss: 0.10087 \n",
      "[ 91/500] train_loss: 0.08197 valid_loss: 0.08853 test_loss: 0.10131 \n",
      "[ 92/500] train_loss: 0.08024 valid_loss: 0.08751 test_loss: 0.10030 \n",
      "验证损失减少 (0.088064 --> 0.087507). 正在保存模型...\n",
      "[ 93/500] train_loss: 0.08001 valid_loss: 0.08846 test_loss: 0.10002 \n",
      "[ 94/500] train_loss: 0.07880 valid_loss: 0.08742 test_loss: 0.09946 \n",
      "验证损失减少 (0.087507 --> 0.087424). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.08232 valid_loss: 0.09026 test_loss: 0.09921 \n",
      "[ 96/500] train_loss: 0.07941 valid_loss: 0.09138 test_loss: 0.10184 \n",
      "[ 97/500] train_loss: 0.07968 valid_loss: 0.08897 test_loss: 0.09994 \n",
      "[ 98/500] train_loss: 0.07995 valid_loss: 0.09443 test_loss: 0.10128 \n",
      "[ 99/500] train_loss: 0.07949 valid_loss: 0.08979 test_loss: 0.09862 \n",
      "[100/500] train_loss: 0.07941 valid_loss: 0.09299 test_loss: 0.09914 \n",
      "[101/500] train_loss: 0.07761 valid_loss: 0.08780 test_loss: 0.09827 \n",
      "[102/500] train_loss: 0.07931 valid_loss: 0.08696 test_loss: 0.09778 \n",
      "验证损失减少 (0.087424 --> 0.086955). 正在保存模型...\n",
      "[103/500] train_loss: 0.07889 valid_loss: 0.08704 test_loss: 0.09786 \n",
      "[104/500] train_loss: 0.07953 valid_loss: 0.08735 test_loss: 0.09787 \n",
      "[105/500] train_loss: 0.07843 valid_loss: 0.08745 test_loss: 0.09699 \n",
      "[106/500] train_loss: 0.07938 valid_loss: 0.08742 test_loss: 0.09744 \n",
      "[107/500] train_loss: 0.07939 valid_loss: 0.08821 test_loss: 0.09731 \n",
      "[108/500] train_loss: 0.07601 valid_loss: 0.08905 test_loss: 0.09869 \n",
      "[109/500] train_loss: 0.07857 valid_loss: 0.08553 test_loss: 0.09657 \n",
      "验证损失减少 (0.086955 --> 0.085534). 正在保存模型...\n",
      "[110/500] train_loss: 0.07667 valid_loss: 0.08775 test_loss: 0.09742 \n",
      "[111/500] train_loss: 0.07840 valid_loss: 0.08606 test_loss: 0.09679 \n",
      "[112/500] train_loss: 0.07690 valid_loss: 0.08525 test_loss: 0.09608 \n",
      "验证损失减少 (0.085534 --> 0.085246). 正在保存模型...\n",
      "[113/500] train_loss: 0.07842 valid_loss: 0.08700 test_loss: 0.09697 \n",
      "[114/500] train_loss: 0.07706 valid_loss: 0.08422 test_loss: 0.09689 \n",
      "验证损失减少 (0.085246 --> 0.084223). 正在保存模型...\n",
      "[115/500] train_loss: 0.07667 valid_loss: 0.08471 test_loss: 0.09704 \n",
      "[116/500] train_loss: 0.07621 valid_loss: 0.08411 test_loss: 0.09586 \n",
      "验证损失减少 (0.084223 --> 0.084112). 正在保存模型...\n",
      "[117/500] train_loss: 0.07664 valid_loss: 0.08538 test_loss: 0.09609 \n",
      "[118/500] train_loss: 0.07662 valid_loss: 0.08609 test_loss: 0.09826 \n",
      "[119/500] train_loss: 0.07613 valid_loss: 0.08627 test_loss: 0.09513 \n",
      "[120/500] train_loss: 0.07458 valid_loss: 0.08753 test_loss: 0.09683 \n",
      "[121/500] train_loss: 0.07517 valid_loss: 0.08582 test_loss: 0.09774 \n",
      "[122/500] train_loss: 0.07501 valid_loss: 0.08607 test_loss: 0.09599 \n",
      "[123/500] train_loss: 0.07571 valid_loss: 0.08446 test_loss: 0.09603 \n",
      "[124/500] train_loss: 0.07489 valid_loss: 0.08707 test_loss: 0.09693 \n",
      "[125/500] train_loss: 0.07651 valid_loss: 0.08607 test_loss: 0.09516 \n",
      "[126/500] train_loss: 0.07553 valid_loss: 0.08445 test_loss: 0.09625 \n",
      "[127/500] train_loss: 0.07678 valid_loss: 0.08448 test_loss: 0.09557 \n",
      "[128/500] train_loss: 0.07402 valid_loss: 0.08618 test_loss: 0.09558 \n",
      "[129/500] train_loss: 0.07622 valid_loss: 0.08713 test_loss: 0.09665 \n",
      "[130/500] train_loss: 0.07209 valid_loss: 0.08432 test_loss: 0.09563 \n",
      "[131/500] train_loss: 0.07294 valid_loss: 0.08532 test_loss: 0.09457 \n",
      "[132/500] train_loss: 0.07510 valid_loss: 0.08342 test_loss: 0.09488 \n",
      "验证损失减少 (0.084112 --> 0.083425). 正在保存模型...\n",
      "[133/500] train_loss: 0.07644 valid_loss: 0.08252 test_loss: 0.09512 \n",
      "验证损失减少 (0.083425 --> 0.082516). 正在保存模型...\n",
      "[134/500] train_loss: 0.07408 valid_loss: 0.08485 test_loss: 0.09523 \n",
      "[135/500] train_loss: 0.07295 valid_loss: 0.08337 test_loss: 0.09567 \n",
      "[136/500] train_loss: 0.07374 valid_loss: 0.08246 test_loss: 0.09376 \n",
      "验证损失减少 (0.082516 --> 0.082460). 正在保存模型...\n",
      "[137/500] train_loss: 0.07396 valid_loss: 0.08418 test_loss: 0.09409 \n",
      "[138/500] train_loss: 0.07297 valid_loss: 0.08440 test_loss: 0.09697 \n",
      "[139/500] train_loss: 0.07236 valid_loss: 0.08366 test_loss: 0.09409 \n",
      "[140/500] train_loss: 0.07299 valid_loss: 0.08193 test_loss: 0.09443 \n",
      "验证损失减少 (0.082460 --> 0.081932). 正在保存模型...\n",
      "[141/500] train_loss: 0.07115 valid_loss: 0.08342 test_loss: 0.09535 \n",
      "[142/500] train_loss: 0.07647 valid_loss: 0.08585 test_loss: 0.09530 \n",
      "[143/500] train_loss: 0.07391 valid_loss: 0.08134 test_loss: 0.09487 \n",
      "验证损失减少 (0.081932 --> 0.081341). 正在保存模型...\n",
      "[144/500] train_loss: 0.07380 valid_loss: 0.08522 test_loss: 0.09409 \n",
      "[145/500] train_loss: 0.07110 valid_loss: 0.08413 test_loss: 0.09423 \n",
      "[146/500] train_loss: 0.07357 valid_loss: 0.08291 test_loss: 0.09418 \n",
      "[147/500] train_loss: 0.07108 valid_loss: 0.08149 test_loss: 0.09342 \n",
      "[148/500] train_loss: 0.07418 valid_loss: 0.08305 test_loss: 0.09421 \n",
      "[149/500] train_loss: 0.07227 valid_loss: 0.08274 test_loss: 0.09504 \n",
      "[150/500] train_loss: 0.07205 valid_loss: 0.08566 test_loss: 0.09504 \n",
      "[151/500] train_loss: 0.07240 valid_loss: 0.08111 test_loss: 0.09349 \n",
      "验证损失减少 (0.081341 --> 0.081106). 正在保存模型...\n",
      "[152/500] train_loss: 0.07141 valid_loss: 0.08289 test_loss: 0.09329 \n",
      "[153/500] train_loss: 0.07221 valid_loss: 0.08313 test_loss: 0.09293 \n",
      "[154/500] train_loss: 0.07024 valid_loss: 0.08108 test_loss: 0.09276 \n",
      "验证损失减少 (0.081106 --> 0.081078). 正在保存模型...\n",
      "[155/500] train_loss: 0.07100 valid_loss: 0.08239 test_loss: 0.09294 \n",
      "[156/500] train_loss: 0.07125 valid_loss: 0.08187 test_loss: 0.09264 \n",
      "[157/500] train_loss: 0.07213 valid_loss: 0.08192 test_loss: 0.09274 \n",
      "[158/500] train_loss: 0.07168 valid_loss: 0.08414 test_loss: 0.09371 \n",
      "[159/500] train_loss: 0.07167 valid_loss: 0.08153 test_loss: 0.09273 \n",
      "[160/500] train_loss: 0.07234 valid_loss: 0.08131 test_loss: 0.09307 \n",
      "[161/500] train_loss: 0.06986 valid_loss: 0.08639 test_loss: 0.09174 \n",
      "[162/500] train_loss: 0.07118 valid_loss: 0.08749 test_loss: 0.09436 \n",
      "[163/500] train_loss: 0.07174 valid_loss: 0.07996 test_loss: 0.09201 \n",
      "验证损失减少 (0.081078 --> 0.079959). 正在保存模型...\n",
      "[164/500] train_loss: 0.07086 valid_loss: 0.08662 test_loss: 0.09275 \n",
      "[165/500] train_loss: 0.07161 valid_loss: 0.08182 test_loss: 0.09387 \n",
      "[166/500] train_loss: 0.07032 valid_loss: 0.08142 test_loss: 0.09282 \n",
      "[167/500] train_loss: 0.06946 valid_loss: 0.08261 test_loss: 0.09151 \n",
      "[168/500] train_loss: 0.06904 valid_loss: 0.08455 test_loss: 0.09283 \n",
      "[169/500] train_loss: 0.07138 valid_loss: 0.08245 test_loss: 0.09356 \n",
      "[170/500] train_loss: 0.06861 valid_loss: 0.08323 test_loss: 0.09265 \n",
      "[171/500] train_loss: 0.07167 valid_loss: 0.08241 test_loss: 0.09176 \n",
      "[172/500] train_loss: 0.06922 valid_loss: 0.08150 test_loss: 0.09189 \n",
      "[173/500] train_loss: 0.07053 valid_loss: 0.08091 test_loss: 0.09241 \n",
      "[174/500] train_loss: 0.07092 valid_loss: 0.07964 test_loss: 0.09122 \n",
      "验证损失减少 (0.079959 --> 0.079639). 正在保存模型...\n",
      "[175/500] train_loss: 0.07020 valid_loss: 0.08352 test_loss: 0.09084 \n",
      "[176/500] train_loss: 0.06895 valid_loss: 0.08589 test_loss: 0.09150 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[177/500] train_loss: 0.06783 valid_loss: 0.08188 test_loss: 0.09095 \n",
      "[178/500] train_loss: 0.06828 valid_loss: 0.07936 test_loss: 0.09179 \n",
      "验证损失减少 (0.079639 --> 0.079365). 正在保存模型...\n",
      "[179/500] train_loss: 0.07046 valid_loss: 0.08101 test_loss: 0.09179 \n",
      "[180/500] train_loss: 0.07151 valid_loss: 0.07956 test_loss: 0.09165 \n",
      "[181/500] train_loss: 0.06921 valid_loss: 0.07986 test_loss: 0.09124 \n",
      "[182/500] train_loss: 0.06890 valid_loss: 0.08062 test_loss: 0.09129 \n",
      "[183/500] train_loss: 0.06887 valid_loss: 0.07908 test_loss: 0.09070 \n",
      "验证损失减少 (0.079365 --> 0.079084). 正在保存模型...\n",
      "[184/500] train_loss: 0.06886 valid_loss: 0.07879 test_loss: 0.09077 \n",
      "验证损失减少 (0.079084 --> 0.078795). 正在保存模型...\n",
      "[185/500] train_loss: 0.06967 valid_loss: 0.07891 test_loss: 0.09100 \n",
      "[186/500] train_loss: 0.06857 valid_loss: 0.08024 test_loss: 0.09019 \n",
      "[187/500] train_loss: 0.06842 valid_loss: 0.07937 test_loss: 0.09080 \n",
      "[188/500] train_loss: 0.06970 valid_loss: 0.07906 test_loss: 0.09079 \n",
      "[189/500] train_loss: 0.06737 valid_loss: 0.07901 test_loss: 0.09152 \n",
      "[190/500] train_loss: 0.06916 valid_loss: 0.07983 test_loss: 0.09046 \n",
      "[191/500] train_loss: 0.06862 valid_loss: 0.07819 test_loss: 0.09021 \n",
      "验证损失减少 (0.078795 --> 0.078191). 正在保存模型...\n",
      "[192/500] train_loss: 0.06639 valid_loss: 0.07822 test_loss: 0.09034 \n",
      "[193/500] train_loss: 0.06750 valid_loss: 0.07793 test_loss: 0.09046 \n",
      "验证损失减少 (0.078191 --> 0.077926). 正在保存模型...\n",
      "[194/500] train_loss: 0.06801 valid_loss: 0.07985 test_loss: 0.09100 \n",
      "[195/500] train_loss: 0.06716 valid_loss: 0.08024 test_loss: 0.09268 \n",
      "[196/500] train_loss: 0.06843 valid_loss: 0.07895 test_loss: 0.09070 \n",
      "[197/500] train_loss: 0.06761 valid_loss: 0.07886 test_loss: 0.09105 \n",
      "[198/500] train_loss: 0.06630 valid_loss: 0.08046 test_loss: 0.08955 \n",
      "[199/500] train_loss: 0.06555 valid_loss: 0.08086 test_loss: 0.09121 \n",
      "[200/500] train_loss: 0.06801 valid_loss: 0.07746 test_loss: 0.09014 \n",
      "验证损失减少 (0.077926 --> 0.077455). 正在保存模型...\n",
      "[201/500] train_loss: 0.06660 valid_loss: 0.08324 test_loss: 0.09016 \n",
      "[202/500] train_loss: 0.06838 valid_loss: 0.07854 test_loss: 0.09097 \n",
      "[203/500] train_loss: 0.06647 valid_loss: 0.07957 test_loss: 0.09063 \n",
      "[204/500] train_loss: 0.06718 valid_loss: 0.08015 test_loss: 0.09023 \n",
      "[205/500] train_loss: 0.06742 valid_loss: 0.07965 test_loss: 0.09092 \n",
      "[206/500] train_loss: 0.06687 valid_loss: 0.07836 test_loss: 0.09037 \n",
      "[207/500] train_loss: 0.06648 valid_loss: 0.07892 test_loss: 0.09016 \n",
      "[208/500] train_loss: 0.06637 valid_loss: 0.07871 test_loss: 0.09190 \n",
      "[209/500] train_loss: 0.06713 valid_loss: 0.07840 test_loss: 0.08945 \n",
      "[210/500] train_loss: 0.06715 valid_loss: 0.07844 test_loss: 0.09110 \n",
      "[211/500] train_loss: 0.06607 valid_loss: 0.07709 test_loss: 0.08944 \n",
      "验证损失减少 (0.077455 --> 0.077089). 正在保存模型...\n",
      "[212/500] train_loss: 0.06460 valid_loss: 0.08039 test_loss: 0.09073 \n",
      "[213/500] train_loss: 0.06656 valid_loss: 0.07880 test_loss: 0.09032 \n",
      "[214/500] train_loss: 0.06652 valid_loss: 0.07655 test_loss: 0.08887 \n",
      "验证损失减少 (0.077089 --> 0.076545). 正在保存模型...\n",
      "[215/500] train_loss: 0.06696 valid_loss: 0.07817 test_loss: 0.09220 \n",
      "[216/500] train_loss: 0.06552 valid_loss: 0.07745 test_loss: 0.08995 \n",
      "[217/500] train_loss: 0.06553 valid_loss: 0.07638 test_loss: 0.08881 \n",
      "验证损失减少 (0.076545 --> 0.076378). 正在保存模型...\n",
      "[218/500] train_loss: 0.06598 valid_loss: 0.07675 test_loss: 0.08903 \n",
      "[219/500] train_loss: 0.06453 valid_loss: 0.07930 test_loss: 0.08996 \n",
      "[220/500] train_loss: 0.06447 valid_loss: 0.07666 test_loss: 0.08937 \n",
      "[221/500] train_loss: 0.06466 valid_loss: 0.07774 test_loss: 0.08979 \n",
      "[222/500] train_loss: 0.06489 valid_loss: 0.07745 test_loss: 0.08939 \n",
      "[223/500] train_loss: 0.06508 valid_loss: 0.07908 test_loss: 0.09088 \n",
      "[224/500] train_loss: 0.06483 valid_loss: 0.07693 test_loss: 0.08848 \n",
      "[225/500] train_loss: 0.06444 valid_loss: 0.07809 test_loss: 0.08879 \n",
      "[226/500] train_loss: 0.06593 valid_loss: 0.07838 test_loss: 0.08960 \n",
      "[227/500] train_loss: 0.06484 valid_loss: 0.07835 test_loss: 0.08988 \n",
      "[228/500] train_loss: 0.06388 valid_loss: 0.07736 test_loss: 0.08892 \n",
      "[229/500] train_loss: 0.06568 valid_loss: 0.07744 test_loss: 0.08944 \n",
      "[230/500] train_loss: 0.06349 valid_loss: 0.07730 test_loss: 0.08911 \n",
      "[231/500] train_loss: 0.06720 valid_loss: 0.08007 test_loss: 0.08920 \n",
      "[232/500] train_loss: 0.06532 valid_loss: 0.08304 test_loss: 0.08868 \n",
      "[233/500] train_loss: 0.06545 valid_loss: 0.07643 test_loss: 0.08962 \n",
      "[234/500] train_loss: 0.06543 valid_loss: 0.07804 test_loss: 0.08876 \n",
      "[235/500] train_loss: 0.06407 valid_loss: 0.08095 test_loss: 0.08892 \n",
      "[236/500] train_loss: 0.06603 valid_loss: 0.07732 test_loss: 0.08903 \n",
      "[237/500] train_loss: 0.06405 valid_loss: 0.07712 test_loss: 0.08993 \n",
      "[238/500] train_loss: 0.06524 valid_loss: 0.07677 test_loss: 0.08854 \n",
      "[239/500] train_loss: 0.06423 valid_loss: 0.07961 test_loss: 0.08796 \n",
      "[240/500] train_loss: 0.06363 valid_loss: 0.07771 test_loss: 0.09004 \n",
      "[241/500] train_loss: 0.06396 valid_loss: 0.07671 test_loss: 0.08952 \n",
      "[242/500] train_loss: 0.06523 valid_loss: 0.07672 test_loss: 0.08923 \n",
      "[243/500] train_loss: 0.06236 valid_loss: 0.07703 test_loss: 0.08907 \n",
      "[244/500] train_loss: 0.06349 valid_loss: 0.07994 test_loss: 0.08957 \n",
      "[245/500] train_loss: 0.06607 valid_loss: 0.07963 test_loss: 0.08997 \n",
      "[246/500] train_loss: 0.06469 valid_loss: 0.07686 test_loss: 0.08871 \n",
      "[247/500] train_loss: 0.06452 valid_loss: 0.07807 test_loss: 0.09102 \n",
      "[248/500] train_loss: 0.06374 valid_loss: 0.07712 test_loss: 0.08934 \n",
      "[249/500] train_loss: 0.06367 valid_loss: 0.07676 test_loss: 0.08844 \n",
      "[250/500] train_loss: 0.06376 valid_loss: 0.07723 test_loss: 0.08844 \n",
      "[251/500] train_loss: 0.06249 valid_loss: 0.07592 test_loss: 0.08812 \n",
      "验证损失减少 (0.076378 --> 0.075917). 正在保存模型...\n",
      "[252/500] train_loss: 0.06262 valid_loss: 0.07687 test_loss: 0.08827 \n",
      "[253/500] train_loss: 0.06391 valid_loss: 0.07612 test_loss: 0.08867 \n",
      "[254/500] train_loss: 0.06424 valid_loss: 0.07613 test_loss: 0.08755 \n",
      "[255/500] train_loss: 0.06349 valid_loss: 0.07590 test_loss: 0.08755 \n",
      "验证损失减少 (0.075917 --> 0.075899). 正在保存模型...\n",
      "[256/500] train_loss: 0.06352 valid_loss: 0.07586 test_loss: 0.08856 \n",
      "验证损失减少 (0.075899 --> 0.075861). 正在保存模型...\n",
      "[257/500] train_loss: 0.06149 valid_loss: 0.07623 test_loss: 0.08808 \n",
      "[258/500] train_loss: 0.06416 valid_loss: 0.07576 test_loss: 0.08802 \n",
      "验证损失减少 (0.075861 --> 0.075757). 正在保存模型...\n",
      "[259/500] train_loss: 0.06233 valid_loss: 0.07559 test_loss: 0.08816 \n",
      "验证损失减少 (0.075757 --> 0.075591). 正在保存模型...\n",
      "[260/500] train_loss: 0.06412 valid_loss: 0.07573 test_loss: 0.08998 \n",
      "[261/500] train_loss: 0.06228 valid_loss: 0.07901 test_loss: 0.08814 \n",
      "[262/500] train_loss: 0.06339 valid_loss: 0.07852 test_loss: 0.08775 \n",
      "[263/500] train_loss: 0.06173 valid_loss: 0.07646 test_loss: 0.08961 \n",
      "[264/500] train_loss: 0.06248 valid_loss: 0.07606 test_loss: 0.09045 \n",
      "[265/500] train_loss: 0.06390 valid_loss: 0.07512 test_loss: 0.08957 \n",
      "验证损失减少 (0.075591 --> 0.075123). 正在保存模型...\n",
      "[266/500] train_loss: 0.06339 valid_loss: 0.07488 test_loss: 0.08792 \n",
      "验证损失减少 (0.075123 --> 0.074880). 正在保存模型...\n",
      "[267/500] train_loss: 0.06234 valid_loss: 0.07494 test_loss: 0.08891 \n",
      "[268/500] train_loss: 0.06103 valid_loss: 0.07572 test_loss: 0.08789 \n",
      "[269/500] train_loss: 0.06346 valid_loss: 0.07589 test_loss: 0.08892 \n",
      "[270/500] train_loss: 0.06328 valid_loss: 0.07459 test_loss: 0.08799 \n",
      "验证损失减少 (0.074880 --> 0.074589). 正在保存模型...\n",
      "[271/500] train_loss: 0.06215 valid_loss: 0.07572 test_loss: 0.09012 \n",
      "[272/500] train_loss: 0.06201 valid_loss: 0.07462 test_loss: 0.08751 \n",
      "[273/500] train_loss: 0.06148 valid_loss: 0.07488 test_loss: 0.08785 \n",
      "[274/500] train_loss: 0.06249 valid_loss: 0.07419 test_loss: 0.08785 \n",
      "验证损失减少 (0.074589 --> 0.074186). 正在保存模型...\n",
      "[275/500] train_loss: 0.06209 valid_loss: 0.07449 test_loss: 0.08801 \n",
      "[276/500] train_loss: 0.06161 valid_loss: 0.07613 test_loss: 0.08950 \n",
      "[277/500] train_loss: 0.06208 valid_loss: 0.07898 test_loss: 0.08956 \n",
      "[278/500] train_loss: 0.06303 valid_loss: 0.07658 test_loss: 0.08899 \n",
      "[279/500] train_loss: 0.05992 valid_loss: 0.07579 test_loss: 0.08833 \n",
      "[280/500] train_loss: 0.06279 valid_loss: 0.07902 test_loss: 0.08876 \n",
      "[281/500] train_loss: 0.06189 valid_loss: 0.07551 test_loss: 0.08787 \n",
      "[282/500] train_loss: 0.06116 valid_loss: 0.07704 test_loss: 0.08779 \n",
      "[283/500] train_loss: 0.05813 valid_loss: 0.08106 test_loss: 0.08808 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284/500] train_loss: 0.06050 valid_loss: 0.07648 test_loss: 0.08898 \n",
      "[285/500] train_loss: 0.05896 valid_loss: 0.07603 test_loss: 0.08813 \n",
      "[286/500] train_loss: 0.06325 valid_loss: 0.07625 test_loss: 0.08755 \n",
      "[287/500] train_loss: 0.05932 valid_loss: 0.07786 test_loss: 0.08842 \n",
      "[288/500] train_loss: 0.06077 valid_loss: 0.07552 test_loss: 0.08843 \n",
      "[289/500] train_loss: 0.05771 valid_loss: 0.07533 test_loss: 0.08816 \n",
      "[290/500] train_loss: 0.06043 valid_loss: 0.07502 test_loss: 0.08852 \n",
      "[291/500] train_loss: 0.06143 valid_loss: 0.08052 test_loss: 0.08882 \n",
      "[292/500] train_loss: 0.06099 valid_loss: 0.08186 test_loss: 0.08688 \n",
      "[293/500] train_loss: 0.06163 valid_loss: 0.08796 test_loss: 0.08832 \n",
      "[294/500] train_loss: 0.06105 valid_loss: 0.07996 test_loss: 0.08942 \n",
      "[295/500] train_loss: 0.06147 valid_loss: 0.07551 test_loss: 0.08769 \n",
      "[296/500] train_loss: 0.05964 valid_loss: 0.07662 test_loss: 0.08944 \n",
      "[297/500] train_loss: 0.06061 valid_loss: 0.07588 test_loss: 0.08872 \n",
      "[298/500] train_loss: 0.05934 valid_loss: 0.07541 test_loss: 0.08752 \n",
      "[299/500] train_loss: 0.06107 valid_loss: 0.07587 test_loss: 0.08830 \n",
      "[300/500] train_loss: 0.05997 valid_loss: 0.07720 test_loss: 0.08647 \n",
      "[301/500] train_loss: 0.06061 valid_loss: 0.07460 test_loss: 0.08712 \n",
      "[302/500] train_loss: 0.05946 valid_loss: 0.07626 test_loss: 0.08849 \n",
      "[303/500] train_loss: 0.05959 valid_loss: 0.07494 test_loss: 0.08869 \n",
      "[304/500] train_loss: 0.05968 valid_loss: 0.07667 test_loss: 0.08737 \n",
      "[305/500] train_loss: 0.06004 valid_loss: 0.07617 test_loss: 0.08861 \n",
      "[306/500] train_loss: 0.05997 valid_loss: 0.07878 test_loss: 0.08751 \n",
      "[307/500] train_loss: 0.05942 valid_loss: 0.07594 test_loss: 0.08719 \n",
      "[308/500] train_loss: 0.05912 valid_loss: 0.08790 test_loss: 0.08743 \n",
      "[309/500] train_loss: 0.06071 valid_loss: 0.07507 test_loss: 0.08784 \n",
      "[310/500] train_loss: 0.06031 valid_loss: 0.07606 test_loss: 0.08894 \n",
      "[311/500] train_loss: 0.06164 valid_loss: 0.07573 test_loss: 0.08809 \n",
      "[312/500] train_loss: 0.05859 valid_loss: 0.07469 test_loss: 0.08673 \n",
      "[313/500] train_loss: 0.05969 valid_loss: 0.07502 test_loss: 0.08747 \n",
      "[314/500] train_loss: 0.05864 valid_loss: 0.07692 test_loss: 0.08709 \n",
      "[315/500] train_loss: 0.05876 valid_loss: 0.07409 test_loss: 0.08745 \n",
      "验证损失减少 (0.074186 --> 0.074088). 正在保存模型...\n",
      "[316/500] train_loss: 0.05816 valid_loss: 0.08052 test_loss: 0.08820 \n",
      "[317/500] train_loss: 0.05915 valid_loss: 0.07747 test_loss: 0.08931 \n",
      "[318/500] train_loss: 0.05945 valid_loss: 0.07409 test_loss: 0.08726 \n",
      "[319/500] train_loss: 0.06008 valid_loss: 0.07712 test_loss: 0.08728 \n",
      "[320/500] train_loss: 0.05755 valid_loss: 0.07989 test_loss: 0.08864 \n",
      "[321/500] train_loss: 0.05894 valid_loss: 0.07551 test_loss: 0.08633 \n",
      "[322/500] train_loss: 0.05698 valid_loss: 0.07546 test_loss: 0.08842 \n",
      "[323/500] train_loss: 0.05856 valid_loss: 0.07473 test_loss: 0.08740 \n",
      "[324/500] train_loss: 0.05847 valid_loss: 0.07681 test_loss: 0.08979 \n",
      "[325/500] train_loss: 0.05976 valid_loss: 0.07630 test_loss: 0.08808 \n",
      "[326/500] train_loss: 0.05892 valid_loss: 0.07501 test_loss: 0.08789 \n",
      "[327/500] train_loss: 0.05888 valid_loss: 0.07426 test_loss: 0.08721 \n",
      "[328/500] train_loss: 0.05805 valid_loss: 0.07518 test_loss: 0.08924 \n",
      "[329/500] train_loss: 0.05970 valid_loss: 0.07489 test_loss: 0.08790 \n",
      "[330/500] train_loss: 0.05879 valid_loss: 0.07434 test_loss: 0.08764 \n",
      "[331/500] train_loss: 0.05839 valid_loss: 0.07600 test_loss: 0.08863 \n",
      "[332/500] train_loss: 0.05895 valid_loss: 0.07448 test_loss: 0.08948 \n",
      "[333/500] train_loss: 0.06006 valid_loss: 0.07432 test_loss: 0.08755 \n",
      "[334/500] train_loss: 0.05787 valid_loss: 0.07446 test_loss: 0.08808 \n",
      "[335/500] train_loss: 0.05744 valid_loss: 0.07633 test_loss: 0.08777 \n",
      "[336/500] train_loss: 0.05958 valid_loss: 0.07464 test_loss: 0.08704 \n",
      "[337/500] train_loss: 0.05832 valid_loss: 0.07578 test_loss: 0.08713 \n",
      "[338/500] train_loss: 0.06047 valid_loss: 0.07429 test_loss: 0.08821 \n",
      "[339/500] train_loss: 0.05894 valid_loss: 0.07390 test_loss: 0.08783 \n",
      "验证损失减少 (0.074088 --> 0.073899). 正在保存模型...\n",
      "[340/500] train_loss: 0.05807 valid_loss: 0.07656 test_loss: 0.08670 \n",
      "[341/500] train_loss: 0.05860 valid_loss: 0.07827 test_loss: 0.08824 \n",
      "[342/500] train_loss: 0.05840 valid_loss: 0.07489 test_loss: 0.08740 \n",
      "[343/500] train_loss: 0.05773 valid_loss: 0.07648 test_loss: 0.08947 \n",
      "[344/500] train_loss: 0.05795 valid_loss: 0.07581 test_loss: 0.08769 \n",
      "[345/500] train_loss: 0.05743 valid_loss: 0.07484 test_loss: 0.08699 \n",
      "[346/500] train_loss: 0.05901 valid_loss: 0.07409 test_loss: 0.08711 \n",
      "[347/500] train_loss: 0.05983 valid_loss: 0.07451 test_loss: 0.08905 \n",
      "[348/500] train_loss: 0.05599 valid_loss: 0.07485 test_loss: 0.08882 \n",
      "[349/500] train_loss: 0.05497 valid_loss: 0.07453 test_loss: 0.08818 \n",
      "[350/500] train_loss: 0.05742 valid_loss: 0.07339 test_loss: 0.08669 \n",
      "验证损失减少 (0.073899 --> 0.073388). 正在保存模型...\n",
      "[351/500] train_loss: 0.05726 valid_loss: 0.07299 test_loss: 0.08620 \n",
      "验证损失减少 (0.073388 --> 0.072990). 正在保存模型...\n",
      "[352/500] train_loss: 0.05641 valid_loss: 0.07495 test_loss: 0.08845 \n",
      "[353/500] train_loss: 0.05694 valid_loss: 0.07400 test_loss: 0.08741 \n",
      "[354/500] train_loss: 0.05828 valid_loss: 0.07605 test_loss: 0.08979 \n",
      "[355/500] train_loss: 0.05645 valid_loss: 0.07657 test_loss: 0.08730 \n",
      "[356/500] train_loss: 0.05547 valid_loss: 0.07438 test_loss: 0.08865 \n",
      "[357/500] train_loss: 0.05642 valid_loss: 0.07361 test_loss: 0.08731 \n",
      "[358/500] train_loss: 0.05678 valid_loss: 0.07426 test_loss: 0.08658 \n",
      "[359/500] train_loss: 0.05697 valid_loss: 0.07536 test_loss: 0.08838 \n",
      "[360/500] train_loss: 0.05774 valid_loss: 0.07391 test_loss: 0.08783 \n",
      "[361/500] train_loss: 0.05734 valid_loss: 0.07606 test_loss: 0.08849 \n",
      "[362/500] train_loss: 0.05851 valid_loss: 0.07508 test_loss: 0.08833 \n",
      "[363/500] train_loss: 0.05501 valid_loss: 0.07411 test_loss: 0.08692 \n",
      "[364/500] train_loss: 0.05641 valid_loss: 0.07400 test_loss: 0.08790 \n",
      "[365/500] train_loss: 0.05828 valid_loss: 0.07851 test_loss: 0.08777 \n",
      "[366/500] train_loss: 0.05583 valid_loss: 0.07662 test_loss: 0.08841 \n",
      "[367/500] train_loss: 0.05630 valid_loss: 0.07414 test_loss: 0.08718 \n",
      "[368/500] train_loss: 0.05687 valid_loss: 0.07428 test_loss: 0.08764 \n",
      "[369/500] train_loss: 0.05735 valid_loss: 0.07537 test_loss: 0.08602 \n",
      "[370/500] train_loss: 0.05528 valid_loss: 0.07398 test_loss: 0.08785 \n",
      "[371/500] train_loss: 0.05688 valid_loss: 0.07388 test_loss: 0.08651 \n",
      "[372/500] train_loss: 0.05638 valid_loss: 0.07471 test_loss: 0.08717 \n",
      "[373/500] train_loss: 0.05909 valid_loss: 0.07372 test_loss: 0.08666 \n",
      "[374/500] train_loss: 0.05847 valid_loss: 0.07431 test_loss: 0.08660 \n",
      "[375/500] train_loss: 0.05793 valid_loss: 0.07334 test_loss: 0.08585 \n",
      "[376/500] train_loss: 0.05671 valid_loss: 0.07482 test_loss: 0.08782 \n",
      "[377/500] train_loss: 0.05696 valid_loss: 0.07314 test_loss: 0.08659 \n",
      "[378/500] train_loss: 0.05491 valid_loss: 0.07410 test_loss: 0.08726 \n",
      "[379/500] train_loss: 0.05642 valid_loss: 0.07569 test_loss: 0.08754 \n",
      "[380/500] train_loss: 0.05435 valid_loss: 0.07406 test_loss: 0.08709 \n",
      "[381/500] train_loss: 0.05502 valid_loss: 0.07783 test_loss: 0.08748 \n",
      "[382/500] train_loss: 0.05488 valid_loss: 0.07423 test_loss: 0.08664 \n",
      "[383/500] train_loss: 0.05582 valid_loss: 0.07293 test_loss: 0.08684 \n",
      "验证损失减少 (0.072990 --> 0.072932). 正在保存模型...\n",
      "[384/500] train_loss: 0.05642 valid_loss: 0.07369 test_loss: 0.08751 \n",
      "[385/500] train_loss: 0.05379 valid_loss: 0.07470 test_loss: 0.08689 \n",
      "[386/500] train_loss: 0.05593 valid_loss: 0.07359 test_loss: 0.08670 \n",
      "[387/500] train_loss: 0.05696 valid_loss: 0.07519 test_loss: 0.08793 \n",
      "[388/500] train_loss: 0.05682 valid_loss: 0.07428 test_loss: 0.08736 \n",
      "[389/500] train_loss: 0.05539 valid_loss: 0.07368 test_loss: 0.08701 \n",
      "[390/500] train_loss: 0.05543 valid_loss: 0.07315 test_loss: 0.08707 \n",
      "[391/500] train_loss: 0.05956 valid_loss: 0.07389 test_loss: 0.08624 \n",
      "[392/500] train_loss: 0.05612 valid_loss: 0.07396 test_loss: 0.08889 \n",
      "[393/500] train_loss: 0.05620 valid_loss: 0.07302 test_loss: 0.08663 \n",
      "[394/500] train_loss: 0.05614 valid_loss: 0.07333 test_loss: 0.08601 \n",
      "[395/500] train_loss: 0.05381 valid_loss: 0.07344 test_loss: 0.08713 \n",
      "[396/500] train_loss: 0.05543 valid_loss: 0.07515 test_loss: 0.08915 \n",
      "[397/500] train_loss: 0.05666 valid_loss: 0.07290 test_loss: 0.08710 \n",
      "验证损失减少 (0.072932 --> 0.072904). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[398/500] train_loss: 0.05718 valid_loss: 0.07296 test_loss: 0.08602 \n",
      "[399/500] train_loss: 0.05454 valid_loss: 0.07355 test_loss: 0.08587 \n",
      "[400/500] train_loss: 0.05559 valid_loss: 0.07303 test_loss: 0.08849 \n",
      "[401/500] train_loss: 0.05612 valid_loss: 0.07401 test_loss: 0.08697 \n",
      "[402/500] train_loss: 0.05529 valid_loss: 0.07474 test_loss: 0.08523 \n",
      "[403/500] train_loss: 0.05404 valid_loss: 0.07420 test_loss: 0.08625 \n",
      "[404/500] train_loss: 0.05499 valid_loss: 0.07226 test_loss: 0.08568 \n",
      "验证损失减少 (0.072904 --> 0.072259). 正在保存模型...\n",
      "[405/500] train_loss: 0.05587 valid_loss: 0.07237 test_loss: 0.08614 \n",
      "[406/500] train_loss: 0.05538 valid_loss: 0.07399 test_loss: 0.08757 \n",
      "[407/500] train_loss: 0.05442 valid_loss: 0.07483 test_loss: 0.08856 \n",
      "[408/500] train_loss: 0.05705 valid_loss: 0.07392 test_loss: 0.08667 \n",
      "[409/500] train_loss: 0.05471 valid_loss: 0.07288 test_loss: 0.08648 \n",
      "[410/500] train_loss: 0.05444 valid_loss: 0.07410 test_loss: 0.08626 \n",
      "[411/500] train_loss: 0.05524 valid_loss: 0.07359 test_loss: 0.08689 \n",
      "[412/500] train_loss: 0.05516 valid_loss: 0.07528 test_loss: 0.08717 \n",
      "[413/500] train_loss: 0.05464 valid_loss: 0.07362 test_loss: 0.08631 \n",
      "[414/500] train_loss: 0.05459 valid_loss: 0.07283 test_loss: 0.08568 \n",
      "[415/500] train_loss: 0.05411 valid_loss: 0.07396 test_loss: 0.08720 \n",
      "[416/500] train_loss: 0.05490 valid_loss: 0.07369 test_loss: 0.08652 \n",
      "[417/500] train_loss: 0.05435 valid_loss: 0.07486 test_loss: 0.08589 \n",
      "[418/500] train_loss: 0.05522 valid_loss: 0.07387 test_loss: 0.08711 \n",
      "[419/500] train_loss: 0.05508 valid_loss: 0.07499 test_loss: 0.08634 \n",
      "[420/500] train_loss: 0.05372 valid_loss: 0.08182 test_loss: 0.08634 \n",
      "[421/500] train_loss: 0.05413 valid_loss: 0.07842 test_loss: 0.08749 \n",
      "[422/500] train_loss: 0.05355 valid_loss: 0.07778 test_loss: 0.08820 \n",
      "[423/500] train_loss: 0.05567 valid_loss: 0.07447 test_loss: 0.08729 \n",
      "[424/500] train_loss: 0.05371 valid_loss: 0.07380 test_loss: 0.08725 \n",
      "[425/500] train_loss: 0.05310 valid_loss: 0.07641 test_loss: 0.08812 \n",
      "[426/500] train_loss: 0.05364 valid_loss: 0.07847 test_loss: 0.08630 \n",
      "[427/500] train_loss: 0.05529 valid_loss: 0.07809 test_loss: 0.08763 \n",
      "[428/500] train_loss: 0.05375 valid_loss: 0.08124 test_loss: 0.08914 \n",
      "[429/500] train_loss: 0.05433 valid_loss: 0.07544 test_loss: 0.08791 \n",
      "[430/500] train_loss: 0.05506 valid_loss: 0.08067 test_loss: 0.08698 \n",
      "[431/500] train_loss: 0.05454 valid_loss: 0.07540 test_loss: 0.08734 \n",
      "[432/500] train_loss: 0.05523 valid_loss: 0.08105 test_loss: 0.08596 \n",
      "[433/500] train_loss: 0.05375 valid_loss: 0.07465 test_loss: 0.08746 \n",
      "[434/500] train_loss: 0.05397 valid_loss: 0.07847 test_loss: 0.08592 \n",
      "[435/500] train_loss: 0.05286 valid_loss: 0.08071 test_loss: 0.08634 \n",
      "[436/500] train_loss: 0.05397 valid_loss: 0.07476 test_loss: 0.08700 \n",
      "[437/500] train_loss: 0.05523 valid_loss: 0.07486 test_loss: 0.08814 \n",
      "[438/500] train_loss: 0.05496 valid_loss: 0.07423 test_loss: 0.08645 \n",
      "[439/500] train_loss: 0.05307 valid_loss: 0.07446 test_loss: 0.08764 \n",
      "[440/500] train_loss: 0.05490 valid_loss: 0.07356 test_loss: 0.08824 \n",
      "[441/500] train_loss: 0.05388 valid_loss: 0.07513 test_loss: 0.08687 \n",
      "[442/500] train_loss: 0.05285 valid_loss: 0.07384 test_loss: 0.08669 \n",
      "[443/500] train_loss: 0.05187 valid_loss: 0.07253 test_loss: 0.08563 \n",
      "[444/500] train_loss: 0.05284 valid_loss: 0.07284 test_loss: 0.08552 \n",
      "[445/500] train_loss: 0.05486 valid_loss: 0.08345 test_loss: 0.08713 \n",
      "[446/500] train_loss: 0.05561 valid_loss: 0.07737 test_loss: 0.08764 \n",
      "[447/500] train_loss: 0.05285 valid_loss: 0.07576 test_loss: 0.08762 \n",
      "[448/500] train_loss: 0.05322 valid_loss: 0.07675 test_loss: 0.08683 \n",
      "[449/500] train_loss: 0.05257 valid_loss: 0.07613 test_loss: 0.08815 \n",
      "[450/500] train_loss: 0.05297 valid_loss: 0.07526 test_loss: 0.08593 \n",
      "[451/500] train_loss: 0.05267 valid_loss: 0.07328 test_loss: 0.08588 \n",
      "[452/500] train_loss: 0.05349 valid_loss: 0.07611 test_loss: 0.08677 \n",
      "[453/500] train_loss: 0.05213 valid_loss: 0.07674 test_loss: 0.08561 \n",
      "[454/500] train_loss: 0.05449 valid_loss: 0.07941 test_loss: 0.08681 \n",
      "[455/500] train_loss: 0.05432 valid_loss: 0.07594 test_loss: 0.08733 \n",
      "[456/500] train_loss: 0.05322 valid_loss: 0.07334 test_loss: 0.08700 \n",
      "[457/500] train_loss: 0.05405 valid_loss: 0.07314 test_loss: 0.08644 \n",
      "[458/500] train_loss: 0.05223 valid_loss: 0.07388 test_loss: 0.08595 \n",
      "[459/500] train_loss: 0.05109 valid_loss: 0.07272 test_loss: 0.08655 \n",
      "[460/500] train_loss: 0.05250 valid_loss: 0.07530 test_loss: 0.08733 \n",
      "[461/500] train_loss: 0.05143 valid_loss: 0.07533 test_loss: 0.08824 \n",
      "[462/500] train_loss: 0.05294 valid_loss: 0.07276 test_loss: 0.08493 \n",
      "[463/500] train_loss: 0.05168 valid_loss: 0.07318 test_loss: 0.08734 \n",
      "[464/500] train_loss: 0.05467 valid_loss: 0.07236 test_loss: 0.08640 \n",
      "[465/500] train_loss: 0.05113 valid_loss: 0.07307 test_loss: 0.08611 \n",
      "[466/500] train_loss: 0.05339 valid_loss: 0.07259 test_loss: 0.08612 \n",
      "[467/500] train_loss: 0.05273 valid_loss: 0.07216 test_loss: 0.08671 \n",
      "验证损失减少 (0.072259 --> 0.072156). 正在保存模型...\n",
      "[468/500] train_loss: 0.05307 valid_loss: 0.07287 test_loss: 0.08643 \n",
      "[469/500] train_loss: 0.05365 valid_loss: 0.07475 test_loss: 0.08730 \n",
      "[470/500] train_loss: 0.05263 valid_loss: 0.07296 test_loss: 0.08648 \n",
      "[471/500] train_loss: 0.05165 valid_loss: 0.07151 test_loss: 0.08674 \n",
      "验证损失减少 (0.072156 --> 0.071507). 正在保存模型...\n",
      "[472/500] train_loss: 0.05438 valid_loss: 0.07344 test_loss: 0.08711 \n",
      "[473/500] train_loss: 0.05349 valid_loss: 0.07304 test_loss: 0.08655 \n",
      "[474/500] train_loss: 0.05146 valid_loss: 0.07829 test_loss: 0.08639 \n",
      "[475/500] train_loss: 0.05197 valid_loss: 0.07573 test_loss: 0.08700 \n",
      "[476/500] train_loss: 0.05181 valid_loss: 0.07361 test_loss: 0.08498 \n",
      "[477/500] train_loss: 0.05110 valid_loss: 0.07228 test_loss: 0.08626 \n",
      "[478/500] train_loss: 0.05226 valid_loss: 0.07323 test_loss: 0.08905 \n",
      "[479/500] train_loss: 0.05268 valid_loss: 0.07538 test_loss: 0.08619 \n",
      "[480/500] train_loss: 0.05347 valid_loss: 0.07312 test_loss: 0.08677 \n",
      "[481/500] train_loss: 0.05253 valid_loss: 0.07266 test_loss: 0.08608 \n",
      "[482/500] train_loss: 0.05208 valid_loss: 0.07760 test_loss: 0.08578 \n",
      "[483/500] train_loss: 0.05114 valid_loss: 0.07365 test_loss: 0.08741 \n",
      "[484/500] train_loss: 0.05038 valid_loss: 0.07317 test_loss: 0.08752 \n",
      "[485/500] train_loss: 0.05269 valid_loss: 0.08181 test_loss: 0.08581 \n",
      "[486/500] train_loss: 0.05205 valid_loss: 0.07317 test_loss: 0.08438 \n",
      "[487/500] train_loss: 0.05043 valid_loss: 0.07478 test_loss: 0.08611 \n",
      "[488/500] train_loss: 0.05231 valid_loss: 0.07990 test_loss: 0.08884 \n",
      "[489/500] train_loss: 0.05138 valid_loss: 0.07876 test_loss: 0.08706 \n",
      "[490/500] train_loss: 0.05320 valid_loss: 0.07376 test_loss: 0.08563 \n",
      "[491/500] train_loss: 0.05130 valid_loss: 0.07335 test_loss: 0.08575 \n",
      "[492/500] train_loss: 0.05110 valid_loss: 0.07818 test_loss: 0.08682 \n",
      "[493/500] train_loss: 0.05020 valid_loss: 0.07463 test_loss: 0.08722 \n",
      "[494/500] train_loss: 0.05130 valid_loss: 0.07676 test_loss: 0.08852 \n",
      "[495/500] train_loss: 0.05065 valid_loss: 0.07792 test_loss: 0.08684 \n",
      "[496/500] train_loss: 0.05241 valid_loss: 0.07334 test_loss: 0.08531 \n",
      "[497/500] train_loss: 0.05149 valid_loss: 0.07414 test_loss: 0.08692 \n",
      "[498/500] train_loss: 0.04986 valid_loss: 0.07285 test_loss: 0.08540 \n",
      "[499/500] train_loss: 0.05280 valid_loss: 0.07325 test_loss: 0.08565 \n",
      "[500/500] train_loss: 0.05165 valid_loss: 0.07291 test_loss: 0.08572 \n",
      "TRAINING MODEL 13\n",
      "[  1/500] train_loss: 0.35063 valid_loss: 0.26520 test_loss: 0.27103 \n",
      "验证损失减少 (inf --> 0.265205). 正在保存模型...\n",
      "[  2/500] train_loss: 0.20448 valid_loss: 0.19453 test_loss: 0.20423 \n",
      "验证损失减少 (0.265205 --> 0.194528). 正在保存模型...\n",
      "[  3/500] train_loss: 0.16955 valid_loss: 0.16760 test_loss: 0.17833 \n",
      "验证损失减少 (0.194528 --> 0.167604). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15425 valid_loss: 0.15636 test_loss: 0.16727 \n",
      "验证损失减少 (0.167604 --> 0.156365). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14356 valid_loss: 0.14817 test_loss: 0.16225 \n",
      "验证损失减少 (0.156365 --> 0.148165). 正在保存模型...\n",
      "[  6/500] train_loss: 0.13969 valid_loss: 0.13973 test_loss: 0.15271 \n",
      "验证损失减少 (0.148165 --> 0.139728). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13347 valid_loss: 0.13615 test_loss: 0.14959 \n",
      "验证损失减少 (0.139728 --> 0.136151). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13027 valid_loss: 0.13737 test_loss: 0.14963 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9/500] train_loss: 0.12840 valid_loss: 0.13109 test_loss: 0.14368 \n",
      "验证损失减少 (0.136151 --> 0.131090). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12323 valid_loss: 0.12881 test_loss: 0.14395 \n",
      "验证损失减少 (0.131090 --> 0.128812). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12170 valid_loss: 0.12652 test_loss: 0.14006 \n",
      "验证损失减少 (0.128812 --> 0.126523). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12338 valid_loss: 0.12467 test_loss: 0.13995 \n",
      "验证损失减少 (0.126523 --> 0.124671). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.11793 valid_loss: 0.12217 test_loss: 0.13831 \n",
      "验证损失减少 (0.124671 --> 0.122172). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.11917 valid_loss: 0.12098 test_loss: 0.13671 \n",
      "验证损失减少 (0.122172 --> 0.120977). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11503 valid_loss: 0.11891 test_loss: 0.13290 \n",
      "验证损失减少 (0.120977 --> 0.118913). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11571 valid_loss: 0.11780 test_loss: 0.13321 \n",
      "验证损失减少 (0.118913 --> 0.117802). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.10926 valid_loss: 0.11656 test_loss: 0.13118 \n",
      "验证损失减少 (0.117802 --> 0.116565). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11508 valid_loss: 0.11672 test_loss: 0.13103 \n",
      "[ 19/500] train_loss: 0.11032 valid_loss: 0.11378 test_loss: 0.12929 \n",
      "验证损失减少 (0.116565 --> 0.113784). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.10940 valid_loss: 0.11397 test_loss: 0.12982 \n",
      "[ 21/500] train_loss: 0.10777 valid_loss: 0.11407 test_loss: 0.12847 \n",
      "[ 22/500] train_loss: 0.10616 valid_loss: 0.11217 test_loss: 0.12692 \n",
      "验证损失减少 (0.113784 --> 0.112170). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10814 valid_loss: 0.11096 test_loss: 0.12355 \n",
      "验证损失减少 (0.112170 --> 0.110959). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10472 valid_loss: 0.11107 test_loss: 0.12554 \n",
      "[ 25/500] train_loss: 0.10278 valid_loss: 0.10994 test_loss: 0.12384 \n",
      "验证损失减少 (0.110959 --> 0.109939). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10201 valid_loss: 0.11324 test_loss: 0.12556 \n",
      "[ 27/500] train_loss: 0.10360 valid_loss: 0.10871 test_loss: 0.12421 \n",
      "验证损失减少 (0.109939 --> 0.108707). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10187 valid_loss: 0.10768 test_loss: 0.12250 \n",
      "验证损失减少 (0.108707 --> 0.107682). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.09890 valid_loss: 0.10571 test_loss: 0.12024 \n",
      "验证损失减少 (0.107682 --> 0.105714). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10234 valid_loss: 0.10512 test_loss: 0.12025 \n",
      "验证损失减少 (0.105714 --> 0.105122). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.09837 valid_loss: 0.10581 test_loss: 0.11942 \n",
      "[ 32/500] train_loss: 0.09792 valid_loss: 0.10573 test_loss: 0.11891 \n",
      "[ 33/500] train_loss: 0.09916 valid_loss: 0.10440 test_loss: 0.12088 \n",
      "验证损失减少 (0.105122 --> 0.104400). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.09806 valid_loss: 0.10167 test_loss: 0.11565 \n",
      "验证损失减少 (0.104400 --> 0.101669). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.09738 valid_loss: 0.10163 test_loss: 0.11534 \n",
      "验证损失减少 (0.101669 --> 0.101631). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09500 valid_loss: 0.10141 test_loss: 0.11527 \n",
      "验证损失减少 (0.101631 --> 0.101411). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.09436 valid_loss: 0.10326 test_loss: 0.11752 \n",
      "[ 38/500] train_loss: 0.09425 valid_loss: 0.10043 test_loss: 0.11453 \n",
      "验证损失减少 (0.101411 --> 0.100431). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.09609 valid_loss: 0.10137 test_loss: 0.11417 \n",
      "[ 40/500] train_loss: 0.09617 valid_loss: 0.10338 test_loss: 0.11588 \n",
      "[ 41/500] train_loss: 0.09414 valid_loss: 0.09946 test_loss: 0.11395 \n",
      "验证损失减少 (0.100431 --> 0.099461). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.09338 valid_loss: 0.10230 test_loss: 0.11312 \n",
      "[ 43/500] train_loss: 0.08997 valid_loss: 0.10202 test_loss: 0.11603 \n",
      "[ 44/500] train_loss: 0.09428 valid_loss: 0.10031 test_loss: 0.11387 \n",
      "[ 45/500] train_loss: 0.09253 valid_loss: 0.09979 test_loss: 0.11223 \n",
      "[ 46/500] train_loss: 0.09189 valid_loss: 0.10126 test_loss: 0.11153 \n",
      "[ 47/500] train_loss: 0.08941 valid_loss: 0.09791 test_loss: 0.11137 \n",
      "验证损失减少 (0.099461 --> 0.097910). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.08971 valid_loss: 0.09759 test_loss: 0.11023 \n",
      "验证损失减少 (0.097910 --> 0.097594). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09032 valid_loss: 0.09738 test_loss: 0.11139 \n",
      "验证损失减少 (0.097594 --> 0.097379). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.08960 valid_loss: 0.09633 test_loss: 0.10735 \n",
      "验证损失减少 (0.097379 --> 0.096328). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.08999 valid_loss: 0.09706 test_loss: 0.11090 \n",
      "[ 52/500] train_loss: 0.09087 valid_loss: 0.09552 test_loss: 0.10751 \n",
      "验证损失减少 (0.096328 --> 0.095517). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.09207 valid_loss: 0.09766 test_loss: 0.10957 \n",
      "[ 54/500] train_loss: 0.08871 valid_loss: 0.09650 test_loss: 0.10915 \n",
      "[ 55/500] train_loss: 0.08905 valid_loss: 0.09518 test_loss: 0.10789 \n",
      "验证损失减少 (0.095517 --> 0.095176). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.08992 valid_loss: 0.09600 test_loss: 0.10891 \n",
      "[ 57/500] train_loss: 0.08860 valid_loss: 0.09545 test_loss: 0.10794 \n",
      "[ 58/500] train_loss: 0.08975 valid_loss: 0.09581 test_loss: 0.10891 \n",
      "[ 59/500] train_loss: 0.08879 valid_loss: 0.09339 test_loss: 0.10724 \n",
      "验证损失减少 (0.095176 --> 0.093387). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.08799 valid_loss: 0.09312 test_loss: 0.10533 \n",
      "验证损失减少 (0.093387 --> 0.093117). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.08804 valid_loss: 0.09581 test_loss: 0.10680 \n",
      "[ 62/500] train_loss: 0.08658 valid_loss: 0.09415 test_loss: 0.10823 \n",
      "[ 63/500] train_loss: 0.08630 valid_loss: 0.09274 test_loss: 0.10498 \n",
      "验证损失减少 (0.093117 --> 0.092743). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.08566 valid_loss: 0.09452 test_loss: 0.10713 \n",
      "[ 65/500] train_loss: 0.08710 valid_loss: 0.09307 test_loss: 0.10565 \n",
      "[ 66/500] train_loss: 0.08616 valid_loss: 0.09228 test_loss: 0.10653 \n",
      "验证损失减少 (0.092743 --> 0.092282). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.08241 valid_loss: 0.09002 test_loss: 0.10440 \n",
      "验证损失减少 (0.092282 --> 0.090024). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.08530 valid_loss: 0.09464 test_loss: 0.10517 \n",
      "[ 69/500] train_loss: 0.08584 valid_loss: 0.09429 test_loss: 0.10625 \n",
      "[ 70/500] train_loss: 0.08417 valid_loss: 0.09239 test_loss: 0.10505 \n",
      "[ 71/500] train_loss: 0.08385 valid_loss: 0.09316 test_loss: 0.10748 \n",
      "[ 72/500] train_loss: 0.08449 valid_loss: 0.09111 test_loss: 0.10454 \n",
      "[ 73/500] train_loss: 0.08464 valid_loss: 0.09212 test_loss: 0.10458 \n",
      "[ 74/500] train_loss: 0.08389 valid_loss: 0.09320 test_loss: 0.10426 \n",
      "[ 75/500] train_loss: 0.08153 valid_loss: 0.09016 test_loss: 0.10255 \n",
      "[ 76/500] train_loss: 0.08247 valid_loss: 0.09465 test_loss: 0.10696 \n",
      "[ 77/500] train_loss: 0.08210 valid_loss: 0.09059 test_loss: 0.10414 \n",
      "[ 78/500] train_loss: 0.08308 valid_loss: 0.09204 test_loss: 0.10324 \n",
      "[ 79/500] train_loss: 0.08290 valid_loss: 0.09307 test_loss: 0.10395 \n",
      "[ 80/500] train_loss: 0.08144 valid_loss: 0.09065 test_loss: 0.10447 \n",
      "[ 81/500] train_loss: 0.07995 valid_loss: 0.09413 test_loss: 0.10309 \n",
      "[ 82/500] train_loss: 0.08085 valid_loss: 0.09030 test_loss: 0.10328 \n",
      "[ 83/500] train_loss: 0.07984 valid_loss: 0.09134 test_loss: 0.10440 \n",
      "[ 84/500] train_loss: 0.08150 valid_loss: 0.09294 test_loss: 0.10157 \n",
      "[ 85/500] train_loss: 0.08076 valid_loss: 0.09123 test_loss: 0.10235 \n",
      "[ 86/500] train_loss: 0.08112 valid_loss: 0.08922 test_loss: 0.10114 \n",
      "验证损失减少 (0.090024 --> 0.089223). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.08114 valid_loss: 0.08906 test_loss: 0.10148 \n",
      "验证损失减少 (0.089223 --> 0.089056). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.07854 valid_loss: 0.08858 test_loss: 0.10240 \n",
      "验证损失减少 (0.089056 --> 0.088583). 正在保存模型...\n",
      "[ 89/500] train_loss: 0.07812 valid_loss: 0.08771 test_loss: 0.09928 \n",
      "验证损失减少 (0.088583 --> 0.087708). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.08157 valid_loss: 0.08999 test_loss: 0.10232 \n",
      "[ 91/500] train_loss: 0.08162 valid_loss: 0.08808 test_loss: 0.09903 \n",
      "[ 92/500] train_loss: 0.07956 valid_loss: 0.08979 test_loss: 0.10188 \n",
      "[ 93/500] train_loss: 0.08069 valid_loss: 0.08930 test_loss: 0.09942 \n",
      "[ 94/500] train_loss: 0.07763 valid_loss: 0.08786 test_loss: 0.09965 \n",
      "[ 95/500] train_loss: 0.07894 valid_loss: 0.08779 test_loss: 0.10026 \n",
      "[ 96/500] train_loss: 0.08212 valid_loss: 0.08877 test_loss: 0.10073 \n",
      "[ 97/500] train_loss: 0.07923 valid_loss: 0.08625 test_loss: 0.09869 \n",
      "验证损失减少 (0.087708 --> 0.086253). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.07988 valid_loss: 0.08823 test_loss: 0.09892 \n",
      "[ 99/500] train_loss: 0.07913 valid_loss: 0.08778 test_loss: 0.10071 \n",
      "[100/500] train_loss: 0.07721 valid_loss: 0.08621 test_loss: 0.09872 \n",
      "验证损失减少 (0.086253 --> 0.086213). 正在保存模型...\n",
      "[101/500] train_loss: 0.07828 valid_loss: 0.08587 test_loss: 0.09857 \n",
      "验证损失减少 (0.086213 --> 0.085873). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102/500] train_loss: 0.07861 valid_loss: 0.09084 test_loss: 0.10031 \n",
      "[103/500] train_loss: 0.07880 valid_loss: 0.08606 test_loss: 0.09868 \n",
      "[104/500] train_loss: 0.07701 valid_loss: 0.08532 test_loss: 0.09808 \n",
      "验证损失减少 (0.085873 --> 0.085323). 正在保存模型...\n",
      "[105/500] train_loss: 0.07770 valid_loss: 0.08708 test_loss: 0.09881 \n",
      "[106/500] train_loss: 0.07788 valid_loss: 0.08521 test_loss: 0.09827 \n",
      "验证损失减少 (0.085323 --> 0.085210). 正在保存模型...\n",
      "[107/500] train_loss: 0.07787 valid_loss: 0.08562 test_loss: 0.10021 \n",
      "[108/500] train_loss: 0.07702 valid_loss: 0.08975 test_loss: 0.09959 \n",
      "[109/500] train_loss: 0.07589 valid_loss: 0.08695 test_loss: 0.09904 \n",
      "[110/500] train_loss: 0.07704 valid_loss: 0.08883 test_loss: 0.09914 \n",
      "[111/500] train_loss: 0.07629 valid_loss: 0.08682 test_loss: 0.09817 \n",
      "[112/500] train_loss: 0.07515 valid_loss: 0.08632 test_loss: 0.09725 \n",
      "[113/500] train_loss: 0.07720 valid_loss: 0.08668 test_loss: 0.09721 \n",
      "[114/500] train_loss: 0.07757 valid_loss: 0.08673 test_loss: 0.09848 \n",
      "[115/500] train_loss: 0.07594 valid_loss: 0.08780 test_loss: 0.09621 \n",
      "[116/500] train_loss: 0.07594 valid_loss: 0.08535 test_loss: 0.09820 \n",
      "[117/500] train_loss: 0.07579 valid_loss: 0.08705 test_loss: 0.09834 \n",
      "[118/500] train_loss: 0.07613 valid_loss: 0.08669 test_loss: 0.09839 \n",
      "[119/500] train_loss: 0.07354 valid_loss: 0.08753 test_loss: 0.09921 \n",
      "[120/500] train_loss: 0.07396 valid_loss: 0.08533 test_loss: 0.09732 \n",
      "[121/500] train_loss: 0.07494 valid_loss: 0.08695 test_loss: 0.09775 \n",
      "[122/500] train_loss: 0.07345 valid_loss: 0.08414 test_loss: 0.09741 \n",
      "验证损失减少 (0.085210 --> 0.084144). 正在保存模型...\n",
      "[123/500] train_loss: 0.07387 valid_loss: 0.08995 test_loss: 0.10024 \n",
      "[124/500] train_loss: 0.07387 valid_loss: 0.08584 test_loss: 0.09725 \n",
      "[125/500] train_loss: 0.07465 valid_loss: 0.08486 test_loss: 0.09777 \n",
      "[126/500] train_loss: 0.07367 valid_loss: 0.08332 test_loss: 0.09682 \n",
      "验证损失减少 (0.084144 --> 0.083325). 正在保存模型...\n",
      "[127/500] train_loss: 0.07304 valid_loss: 0.08427 test_loss: 0.09688 \n",
      "[128/500] train_loss: 0.07429 valid_loss: 0.08690 test_loss: 0.09713 \n",
      "[129/500] train_loss: 0.07286 valid_loss: 0.08810 test_loss: 0.09773 \n",
      "[130/500] train_loss: 0.07502 valid_loss: 0.08690 test_loss: 0.09669 \n",
      "[131/500] train_loss: 0.07315 valid_loss: 0.08582 test_loss: 0.09890 \n",
      "[132/500] train_loss: 0.07681 valid_loss: 0.08564 test_loss: 0.09551 \n",
      "[133/500] train_loss: 0.07497 valid_loss: 0.08494 test_loss: 0.09593 \n",
      "[134/500] train_loss: 0.07462 valid_loss: 0.08462 test_loss: 0.09504 \n",
      "[135/500] train_loss: 0.07375 valid_loss: 0.08457 test_loss: 0.09586 \n",
      "[136/500] train_loss: 0.07366 valid_loss: 0.08343 test_loss: 0.09532 \n",
      "[137/500] train_loss: 0.07211 valid_loss: 0.08197 test_loss: 0.09445 \n",
      "验证损失减少 (0.083325 --> 0.081974). 正在保存模型...\n",
      "[138/500] train_loss: 0.07337 valid_loss: 0.08333 test_loss: 0.09499 \n",
      "[139/500] train_loss: 0.07124 valid_loss: 0.08438 test_loss: 0.09557 \n",
      "[140/500] train_loss: 0.07341 valid_loss: 0.08574 test_loss: 0.09452 \n",
      "[141/500] train_loss: 0.07280 valid_loss: 0.08475 test_loss: 0.09413 \n",
      "[142/500] train_loss: 0.07291 valid_loss: 0.08474 test_loss: 0.09491 \n",
      "[143/500] train_loss: 0.07264 valid_loss: 0.08428 test_loss: 0.09630 \n",
      "[144/500] train_loss: 0.06855 valid_loss: 0.08313 test_loss: 0.09379 \n",
      "[145/500] train_loss: 0.07061 valid_loss: 0.08299 test_loss: 0.09456 \n",
      "[146/500] train_loss: 0.07363 valid_loss: 0.08332 test_loss: 0.09463 \n",
      "[147/500] train_loss: 0.07200 valid_loss: 0.08744 test_loss: 0.09612 \n",
      "[148/500] train_loss: 0.07089 valid_loss: 0.08424 test_loss: 0.09377 \n",
      "[149/500] train_loss: 0.07225 valid_loss: 0.08532 test_loss: 0.09605 \n",
      "[150/500] train_loss: 0.07150 valid_loss: 0.08481 test_loss: 0.09432 \n",
      "[151/500] train_loss: 0.06993 valid_loss: 0.08145 test_loss: 0.09438 \n",
      "验证损失减少 (0.081974 --> 0.081447). 正在保存模型...\n",
      "[152/500] train_loss: 0.07148 valid_loss: 0.08367 test_loss: 0.09378 \n",
      "[153/500] train_loss: 0.07173 valid_loss: 0.08326 test_loss: 0.09446 \n",
      "[154/500] train_loss: 0.07143 valid_loss: 0.08313 test_loss: 0.09392 \n",
      "[155/500] train_loss: 0.06924 valid_loss: 0.08830 test_loss: 0.09414 \n",
      "[156/500] train_loss: 0.06801 valid_loss: 0.08368 test_loss: 0.09257 \n",
      "[157/500] train_loss: 0.07080 valid_loss: 0.08273 test_loss: 0.09390 \n",
      "[158/500] train_loss: 0.06983 valid_loss: 0.08231 test_loss: 0.09475 \n",
      "[159/500] train_loss: 0.06977 valid_loss: 0.08277 test_loss: 0.09389 \n",
      "[160/500] train_loss: 0.07050 valid_loss: 0.08412 test_loss: 0.09347 \n",
      "[161/500] train_loss: 0.07033 valid_loss: 0.08259 test_loss: 0.09431 \n",
      "[162/500] train_loss: 0.06862 valid_loss: 0.08194 test_loss: 0.09420 \n",
      "[163/500] train_loss: 0.06890 valid_loss: 0.08192 test_loss: 0.09427 \n",
      "[164/500] train_loss: 0.07089 valid_loss: 0.08514 test_loss: 0.09398 \n",
      "[165/500] train_loss: 0.06969 valid_loss: 0.08409 test_loss: 0.09325 \n",
      "[166/500] train_loss: 0.07067 valid_loss: 0.08123 test_loss: 0.09449 \n",
      "验证损失减少 (0.081447 --> 0.081231). 正在保存模型...\n",
      "[167/500] train_loss: 0.06928 valid_loss: 0.08136 test_loss: 0.09293 \n",
      "[168/500] train_loss: 0.06926 valid_loss: 0.08079 test_loss: 0.09265 \n",
      "验证损失减少 (0.081231 --> 0.080789). 正在保存模型...\n",
      "[169/500] train_loss: 0.06883 valid_loss: 0.08158 test_loss: 0.09333 \n",
      "[170/500] train_loss: 0.06759 valid_loss: 0.08296 test_loss: 0.09258 \n",
      "[171/500] train_loss: 0.07036 valid_loss: 0.08132 test_loss: 0.09319 \n",
      "[172/500] train_loss: 0.06841 valid_loss: 0.08222 test_loss: 0.09408 \n",
      "[173/500] train_loss: 0.06993 valid_loss: 0.08101 test_loss: 0.09324 \n",
      "[174/500] train_loss: 0.07013 valid_loss: 0.08097 test_loss: 0.09309 \n",
      "[175/500] train_loss: 0.07024 valid_loss: 0.08162 test_loss: 0.09362 \n",
      "[176/500] train_loss: 0.06737 valid_loss: 0.08184 test_loss: 0.09408 \n",
      "[177/500] train_loss: 0.06844 valid_loss: 0.08147 test_loss: 0.09371 \n",
      "[178/500] train_loss: 0.06771 valid_loss: 0.08247 test_loss: 0.09305 \n",
      "[179/500] train_loss: 0.06940 valid_loss: 0.07917 test_loss: 0.09115 \n",
      "验证损失减少 (0.080789 --> 0.079173). 正在保存模型...\n",
      "[180/500] train_loss: 0.06796 valid_loss: 0.08347 test_loss: 0.09397 \n",
      "[181/500] train_loss: 0.06959 valid_loss: 0.08343 test_loss: 0.09343 \n",
      "[182/500] train_loss: 0.06922 valid_loss: 0.08189 test_loss: 0.09152 \n",
      "[183/500] train_loss: 0.06859 valid_loss: 0.08121 test_loss: 0.09264 \n",
      "[184/500] train_loss: 0.06963 valid_loss: 0.08525 test_loss: 0.09327 \n",
      "[185/500] train_loss: 0.06783 valid_loss: 0.08695 test_loss: 0.09411 \n",
      "[186/500] train_loss: 0.06869 valid_loss: 0.08182 test_loss: 0.09338 \n",
      "[187/500] train_loss: 0.06714 valid_loss: 0.09120 test_loss: 0.09254 \n",
      "[188/500] train_loss: 0.06812 valid_loss: 0.08170 test_loss: 0.09218 \n",
      "[189/500] train_loss: 0.06719 valid_loss: 0.08103 test_loss: 0.09293 \n",
      "[190/500] train_loss: 0.06747 valid_loss: 0.07954 test_loss: 0.09178 \n",
      "[191/500] train_loss: 0.06808 valid_loss: 0.08053 test_loss: 0.09230 \n",
      "[192/500] train_loss: 0.06746 valid_loss: 0.07902 test_loss: 0.09236 \n",
      "验证损失减少 (0.079173 --> 0.079022). 正在保存模型...\n",
      "[193/500] train_loss: 0.06668 valid_loss: 0.08071 test_loss: 0.09204 \n",
      "[194/500] train_loss: 0.06662 valid_loss: 0.08239 test_loss: 0.09348 \n",
      "[195/500] train_loss: 0.06841 valid_loss: 0.08002 test_loss: 0.09266 \n",
      "[196/500] train_loss: 0.06872 valid_loss: 0.08195 test_loss: 0.09913 \n",
      "[197/500] train_loss: 0.06908 valid_loss: 0.07925 test_loss: 0.09150 \n",
      "[198/500] train_loss: 0.06779 valid_loss: 0.07905 test_loss: 0.09133 \n",
      "[199/500] train_loss: 0.06822 valid_loss: 0.07963 test_loss: 0.09152 \n",
      "[200/500] train_loss: 0.06609 valid_loss: 0.07870 test_loss: 0.09055 \n",
      "验证损失减少 (0.079022 --> 0.078698). 正在保存模型...\n",
      "[201/500] train_loss: 0.06707 valid_loss: 0.08056 test_loss: 0.09240 \n",
      "[202/500] train_loss: 0.06747 valid_loss: 0.07883 test_loss: 0.09153 \n",
      "[203/500] train_loss: 0.06814 valid_loss: 0.08461 test_loss: 0.09347 \n",
      "[204/500] train_loss: 0.06875 valid_loss: 0.07856 test_loss: 0.09100 \n",
      "验证损失减少 (0.078698 --> 0.078560). 正在保存模型...\n",
      "[205/500] train_loss: 0.06579 valid_loss: 0.08027 test_loss: 0.09276 \n",
      "[206/500] train_loss: 0.06794 valid_loss: 0.07906 test_loss: 0.09150 \n",
      "[207/500] train_loss: 0.06738 valid_loss: 0.07925 test_loss: 0.09073 \n",
      "[208/500] train_loss: 0.06555 valid_loss: 0.07813 test_loss: 0.09158 \n",
      "验证损失减少 (0.078560 --> 0.078131). 正在保存模型...\n",
      "[209/500] train_loss: 0.06463 valid_loss: 0.07863 test_loss: 0.09134 \n",
      "[210/500] train_loss: 0.06634 valid_loss: 0.07889 test_loss: 0.09151 \n",
      "[211/500] train_loss: 0.06510 valid_loss: 0.07869 test_loss: 0.09172 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[212/500] train_loss: 0.06549 valid_loss: 0.07916 test_loss: 0.09205 \n",
      "[213/500] train_loss: 0.06528 valid_loss: 0.08004 test_loss: 0.09278 \n",
      "[214/500] train_loss: 0.06653 valid_loss: 0.07994 test_loss: 0.09336 \n",
      "[215/500] train_loss: 0.06542 valid_loss: 0.07947 test_loss: 0.09206 \n",
      "[216/500] train_loss: 0.06306 valid_loss: 0.07965 test_loss: 0.09391 \n",
      "[217/500] train_loss: 0.06321 valid_loss: 0.07793 test_loss: 0.09241 \n",
      "验证损失减少 (0.078131 --> 0.077933). 正在保存模型...\n",
      "[218/500] train_loss: 0.06369 valid_loss: 0.07927 test_loss: 0.09244 \n",
      "[219/500] train_loss: 0.06493 valid_loss: 0.07890 test_loss: 0.09176 \n",
      "[220/500] train_loss: 0.06540 valid_loss: 0.07880 test_loss: 0.09070 \n",
      "[221/500] train_loss: 0.06445 valid_loss: 0.07933 test_loss: 0.09222 \n",
      "[222/500] train_loss: 0.06398 valid_loss: 0.07845 test_loss: 0.09065 \n",
      "[223/500] train_loss: 0.06497 valid_loss: 0.07826 test_loss: 0.09254 \n",
      "[224/500] train_loss: 0.06659 valid_loss: 0.07902 test_loss: 0.09214 \n",
      "[225/500] train_loss: 0.06501 valid_loss: 0.07768 test_loss: 0.09085 \n",
      "验证损失减少 (0.077933 --> 0.077683). 正在保存模型...\n",
      "[226/500] train_loss: 0.06501 valid_loss: 0.07703 test_loss: 0.09169 \n",
      "验证损失减少 (0.077683 --> 0.077034). 正在保存模型...\n",
      "[227/500] train_loss: 0.06360 valid_loss: 0.07628 test_loss: 0.09058 \n",
      "验证损失减少 (0.077034 --> 0.076280). 正在保存模型...\n",
      "[228/500] train_loss: 0.06424 valid_loss: 0.07800 test_loss: 0.09114 \n",
      "[229/500] train_loss: 0.06309 valid_loss: 0.07799 test_loss: 0.09181 \n",
      "[230/500] train_loss: 0.06581 valid_loss: 0.07864 test_loss: 0.09193 \n",
      "[231/500] train_loss: 0.06340 valid_loss: 0.07913 test_loss: 0.09500 \n",
      "[232/500] train_loss: 0.06495 valid_loss: 0.08099 test_loss: 0.09227 \n",
      "[233/500] train_loss: 0.06434 valid_loss: 0.07956 test_loss: 0.09198 \n",
      "[234/500] train_loss: 0.06380 valid_loss: 0.07838 test_loss: 0.09125 \n",
      "[235/500] train_loss: 0.06611 valid_loss: 0.08065 test_loss: 0.09218 \n",
      "[236/500] train_loss: 0.06202 valid_loss: 0.07871 test_loss: 0.09171 \n",
      "[237/500] train_loss: 0.06457 valid_loss: 0.08008 test_loss: 0.09086 \n",
      "[238/500] train_loss: 0.06309 valid_loss: 0.08143 test_loss: 0.09090 \n",
      "[239/500] train_loss: 0.06374 valid_loss: 0.07935 test_loss: 0.09110 \n",
      "[240/500] train_loss: 0.06279 valid_loss: 0.07769 test_loss: 0.09124 \n",
      "[241/500] train_loss: 0.06392 valid_loss: 0.07771 test_loss: 0.09076 \n",
      "[242/500] train_loss: 0.06367 valid_loss: 0.07804 test_loss: 0.09091 \n",
      "[243/500] train_loss: 0.06344 valid_loss: 0.07774 test_loss: 0.09038 \n",
      "[244/500] train_loss: 0.06504 valid_loss: 0.07974 test_loss: 0.09308 \n",
      "[245/500] train_loss: 0.06304 valid_loss: 0.07781 test_loss: 0.09078 \n",
      "[246/500] train_loss: 0.06388 valid_loss: 0.07787 test_loss: 0.09071 \n",
      "[247/500] train_loss: 0.06320 valid_loss: 0.07903 test_loss: 0.09217 \n",
      "[248/500] train_loss: 0.06408 valid_loss: 0.07749 test_loss: 0.09240 \n",
      "[249/500] train_loss: 0.06250 valid_loss: 0.07968 test_loss: 0.09455 \n",
      "[250/500] train_loss: 0.06236 valid_loss: 0.07875 test_loss: 0.09321 \n",
      "[251/500] train_loss: 0.06280 valid_loss: 0.07912 test_loss: 0.09310 \n",
      "[252/500] train_loss: 0.06372 valid_loss: 0.07876 test_loss: 0.09275 \n",
      "[253/500] train_loss: 0.06301 valid_loss: 0.08244 test_loss: 0.09408 \n",
      "[254/500] train_loss: 0.06458 valid_loss: 0.07917 test_loss: 0.09158 \n",
      "[255/500] train_loss: 0.06045 valid_loss: 0.07770 test_loss: 0.09231 \n",
      "[256/500] train_loss: 0.06306 valid_loss: 0.07904 test_loss: 0.09249 \n",
      "[257/500] train_loss: 0.06142 valid_loss: 0.07881 test_loss: 0.09199 \n",
      "[258/500] train_loss: 0.06296 valid_loss: 0.07978 test_loss: 0.09305 \n",
      "[259/500] train_loss: 0.06118 valid_loss: 0.07679 test_loss: 0.09176 \n",
      "[260/500] train_loss: 0.06292 valid_loss: 0.07983 test_loss: 0.09252 \n",
      "[261/500] train_loss: 0.06285 valid_loss: 0.07721 test_loss: 0.09106 \n",
      "[262/500] train_loss: 0.06193 valid_loss: 0.07839 test_loss: 0.09158 \n",
      "[263/500] train_loss: 0.06235 valid_loss: 0.07787 test_loss: 0.09080 \n",
      "[264/500] train_loss: 0.06230 valid_loss: 0.07800 test_loss: 0.09153 \n",
      "[265/500] train_loss: 0.06222 valid_loss: 0.07793 test_loss: 0.09077 \n",
      "[266/500] train_loss: 0.06340 valid_loss: 0.07933 test_loss: 0.09385 \n",
      "[267/500] train_loss: 0.06319 valid_loss: 0.07749 test_loss: 0.09149 \n",
      "[268/500] train_loss: 0.06245 valid_loss: 0.07841 test_loss: 0.09145 \n",
      "[269/500] train_loss: 0.06182 valid_loss: 0.07803 test_loss: 0.09167 \n",
      "[270/500] train_loss: 0.06097 valid_loss: 0.07747 test_loss: 0.09141 \n",
      "[271/500] train_loss: 0.06147 valid_loss: 0.07622 test_loss: 0.09112 \n",
      "验证损失减少 (0.076280 --> 0.076216). 正在保存模型...\n",
      "[272/500] train_loss: 0.06278 valid_loss: 0.08020 test_loss: 0.09279 \n",
      "[273/500] train_loss: 0.06166 valid_loss: 0.07918 test_loss: 0.08914 \n",
      "[274/500] train_loss: 0.06261 valid_loss: 0.07653 test_loss: 0.09231 \n",
      "[275/500] train_loss: 0.06149 valid_loss: 0.07792 test_loss: 0.09219 \n",
      "[276/500] train_loss: 0.06086 valid_loss: 0.07915 test_loss: 0.09122 \n",
      "[277/500] train_loss: 0.06347 valid_loss: 0.07926 test_loss: 0.09200 \n",
      "[278/500] train_loss: 0.06058 valid_loss: 0.07836 test_loss: 0.08993 \n",
      "[279/500] train_loss: 0.06144 valid_loss: 0.07799 test_loss: 0.09070 \n",
      "[280/500] train_loss: 0.06540 valid_loss: 0.07986 test_loss: 0.09126 \n",
      "[281/500] train_loss: 0.06230 valid_loss: 0.07936 test_loss: 0.09103 \n",
      "[282/500] train_loss: 0.06101 valid_loss: 0.07798 test_loss: 0.09167 \n",
      "[283/500] train_loss: 0.06281 valid_loss: 0.07873 test_loss: 0.09015 \n",
      "[284/500] train_loss: 0.06114 valid_loss: 0.07839 test_loss: 0.09206 \n",
      "[285/500] train_loss: 0.05902 valid_loss: 0.07667 test_loss: 0.08966 \n",
      "[286/500] train_loss: 0.05986 valid_loss: 0.07616 test_loss: 0.09021 \n",
      "验证损失减少 (0.076216 --> 0.076157). 正在保存模型...\n",
      "[287/500] train_loss: 0.06053 valid_loss: 0.07816 test_loss: 0.09113 \n",
      "[288/500] train_loss: 0.06121 valid_loss: 0.07842 test_loss: 0.09056 \n",
      "[289/500] train_loss: 0.06106 valid_loss: 0.07927 test_loss: 0.08996 \n",
      "[290/500] train_loss: 0.06005 valid_loss: 0.07905 test_loss: 0.09173 \n",
      "[291/500] train_loss: 0.06015 valid_loss: 0.07859 test_loss: 0.08944 \n",
      "[292/500] train_loss: 0.05994 valid_loss: 0.07914 test_loss: 0.09013 \n",
      "[293/500] train_loss: 0.06050 valid_loss: 0.07810 test_loss: 0.09058 \n",
      "[294/500] train_loss: 0.06004 valid_loss: 0.07771 test_loss: 0.09270 \n",
      "[295/500] train_loss: 0.06176 valid_loss: 0.07649 test_loss: 0.08971 \n",
      "[296/500] train_loss: 0.06162 valid_loss: 0.07916 test_loss: 0.08971 \n",
      "[297/500] train_loss: 0.05971 valid_loss: 0.08041 test_loss: 0.09066 \n",
      "[298/500] train_loss: 0.06072 valid_loss: 0.07728 test_loss: 0.08941 \n",
      "[299/500] train_loss: 0.06004 valid_loss: 0.07683 test_loss: 0.08913 \n",
      "[300/500] train_loss: 0.05904 valid_loss: 0.07673 test_loss: 0.08948 \n",
      "[301/500] train_loss: 0.05977 valid_loss: 0.08015 test_loss: 0.09088 \n",
      "[302/500] train_loss: 0.05973 valid_loss: 0.07880 test_loss: 0.09171 \n",
      "[303/500] train_loss: 0.06113 valid_loss: 0.08016 test_loss: 0.09003 \n",
      "[304/500] train_loss: 0.05988 valid_loss: 0.07949 test_loss: 0.08851 \n",
      "[305/500] train_loss: 0.05944 valid_loss: 0.07716 test_loss: 0.08939 \n",
      "[306/500] train_loss: 0.06056 valid_loss: 0.07580 test_loss: 0.08832 \n",
      "验证损失减少 (0.076157 --> 0.075799). 正在保存模型...\n",
      "[307/500] train_loss: 0.05816 valid_loss: 0.07936 test_loss: 0.08998 \n",
      "[308/500] train_loss: 0.06062 valid_loss: 0.07759 test_loss: 0.08968 \n",
      "[309/500] train_loss: 0.06020 valid_loss: 0.07733 test_loss: 0.09041 \n",
      "[310/500] train_loss: 0.05876 valid_loss: 0.07782 test_loss: 0.08898 \n",
      "[311/500] train_loss: 0.06026 valid_loss: 0.07796 test_loss: 0.08997 \n",
      "[312/500] train_loss: 0.06029 valid_loss: 0.07743 test_loss: 0.08934 \n",
      "[313/500] train_loss: 0.05909 valid_loss: 0.07696 test_loss: 0.09029 \n",
      "[314/500] train_loss: 0.05866 valid_loss: 0.07662 test_loss: 0.08932 \n",
      "[315/500] train_loss: 0.05911 valid_loss: 0.07839 test_loss: 0.09204 \n",
      "[316/500] train_loss: 0.05945 valid_loss: 0.07634 test_loss: 0.09135 \n",
      "[317/500] train_loss: 0.05762 valid_loss: 0.07767 test_loss: 0.09037 \n",
      "[318/500] train_loss: 0.06068 valid_loss: 0.07734 test_loss: 0.08877 \n",
      "[319/500] train_loss: 0.05742 valid_loss: 0.07725 test_loss: 0.09083 \n",
      "[320/500] train_loss: 0.05967 valid_loss: 0.07738 test_loss: 0.09066 \n",
      "[321/500] train_loss: 0.05884 valid_loss: 0.07778 test_loss: 0.09033 \n",
      "[322/500] train_loss: 0.05884 valid_loss: 0.07687 test_loss: 0.09083 \n",
      "[323/500] train_loss: 0.05848 valid_loss: 0.07706 test_loss: 0.09134 \n",
      "[324/500] train_loss: 0.05865 valid_loss: 0.07770 test_loss: 0.08933 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325/500] train_loss: 0.05904 valid_loss: 0.07798 test_loss: 0.09023 \n",
      "[326/500] train_loss: 0.05957 valid_loss: 0.08489 test_loss: 0.08967 \n",
      "[327/500] train_loss: 0.05860 valid_loss: 0.08044 test_loss: 0.09168 \n",
      "[328/500] train_loss: 0.06047 valid_loss: 0.07814 test_loss: 0.09158 \n",
      "[329/500] train_loss: 0.05800 valid_loss: 0.07723 test_loss: 0.09031 \n",
      "[330/500] train_loss: 0.05832 valid_loss: 0.07922 test_loss: 0.09042 \n",
      "[331/500] train_loss: 0.05981 valid_loss: 0.07661 test_loss: 0.09003 \n",
      "[332/500] train_loss: 0.05822 valid_loss: 0.07809 test_loss: 0.08988 \n",
      "[333/500] train_loss: 0.05881 valid_loss: 0.07874 test_loss: 0.09079 \n",
      "[334/500] train_loss: 0.05889 valid_loss: 0.07821 test_loss: 0.09107 \n",
      "[335/500] train_loss: 0.05946 valid_loss: 0.08459 test_loss: 0.09141 \n",
      "[336/500] train_loss: 0.05710 valid_loss: 0.07606 test_loss: 0.09001 \n",
      "[337/500] train_loss: 0.05674 valid_loss: 0.07638 test_loss: 0.09041 \n",
      "[338/500] train_loss: 0.05681 valid_loss: 0.08996 test_loss: 0.09082 \n",
      "[339/500] train_loss: 0.05851 valid_loss: 0.08160 test_loss: 0.09029 \n",
      "[340/500] train_loss: 0.05693 valid_loss: 0.07684 test_loss: 0.09138 \n",
      "[341/500] train_loss: 0.05954 valid_loss: 0.07894 test_loss: 0.09157 \n",
      "[342/500] train_loss: 0.05984 valid_loss: 0.07704 test_loss: 0.08965 \n",
      "[343/500] train_loss: 0.05888 valid_loss: 0.07582 test_loss: 0.08998 \n",
      "[344/500] train_loss: 0.05815 valid_loss: 0.07829 test_loss: 0.09020 \n",
      "[345/500] train_loss: 0.05758 valid_loss: 0.07895 test_loss: 0.08964 \n",
      "[346/500] train_loss: 0.05722 valid_loss: 0.07601 test_loss: 0.08973 \n",
      "[347/500] train_loss: 0.05577 valid_loss: 0.07658 test_loss: 0.08833 \n",
      "[348/500] train_loss: 0.05601 valid_loss: 0.07796 test_loss: 0.09041 \n",
      "[349/500] train_loss: 0.05711 valid_loss: 0.07626 test_loss: 0.09080 \n",
      "[350/500] train_loss: 0.05838 valid_loss: 0.07871 test_loss: 0.09109 \n",
      "[351/500] train_loss: 0.05826 valid_loss: 0.08435 test_loss: 0.09099 \n",
      "[352/500] train_loss: 0.05857 valid_loss: 0.07546 test_loss: 0.09035 \n",
      "验证损失减少 (0.075799 --> 0.075458). 正在保存模型...\n",
      "[353/500] train_loss: 0.05827 valid_loss: 0.07501 test_loss: 0.08901 \n",
      "验证损失减少 (0.075458 --> 0.075011). 正在保存模型...\n",
      "[354/500] train_loss: 0.05690 valid_loss: 0.07659 test_loss: 0.09159 \n",
      "[355/500] train_loss: 0.05673 valid_loss: 0.07796 test_loss: 0.09022 \n",
      "[356/500] train_loss: 0.05659 valid_loss: 0.07629 test_loss: 0.09088 \n",
      "[357/500] train_loss: 0.05655 valid_loss: 0.08719 test_loss: 0.09015 \n",
      "[358/500] train_loss: 0.05913 valid_loss: 0.07737 test_loss: 0.08941 \n",
      "[359/500] train_loss: 0.05674 valid_loss: 0.07712 test_loss: 0.09020 \n",
      "[360/500] train_loss: 0.05770 valid_loss: 0.07693 test_loss: 0.09064 \n",
      "[361/500] train_loss: 0.05628 valid_loss: 0.07611 test_loss: 0.08790 \n",
      "[362/500] train_loss: 0.05938 valid_loss: 0.08704 test_loss: 0.08876 \n",
      "[363/500] train_loss: 0.05724 valid_loss: 0.08054 test_loss: 0.09015 \n",
      "[364/500] train_loss: 0.05765 valid_loss: 0.07836 test_loss: 0.09013 \n",
      "[365/500] train_loss: 0.05640 valid_loss: 0.08118 test_loss: 0.08975 \n",
      "[366/500] train_loss: 0.05639 valid_loss: 0.07469 test_loss: 0.08847 \n",
      "验证损失减少 (0.075011 --> 0.074687). 正在保存模型...\n",
      "[367/500] train_loss: 0.05620 valid_loss: 0.07654 test_loss: 0.09093 \n",
      "[368/500] train_loss: 0.05580 valid_loss: 0.09017 test_loss: 0.08921 \n",
      "[369/500] train_loss: 0.05759 valid_loss: 0.09270 test_loss: 0.08956 \n",
      "[370/500] train_loss: 0.05616 valid_loss: 0.08581 test_loss: 0.08870 \n",
      "[371/500] train_loss: 0.05786 valid_loss: 0.07573 test_loss: 0.08871 \n",
      "[372/500] train_loss: 0.05622 valid_loss: 0.08701 test_loss: 0.08865 \n",
      "[373/500] train_loss: 0.05641 valid_loss: 0.08761 test_loss: 0.08962 \n",
      "[374/500] train_loss: 0.05763 valid_loss: 0.08093 test_loss: 0.08957 \n",
      "[375/500] train_loss: 0.05715 valid_loss: 0.08401 test_loss: 0.08860 \n",
      "[376/500] train_loss: 0.05752 valid_loss: 0.07574 test_loss: 0.08890 \n",
      "[377/500] train_loss: 0.05621 valid_loss: 0.07607 test_loss: 0.08837 \n",
      "[378/500] train_loss: 0.05803 valid_loss: 0.07620 test_loss: 0.08941 \n",
      "[379/500] train_loss: 0.05491 valid_loss: 0.07586 test_loss: 0.08844 \n",
      "[380/500] train_loss: 0.05619 valid_loss: 0.07622 test_loss: 0.08960 \n",
      "[381/500] train_loss: 0.05616 valid_loss: 0.07504 test_loss: 0.08784 \n",
      "[382/500] train_loss: 0.05553 valid_loss: 0.07549 test_loss: 0.08736 \n",
      "[383/500] train_loss: 0.05619 valid_loss: 0.07625 test_loss: 0.08872 \n",
      "[384/500] train_loss: 0.05704 valid_loss: 0.07545 test_loss: 0.08973 \n",
      "[385/500] train_loss: 0.05591 valid_loss: 0.07523 test_loss: 0.08825 \n",
      "[386/500] train_loss: 0.05737 valid_loss: 0.07574 test_loss: 0.09008 \n",
      "[387/500] train_loss: 0.05663 valid_loss: 0.07689 test_loss: 0.09003 \n",
      "[388/500] train_loss: 0.05686 valid_loss: 0.07575 test_loss: 0.08934 \n",
      "[389/500] train_loss: 0.05658 valid_loss: 0.07531 test_loss: 0.08949 \n",
      "[390/500] train_loss: 0.05641 valid_loss: 0.07535 test_loss: 0.08887 \n",
      "[391/500] train_loss: 0.05675 valid_loss: 0.07563 test_loss: 0.08977 \n",
      "[392/500] train_loss: 0.05456 valid_loss: 0.07634 test_loss: 0.09013 \n",
      "[393/500] train_loss: 0.05582 valid_loss: 0.07592 test_loss: 0.08870 \n",
      "[394/500] train_loss: 0.05356 valid_loss: 0.07526 test_loss: 0.08989 \n",
      "[395/500] train_loss: 0.05684 valid_loss: 0.07564 test_loss: 0.08917 \n",
      "[396/500] train_loss: 0.05620 valid_loss: 0.07572 test_loss: 0.08971 \n",
      "[397/500] train_loss: 0.05620 valid_loss: 0.07557 test_loss: 0.08953 \n",
      "[398/500] train_loss: 0.05591 valid_loss: 0.07485 test_loss: 0.08837 \n",
      "[399/500] train_loss: 0.05549 valid_loss: 0.07650 test_loss: 0.08928 \n",
      "[400/500] train_loss: 0.05594 valid_loss: 0.07581 test_loss: 0.09015 \n",
      "[401/500] train_loss: 0.05569 valid_loss: 0.07479 test_loss: 0.08864 \n",
      "[402/500] train_loss: 0.05464 valid_loss: 0.07673 test_loss: 0.09081 \n",
      "[403/500] train_loss: 0.05539 valid_loss: 0.07499 test_loss: 0.08937 \n",
      "[404/500] train_loss: 0.05482 valid_loss: 0.07581 test_loss: 0.09099 \n",
      "[405/500] train_loss: 0.05422 valid_loss: 0.07500 test_loss: 0.09156 \n",
      "[406/500] train_loss: 0.05447 valid_loss: 0.07443 test_loss: 0.08867 \n",
      "验证损失减少 (0.074687 --> 0.074430). 正在保存模型...\n",
      "[407/500] train_loss: 0.05551 valid_loss: 0.07568 test_loss: 0.08936 \n",
      "[408/500] train_loss: 0.05389 valid_loss: 0.07692 test_loss: 0.09118 \n",
      "[409/500] train_loss: 0.05722 valid_loss: 0.07630 test_loss: 0.09035 \n",
      "[410/500] train_loss: 0.05451 valid_loss: 0.07698 test_loss: 0.09127 \n",
      "[411/500] train_loss: 0.05606 valid_loss: 0.07604 test_loss: 0.09056 \n",
      "[412/500] train_loss: 0.05481 valid_loss: 0.07630 test_loss: 0.09067 \n",
      "[413/500] train_loss: 0.05736 valid_loss: 0.08321 test_loss: 0.08890 \n",
      "[414/500] train_loss: 0.05337 valid_loss: 0.07687 test_loss: 0.09199 \n",
      "[415/500] train_loss: 0.05404 valid_loss: 0.07629 test_loss: 0.08908 \n",
      "[416/500] train_loss: 0.05397 valid_loss: 0.07628 test_loss: 0.08916 \n",
      "[417/500] train_loss: 0.05632 valid_loss: 0.07623 test_loss: 0.08961 \n",
      "[418/500] train_loss: 0.05425 valid_loss: 0.07909 test_loss: 0.08865 \n",
      "[419/500] train_loss: 0.05519 valid_loss: 0.08932 test_loss: 0.08936 \n",
      "[420/500] train_loss: 0.05347 valid_loss: 0.07661 test_loss: 0.08992 \n",
      "[421/500] train_loss: 0.05531 valid_loss: 0.08728 test_loss: 0.08930 \n",
      "[422/500] train_loss: 0.05529 valid_loss: 0.07633 test_loss: 0.08876 \n",
      "[423/500] train_loss: 0.05511 valid_loss: 0.07537 test_loss: 0.08873 \n",
      "[424/500] train_loss: 0.05430 valid_loss: 0.07760 test_loss: 0.09196 \n",
      "[425/500] train_loss: 0.05520 valid_loss: 0.08227 test_loss: 0.08999 \n",
      "[426/500] train_loss: 0.05391 valid_loss: 0.07647 test_loss: 0.08926 \n",
      "[427/500] train_loss: 0.05554 valid_loss: 0.07656 test_loss: 0.08920 \n",
      "[428/500] train_loss: 0.05506 valid_loss: 0.08158 test_loss: 0.08957 \n",
      "[429/500] train_loss: 0.05509 valid_loss: 0.08476 test_loss: 0.08965 \n",
      "[430/500] train_loss: 0.05441 valid_loss: 0.08031 test_loss: 0.08894 \n",
      "[431/500] train_loss: 0.05399 valid_loss: 0.07895 test_loss: 0.08905 \n",
      "[432/500] train_loss: 0.05359 valid_loss: 0.08055 test_loss: 0.08853 \n",
      "[433/500] train_loss: 0.05318 valid_loss: 0.08210 test_loss: 0.08932 \n",
      "[434/500] train_loss: 0.05491 valid_loss: 0.07644 test_loss: 0.08906 \n",
      "[435/500] train_loss: 0.05345 valid_loss: 0.07801 test_loss: 0.09149 \n",
      "[436/500] train_loss: 0.05527 valid_loss: 0.08244 test_loss: 0.09062 \n",
      "[437/500] train_loss: 0.05467 valid_loss: 0.07650 test_loss: 0.08940 \n",
      "[438/500] train_loss: 0.05361 valid_loss: 0.07613 test_loss: 0.08945 \n",
      "[439/500] train_loss: 0.05541 valid_loss: 0.07882 test_loss: 0.08973 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[440/500] train_loss: 0.05512 valid_loss: 0.08464 test_loss: 0.08911 \n",
      "[441/500] train_loss: 0.05337 valid_loss: 0.07714 test_loss: 0.08822 \n",
      "[442/500] train_loss: 0.05337 valid_loss: 0.07793 test_loss: 0.08761 \n",
      "[443/500] train_loss: 0.05382 valid_loss: 0.08311 test_loss: 0.08729 \n",
      "[444/500] train_loss: 0.05291 valid_loss: 0.09570 test_loss: 0.08901 \n",
      "[445/500] train_loss: 0.05433 valid_loss: 0.08046 test_loss: 0.08828 \n",
      "[446/500] train_loss: 0.05339 valid_loss: 0.08731 test_loss: 0.08817 \n",
      "[447/500] train_loss: 0.05337 valid_loss: 0.08437 test_loss: 0.08959 \n",
      "[448/500] train_loss: 0.05393 valid_loss: 0.08489 test_loss: 0.08997 \n",
      "[449/500] train_loss: 0.05344 valid_loss: 0.07916 test_loss: 0.09124 \n",
      "[450/500] train_loss: 0.05333 valid_loss: 0.08141 test_loss: 0.09015 \n",
      "[451/500] train_loss: 0.05394 valid_loss: 0.09857 test_loss: 0.08949 \n",
      "[452/500] train_loss: 0.05286 valid_loss: 0.08621 test_loss: 0.09043 \n",
      "[453/500] train_loss: 0.05293 valid_loss: 0.09046 test_loss: 0.08988 \n",
      "[454/500] train_loss: 0.05376 valid_loss: 0.09654 test_loss: 0.09032 \n",
      "[455/500] train_loss: 0.05552 valid_loss: 0.07587 test_loss: 0.08956 \n",
      "[456/500] train_loss: 0.05263 valid_loss: 0.08536 test_loss: 0.08912 \n",
      "[457/500] train_loss: 0.05333 valid_loss: 0.08783 test_loss: 0.08851 \n",
      "[458/500] train_loss: 0.05361 valid_loss: 0.07890 test_loss: 0.08835 \n",
      "[459/500] train_loss: 0.05354 valid_loss: 0.07620 test_loss: 0.08929 \n",
      "[460/500] train_loss: 0.05258 valid_loss: 0.07623 test_loss: 0.09086 \n",
      "[461/500] train_loss: 0.05556 valid_loss: 0.08885 test_loss: 0.08883 \n",
      "[462/500] train_loss: 0.05390 valid_loss: 0.07781 test_loss: 0.08864 \n",
      "[463/500] train_loss: 0.05245 valid_loss: 0.08355 test_loss: 0.08911 \n",
      "[464/500] train_loss: 0.05273 valid_loss: 0.08650 test_loss: 0.09111 \n",
      "[465/500] train_loss: 0.05380 valid_loss: 0.08664 test_loss: 0.09040 \n",
      "[466/500] train_loss: 0.05454 valid_loss: 0.07581 test_loss: 0.08895 \n",
      "[467/500] train_loss: 0.05398 valid_loss: 0.07645 test_loss: 0.08935 \n",
      "[468/500] train_loss: 0.05331 valid_loss: 0.07593 test_loss: 0.09124 \n",
      "[469/500] train_loss: 0.05289 valid_loss: 0.09290 test_loss: 0.08895 \n",
      "[470/500] train_loss: 0.05308 valid_loss: 0.08617 test_loss: 0.08986 \n",
      "[471/500] train_loss: 0.05388 valid_loss: 0.08297 test_loss: 0.08966 \n",
      "[472/500] train_loss: 0.05205 valid_loss: 0.08306 test_loss: 0.08966 \n",
      "[473/500] train_loss: 0.05326 valid_loss: 0.08437 test_loss: 0.09002 \n",
      "[474/500] train_loss: 0.05391 valid_loss: 0.08327 test_loss: 0.08843 \n",
      "[475/500] train_loss: 0.05177 valid_loss: 0.07649 test_loss: 0.09045 \n",
      "[476/500] train_loss: 0.05524 valid_loss: 0.09164 test_loss: 0.09022 \n",
      "[477/500] train_loss: 0.05179 valid_loss: 0.08533 test_loss: 0.09165 \n",
      "[478/500] train_loss: 0.05308 valid_loss: 0.07560 test_loss: 0.09010 \n",
      "[479/500] train_loss: 0.05182 valid_loss: 0.07628 test_loss: 0.08951 \n",
      "[480/500] train_loss: 0.05234 valid_loss: 0.08878 test_loss: 0.08981 \n",
      "[481/500] train_loss: 0.05349 valid_loss: 0.08570 test_loss: 0.09105 \n",
      "[482/500] train_loss: 0.05215 valid_loss: 0.07764 test_loss: 0.09132 \n",
      "[483/500] train_loss: 0.05185 valid_loss: 0.07810 test_loss: 0.09056 \n",
      "[484/500] train_loss: 0.05516 valid_loss: 0.09304 test_loss: 0.09507 \n",
      "[485/500] train_loss: 0.05490 valid_loss: 0.08124 test_loss: 0.08889 \n",
      "[486/500] train_loss: 0.05226 valid_loss: 0.07545 test_loss: 0.08969 \n",
      "[487/500] train_loss: 0.05128 valid_loss: 0.07761 test_loss: 0.08878 \n",
      "[488/500] train_loss: 0.05181 valid_loss: 0.07693 test_loss: 0.08886 \n",
      "[489/500] train_loss: 0.05258 valid_loss: 0.08264 test_loss: 0.09029 \n",
      "[490/500] train_loss: 0.05275 valid_loss: 0.07583 test_loss: 0.09049 \n",
      "[491/500] train_loss: 0.05278 valid_loss: 0.07889 test_loss: 0.08987 \n",
      "[492/500] train_loss: 0.05367 valid_loss: 0.08451 test_loss: 0.08938 \n",
      "[493/500] train_loss: 0.05282 valid_loss: 0.07652 test_loss: 0.08988 \n",
      "[494/500] train_loss: 0.05173 valid_loss: 0.08578 test_loss: 0.08859 \n",
      "[495/500] train_loss: 0.05232 valid_loss: 0.08910 test_loss: 0.08921 \n",
      "[496/500] train_loss: 0.05066 valid_loss: 0.08127 test_loss: 0.08930 \n",
      "[497/500] train_loss: 0.05098 valid_loss: 0.08747 test_loss: 0.08867 \n",
      "[498/500] train_loss: 0.05087 valid_loss: 0.07661 test_loss: 0.08948 \n",
      "[499/500] train_loss: 0.05149 valid_loss: 0.07887 test_loss: 0.08987 \n",
      "[500/500] train_loss: 0.05261 valid_loss: 0.07667 test_loss: 0.08962 \n",
      "TRAINING MODEL 14\n",
      "[  1/500] train_loss: 0.42570 valid_loss: 0.29294 test_loss: 0.29809 \n",
      "验证损失减少 (inf --> 0.292939). 正在保存模型...\n",
      "[  2/500] train_loss: 0.23167 valid_loss: 0.21081 test_loss: 0.21885 \n",
      "验证损失减少 (0.292939 --> 0.210813). 正在保存模型...\n",
      "[  3/500] train_loss: 0.18415 valid_loss: 0.17958 test_loss: 0.19026 \n",
      "验证损失减少 (0.210813 --> 0.179579). 正在保存模型...\n",
      "[  4/500] train_loss: 0.16243 valid_loss: 0.16168 test_loss: 0.17371 \n",
      "验证损失减少 (0.179579 --> 0.161676). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15072 valid_loss: 0.15776 test_loss: 0.16850 \n",
      "验证损失减少 (0.161676 --> 0.157765). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14253 valid_loss: 0.14888 test_loss: 0.15991 \n",
      "验证损失减少 (0.157765 --> 0.148884). 正在保存模型...\n",
      "[  7/500] train_loss: 0.14108 valid_loss: 0.14351 test_loss: 0.15718 \n",
      "验证损失减少 (0.148884 --> 0.143510). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13582 valid_loss: 0.13846 test_loss: 0.15231 \n",
      "验证损失减少 (0.143510 --> 0.138461). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13263 valid_loss: 0.14365 test_loss: 0.15882 \n",
      "[ 10/500] train_loss: 0.12979 valid_loss: 0.13575 test_loss: 0.14901 \n",
      "验证损失减少 (0.138461 --> 0.135750). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12677 valid_loss: 0.12926 test_loss: 0.14533 \n",
      "验证损失减少 (0.135750 --> 0.129264). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12500 valid_loss: 0.13370 test_loss: 0.15124 \n",
      "[ 13/500] train_loss: 0.12248 valid_loss: 0.12782 test_loss: 0.14131 \n",
      "验证损失减少 (0.129264 --> 0.127818). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.12086 valid_loss: 0.12850 test_loss: 0.14114 \n",
      "[ 15/500] train_loss: 0.11980 valid_loss: 0.12571 test_loss: 0.13895 \n",
      "验证损失减少 (0.127818 --> 0.125707). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11707 valid_loss: 0.12252 test_loss: 0.13814 \n",
      "验证损失减少 (0.125707 --> 0.122519). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11794 valid_loss: 0.12073 test_loss: 0.13598 \n",
      "验证损失减少 (0.122519 --> 0.120733). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11836 valid_loss: 0.12695 test_loss: 0.14118 \n",
      "[ 19/500] train_loss: 0.11361 valid_loss: 0.11874 test_loss: 0.13336 \n",
      "验证损失减少 (0.120733 --> 0.118744). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11098 valid_loss: 0.11721 test_loss: 0.13082 \n",
      "验证损失减少 (0.118744 --> 0.117211). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.11363 valid_loss: 0.12297 test_loss: 0.13706 \n",
      "[ 22/500] train_loss: 0.11170 valid_loss: 0.11873 test_loss: 0.13257 \n",
      "[ 23/500] train_loss: 0.10915 valid_loss: 0.11840 test_loss: 0.13218 \n",
      "[ 24/500] train_loss: 0.10946 valid_loss: 0.11250 test_loss: 0.12708 \n",
      "验证损失减少 (0.117211 --> 0.112499). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10823 valid_loss: 0.11672 test_loss: 0.13098 \n",
      "[ 26/500] train_loss: 0.10533 valid_loss: 0.11583 test_loss: 0.12889 \n",
      "[ 27/500] train_loss: 0.10608 valid_loss: 0.10912 test_loss: 0.12439 \n",
      "验证损失减少 (0.112499 --> 0.109120). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10798 valid_loss: 0.11078 test_loss: 0.12520 \n",
      "[ 29/500] train_loss: 0.10662 valid_loss: 0.11278 test_loss: 0.12793 \n",
      "[ 30/500] train_loss: 0.10309 valid_loss: 0.10770 test_loss: 0.12261 \n",
      "验证损失减少 (0.109120 --> 0.107697). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.10388 valid_loss: 0.10921 test_loss: 0.12412 \n",
      "[ 32/500] train_loss: 0.10025 valid_loss: 0.10820 test_loss: 0.12124 \n",
      "[ 33/500] train_loss: 0.10173 valid_loss: 0.11024 test_loss: 0.12110 \n",
      "[ 34/500] train_loss: 0.10213 valid_loss: 0.10596 test_loss: 0.12065 \n",
      "验证损失减少 (0.107697 --> 0.105963). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.10115 valid_loss: 0.10499 test_loss: 0.11906 \n",
      "验证损失减少 (0.105963 --> 0.104988). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.10183 valid_loss: 0.10603 test_loss: 0.12036 \n",
      "[ 37/500] train_loss: 0.10197 valid_loss: 0.10688 test_loss: 0.12009 \n",
      "[ 38/500] train_loss: 0.09790 valid_loss: 0.10307 test_loss: 0.11769 \n",
      "验证损失减少 (0.104988 --> 0.103065). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.09989 valid_loss: 0.10258 test_loss: 0.11623 \n",
      "验证损失减少 (0.103065 --> 0.102583). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.10050 valid_loss: 0.10354 test_loss: 0.11545 \n",
      "[ 41/500] train_loss: 0.09863 valid_loss: 0.10407 test_loss: 0.11445 \n",
      "[ 42/500] train_loss: 0.09716 valid_loss: 0.10200 test_loss: 0.11460 \n",
      "验证损失减少 (0.102583 --> 0.101999). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 43/500] train_loss: 0.09767 valid_loss: 0.10188 test_loss: 0.11549 \n",
      "验证损失减少 (0.101999 --> 0.101885). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.09632 valid_loss: 0.10040 test_loss: 0.11621 \n",
      "验证损失减少 (0.101885 --> 0.100401). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.09616 valid_loss: 0.10049 test_loss: 0.11508 \n",
      "[ 46/500] train_loss: 0.09437 valid_loss: 0.10358 test_loss: 0.11480 \n",
      "[ 47/500] train_loss: 0.09586 valid_loss: 0.10192 test_loss: 0.11497 \n",
      "[ 48/500] train_loss: 0.09347 valid_loss: 0.09905 test_loss: 0.11391 \n",
      "验证损失减少 (0.100401 --> 0.099048). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09416 valid_loss: 0.10010 test_loss: 0.11374 \n",
      "[ 50/500] train_loss: 0.09266 valid_loss: 0.10393 test_loss: 0.11679 \n",
      "[ 51/500] train_loss: 0.09094 valid_loss: 0.09946 test_loss: 0.11282 \n",
      "[ 52/500] train_loss: 0.09431 valid_loss: 0.10191 test_loss: 0.11238 \n",
      "[ 53/500] train_loss: 0.09404 valid_loss: 0.09897 test_loss: 0.11157 \n",
      "验证损失减少 (0.099048 --> 0.098965). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.09491 valid_loss: 0.09741 test_loss: 0.11117 \n",
      "验证损失减少 (0.098965 --> 0.097413). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.09329 valid_loss: 0.09575 test_loss: 0.10947 \n",
      "验证损失减少 (0.097413 --> 0.095747). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.09229 valid_loss: 0.09736 test_loss: 0.10970 \n",
      "[ 57/500] train_loss: 0.09017 valid_loss: 0.09564 test_loss: 0.10934 \n",
      "验证损失减少 (0.095747 --> 0.095635). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.09164 valid_loss: 0.09621 test_loss: 0.10888 \n",
      "[ 59/500] train_loss: 0.09132 valid_loss: 0.09567 test_loss: 0.10797 \n",
      "[ 60/500] train_loss: 0.08839 valid_loss: 0.09578 test_loss: 0.10894 \n",
      "[ 61/500] train_loss: 0.08991 valid_loss: 0.09515 test_loss: 0.10728 \n",
      "验证损失减少 (0.095635 --> 0.095146). 正在保存模型...\n",
      "[ 62/500] train_loss: 0.08948 valid_loss: 0.09552 test_loss: 0.10782 \n",
      "[ 63/500] train_loss: 0.08911 valid_loss: 0.09437 test_loss: 0.10669 \n",
      "验证损失减少 (0.095146 --> 0.094374). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.08975 valid_loss: 0.09511 test_loss: 0.10930 \n",
      "[ 65/500] train_loss: 0.08789 valid_loss: 0.09778 test_loss: 0.10913 \n",
      "[ 66/500] train_loss: 0.08799 valid_loss: 0.09284 test_loss: 0.10682 \n",
      "验证损失减少 (0.094374 --> 0.092840). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.08901 valid_loss: 0.09321 test_loss: 0.10618 \n",
      "[ 68/500] train_loss: 0.08780 valid_loss: 0.09526 test_loss: 0.10699 \n",
      "[ 69/500] train_loss: 0.08541 valid_loss: 0.09444 test_loss: 0.10775 \n",
      "[ 70/500] train_loss: 0.08689 valid_loss: 0.09520 test_loss: 0.10506 \n",
      "[ 71/500] train_loss: 0.08729 valid_loss: 0.09524 test_loss: 0.10593 \n",
      "[ 72/500] train_loss: 0.08536 valid_loss: 0.09432 test_loss: 0.10543 \n",
      "[ 73/500] train_loss: 0.08525 valid_loss: 0.09375 test_loss: 0.10620 \n",
      "[ 74/500] train_loss: 0.08654 valid_loss: 0.09124 test_loss: 0.10393 \n",
      "验证损失减少 (0.092840 --> 0.091242). 正在保存模型...\n",
      "[ 75/500] train_loss: 0.08514 valid_loss: 0.09345 test_loss: 0.10573 \n",
      "[ 76/500] train_loss: 0.08273 valid_loss: 0.09419 test_loss: 0.10477 \n",
      "[ 77/500] train_loss: 0.08480 valid_loss: 0.09165 test_loss: 0.10412 \n",
      "[ 78/500] train_loss: 0.08424 valid_loss: 0.09196 test_loss: 0.10369 \n",
      "[ 79/500] train_loss: 0.08508 valid_loss: 0.09142 test_loss: 0.10376 \n",
      "[ 80/500] train_loss: 0.08271 valid_loss: 0.08945 test_loss: 0.10248 \n",
      "验证损失减少 (0.091242 --> 0.089452). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.08356 valid_loss: 0.09090 test_loss: 0.10635 \n",
      "[ 82/500] train_loss: 0.08410 valid_loss: 0.09076 test_loss: 0.10186 \n",
      "[ 83/500] train_loss: 0.08546 valid_loss: 0.09184 test_loss: 0.10369 \n",
      "[ 84/500] train_loss: 0.08269 valid_loss: 0.08914 test_loss: 0.10145 \n",
      "验证损失减少 (0.089452 --> 0.089136). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.08376 valid_loss: 0.09072 test_loss: 0.10346 \n",
      "[ 86/500] train_loss: 0.08352 valid_loss: 0.08967 test_loss: 0.10344 \n",
      "[ 87/500] train_loss: 0.08166 valid_loss: 0.08920 test_loss: 0.10141 \n",
      "[ 88/500] train_loss: 0.08469 valid_loss: 0.08895 test_loss: 0.10100 \n",
      "验证损失减少 (0.089136 --> 0.088947). 正在保存模型...\n",
      "[ 89/500] train_loss: 0.08207 valid_loss: 0.09161 test_loss: 0.10213 \n",
      "[ 90/500] train_loss: 0.08494 valid_loss: 0.09109 test_loss: 0.10148 \n",
      "[ 91/500] train_loss: 0.08140 valid_loss: 0.08787 test_loss: 0.10097 \n",
      "验证损失减少 (0.088947 --> 0.087868). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.08317 valid_loss: 0.08818 test_loss: 0.10047 \n",
      "[ 93/500] train_loss: 0.08084 valid_loss: 0.08904 test_loss: 0.10144 \n",
      "[ 94/500] train_loss: 0.08313 valid_loss: 0.09024 test_loss: 0.10174 \n",
      "[ 95/500] train_loss: 0.07996 valid_loss: 0.08687 test_loss: 0.09949 \n",
      "验证损失减少 (0.087868 --> 0.086874). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.08096 valid_loss: 0.08996 test_loss: 0.10299 \n",
      "[ 97/500] train_loss: 0.08167 valid_loss: 0.08921 test_loss: 0.10114 \n",
      "[ 98/500] train_loss: 0.08000 valid_loss: 0.08985 test_loss: 0.09934 \n",
      "[ 99/500] train_loss: 0.08088 valid_loss: 0.08810 test_loss: 0.10055 \n",
      "[100/500] train_loss: 0.07976 valid_loss: 0.08757 test_loss: 0.09954 \n",
      "[101/500] train_loss: 0.08006 valid_loss: 0.08960 test_loss: 0.10140 \n",
      "[102/500] train_loss: 0.08134 valid_loss: 0.08901 test_loss: 0.10145 \n",
      "[103/500] train_loss: 0.08064 valid_loss: 0.08782 test_loss: 0.09926 \n",
      "[104/500] train_loss: 0.07960 valid_loss: 0.08741 test_loss: 0.09870 \n",
      "[105/500] train_loss: 0.07776 valid_loss: 0.08999 test_loss: 0.09941 \n",
      "[106/500] train_loss: 0.07949 valid_loss: 0.08940 test_loss: 0.09802 \n",
      "[107/500] train_loss: 0.08041 valid_loss: 0.08990 test_loss: 0.09883 \n",
      "[108/500] train_loss: 0.07729 valid_loss: 0.08612 test_loss: 0.09743 \n",
      "验证损失减少 (0.086874 --> 0.086116). 正在保存模型...\n",
      "[109/500] train_loss: 0.07889 valid_loss: 0.08592 test_loss: 0.09722 \n",
      "验证损失减少 (0.086116 --> 0.085916). 正在保存模型...\n",
      "[110/500] train_loss: 0.07874 valid_loss: 0.08763 test_loss: 0.09829 \n",
      "[111/500] train_loss: 0.07707 valid_loss: 0.08818 test_loss: 0.09919 \n",
      "[112/500] train_loss: 0.07804 valid_loss: 0.08848 test_loss: 0.09849 \n",
      "[113/500] train_loss: 0.07671 valid_loss: 0.08591 test_loss: 0.09653 \n",
      "验证损失减少 (0.085916 --> 0.085914). 正在保存模型...\n",
      "[114/500] train_loss: 0.07996 valid_loss: 0.08866 test_loss: 0.09664 \n",
      "[115/500] train_loss: 0.07866 valid_loss: 0.08793 test_loss: 0.09670 \n",
      "[116/500] train_loss: 0.07628 valid_loss: 0.08497 test_loss: 0.09658 \n",
      "验证损失减少 (0.085914 --> 0.084968). 正在保存模型...\n",
      "[117/500] train_loss: 0.07750 valid_loss: 0.08891 test_loss: 0.09789 \n",
      "[118/500] train_loss: 0.07869 valid_loss: 0.08568 test_loss: 0.09646 \n",
      "[119/500] train_loss: 0.07636 valid_loss: 0.08710 test_loss: 0.09764 \n",
      "[120/500] train_loss: 0.07849 valid_loss: 0.08538 test_loss: 0.09738 \n",
      "[121/500] train_loss: 0.07596 valid_loss: 0.08506 test_loss: 0.09553 \n",
      "[122/500] train_loss: 0.07641 valid_loss: 0.08434 test_loss: 0.09630 \n",
      "验证损失减少 (0.084968 --> 0.084342). 正在保存模型...\n",
      "[123/500] train_loss: 0.07678 valid_loss: 0.08625 test_loss: 0.09547 \n",
      "[124/500] train_loss: 0.07630 valid_loss: 0.08472 test_loss: 0.09649 \n",
      "[125/500] train_loss: 0.07748 valid_loss: 0.08487 test_loss: 0.09504 \n",
      "[126/500] train_loss: 0.07649 valid_loss: 0.08475 test_loss: 0.09466 \n",
      "[127/500] train_loss: 0.07535 valid_loss: 0.08509 test_loss: 0.09517 \n",
      "[128/500] train_loss: 0.07863 valid_loss: 0.08541 test_loss: 0.09582 \n",
      "[129/500] train_loss: 0.07624 valid_loss: 0.08422 test_loss: 0.09453 \n",
      "验证损失减少 (0.084342 --> 0.084222). 正在保存模型...\n",
      "[130/500] train_loss: 0.07510 valid_loss: 0.08571 test_loss: 0.09543 \n",
      "[131/500] train_loss: 0.07632 valid_loss: 0.08475 test_loss: 0.09619 \n",
      "[132/500] train_loss: 0.07797 valid_loss: 0.08427 test_loss: 0.09699 \n",
      "[133/500] train_loss: 0.07298 valid_loss: 0.08406 test_loss: 0.09642 \n",
      "验证损失减少 (0.084222 --> 0.084058). 正在保存模型...\n",
      "[134/500] train_loss: 0.07565 valid_loss: 0.08499 test_loss: 0.09473 \n",
      "[135/500] train_loss: 0.07341 valid_loss: 0.09038 test_loss: 0.09748 \n",
      "[136/500] train_loss: 0.07294 valid_loss: 0.08349 test_loss: 0.09517 \n",
      "验证损失减少 (0.084058 --> 0.083485). 正在保存模型...\n",
      "[137/500] train_loss: 0.07171 valid_loss: 0.08571 test_loss: 0.09483 \n",
      "[138/500] train_loss: 0.07253 valid_loss: 0.08893 test_loss: 0.09484 \n",
      "[139/500] train_loss: 0.07533 valid_loss: 0.08236 test_loss: 0.09472 \n",
      "验证损失减少 (0.083485 --> 0.082358). 正在保存模型...\n",
      "[140/500] train_loss: 0.07378 valid_loss: 0.08330 test_loss: 0.09411 \n",
      "[141/500] train_loss: 0.07311 valid_loss: 0.08379 test_loss: 0.09415 \n",
      "[142/500] train_loss: 0.07214 valid_loss: 0.08485 test_loss: 0.09505 \n",
      "[143/500] train_loss: 0.07549 valid_loss: 0.08411 test_loss: 0.09454 \n",
      "[144/500] train_loss: 0.07438 valid_loss: 0.08232 test_loss: 0.09468 \n",
      "验证损失减少 (0.082358 --> 0.082318). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145/500] train_loss: 0.07255 valid_loss: 0.08296 test_loss: 0.09626 \n",
      "[146/500] train_loss: 0.07490 valid_loss: 0.08127 test_loss: 0.09415 \n",
      "验证损失减少 (0.082318 --> 0.081270). 正在保存模型...\n",
      "[147/500] train_loss: 0.07268 valid_loss: 0.08558 test_loss: 0.09522 \n",
      "[148/500] train_loss: 0.07219 valid_loss: 0.08279 test_loss: 0.09389 \n",
      "[149/500] train_loss: 0.07150 valid_loss: 0.08182 test_loss: 0.09304 \n",
      "[150/500] train_loss: 0.07306 valid_loss: 0.08374 test_loss: 0.09482 \n",
      "[151/500] train_loss: 0.07311 valid_loss: 0.08157 test_loss: 0.09322 \n",
      "[152/500] train_loss: 0.07151 valid_loss: 0.08178 test_loss: 0.09392 \n",
      "[153/500] train_loss: 0.07181 valid_loss: 0.08279 test_loss: 0.09339 \n",
      "[154/500] train_loss: 0.07225 valid_loss: 0.08140 test_loss: 0.09424 \n",
      "[155/500] train_loss: 0.07417 valid_loss: 0.08167 test_loss: 0.09416 \n",
      "[156/500] train_loss: 0.07267 valid_loss: 0.08582 test_loss: 0.09371 \n",
      "[157/500] train_loss: 0.07267 valid_loss: 0.08148 test_loss: 0.09447 \n",
      "[158/500] train_loss: 0.07282 valid_loss: 0.08403 test_loss: 0.09525 \n",
      "[159/500] train_loss: 0.07058 valid_loss: 0.08733 test_loss: 0.09422 \n",
      "[160/500] train_loss: 0.07119 valid_loss: 0.08182 test_loss: 0.09369 \n",
      "[161/500] train_loss: 0.07230 valid_loss: 0.08303 test_loss: 0.09399 \n",
      "[162/500] train_loss: 0.07174 valid_loss: 0.08365 test_loss: 0.09228 \n",
      "[163/500] train_loss: 0.07331 valid_loss: 0.08315 test_loss: 0.09271 \n",
      "[164/500] train_loss: 0.07160 valid_loss: 0.08238 test_loss: 0.09294 \n",
      "[165/500] train_loss: 0.07192 valid_loss: 0.08604 test_loss: 0.09369 \n",
      "[166/500] train_loss: 0.07011 valid_loss: 0.08644 test_loss: 0.09303 \n",
      "[167/500] train_loss: 0.07159 valid_loss: 0.08256 test_loss: 0.09178 \n",
      "[168/500] train_loss: 0.07023 valid_loss: 0.08124 test_loss: 0.09340 \n",
      "验证损失减少 (0.081270 --> 0.081244). 正在保存模型...\n",
      "[169/500] train_loss: 0.06976 valid_loss: 0.08375 test_loss: 0.09433 \n",
      "[170/500] train_loss: 0.06967 valid_loss: 0.08215 test_loss: 0.09251 \n",
      "[171/500] train_loss: 0.06879 valid_loss: 0.08150 test_loss: 0.09319 \n",
      "[172/500] train_loss: 0.07211 valid_loss: 0.08284 test_loss: 0.09235 \n",
      "[173/500] train_loss: 0.07148 valid_loss: 0.08020 test_loss: 0.09225 \n",
      "验证损失减少 (0.081244 --> 0.080200). 正在保存模型...\n",
      "[174/500] train_loss: 0.07144 valid_loss: 0.08431 test_loss: 0.09170 \n",
      "[175/500] train_loss: 0.07017 valid_loss: 0.08373 test_loss: 0.09150 \n",
      "[176/500] train_loss: 0.06977 valid_loss: 0.08273 test_loss: 0.09231 \n",
      "[177/500] train_loss: 0.07020 valid_loss: 0.08030 test_loss: 0.09116 \n",
      "[178/500] train_loss: 0.06853 valid_loss: 0.08087 test_loss: 0.09187 \n",
      "[179/500] train_loss: 0.07140 valid_loss: 0.08150 test_loss: 0.09195 \n",
      "[180/500] train_loss: 0.07045 valid_loss: 0.08274 test_loss: 0.09296 \n",
      "[181/500] train_loss: 0.07040 valid_loss: 0.08059 test_loss: 0.09095 \n",
      "[182/500] train_loss: 0.07128 valid_loss: 0.08360 test_loss: 0.09165 \n",
      "[183/500] train_loss: 0.06905 valid_loss: 0.08174 test_loss: 0.09153 \n",
      "[184/500] train_loss: 0.06895 valid_loss: 0.08268 test_loss: 0.09233 \n",
      "[185/500] train_loss: 0.06876 valid_loss: 0.07988 test_loss: 0.09199 \n",
      "验证损失减少 (0.080200 --> 0.079879). 正在保存模型...\n",
      "[186/500] train_loss: 0.06833 valid_loss: 0.08349 test_loss: 0.09111 \n",
      "[187/500] train_loss: 0.06847 valid_loss: 0.08153 test_loss: 0.09206 \n",
      "[188/500] train_loss: 0.06890 valid_loss: 0.08530 test_loss: 0.09191 \n",
      "[189/500] train_loss: 0.06785 valid_loss: 0.07979 test_loss: 0.09225 \n",
      "验证损失减少 (0.079879 --> 0.079786). 正在保存模型...\n",
      "[190/500] train_loss: 0.06836 valid_loss: 0.08056 test_loss: 0.09128 \n",
      "[191/500] train_loss: 0.06596 valid_loss: 0.07863 test_loss: 0.09041 \n",
      "验证损失减少 (0.079786 --> 0.078633). 正在保存模型...\n",
      "[192/500] train_loss: 0.06765 valid_loss: 0.07916 test_loss: 0.09022 \n",
      "[193/500] train_loss: 0.06863 valid_loss: 0.07946 test_loss: 0.09209 \n",
      "[194/500] train_loss: 0.06648 valid_loss: 0.07846 test_loss: 0.09001 \n",
      "验证损失减少 (0.078633 --> 0.078462). 正在保存模型...\n",
      "[195/500] train_loss: 0.06765 valid_loss: 0.08340 test_loss: 0.09127 \n",
      "[196/500] train_loss: 0.06878 valid_loss: 0.07873 test_loss: 0.09058 \n",
      "[197/500] train_loss: 0.06586 valid_loss: 0.07946 test_loss: 0.09024 \n",
      "[198/500] train_loss: 0.06798 valid_loss: 0.07931 test_loss: 0.09065 \n",
      "[199/500] train_loss: 0.06656 valid_loss: 0.07907 test_loss: 0.09053 \n",
      "[200/500] train_loss: 0.06690 valid_loss: 0.07965 test_loss: 0.08997 \n",
      "[201/500] train_loss: 0.06794 valid_loss: 0.08028 test_loss: 0.09065 \n",
      "[202/500] train_loss: 0.06633 valid_loss: 0.07937 test_loss: 0.08999 \n",
      "[203/500] train_loss: 0.06648 valid_loss: 0.08027 test_loss: 0.09073 \n",
      "[204/500] train_loss: 0.06558 valid_loss: 0.08040 test_loss: 0.08957 \n",
      "[205/500] train_loss: 0.06655 valid_loss: 0.07988 test_loss: 0.09035 \n",
      "[206/500] train_loss: 0.06679 valid_loss: 0.07923 test_loss: 0.09175 \n",
      "[207/500] train_loss: 0.06681 valid_loss: 0.07983 test_loss: 0.09030 \n",
      "[208/500] train_loss: 0.06632 valid_loss: 0.07992 test_loss: 0.09090 \n",
      "[209/500] train_loss: 0.06841 valid_loss: 0.08135 test_loss: 0.08950 \n",
      "[210/500] train_loss: 0.06597 valid_loss: 0.07758 test_loss: 0.08903 \n",
      "验证损失减少 (0.078462 --> 0.077579). 正在保存模型...\n",
      "[211/500] train_loss: 0.06864 valid_loss: 0.07912 test_loss: 0.08992 \n",
      "[212/500] train_loss: 0.06418 valid_loss: 0.07779 test_loss: 0.09017 \n",
      "[213/500] train_loss: 0.06804 valid_loss: 0.07954 test_loss: 0.08995 \n",
      "[214/500] train_loss: 0.06659 valid_loss: 0.07736 test_loss: 0.08960 \n",
      "验证损失减少 (0.077579 --> 0.077357). 正在保存模型...\n",
      "[215/500] train_loss: 0.06697 valid_loss: 0.07938 test_loss: 0.08939 \n",
      "[216/500] train_loss: 0.06626 valid_loss: 0.07956 test_loss: 0.09007 \n",
      "[217/500] train_loss: 0.06603 valid_loss: 0.08032 test_loss: 0.09027 \n",
      "[218/500] train_loss: 0.06683 valid_loss: 0.08319 test_loss: 0.09056 \n",
      "[219/500] train_loss: 0.06645 valid_loss: 0.08060 test_loss: 0.09058 \n",
      "[220/500] train_loss: 0.06496 valid_loss: 0.07979 test_loss: 0.08925 \n",
      "[221/500] train_loss: 0.06586 valid_loss: 0.07909 test_loss: 0.09046 \n",
      "[222/500] train_loss: 0.06559 valid_loss: 0.07980 test_loss: 0.09043 \n",
      "[223/500] train_loss: 0.06507 valid_loss: 0.07901 test_loss: 0.08938 \n",
      "[224/500] train_loss: 0.06575 valid_loss: 0.08083 test_loss: 0.09095 \n",
      "[225/500] train_loss: 0.06595 valid_loss: 0.07834 test_loss: 0.08903 \n",
      "[226/500] train_loss: 0.06615 valid_loss: 0.07964 test_loss: 0.08910 \n",
      "[227/500] train_loss: 0.06597 valid_loss: 0.07893 test_loss: 0.08952 \n",
      "[228/500] train_loss: 0.06440 valid_loss: 0.07950 test_loss: 0.09076 \n",
      "[229/500] train_loss: 0.06563 valid_loss: 0.07919 test_loss: 0.08998 \n",
      "[230/500] train_loss: 0.06496 valid_loss: 0.08354 test_loss: 0.09077 \n",
      "[231/500] train_loss: 0.06721 valid_loss: 0.08145 test_loss: 0.08963 \n",
      "[232/500] train_loss: 0.06346 valid_loss: 0.07971 test_loss: 0.08890 \n",
      "[233/500] train_loss: 0.06463 valid_loss: 0.07869 test_loss: 0.08978 \n",
      "[234/500] train_loss: 0.06340 valid_loss: 0.07907 test_loss: 0.08985 \n",
      "[235/500] train_loss: 0.06434 valid_loss: 0.07822 test_loss: 0.08771 \n",
      "[236/500] train_loss: 0.06333 valid_loss: 0.07737 test_loss: 0.08893 \n",
      "[237/500] train_loss: 0.06238 valid_loss: 0.07851 test_loss: 0.08852 \n",
      "[238/500] train_loss: 0.06462 valid_loss: 0.07980 test_loss: 0.08941 \n",
      "[239/500] train_loss: 0.06301 valid_loss: 0.07831 test_loss: 0.08937 \n",
      "[240/500] train_loss: 0.06514 valid_loss: 0.07788 test_loss: 0.08896 \n",
      "[241/500] train_loss: 0.06373 valid_loss: 0.08070 test_loss: 0.09007 \n",
      "[242/500] train_loss: 0.06448 valid_loss: 0.07792 test_loss: 0.08913 \n",
      "[243/500] train_loss: 0.06533 valid_loss: 0.08287 test_loss: 0.08851 \n",
      "[244/500] train_loss: 0.06573 valid_loss: 0.07903 test_loss: 0.08858 \n",
      "[245/500] train_loss: 0.06356 valid_loss: 0.07901 test_loss: 0.08806 \n",
      "[246/500] train_loss: 0.06242 valid_loss: 0.07843 test_loss: 0.08841 \n",
      "[247/500] train_loss: 0.06271 valid_loss: 0.07781 test_loss: 0.08855 \n",
      "[248/500] train_loss: 0.06418 valid_loss: 0.07910 test_loss: 0.08833 \n",
      "[249/500] train_loss: 0.06388 valid_loss: 0.08183 test_loss: 0.08800 \n",
      "[250/500] train_loss: 0.06356 valid_loss: 0.07956 test_loss: 0.09067 \n",
      "[251/500] train_loss: 0.06451 valid_loss: 0.07915 test_loss: 0.08847 \n",
      "[252/500] train_loss: 0.06327 valid_loss: 0.07858 test_loss: 0.08780 \n",
      "[253/500] train_loss: 0.06437 valid_loss: 0.07860 test_loss: 0.08813 \n",
      "[254/500] train_loss: 0.06337 valid_loss: 0.07732 test_loss: 0.08727 \n",
      "验证损失减少 (0.077357 --> 0.077318). 正在保存模型...\n",
      "[255/500] train_loss: 0.06382 valid_loss: 0.07890 test_loss: 0.08799 \n",
      "[256/500] train_loss: 0.06412 valid_loss: 0.07845 test_loss: 0.08819 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[257/500] train_loss: 0.06267 valid_loss: 0.07849 test_loss: 0.08995 \n",
      "[258/500] train_loss: 0.06401 valid_loss: 0.07911 test_loss: 0.08797 \n",
      "[259/500] train_loss: 0.06335 valid_loss: 0.07669 test_loss: 0.08795 \n",
      "验证损失减少 (0.077318 --> 0.076689). 正在保存模型...\n",
      "[260/500] train_loss: 0.06289 valid_loss: 0.08127 test_loss: 0.08838 \n",
      "[261/500] train_loss: 0.06449 valid_loss: 0.08511 test_loss: 0.08936 \n",
      "[262/500] train_loss: 0.06357 valid_loss: 0.08434 test_loss: 0.08914 \n",
      "[263/500] train_loss: 0.06336 valid_loss: 0.08157 test_loss: 0.08891 \n",
      "[264/500] train_loss: 0.06316 valid_loss: 0.07922 test_loss: 0.08940 \n",
      "[265/500] train_loss: 0.06278 valid_loss: 0.08024 test_loss: 0.08894 \n",
      "[266/500] train_loss: 0.06229 valid_loss: 0.07911 test_loss: 0.08765 \n",
      "[267/500] train_loss: 0.06206 valid_loss: 0.07743 test_loss: 0.08842 \n",
      "[268/500] train_loss: 0.06255 valid_loss: 0.07767 test_loss: 0.08823 \n",
      "[269/500] train_loss: 0.06285 valid_loss: 0.07782 test_loss: 0.08775 \n",
      "[270/500] train_loss: 0.06344 valid_loss: 0.07842 test_loss: 0.08777 \n",
      "[271/500] train_loss: 0.06134 valid_loss: 0.07891 test_loss: 0.08789 \n",
      "[272/500] train_loss: 0.06121 valid_loss: 0.08053 test_loss: 0.08798 \n",
      "[273/500] train_loss: 0.06147 valid_loss: 0.07838 test_loss: 0.08710 \n",
      "[274/500] train_loss: 0.06068 valid_loss: 0.07881 test_loss: 0.08778 \n",
      "[275/500] train_loss: 0.06217 valid_loss: 0.07812 test_loss: 0.08741 \n",
      "[276/500] train_loss: 0.06253 valid_loss: 0.07734 test_loss: 0.08790 \n",
      "[277/500] train_loss: 0.06076 valid_loss: 0.07848 test_loss: 0.08857 \n",
      "[278/500] train_loss: 0.06106 valid_loss: 0.07808 test_loss: 0.08981 \n",
      "[279/500] train_loss: 0.06292 valid_loss: 0.07916 test_loss: 0.08967 \n",
      "[280/500] train_loss: 0.06045 valid_loss: 0.07788 test_loss: 0.08901 \n",
      "[281/500] train_loss: 0.06216 valid_loss: 0.07724 test_loss: 0.08774 \n",
      "[282/500] train_loss: 0.06243 valid_loss: 0.07738 test_loss: 0.08873 \n",
      "[283/500] train_loss: 0.06150 valid_loss: 0.07744 test_loss: 0.08848 \n",
      "[284/500] train_loss: 0.06075 valid_loss: 0.07666 test_loss: 0.08763 \n",
      "验证损失减少 (0.076689 --> 0.076659). 正在保存模型...\n",
      "[285/500] train_loss: 0.06292 valid_loss: 0.07623 test_loss: 0.08675 \n",
      "验证损失减少 (0.076659 --> 0.076234). 正在保存模型...\n",
      "[286/500] train_loss: 0.05933 valid_loss: 0.07715 test_loss: 0.08865 \n",
      "[287/500] train_loss: 0.06087 valid_loss: 0.07793 test_loss: 0.08829 \n",
      "[288/500] train_loss: 0.06237 valid_loss: 0.07874 test_loss: 0.08920 \n",
      "[289/500] train_loss: 0.05953 valid_loss: 0.07676 test_loss: 0.08861 \n",
      "[290/500] train_loss: 0.06177 valid_loss: 0.07663 test_loss: 0.08737 \n",
      "[291/500] train_loss: 0.06228 valid_loss: 0.07669 test_loss: 0.08758 \n",
      "[292/500] train_loss: 0.06138 valid_loss: 0.07717 test_loss: 0.08954 \n",
      "[293/500] train_loss: 0.06147 valid_loss: 0.07618 test_loss: 0.08776 \n",
      "验证损失减少 (0.076234 --> 0.076181). 正在保存模型...\n",
      "[294/500] train_loss: 0.05985 valid_loss: 0.07892 test_loss: 0.08792 \n",
      "[295/500] train_loss: 0.06116 valid_loss: 0.07663 test_loss: 0.08832 \n",
      "[296/500] train_loss: 0.06056 valid_loss: 0.07728 test_loss: 0.08857 \n",
      "[297/500] train_loss: 0.06228 valid_loss: 0.07766 test_loss: 0.08883 \n",
      "[298/500] train_loss: 0.06208 valid_loss: 0.07757 test_loss: 0.08820 \n",
      "[299/500] train_loss: 0.06143 valid_loss: 0.07845 test_loss: 0.08969 \n",
      "[300/500] train_loss: 0.05978 valid_loss: 0.07775 test_loss: 0.08866 \n",
      "[301/500] train_loss: 0.06060 valid_loss: 0.07714 test_loss: 0.08787 \n",
      "[302/500] train_loss: 0.06118 valid_loss: 0.07746 test_loss: 0.08921 \n",
      "[303/500] train_loss: 0.06041 valid_loss: 0.07661 test_loss: 0.08779 \n",
      "[304/500] train_loss: 0.06143 valid_loss: 0.07642 test_loss: 0.08788 \n",
      "[305/500] train_loss: 0.06107 valid_loss: 0.07843 test_loss: 0.09002 \n",
      "[306/500] train_loss: 0.06127 valid_loss: 0.07713 test_loss: 0.08734 \n",
      "[307/500] train_loss: 0.06082 valid_loss: 0.07664 test_loss: 0.08771 \n",
      "[308/500] train_loss: 0.06054 valid_loss: 0.07839 test_loss: 0.08911 \n",
      "[309/500] train_loss: 0.06030 valid_loss: 0.07602 test_loss: 0.08707 \n",
      "验证损失减少 (0.076181 --> 0.076021). 正在保存模型...\n",
      "[310/500] train_loss: 0.05989 valid_loss: 0.07781 test_loss: 0.08774 \n",
      "[311/500] train_loss: 0.05935 valid_loss: 0.07711 test_loss: 0.08752 \n",
      "[312/500] train_loss: 0.06061 valid_loss: 0.07731 test_loss: 0.08810 \n",
      "[313/500] train_loss: 0.06023 valid_loss: 0.07659 test_loss: 0.08753 \n",
      "[314/500] train_loss: 0.05938 valid_loss: 0.07653 test_loss: 0.08733 \n",
      "[315/500] train_loss: 0.05949 valid_loss: 0.07588 test_loss: 0.08743 \n",
      "验证损失减少 (0.076021 --> 0.075883). 正在保存模型...\n",
      "[316/500] train_loss: 0.05809 valid_loss: 0.07780 test_loss: 0.08876 \n",
      "[317/500] train_loss: 0.05955 valid_loss: 0.07594 test_loss: 0.08726 \n",
      "[318/500] train_loss: 0.06052 valid_loss: 0.07663 test_loss: 0.08674 \n",
      "[319/500] train_loss: 0.06037 valid_loss: 0.07738 test_loss: 0.08710 \n",
      "[320/500] train_loss: 0.05985 valid_loss: 0.07849 test_loss: 0.08742 \n",
      "[321/500] train_loss: 0.06010 valid_loss: 0.07747 test_loss: 0.08662 \n",
      "[322/500] train_loss: 0.05870 valid_loss: 0.07720 test_loss: 0.08699 \n",
      "[323/500] train_loss: 0.05866 valid_loss: 0.07587 test_loss: 0.08678 \n",
      "验证损失减少 (0.075883 --> 0.075870). 正在保存模型...\n",
      "[324/500] train_loss: 0.05805 valid_loss: 0.07731 test_loss: 0.08735 \n",
      "[325/500] train_loss: 0.06101 valid_loss: 0.07641 test_loss: 0.08753 \n",
      "[326/500] train_loss: 0.06023 valid_loss: 0.07632 test_loss: 0.08797 \n",
      "[327/500] train_loss: 0.05959 valid_loss: 0.07563 test_loss: 0.08723 \n",
      "验证损失减少 (0.075870 --> 0.075634). 正在保存模型...\n",
      "[328/500] train_loss: 0.05889 valid_loss: 0.07702 test_loss: 0.08738 \n",
      "[329/500] train_loss: 0.05827 valid_loss: 0.07756 test_loss: 0.08700 \n",
      "[330/500] train_loss: 0.05785 valid_loss: 0.07624 test_loss: 0.08735 \n",
      "[331/500] train_loss: 0.05989 valid_loss: 0.07754 test_loss: 0.08735 \n",
      "[332/500] train_loss: 0.05911 valid_loss: 0.07646 test_loss: 0.08682 \n",
      "[333/500] train_loss: 0.05864 valid_loss: 0.07707 test_loss: 0.08875 \n",
      "[334/500] train_loss: 0.05872 valid_loss: 0.07727 test_loss: 0.08823 \n",
      "[335/500] train_loss: 0.05851 valid_loss: 0.07852 test_loss: 0.09005 \n",
      "[336/500] train_loss: 0.05935 valid_loss: 0.07886 test_loss: 0.08676 \n",
      "[337/500] train_loss: 0.05952 valid_loss: 0.08613 test_loss: 0.08791 \n",
      "[338/500] train_loss: 0.05854 valid_loss: 0.07931 test_loss: 0.08792 \n",
      "[339/500] train_loss: 0.06022 valid_loss: 0.08486 test_loss: 0.08891 \n",
      "[340/500] train_loss: 0.05553 valid_loss: 0.07688 test_loss: 0.08659 \n",
      "[341/500] train_loss: 0.05842 valid_loss: 0.07884 test_loss: 0.08734 \n",
      "[342/500] train_loss: 0.05947 valid_loss: 0.07697 test_loss: 0.08856 \n",
      "[343/500] train_loss: 0.05753 valid_loss: 0.07650 test_loss: 0.08852 \n",
      "[344/500] train_loss: 0.05956 valid_loss: 0.07849 test_loss: 0.08808 \n",
      "[345/500] train_loss: 0.05512 valid_loss: 0.07667 test_loss: 0.08838 \n",
      "[346/500] train_loss: 0.05860 valid_loss: 0.07614 test_loss: 0.08755 \n",
      "[347/500] train_loss: 0.05954 valid_loss: 0.07749 test_loss: 0.08796 \n",
      "[348/500] train_loss: 0.05818 valid_loss: 0.08230 test_loss: 0.08800 \n",
      "[349/500] train_loss: 0.05856 valid_loss: 0.07742 test_loss: 0.08735 \n",
      "[350/500] train_loss: 0.05717 valid_loss: 0.07805 test_loss: 0.08738 \n",
      "[351/500] train_loss: 0.05595 valid_loss: 0.07971 test_loss: 0.08849 \n",
      "[352/500] train_loss: 0.05821 valid_loss: 0.07960 test_loss: 0.08694 \n",
      "[353/500] train_loss: 0.05943 valid_loss: 0.07806 test_loss: 0.08757 \n",
      "[354/500] train_loss: 0.05786 valid_loss: 0.07893 test_loss: 0.08762 \n",
      "[355/500] train_loss: 0.05836 valid_loss: 0.07986 test_loss: 0.08817 \n",
      "[356/500] train_loss: 0.05971 valid_loss: 0.07787 test_loss: 0.08758 \n",
      "[357/500] train_loss: 0.05837 valid_loss: 0.08710 test_loss: 0.08675 \n",
      "[358/500] train_loss: 0.05768 valid_loss: 0.07725 test_loss: 0.08715 \n",
      "[359/500] train_loss: 0.05729 valid_loss: 0.07612 test_loss: 0.08651 \n",
      "[360/500] train_loss: 0.05755 valid_loss: 0.07768 test_loss: 0.08770 \n",
      "[361/500] train_loss: 0.05746 valid_loss: 0.07708 test_loss: 0.08742 \n",
      "[362/500] train_loss: 0.05852 valid_loss: 0.07565 test_loss: 0.08608 \n",
      "[363/500] train_loss: 0.05692 valid_loss: 0.07608 test_loss: 0.08633 \n",
      "[364/500] train_loss: 0.05671 valid_loss: 0.07805 test_loss: 0.08809 \n",
      "[365/500] train_loss: 0.05809 valid_loss: 0.07616 test_loss: 0.08677 \n",
      "[366/500] train_loss: 0.05702 valid_loss: 0.07565 test_loss: 0.08578 \n",
      "[367/500] train_loss: 0.05676 valid_loss: 0.07635 test_loss: 0.08694 \n",
      "[368/500] train_loss: 0.05817 valid_loss: 0.07784 test_loss: 0.08684 \n",
      "[369/500] train_loss: 0.05728 valid_loss: 0.07601 test_loss: 0.08625 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[370/500] train_loss: 0.05640 valid_loss: 0.07761 test_loss: 0.08788 \n",
      "[371/500] train_loss: 0.05935 valid_loss: 0.07721 test_loss: 0.08781 \n",
      "[372/500] train_loss: 0.05571 valid_loss: 0.07680 test_loss: 0.08651 \n",
      "[373/500] train_loss: 0.05652 valid_loss: 0.07653 test_loss: 0.08732 \n",
      "[374/500] train_loss: 0.05668 valid_loss: 0.07628 test_loss: 0.08643 \n",
      "[375/500] train_loss: 0.05589 valid_loss: 0.07696 test_loss: 0.08709 \n",
      "[376/500] train_loss: 0.05720 valid_loss: 0.07615 test_loss: 0.08657 \n",
      "[377/500] train_loss: 0.05665 valid_loss: 0.07611 test_loss: 0.08888 \n",
      "[378/500] train_loss: 0.05637 valid_loss: 0.07600 test_loss: 0.08690 \n",
      "[379/500] train_loss: 0.05838 valid_loss: 0.07680 test_loss: 0.08790 \n",
      "[380/500] train_loss: 0.05663 valid_loss: 0.07762 test_loss: 0.08854 \n",
      "[381/500] train_loss: 0.05673 valid_loss: 0.07533 test_loss: 0.08675 \n",
      "验证损失减少 (0.075634 --> 0.075330). 正在保存模型...\n",
      "[382/500] train_loss: 0.05747 valid_loss: 0.07782 test_loss: 0.08765 \n",
      "[383/500] train_loss: 0.05674 valid_loss: 0.07701 test_loss: 0.08834 \n",
      "[384/500] train_loss: 0.05657 valid_loss: 0.07886 test_loss: 0.08763 \n",
      "[385/500] train_loss: 0.05418 valid_loss: 0.07633 test_loss: 0.08765 \n",
      "[386/500] train_loss: 0.05462 valid_loss: 0.07736 test_loss: 0.08823 \n",
      "[387/500] train_loss: 0.05618 valid_loss: 0.07646 test_loss: 0.08663 \n",
      "[388/500] train_loss: 0.05621 valid_loss: 0.07556 test_loss: 0.08810 \n",
      "[389/500] train_loss: 0.05652 valid_loss: 0.07605 test_loss: 0.08711 \n",
      "[390/500] train_loss: 0.05700 valid_loss: 0.07622 test_loss: 0.08811 \n",
      "[391/500] train_loss: 0.05598 valid_loss: 0.07696 test_loss: 0.08648 \n",
      "[392/500] train_loss: 0.05763 valid_loss: 0.07699 test_loss: 0.08627 \n",
      "[393/500] train_loss: 0.05517 valid_loss: 0.07558 test_loss: 0.08674 \n",
      "[394/500] train_loss: 0.05529 valid_loss: 0.07633 test_loss: 0.08725 \n",
      "[395/500] train_loss: 0.05645 valid_loss: 0.07551 test_loss: 0.08709 \n",
      "[396/500] train_loss: 0.05518 valid_loss: 0.07693 test_loss: 0.08772 \n",
      "[397/500] train_loss: 0.05767 valid_loss: 0.07705 test_loss: 0.08674 \n",
      "[398/500] train_loss: 0.05624 valid_loss: 0.07755 test_loss: 0.08835 \n",
      "[399/500] train_loss: 0.05792 valid_loss: 0.07814 test_loss: 0.08728 \n",
      "[400/500] train_loss: 0.05625 valid_loss: 0.07776 test_loss: 0.08634 \n",
      "[401/500] train_loss: 0.05528 valid_loss: 0.07674 test_loss: 0.08734 \n",
      "[402/500] train_loss: 0.05314 valid_loss: 0.07642 test_loss: 0.08740 \n",
      "[403/500] train_loss: 0.05538 valid_loss: 0.07662 test_loss: 0.08748 \n",
      "[404/500] train_loss: 0.05624 valid_loss: 0.07727 test_loss: 0.08659 \n",
      "[405/500] train_loss: 0.05530 valid_loss: 0.07755 test_loss: 0.08803 \n",
      "[406/500] train_loss: 0.05703 valid_loss: 0.07632 test_loss: 0.08724 \n",
      "[407/500] train_loss: 0.05490 valid_loss: 0.07611 test_loss: 0.08655 \n",
      "[408/500] train_loss: 0.05640 valid_loss: 0.07828 test_loss: 0.08752 \n",
      "[409/500] train_loss: 0.05441 valid_loss: 0.07587 test_loss: 0.08622 \n",
      "[410/500] train_loss: 0.05559 valid_loss: 0.07773 test_loss: 0.08863 \n",
      "[411/500] train_loss: 0.05542 valid_loss: 0.07727 test_loss: 0.08619 \n",
      "[412/500] train_loss: 0.05610 valid_loss: 0.07633 test_loss: 0.08571 \n",
      "[413/500] train_loss: 0.05723 valid_loss: 0.07591 test_loss: 0.08692 \n",
      "[414/500] train_loss: 0.05574 valid_loss: 0.07660 test_loss: 0.08761 \n",
      "[415/500] train_loss: 0.05530 valid_loss: 0.07962 test_loss: 0.08668 \n",
      "[416/500] train_loss: 0.05564 valid_loss: 0.08301 test_loss: 0.09059 \n",
      "[417/500] train_loss: 0.05530 valid_loss: 0.07656 test_loss: 0.08665 \n",
      "[418/500] train_loss: 0.05456 valid_loss: 0.07800 test_loss: 0.08673 \n",
      "[419/500] train_loss: 0.05560 valid_loss: 0.07864 test_loss: 0.08783 \n",
      "[420/500] train_loss: 0.05448 valid_loss: 0.07723 test_loss: 0.08822 \n",
      "[421/500] train_loss: 0.05404 valid_loss: 0.07780 test_loss: 0.08827 \n",
      "[422/500] train_loss: 0.05458 valid_loss: 0.07739 test_loss: 0.08892 \n",
      "[423/500] train_loss: 0.05536 valid_loss: 0.08155 test_loss: 0.08730 \n",
      "[424/500] train_loss: 0.05401 valid_loss: 0.07561 test_loss: 0.08801 \n",
      "[425/500] train_loss: 0.05443 valid_loss: 0.07864 test_loss: 0.08921 \n",
      "[426/500] train_loss: 0.05397 valid_loss: 0.07649 test_loss: 0.08787 \n",
      "[427/500] train_loss: 0.05422 valid_loss: 0.07685 test_loss: 0.08675 \n",
      "[428/500] train_loss: 0.05288 valid_loss: 0.07846 test_loss: 0.08805 \n",
      "[429/500] train_loss: 0.05557 valid_loss: 0.07742 test_loss: 0.08796 \n",
      "[430/500] train_loss: 0.05411 valid_loss: 0.07549 test_loss: 0.08699 \n",
      "[431/500] train_loss: 0.05584 valid_loss: 0.07583 test_loss: 0.08680 \n",
      "[432/500] train_loss: 0.05273 valid_loss: 0.07618 test_loss: 0.08789 \n",
      "[433/500] train_loss: 0.05472 valid_loss: 0.07686 test_loss: 0.08678 \n",
      "[434/500] train_loss: 0.05390 valid_loss: 0.07765 test_loss: 0.08810 \n",
      "[435/500] train_loss: 0.05462 valid_loss: 0.08215 test_loss: 0.08784 \n",
      "[436/500] train_loss: 0.05421 valid_loss: 0.07877 test_loss: 0.08997 \n",
      "[437/500] train_loss: 0.05269 valid_loss: 0.07621 test_loss: 0.08673 \n",
      "[438/500] train_loss: 0.05456 valid_loss: 0.07662 test_loss: 0.08873 \n",
      "[439/500] train_loss: 0.05472 valid_loss: 0.07637 test_loss: 0.08759 \n",
      "[440/500] train_loss: 0.05504 valid_loss: 0.07762 test_loss: 0.08760 \n",
      "[441/500] train_loss: 0.05380 valid_loss: 0.07728 test_loss: 0.08680 \n",
      "[442/500] train_loss: 0.05577 valid_loss: 0.07605 test_loss: 0.08615 \n",
      "[443/500] train_loss: 0.05322 valid_loss: 0.07731 test_loss: 0.08680 \n",
      "[444/500] train_loss: 0.05316 valid_loss: 0.07785 test_loss: 0.08883 \n",
      "[445/500] train_loss: 0.05402 valid_loss: 0.07643 test_loss: 0.08756 \n",
      "[446/500] train_loss: 0.05362 valid_loss: 0.07577 test_loss: 0.08586 \n",
      "[447/500] train_loss: 0.05360 valid_loss: 0.07602 test_loss: 0.08722 \n",
      "[448/500] train_loss: 0.05466 valid_loss: 0.07691 test_loss: 0.08840 \n",
      "[449/500] train_loss: 0.05348 valid_loss: 0.07695 test_loss: 0.08765 \n",
      "[450/500] train_loss: 0.05398 valid_loss: 0.07644 test_loss: 0.08583 \n",
      "[451/500] train_loss: 0.05289 valid_loss: 0.07619 test_loss: 0.08701 \n",
      "[452/500] train_loss: 0.05412 valid_loss: 0.07693 test_loss: 0.08789 \n",
      "[453/500] train_loss: 0.05445 valid_loss: 0.07575 test_loss: 0.08778 \n",
      "[454/500] train_loss: 0.05214 valid_loss: 0.07611 test_loss: 0.08685 \n",
      "[455/500] train_loss: 0.05300 valid_loss: 0.07642 test_loss: 0.08735 \n",
      "[456/500] train_loss: 0.05362 valid_loss: 0.07774 test_loss: 0.08835 \n",
      "[457/500] train_loss: 0.05358 valid_loss: 0.08023 test_loss: 0.08900 \n",
      "[458/500] train_loss: 0.05421 valid_loss: 0.07736 test_loss: 0.08760 \n",
      "[459/500] train_loss: 0.05254 valid_loss: 0.07523 test_loss: 0.08725 \n",
      "验证损失减少 (0.075330 --> 0.075227). 正在保存模型...\n",
      "[460/500] train_loss: 0.05314 valid_loss: 0.07980 test_loss: 0.08778 \n",
      "[461/500] train_loss: 0.05451 valid_loss: 0.07856 test_loss: 0.08653 \n",
      "[462/500] train_loss: 0.05200 valid_loss: 0.07589 test_loss: 0.08671 \n",
      "[463/500] train_loss: 0.05374 valid_loss: 0.07692 test_loss: 0.08747 \n",
      "[464/500] train_loss: 0.05362 valid_loss: 0.08008 test_loss: 0.08749 \n",
      "[465/500] train_loss: 0.05393 valid_loss: 0.07692 test_loss: 0.08772 \n",
      "[466/500] train_loss: 0.05509 valid_loss: 0.07915 test_loss: 0.08748 \n",
      "[467/500] train_loss: 0.05328 valid_loss: 0.07644 test_loss: 0.08610 \n",
      "[468/500] train_loss: 0.05523 valid_loss: 0.07812 test_loss: 0.08758 \n",
      "[469/500] train_loss: 0.05550 valid_loss: 0.07715 test_loss: 0.08754 \n",
      "[470/500] train_loss: 0.05369 valid_loss: 0.07681 test_loss: 0.08563 \n",
      "[471/500] train_loss: 0.05327 valid_loss: 0.07706 test_loss: 0.08679 \n",
      "[472/500] train_loss: 0.05313 valid_loss: 0.07790 test_loss: 0.08801 \n",
      "[473/500] train_loss: 0.05397 valid_loss: 0.07546 test_loss: 0.08573 \n",
      "[474/500] train_loss: 0.05292 valid_loss: 0.07638 test_loss: 0.08705 \n",
      "[475/500] train_loss: 0.05083 valid_loss: 0.07549 test_loss: 0.08713 \n",
      "[476/500] train_loss: 0.05221 valid_loss: 0.08205 test_loss: 0.08973 \n",
      "[477/500] train_loss: 0.05193 valid_loss: 0.08053 test_loss: 0.08523 \n",
      "[478/500] train_loss: 0.05259 valid_loss: 0.07639 test_loss: 0.08688 \n",
      "[479/500] train_loss: 0.05344 valid_loss: 0.07710 test_loss: 0.08895 \n",
      "[480/500] train_loss: 0.05205 valid_loss: 0.07634 test_loss: 0.08767 \n",
      "[481/500] train_loss: 0.05216 valid_loss: 0.07745 test_loss: 0.08727 \n",
      "[482/500] train_loss: 0.05188 valid_loss: 0.07568 test_loss: 0.08704 \n",
      "[483/500] train_loss: 0.05265 valid_loss: 0.07955 test_loss: 0.08696 \n",
      "[484/500] train_loss: 0.05317 valid_loss: 0.08050 test_loss: 0.08708 \n",
      "[485/500] train_loss: 0.05309 valid_loss: 0.08079 test_loss: 0.08736 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[486/500] train_loss: 0.05252 valid_loss: 0.07441 test_loss: 0.08637 \n",
      "验证损失减少 (0.075227 --> 0.074406). 正在保存模型...\n",
      "[487/500] train_loss: 0.05315 valid_loss: 0.07971 test_loss: 0.08757 \n",
      "[488/500] train_loss: 0.05265 valid_loss: 0.07768 test_loss: 0.08956 \n",
      "[489/500] train_loss: 0.05252 valid_loss: 0.07771 test_loss: 0.08723 \n",
      "[490/500] train_loss: 0.05265 valid_loss: 0.07946 test_loss: 0.08715 \n",
      "[491/500] train_loss: 0.05175 valid_loss: 0.07754 test_loss: 0.08710 \n",
      "[492/500] train_loss: 0.05223 valid_loss: 0.08008 test_loss: 0.08816 \n",
      "[493/500] train_loss: 0.05696 valid_loss: 0.08025 test_loss: 0.09039 \n",
      "[494/500] train_loss: 0.05549 valid_loss: 0.08109 test_loss: 0.08637 \n",
      "[495/500] train_loss: 0.05314 valid_loss: 0.07645 test_loss: 0.08853 \n",
      "[496/500] train_loss: 0.05382 valid_loss: 0.07710 test_loss: 0.08890 \n",
      "[497/500] train_loss: 0.05229 valid_loss: 0.07577 test_loss: 0.08778 \n",
      "[498/500] train_loss: 0.05087 valid_loss: 0.07703 test_loss: 0.08925 \n",
      "[499/500] train_loss: 0.05062 valid_loss: 0.07826 test_loss: 0.08667 \n",
      "[500/500] train_loss: 0.05041 valid_loss: 0.07815 test_loss: 0.08789 \n",
      "TRAINING MODEL 15\n",
      "[  1/500] train_loss: 0.36390 valid_loss: 0.27007 test_loss: 0.27461 \n",
      "验证损失减少 (inf --> 0.270070). 正在保存模型...\n",
      "[  2/500] train_loss: 0.21605 valid_loss: 0.20083 test_loss: 0.20947 \n",
      "验证损失减少 (0.270070 --> 0.200835). 正在保存模型...\n",
      "[  3/500] train_loss: 0.17465 valid_loss: 0.17210 test_loss: 0.18352 \n",
      "验证损失减少 (0.200835 --> 0.172097). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15880 valid_loss: 0.15835 test_loss: 0.17099 \n",
      "验证损失减少 (0.172097 --> 0.158349). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14884 valid_loss: 0.15314 test_loss: 0.16495 \n",
      "验证损失减少 (0.158349 --> 0.153137). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14497 valid_loss: 0.14872 test_loss: 0.15882 \n",
      "验证损失减少 (0.153137 --> 0.148720). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13562 valid_loss: 0.14009 test_loss: 0.15320 \n",
      "验证损失减少 (0.148720 --> 0.140088). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13459 valid_loss: 0.13606 test_loss: 0.15038 \n",
      "验证损失减少 (0.140088 --> 0.136056). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13115 valid_loss: 0.13420 test_loss: 0.14844 \n",
      "验证损失减少 (0.136056 --> 0.134198). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12857 valid_loss: 0.13179 test_loss: 0.14612 \n",
      "验证损失减少 (0.134198 --> 0.131791). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12555 valid_loss: 0.12983 test_loss: 0.14560 \n",
      "验证损失减少 (0.131791 --> 0.129826). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12547 valid_loss: 0.12640 test_loss: 0.14025 \n",
      "验证损失减少 (0.129826 --> 0.126405). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12118 valid_loss: 0.12372 test_loss: 0.13876 \n",
      "验证损失减少 (0.126405 --> 0.123717). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.12458 valid_loss: 0.12410 test_loss: 0.13974 \n",
      "[ 15/500] train_loss: 0.12185 valid_loss: 0.12177 test_loss: 0.13710 \n",
      "验证损失减少 (0.123717 --> 0.121769). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11820 valid_loss: 0.12115 test_loss: 0.13625 \n",
      "验证损失减少 (0.121769 --> 0.121154). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11304 valid_loss: 0.12686 test_loss: 0.13917 \n",
      "[ 18/500] train_loss: 0.11617 valid_loss: 0.11822 test_loss: 0.13400 \n",
      "验证损失减少 (0.121154 --> 0.118223). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11452 valid_loss: 0.11813 test_loss: 0.13242 \n",
      "验证损失减少 (0.118223 --> 0.118128). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11201 valid_loss: 0.11901 test_loss: 0.13138 \n",
      "[ 21/500] train_loss: 0.11217 valid_loss: 0.11634 test_loss: 0.13185 \n",
      "验证损失减少 (0.118128 --> 0.116344). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.11117 valid_loss: 0.11310 test_loss: 0.12863 \n",
      "验证损失减少 (0.116344 --> 0.113105). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.11002 valid_loss: 0.11297 test_loss: 0.12788 \n",
      "验证损失减少 (0.113105 --> 0.112967). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.11062 valid_loss: 0.11799 test_loss: 0.13152 \n",
      "[ 25/500] train_loss: 0.10808 valid_loss: 0.11531 test_loss: 0.12833 \n",
      "[ 26/500] train_loss: 0.10999 valid_loss: 0.11826 test_loss: 0.12923 \n",
      "[ 27/500] train_loss: 0.11017 valid_loss: 0.11366 test_loss: 0.12849 \n",
      "[ 28/500] train_loss: 0.10509 valid_loss: 0.11083 test_loss: 0.12451 \n",
      "验证损失减少 (0.112967 --> 0.110832). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10770 valid_loss: 0.11427 test_loss: 0.12542 \n",
      "[ 30/500] train_loss: 0.10313 valid_loss: 0.10913 test_loss: 0.12271 \n",
      "验证损失减少 (0.110832 --> 0.109130). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.10362 valid_loss: 0.10940 test_loss: 0.12446 \n",
      "[ 32/500] train_loss: 0.10892 valid_loss: 0.10821 test_loss: 0.12208 \n",
      "验证损失减少 (0.109130 --> 0.108209). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10156 valid_loss: 0.10836 test_loss: 0.12074 \n",
      "[ 34/500] train_loss: 0.10192 valid_loss: 0.10716 test_loss: 0.12152 \n",
      "验证损失减少 (0.108209 --> 0.107160). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.10483 valid_loss: 0.10866 test_loss: 0.12067 \n",
      "[ 36/500] train_loss: 0.09853 valid_loss: 0.10573 test_loss: 0.11813 \n",
      "验证损失减少 (0.107160 --> 0.105729). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.10131 valid_loss: 0.10509 test_loss: 0.11954 \n",
      "验证损失减少 (0.105729 --> 0.105088). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.09972 valid_loss: 0.10438 test_loss: 0.11932 \n",
      "验证损失减少 (0.105088 --> 0.104381). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.10048 valid_loss: 0.10364 test_loss: 0.11631 \n",
      "验证损失减少 (0.104381 --> 0.103643). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09786 valid_loss: 0.10466 test_loss: 0.11899 \n",
      "[ 41/500] train_loss: 0.09708 valid_loss: 0.10536 test_loss: 0.11712 \n",
      "[ 42/500] train_loss: 0.10000 valid_loss: 0.10353 test_loss: 0.11712 \n",
      "验证损失减少 (0.103643 --> 0.103529). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.09591 valid_loss: 0.10251 test_loss: 0.11352 \n",
      "验证损失减少 (0.103529 --> 0.102513). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.09455 valid_loss: 0.10371 test_loss: 0.11508 \n",
      "[ 45/500] train_loss: 0.09514 valid_loss: 0.10247 test_loss: 0.11352 \n",
      "验证损失减少 (0.102513 --> 0.102467). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.09555 valid_loss: 0.10013 test_loss: 0.11379 \n",
      "验证损失减少 (0.102467 --> 0.100129). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.09691 valid_loss: 0.10183 test_loss: 0.11561 \n",
      "[ 48/500] train_loss: 0.09623 valid_loss: 0.10043 test_loss: 0.11276 \n",
      "[ 49/500] train_loss: 0.09361 valid_loss: 0.10051 test_loss: 0.11356 \n",
      "[ 50/500] train_loss: 0.09481 valid_loss: 0.10148 test_loss: 0.11308 \n",
      "[ 51/500] train_loss: 0.09480 valid_loss: 0.10009 test_loss: 0.11109 \n",
      "验证损失减少 (0.100129 --> 0.100086). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.09356 valid_loss: 0.10227 test_loss: 0.11325 \n",
      "[ 53/500] train_loss: 0.09205 valid_loss: 0.09815 test_loss: 0.11180 \n",
      "验证损失减少 (0.100086 --> 0.098154). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.09133 valid_loss: 0.09817 test_loss: 0.11013 \n",
      "[ 55/500] train_loss: 0.09077 valid_loss: 0.09832 test_loss: 0.11044 \n",
      "[ 56/500] train_loss: 0.09274 valid_loss: 0.09719 test_loss: 0.10959 \n",
      "验证损失减少 (0.098154 --> 0.097191). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.09323 valid_loss: 0.09676 test_loss: 0.10959 \n",
      "验证损失减少 (0.097191 --> 0.096761). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.09092 valid_loss: 0.09619 test_loss: 0.10897 \n",
      "验证损失减少 (0.096761 --> 0.096190). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.09092 valid_loss: 0.09952 test_loss: 0.10939 \n",
      "[ 60/500] train_loss: 0.08957 valid_loss: 0.09624 test_loss: 0.10710 \n",
      "[ 61/500] train_loss: 0.08870 valid_loss: 0.09877 test_loss: 0.10965 \n",
      "[ 62/500] train_loss: 0.09122 valid_loss: 0.09671 test_loss: 0.10861 \n",
      "[ 63/500] train_loss: 0.09049 valid_loss: 0.09691 test_loss: 0.10999 \n",
      "[ 64/500] train_loss: 0.08751 valid_loss: 0.09589 test_loss: 0.10881 \n",
      "验证损失减少 (0.096190 --> 0.095889). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.09018 valid_loss: 0.09549 test_loss: 0.10794 \n",
      "验证损失减少 (0.095889 --> 0.095489). 正在保存模型...\n",
      "[ 66/500] train_loss: 0.08740 valid_loss: 0.09451 test_loss: 0.10882 \n",
      "验证损失减少 (0.095489 --> 0.094509). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.09192 valid_loss: 0.09673 test_loss: 0.10725 \n",
      "[ 68/500] train_loss: 0.08767 valid_loss: 0.09481 test_loss: 0.10643 \n",
      "[ 69/500] train_loss: 0.08803 valid_loss: 0.09540 test_loss: 0.10930 \n",
      "[ 70/500] train_loss: 0.08693 valid_loss: 0.09787 test_loss: 0.10553 \n",
      "[ 71/500] train_loss: 0.08624 valid_loss: 0.09259 test_loss: 0.10517 \n",
      "验证损失减少 (0.094509 --> 0.092587). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.08702 valid_loss: 0.09126 test_loss: 0.10455 \n",
      "验证损失减少 (0.092587 --> 0.091260). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.08875 valid_loss: 0.09442 test_loss: 0.10315 \n",
      "[ 74/500] train_loss: 0.08592 valid_loss: 0.09352 test_loss: 0.10560 \n",
      "[ 75/500] train_loss: 0.08707 valid_loss: 0.09371 test_loss: 0.10461 \n",
      "[ 76/500] train_loss: 0.08949 valid_loss: 0.09153 test_loss: 0.10401 \n",
      "[ 77/500] train_loss: 0.08590 valid_loss: 0.09328 test_loss: 0.10459 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 78/500] train_loss: 0.08370 valid_loss: 0.09419 test_loss: 0.10578 \n",
      "[ 79/500] train_loss: 0.08539 valid_loss: 0.09530 test_loss: 0.10402 \n",
      "[ 80/500] train_loss: 0.08643 valid_loss: 0.09191 test_loss: 0.10351 \n",
      "[ 81/500] train_loss: 0.08513 valid_loss: 0.09198 test_loss: 0.10376 \n",
      "[ 82/500] train_loss: 0.08434 valid_loss: 0.09288 test_loss: 0.10232 \n",
      "[ 83/500] train_loss: 0.08218 valid_loss: 0.09194 test_loss: 0.10343 \n",
      "[ 84/500] train_loss: 0.08570 valid_loss: 0.09047 test_loss: 0.10385 \n",
      "验证损失减少 (0.091260 --> 0.090474). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.08439 valid_loss: 0.08915 test_loss: 0.10234 \n",
      "验证损失减少 (0.090474 --> 0.089155). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08389 valid_loss: 0.09237 test_loss: 0.10285 \n",
      "[ 87/500] train_loss: 0.08226 valid_loss: 0.09040 test_loss: 0.10252 \n",
      "[ 88/500] train_loss: 0.08200 valid_loss: 0.09119 test_loss: 0.10418 \n",
      "[ 89/500] train_loss: 0.08258 valid_loss: 0.09082 test_loss: 0.10144 \n",
      "[ 90/500] train_loss: 0.08141 valid_loss: 0.09206 test_loss: 0.10267 \n",
      "[ 91/500] train_loss: 0.08136 valid_loss: 0.09268 test_loss: 0.10288 \n",
      "[ 92/500] train_loss: 0.08574 valid_loss: 0.08915 test_loss: 0.10195 \n",
      "验证损失减少 (0.089155 --> 0.089151). 正在保存模型...\n",
      "[ 93/500] train_loss: 0.08340 valid_loss: 0.09077 test_loss: 0.10229 \n",
      "[ 94/500] train_loss: 0.08263 valid_loss: 0.09104 test_loss: 0.10270 \n",
      "[ 95/500] train_loss: 0.08244 valid_loss: 0.09023 test_loss: 0.10056 \n",
      "[ 96/500] train_loss: 0.08274 valid_loss: 0.08861 test_loss: 0.10072 \n",
      "验证损失减少 (0.089151 --> 0.088612). 正在保存模型...\n",
      "[ 97/500] train_loss: 0.08097 valid_loss: 0.08779 test_loss: 0.10002 \n",
      "验证损失减少 (0.088612 --> 0.087793). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.08219 valid_loss: 0.08911 test_loss: 0.10282 \n",
      "[ 99/500] train_loss: 0.08238 valid_loss: 0.08673 test_loss: 0.10035 \n",
      "验证损失减少 (0.087793 --> 0.086727). 正在保存模型...\n",
      "[100/500] train_loss: 0.07921 valid_loss: 0.08816 test_loss: 0.10190 \n",
      "[101/500] train_loss: 0.08040 valid_loss: 0.08780 test_loss: 0.09945 \n",
      "[102/500] train_loss: 0.08039 valid_loss: 0.08785 test_loss: 0.10026 \n",
      "[103/500] train_loss: 0.08005 valid_loss: 0.08825 test_loss: 0.09997 \n",
      "[104/500] train_loss: 0.07872 valid_loss: 0.08794 test_loss: 0.09990 \n",
      "[105/500] train_loss: 0.08105 valid_loss: 0.09007 test_loss: 0.10067 \n",
      "[106/500] train_loss: 0.07866 valid_loss: 0.09030 test_loss: 0.10244 \n",
      "[107/500] train_loss: 0.08078 valid_loss: 0.08916 test_loss: 0.10078 \n",
      "[108/500] train_loss: 0.07852 valid_loss: 0.08617 test_loss: 0.10002 \n",
      "验证损失减少 (0.086727 --> 0.086169). 正在保存模型...\n",
      "[109/500] train_loss: 0.07867 valid_loss: 0.08712 test_loss: 0.09960 \n",
      "[110/500] train_loss: 0.07919 valid_loss: 0.08817 test_loss: 0.10040 \n",
      "[111/500] train_loss: 0.07951 valid_loss: 0.08598 test_loss: 0.09800 \n",
      "验证损失减少 (0.086169 --> 0.085981). 正在保存模型...\n",
      "[112/500] train_loss: 0.07766 valid_loss: 0.08600 test_loss: 0.09834 \n",
      "[113/500] train_loss: 0.07860 valid_loss: 0.08968 test_loss: 0.09829 \n",
      "[114/500] train_loss: 0.07739 valid_loss: 0.09146 test_loss: 0.10207 \n",
      "[115/500] train_loss: 0.07707 valid_loss: 0.08827 test_loss: 0.10046 \n",
      "[116/500] train_loss: 0.07710 valid_loss: 0.08869 test_loss: 0.10169 \n",
      "[117/500] train_loss: 0.07824 valid_loss: 0.08411 test_loss: 0.09848 \n",
      "验证损失减少 (0.085981 --> 0.084110). 正在保存模型...\n",
      "[118/500] train_loss: 0.07780 valid_loss: 0.08348 test_loss: 0.09878 \n",
      "验证损失减少 (0.084110 --> 0.083477). 正在保存模型...\n",
      "[119/500] train_loss: 0.07809 valid_loss: 0.08531 test_loss: 0.09665 \n",
      "[120/500] train_loss: 0.07526 valid_loss: 0.08555 test_loss: 0.09776 \n",
      "[121/500] train_loss: 0.07907 valid_loss: 0.08492 test_loss: 0.09704 \n",
      "[122/500] train_loss: 0.07653 valid_loss: 0.08516 test_loss: 0.09766 \n",
      "[123/500] train_loss: 0.07629 valid_loss: 0.08714 test_loss: 0.09849 \n",
      "[124/500] train_loss: 0.07585 valid_loss: 0.08731 test_loss: 0.09687 \n",
      "[125/500] train_loss: 0.07943 valid_loss: 0.08886 test_loss: 0.09656 \n",
      "[126/500] train_loss: 0.07634 valid_loss: 0.08548 test_loss: 0.09644 \n",
      "[127/500] train_loss: 0.07526 valid_loss: 0.08664 test_loss: 0.09655 \n",
      "[128/500] train_loss: 0.07657 valid_loss: 0.09217 test_loss: 0.09906 \n",
      "[129/500] train_loss: 0.07663 valid_loss: 0.08803 test_loss: 0.09587 \n",
      "[130/500] train_loss: 0.07718 valid_loss: 0.08372 test_loss: 0.09655 \n",
      "[131/500] train_loss: 0.07673 valid_loss: 0.08535 test_loss: 0.09504 \n",
      "[132/500] train_loss: 0.07625 valid_loss: 0.08440 test_loss: 0.09617 \n",
      "[133/500] train_loss: 0.07753 valid_loss: 0.08771 test_loss: 0.09583 \n",
      "[134/500] train_loss: 0.07560 valid_loss: 0.08973 test_loss: 0.09811 \n",
      "[135/500] train_loss: 0.07575 valid_loss: 0.08699 test_loss: 0.09631 \n",
      "[136/500] train_loss: 0.07555 valid_loss: 0.08817 test_loss: 0.09641 \n",
      "[137/500] train_loss: 0.07451 valid_loss: 0.08945 test_loss: 0.09610 \n",
      "[138/500] train_loss: 0.07556 valid_loss: 0.08707 test_loss: 0.09647 \n",
      "[139/500] train_loss: 0.07387 valid_loss: 0.08458 test_loss: 0.09716 \n",
      "[140/500] train_loss: 0.07585 valid_loss: 0.08618 test_loss: 0.09408 \n",
      "[141/500] train_loss: 0.07421 valid_loss: 0.08565 test_loss: 0.09466 \n",
      "[142/500] train_loss: 0.07573 valid_loss: 0.08947 test_loss: 0.09556 \n",
      "[143/500] train_loss: 0.07480 valid_loss: 0.08429 test_loss: 0.09482 \n",
      "[144/500] train_loss: 0.07507 valid_loss: 0.08309 test_loss: 0.09528 \n",
      "验证损失减少 (0.083477 --> 0.083091). 正在保存模型...\n",
      "[145/500] train_loss: 0.07300 valid_loss: 0.08639 test_loss: 0.09514 \n",
      "[146/500] train_loss: 0.07452 valid_loss: 0.08545 test_loss: 0.09495 \n",
      "[147/500] train_loss: 0.07404 valid_loss: 0.08319 test_loss: 0.09689 \n",
      "[148/500] train_loss: 0.07310 valid_loss: 0.08329 test_loss: 0.09418 \n",
      "[149/500] train_loss: 0.07297 valid_loss: 0.08255 test_loss: 0.09387 \n",
      "验证损失减少 (0.083091 --> 0.082555). 正在保存模型...\n",
      "[150/500] train_loss: 0.07409 valid_loss: 0.08422 test_loss: 0.09511 \n",
      "[151/500] train_loss: 0.07268 valid_loss: 0.08406 test_loss: 0.09531 \n",
      "[152/500] train_loss: 0.07568 valid_loss: 0.08272 test_loss: 0.09438 \n",
      "[153/500] train_loss: 0.07311 valid_loss: 0.08252 test_loss: 0.09384 \n",
      "验证损失减少 (0.082555 --> 0.082520). 正在保存模型...\n",
      "[154/500] train_loss: 0.07437 valid_loss: 0.08571 test_loss: 0.09486 \n",
      "[155/500] train_loss: 0.07294 valid_loss: 0.08359 test_loss: 0.09339 \n",
      "[156/500] train_loss: 0.07240 valid_loss: 0.08659 test_loss: 0.09593 \n",
      "[157/500] train_loss: 0.07526 valid_loss: 0.08411 test_loss: 0.09368 \n",
      "[158/500] train_loss: 0.07389 valid_loss: 0.08672 test_loss: 0.09400 \n",
      "[159/500] train_loss: 0.07216 valid_loss: 0.08215 test_loss: 0.09382 \n",
      "验证损失减少 (0.082520 --> 0.082155). 正在保存模型...\n",
      "[160/500] train_loss: 0.07256 valid_loss: 0.08182 test_loss: 0.09313 \n",
      "验证损失减少 (0.082155 --> 0.081817). 正在保存模型...\n",
      "[161/500] train_loss: 0.07293 valid_loss: 0.08097 test_loss: 0.09315 \n",
      "验证损失减少 (0.081817 --> 0.080974). 正在保存模型...\n",
      "[162/500] train_loss: 0.07335 valid_loss: 0.08307 test_loss: 0.09378 \n",
      "[163/500] train_loss: 0.07151 valid_loss: 0.08394 test_loss: 0.09477 \n",
      "[164/500] train_loss: 0.07361 valid_loss: 0.08269 test_loss: 0.09405 \n",
      "[165/500] train_loss: 0.07389 valid_loss: 0.08218 test_loss: 0.09349 \n",
      "[166/500] train_loss: 0.07353 valid_loss: 0.08182 test_loss: 0.09280 \n",
      "[167/500] train_loss: 0.07331 valid_loss: 0.08364 test_loss: 0.09284 \n",
      "[168/500] train_loss: 0.07184 valid_loss: 0.08266 test_loss: 0.09314 \n",
      "[169/500] train_loss: 0.06942 valid_loss: 0.08323 test_loss: 0.09312 \n",
      "[170/500] train_loss: 0.07292 valid_loss: 0.08266 test_loss: 0.09141 \n",
      "[171/500] train_loss: 0.07180 valid_loss: 0.08256 test_loss: 0.09246 \n",
      "[172/500] train_loss: 0.07032 valid_loss: 0.08581 test_loss: 0.09230 \n",
      "[173/500] train_loss: 0.07216 valid_loss: 0.08131 test_loss: 0.09140 \n",
      "[174/500] train_loss: 0.06900 valid_loss: 0.08321 test_loss: 0.09184 \n",
      "[175/500] train_loss: 0.07065 valid_loss: 0.08110 test_loss: 0.09234 \n",
      "[176/500] train_loss: 0.06982 valid_loss: 0.08770 test_loss: 0.09264 \n",
      "[177/500] train_loss: 0.06985 valid_loss: 0.08526 test_loss: 0.09061 \n",
      "[178/500] train_loss: 0.06993 valid_loss: 0.08086 test_loss: 0.09205 \n",
      "验证损失减少 (0.080974 --> 0.080862). 正在保存模型...\n",
      "[179/500] train_loss: 0.06972 valid_loss: 0.08310 test_loss: 0.09306 \n",
      "[180/500] train_loss: 0.06942 valid_loss: 0.08295 test_loss: 0.09181 \n",
      "[181/500] train_loss: 0.07087 valid_loss: 0.08097 test_loss: 0.09164 \n",
      "[182/500] train_loss: 0.06837 valid_loss: 0.08279 test_loss: 0.09363 \n",
      "[183/500] train_loss: 0.06927 valid_loss: 0.08060 test_loss: 0.09272 \n",
      "验证损失减少 (0.080862 --> 0.080604). 正在保存模型...\n",
      "[184/500] train_loss: 0.07140 valid_loss: 0.08265 test_loss: 0.09106 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185/500] train_loss: 0.06767 valid_loss: 0.08391 test_loss: 0.09281 \n",
      "[186/500] train_loss: 0.07229 valid_loss: 0.08446 test_loss: 0.09350 \n",
      "[187/500] train_loss: 0.07151 valid_loss: 0.08125 test_loss: 0.09156 \n",
      "[188/500] train_loss: 0.07187 valid_loss: 0.08116 test_loss: 0.09149 \n",
      "[189/500] train_loss: 0.06808 valid_loss: 0.08055 test_loss: 0.09098 \n",
      "验证损失减少 (0.080604 --> 0.080549). 正在保存模型...\n",
      "[190/500] train_loss: 0.06923 valid_loss: 0.07905 test_loss: 0.09067 \n",
      "验证损失减少 (0.080549 --> 0.079046). 正在保存模型...\n",
      "[191/500] train_loss: 0.07034 valid_loss: 0.08016 test_loss: 0.09192 \n",
      "[192/500] train_loss: 0.06945 valid_loss: 0.08115 test_loss: 0.09209 \n",
      "[193/500] train_loss: 0.06990 valid_loss: 0.08098 test_loss: 0.09098 \n",
      "[194/500] train_loss: 0.06925 valid_loss: 0.07988 test_loss: 0.09254 \n",
      "[195/500] train_loss: 0.06818 valid_loss: 0.08012 test_loss: 0.09139 \n",
      "[196/500] train_loss: 0.06862 valid_loss: 0.08057 test_loss: 0.09234 \n",
      "[197/500] train_loss: 0.06874 valid_loss: 0.08040 test_loss: 0.09178 \n",
      "[198/500] train_loss: 0.06844 valid_loss: 0.08065 test_loss: 0.09198 \n",
      "[199/500] train_loss: 0.06801 valid_loss: 0.07975 test_loss: 0.09172 \n",
      "[200/500] train_loss: 0.06825 valid_loss: 0.08134 test_loss: 0.09162 \n",
      "[201/500] train_loss: 0.06827 valid_loss: 0.08498 test_loss: 0.09322 \n",
      "[202/500] train_loss: 0.06767 valid_loss: 0.08109 test_loss: 0.09053 \n",
      "[203/500] train_loss: 0.06789 valid_loss: 0.07858 test_loss: 0.08959 \n",
      "验证损失减少 (0.079046 --> 0.078578). 正在保存模型...\n",
      "[204/500] train_loss: 0.06690 valid_loss: 0.08064 test_loss: 0.09141 \n",
      "[205/500] train_loss: 0.06909 valid_loss: 0.08146 test_loss: 0.09191 \n",
      "[206/500] train_loss: 0.06770 valid_loss: 0.07994 test_loss: 0.09203 \n",
      "[207/500] train_loss: 0.06800 valid_loss: 0.07989 test_loss: 0.09171 \n",
      "[208/500] train_loss: 0.06926 valid_loss: 0.08003 test_loss: 0.09086 \n",
      "[209/500] train_loss: 0.06673 valid_loss: 0.07932 test_loss: 0.09095 \n",
      "[210/500] train_loss: 0.06755 valid_loss: 0.08046 test_loss: 0.09118 \n",
      "[211/500] train_loss: 0.06927 valid_loss: 0.08251 test_loss: 0.09077 \n",
      "[212/500] train_loss: 0.06753 valid_loss: 0.07942 test_loss: 0.09054 \n",
      "[213/500] train_loss: 0.06591 valid_loss: 0.07937 test_loss: 0.09072 \n",
      "[214/500] train_loss: 0.06656 valid_loss: 0.07876 test_loss: 0.09030 \n",
      "[215/500] train_loss: 0.06744 valid_loss: 0.08052 test_loss: 0.09345 \n",
      "[216/500] train_loss: 0.06518 valid_loss: 0.07846 test_loss: 0.08964 \n",
      "验证损失减少 (0.078578 --> 0.078460). 正在保存模型...\n",
      "[217/500] train_loss: 0.06809 valid_loss: 0.07862 test_loss: 0.08974 \n",
      "[218/500] train_loss: 0.06591 valid_loss: 0.07767 test_loss: 0.09042 \n",
      "验证损失减少 (0.078460 --> 0.077669). 正在保存模型...\n",
      "[219/500] train_loss: 0.06774 valid_loss: 0.07828 test_loss: 0.09094 \n",
      "[220/500] train_loss: 0.06504 valid_loss: 0.07921 test_loss: 0.09123 \n",
      "[221/500] train_loss: 0.06639 valid_loss: 0.07780 test_loss: 0.08953 \n",
      "[222/500] train_loss: 0.06721 valid_loss: 0.07861 test_loss: 0.09036 \n",
      "[223/500] train_loss: 0.06554 valid_loss: 0.07830 test_loss: 0.09133 \n",
      "[224/500] train_loss: 0.06672 valid_loss: 0.07906 test_loss: 0.09083 \n",
      "[225/500] train_loss: 0.06600 valid_loss: 0.07895 test_loss: 0.09035 \n",
      "[226/500] train_loss: 0.06757 valid_loss: 0.07789 test_loss: 0.09023 \n",
      "[227/500] train_loss: 0.06736 valid_loss: 0.07776 test_loss: 0.09147 \n",
      "[228/500] train_loss: 0.06439 valid_loss: 0.08029 test_loss: 0.09112 \n",
      "[229/500] train_loss: 0.06603 valid_loss: 0.08078 test_loss: 0.09000 \n",
      "[230/500] train_loss: 0.06596 valid_loss: 0.07808 test_loss: 0.09096 \n",
      "[231/500] train_loss: 0.06580 valid_loss: 0.07980 test_loss: 0.09121 \n",
      "[232/500] train_loss: 0.06482 valid_loss: 0.07970 test_loss: 0.09131 \n",
      "[233/500] train_loss: 0.06510 valid_loss: 0.07924 test_loss: 0.09006 \n",
      "[234/500] train_loss: 0.06643 valid_loss: 0.08171 test_loss: 0.09042 \n",
      "[235/500] train_loss: 0.06391 valid_loss: 0.07945 test_loss: 0.09294 \n",
      "[236/500] train_loss: 0.06523 valid_loss: 0.07835 test_loss: 0.09048 \n",
      "[237/500] train_loss: 0.06534 valid_loss: 0.07958 test_loss: 0.09148 \n",
      "[238/500] train_loss: 0.06494 valid_loss: 0.07967 test_loss: 0.09065 \n",
      "[239/500] train_loss: 0.06489 valid_loss: 0.07842 test_loss: 0.09156 \n",
      "[240/500] train_loss: 0.06624 valid_loss: 0.07650 test_loss: 0.09105 \n",
      "验证损失减少 (0.077669 --> 0.076503). 正在保存模型...\n",
      "[241/500] train_loss: 0.06329 valid_loss: 0.07660 test_loss: 0.09061 \n",
      "[242/500] train_loss: 0.06560 valid_loss: 0.07978 test_loss: 0.09203 \n",
      "[243/500] train_loss: 0.06521 valid_loss: 0.07742 test_loss: 0.09046 \n",
      "[244/500] train_loss: 0.06458 valid_loss: 0.07854 test_loss: 0.09144 \n",
      "[245/500] train_loss: 0.06471 valid_loss: 0.07716 test_loss: 0.09045 \n",
      "[246/500] train_loss: 0.06526 valid_loss: 0.07731 test_loss: 0.09086 \n",
      "[247/500] train_loss: 0.06379 valid_loss: 0.07604 test_loss: 0.09080 \n",
      "验证损失减少 (0.076503 --> 0.076041). 正在保存模型...\n",
      "[248/500] train_loss: 0.06394 valid_loss: 0.07794 test_loss: 0.09144 \n",
      "[249/500] train_loss: 0.06552 valid_loss: 0.07678 test_loss: 0.09029 \n",
      "[250/500] train_loss: 0.06423 valid_loss: 0.07963 test_loss: 0.09324 \n",
      "[251/500] train_loss: 0.06393 valid_loss: 0.07996 test_loss: 0.09052 \n",
      "[252/500] train_loss: 0.06310 valid_loss: 0.07725 test_loss: 0.09109 \n",
      "[253/500] train_loss: 0.06526 valid_loss: 0.09124 test_loss: 0.09161 \n",
      "[254/500] train_loss: 0.06330 valid_loss: 0.07727 test_loss: 0.08847 \n",
      "[255/500] train_loss: 0.06553 valid_loss: 0.07753 test_loss: 0.09019 \n",
      "[256/500] train_loss: 0.06351 valid_loss: 0.07666 test_loss: 0.08868 \n",
      "[257/500] train_loss: 0.06309 valid_loss: 0.07810 test_loss: 0.09014 \n",
      "[258/500] train_loss: 0.06192 valid_loss: 0.07941 test_loss: 0.09013 \n",
      "[259/500] train_loss: 0.06435 valid_loss: 0.09177 test_loss: 0.09050 \n",
      "[260/500] train_loss: 0.06345 valid_loss: 0.07918 test_loss: 0.09186 \n",
      "[261/500] train_loss: 0.06417 valid_loss: 0.07899 test_loss: 0.09107 \n",
      "[262/500] train_loss: 0.06381 valid_loss: 0.07665 test_loss: 0.09061 \n",
      "[263/500] train_loss: 0.06252 valid_loss: 0.07658 test_loss: 0.08905 \n",
      "[264/500] train_loss: 0.06177 valid_loss: 0.07753 test_loss: 0.09048 \n",
      "[265/500] train_loss: 0.06241 valid_loss: 0.07716 test_loss: 0.09014 \n",
      "[266/500] train_loss: 0.06417 valid_loss: 0.07795 test_loss: 0.08999 \n",
      "[267/500] train_loss: 0.06310 valid_loss: 0.07780 test_loss: 0.09030 \n",
      "[268/500] train_loss: 0.06277 valid_loss: 0.09079 test_loss: 0.08963 \n",
      "[269/500] train_loss: 0.06239 valid_loss: 0.07698 test_loss: 0.09114 \n",
      "[270/500] train_loss: 0.06330 valid_loss: 0.07865 test_loss: 0.08915 \n",
      "[271/500] train_loss: 0.05978 valid_loss: 0.08171 test_loss: 0.09208 \n",
      "[272/500] train_loss: 0.06252 valid_loss: 0.08040 test_loss: 0.08964 \n",
      "[273/500] train_loss: 0.06292 valid_loss: 0.08137 test_loss: 0.09000 \n",
      "[274/500] train_loss: 0.06088 valid_loss: 0.07947 test_loss: 0.09030 \n",
      "[275/500] train_loss: 0.06176 valid_loss: 0.07818 test_loss: 0.08926 \n",
      "[276/500] train_loss: 0.06471 valid_loss: 0.07788 test_loss: 0.08973 \n",
      "[277/500] train_loss: 0.06109 valid_loss: 0.07710 test_loss: 0.09030 \n",
      "[278/500] train_loss: 0.06090 valid_loss: 0.07631 test_loss: 0.09055 \n",
      "[279/500] train_loss: 0.06208 valid_loss: 0.07572 test_loss: 0.09047 \n",
      "验证损失减少 (0.076041 --> 0.075718). 正在保存模型...\n",
      "[280/500] train_loss: 0.06198 valid_loss: 0.07598 test_loss: 0.08995 \n",
      "[281/500] train_loss: 0.06334 valid_loss: 0.07751 test_loss: 0.09114 \n",
      "[282/500] train_loss: 0.06398 valid_loss: 0.07645 test_loss: 0.08902 \n",
      "[283/500] train_loss: 0.06397 valid_loss: 0.08011 test_loss: 0.08962 \n",
      "[284/500] train_loss: 0.06250 valid_loss: 0.07688 test_loss: 0.09049 \n",
      "[285/500] train_loss: 0.06347 valid_loss: 0.07563 test_loss: 0.09046 \n",
      "验证损失减少 (0.075718 --> 0.075634). 正在保存模型...\n",
      "[286/500] train_loss: 0.06253 valid_loss: 0.07657 test_loss: 0.08961 \n",
      "[287/500] train_loss: 0.06122 valid_loss: 0.07641 test_loss: 0.08905 \n",
      "[288/500] train_loss: 0.06121 valid_loss: 0.07596 test_loss: 0.08956 \n",
      "[289/500] train_loss: 0.05996 valid_loss: 0.07620 test_loss: 0.08832 \n",
      "[290/500] train_loss: 0.06182 valid_loss: 0.07691 test_loss: 0.08831 \n",
      "[291/500] train_loss: 0.06209 valid_loss: 0.07801 test_loss: 0.09039 \n",
      "[292/500] train_loss: 0.06090 valid_loss: 0.07568 test_loss: 0.09018 \n",
      "[293/500] train_loss: 0.06013 valid_loss: 0.08244 test_loss: 0.09059 \n",
      "[294/500] train_loss: 0.06088 valid_loss: 0.07572 test_loss: 0.08891 \n",
      "[295/500] train_loss: 0.06107 valid_loss: 0.08412 test_loss: 0.08860 \n",
      "[296/500] train_loss: 0.06084 valid_loss: 0.07579 test_loss: 0.08983 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[297/500] train_loss: 0.06198 valid_loss: 0.07924 test_loss: 0.08964 \n",
      "[298/500] train_loss: 0.06078 valid_loss: 0.07699 test_loss: 0.08957 \n",
      "[299/500] train_loss: 0.06121 valid_loss: 0.07896 test_loss: 0.08899 \n",
      "[300/500] train_loss: 0.06064 valid_loss: 0.07566 test_loss: 0.08978 \n",
      "[301/500] train_loss: 0.06030 valid_loss: 0.07721 test_loss: 0.08944 \n",
      "[302/500] train_loss: 0.06008 valid_loss: 0.07951 test_loss: 0.08919 \n",
      "[303/500] train_loss: 0.06114 valid_loss: 0.07984 test_loss: 0.08914 \n",
      "[304/500] train_loss: 0.05819 valid_loss: 0.08001 test_loss: 0.08937 \n",
      "[305/500] train_loss: 0.06099 valid_loss: 0.07855 test_loss: 0.08989 \n",
      "[306/500] train_loss: 0.05941 valid_loss: 0.07705 test_loss: 0.09059 \n",
      "[307/500] train_loss: 0.06061 valid_loss: 0.08121 test_loss: 0.09290 \n",
      "[308/500] train_loss: 0.06235 valid_loss: 0.07651 test_loss: 0.08923 \n",
      "[309/500] train_loss: 0.06041 valid_loss: 0.07600 test_loss: 0.08818 \n",
      "[310/500] train_loss: 0.06181 valid_loss: 0.07757 test_loss: 0.08881 \n",
      "[311/500] train_loss: 0.05994 valid_loss: 0.07730 test_loss: 0.09002 \n",
      "[312/500] train_loss: 0.05972 valid_loss: 0.07722 test_loss: 0.08920 \n",
      "[313/500] train_loss: 0.06017 valid_loss: 0.07580 test_loss: 0.08830 \n",
      "[314/500] train_loss: 0.06143 valid_loss: 0.07567 test_loss: 0.09075 \n",
      "[315/500] train_loss: 0.06072 valid_loss: 0.07602 test_loss: 0.08871 \n",
      "[316/500] train_loss: 0.05927 valid_loss: 0.07837 test_loss: 0.08888 \n",
      "[317/500] train_loss: 0.05947 valid_loss: 0.07729 test_loss: 0.08898 \n",
      "[318/500] train_loss: 0.06047 valid_loss: 0.07888 test_loss: 0.08873 \n",
      "[319/500] train_loss: 0.05809 valid_loss: 0.07721 test_loss: 0.08947 \n",
      "[320/500] train_loss: 0.05984 valid_loss: 0.07649 test_loss: 0.08983 \n",
      "[321/500] train_loss: 0.05943 valid_loss: 0.07542 test_loss: 0.08840 \n",
      "验证损失减少 (0.075634 --> 0.075417). 正在保存模型...\n",
      "[322/500] train_loss: 0.05976 valid_loss: 0.07772 test_loss: 0.08941 \n",
      "[323/500] train_loss: 0.05877 valid_loss: 0.07676 test_loss: 0.08846 \n",
      "[324/500] train_loss: 0.05979 valid_loss: 0.07851 test_loss: 0.08952 \n",
      "[325/500] train_loss: 0.05866 valid_loss: 0.07744 test_loss: 0.08972 \n",
      "[326/500] train_loss: 0.05694 valid_loss: 0.07602 test_loss: 0.08874 \n",
      "[327/500] train_loss: 0.05881 valid_loss: 0.07479 test_loss: 0.08850 \n",
      "验证损失减少 (0.075417 --> 0.074786). 正在保存模型...\n",
      "[328/500] train_loss: 0.05864 valid_loss: 0.07679 test_loss: 0.08865 \n",
      "[329/500] train_loss: 0.06049 valid_loss: 0.07641 test_loss: 0.08880 \n",
      "[330/500] train_loss: 0.06040 valid_loss: 0.07753 test_loss: 0.09013 \n",
      "[331/500] train_loss: 0.05935 valid_loss: 0.07583 test_loss: 0.08882 \n",
      "[332/500] train_loss: 0.05985 valid_loss: 0.07761 test_loss: 0.09047 \n",
      "[333/500] train_loss: 0.05738 valid_loss: 0.07679 test_loss: 0.08949 \n",
      "[334/500] train_loss: 0.05809 valid_loss: 0.07721 test_loss: 0.08971 \n",
      "[335/500] train_loss: 0.05975 valid_loss: 0.07810 test_loss: 0.08992 \n",
      "[336/500] train_loss: 0.06048 valid_loss: 0.07580 test_loss: 0.08854 \n",
      "[337/500] train_loss: 0.05859 valid_loss: 0.07564 test_loss: 0.08933 \n",
      "[338/500] train_loss: 0.05926 valid_loss: 0.07551 test_loss: 0.08922 \n",
      "[339/500] train_loss: 0.05917 valid_loss: 0.07569 test_loss: 0.08996 \n",
      "[340/500] train_loss: 0.06003 valid_loss: 0.07896 test_loss: 0.08933 \n",
      "[341/500] train_loss: 0.05952 valid_loss: 0.07508 test_loss: 0.08921 \n",
      "[342/500] train_loss: 0.05925 valid_loss: 0.07521 test_loss: 0.09068 \n",
      "[343/500] train_loss: 0.05808 valid_loss: 0.07541 test_loss: 0.08921 \n",
      "[344/500] train_loss: 0.05971 valid_loss: 0.07670 test_loss: 0.09027 \n",
      "[345/500] train_loss: 0.05879 valid_loss: 0.07701 test_loss: 0.09036 \n",
      "[346/500] train_loss: 0.05931 valid_loss: 0.07499 test_loss: 0.08830 \n",
      "[347/500] train_loss: 0.05884 valid_loss: 0.07655 test_loss: 0.08980 \n",
      "[348/500] train_loss: 0.05748 valid_loss: 0.07529 test_loss: 0.08950 \n",
      "[349/500] train_loss: 0.05905 valid_loss: 0.07706 test_loss: 0.09049 \n",
      "[350/500] train_loss: 0.05878 valid_loss: 0.07565 test_loss: 0.08931 \n",
      "[351/500] train_loss: 0.05751 valid_loss: 0.07537 test_loss: 0.09021 \n",
      "[352/500] train_loss: 0.05854 valid_loss: 0.07455 test_loss: 0.08765 \n",
      "验证损失减少 (0.074786 --> 0.074552). 正在保存模型...\n",
      "[353/500] train_loss: 0.05857 valid_loss: 0.07548 test_loss: 0.09004 \n",
      "[354/500] train_loss: 0.05645 valid_loss: 0.07457 test_loss: 0.08910 \n",
      "[355/500] train_loss: 0.05821 valid_loss: 0.07574 test_loss: 0.09115 \n",
      "[356/500] train_loss: 0.05932 valid_loss: 0.07496 test_loss: 0.08973 \n",
      "[357/500] train_loss: 0.05915 valid_loss: 0.07340 test_loss: 0.08856 \n",
      "验证损失减少 (0.074552 --> 0.073400). 正在保存模型...\n",
      "[358/500] train_loss: 0.05686 valid_loss: 0.07490 test_loss: 0.08881 \n",
      "[359/500] train_loss: 0.05719 valid_loss: 0.07511 test_loss: 0.09070 \n",
      "[360/500] train_loss: 0.05814 valid_loss: 0.08045 test_loss: 0.08970 \n",
      "[361/500] train_loss: 0.05749 valid_loss: 0.07632 test_loss: 0.08924 \n",
      "[362/500] train_loss: 0.05824 valid_loss: 0.07555 test_loss: 0.08803 \n",
      "[363/500] train_loss: 0.05854 valid_loss: 0.07475 test_loss: 0.08870 \n",
      "[364/500] train_loss: 0.05819 valid_loss: 0.07577 test_loss: 0.08965 \n",
      "[365/500] train_loss: 0.05750 valid_loss: 0.07663 test_loss: 0.08992 \n",
      "[366/500] train_loss: 0.05894 valid_loss: 0.07654 test_loss: 0.09014 \n",
      "[367/500] train_loss: 0.05831 valid_loss: 0.07481 test_loss: 0.08880 \n",
      "[368/500] train_loss: 0.05777 valid_loss: 0.07474 test_loss: 0.08913 \n",
      "[369/500] train_loss: 0.05847 valid_loss: 0.07619 test_loss: 0.08979 \n",
      "[370/500] train_loss: 0.05718 valid_loss: 0.07643 test_loss: 0.08960 \n",
      "[371/500] train_loss: 0.05712 valid_loss: 0.07521 test_loss: 0.09026 \n",
      "[372/500] train_loss: 0.05718 valid_loss: 0.07693 test_loss: 0.08918 \n",
      "[373/500] train_loss: 0.05758 valid_loss: 0.07820 test_loss: 0.08858 \n",
      "[374/500] train_loss: 0.05704 valid_loss: 0.07916 test_loss: 0.08909 \n",
      "[375/500] train_loss: 0.05642 valid_loss: 0.07827 test_loss: 0.08960 \n",
      "[376/500] train_loss: 0.05636 valid_loss: 0.07636 test_loss: 0.08996 \n",
      "[377/500] train_loss: 0.05808 valid_loss: 0.07508 test_loss: 0.08913 \n",
      "[378/500] train_loss: 0.05769 valid_loss: 0.07603 test_loss: 0.08991 \n",
      "[379/500] train_loss: 0.05672 valid_loss: 0.07715 test_loss: 0.08989 \n",
      "[380/500] train_loss: 0.05726 valid_loss: 0.07453 test_loss: 0.08907 \n",
      "[381/500] train_loss: 0.05549 valid_loss: 0.07565 test_loss: 0.08982 \n",
      "[382/500] train_loss: 0.05659 valid_loss: 0.07620 test_loss: 0.08923 \n",
      "[383/500] train_loss: 0.05601 valid_loss: 0.07561 test_loss: 0.08907 \n",
      "[384/500] train_loss: 0.05654 valid_loss: 0.07521 test_loss: 0.09059 \n",
      "[385/500] train_loss: 0.05612 valid_loss: 0.07482 test_loss: 0.08844 \n",
      "[386/500] train_loss: 0.05815 valid_loss: 0.07736 test_loss: 0.08829 \n",
      "[387/500] train_loss: 0.05658 valid_loss: 0.07609 test_loss: 0.08879 \n",
      "[388/500] train_loss: 0.05647 valid_loss: 0.08032 test_loss: 0.09017 \n",
      "[389/500] train_loss: 0.05714 valid_loss: 0.07651 test_loss: 0.08904 \n",
      "[390/500] train_loss: 0.05592 valid_loss: 0.07722 test_loss: 0.08902 \n",
      "[391/500] train_loss: 0.05701 valid_loss: 0.08645 test_loss: 0.08876 \n",
      "[392/500] train_loss: 0.05601 valid_loss: 0.08772 test_loss: 0.08952 \n",
      "[393/500] train_loss: 0.05655 valid_loss: 0.07940 test_loss: 0.09015 \n",
      "[394/500] train_loss: 0.05707 valid_loss: 0.08792 test_loss: 0.08981 \n",
      "[395/500] train_loss: 0.05762 valid_loss: 0.07571 test_loss: 0.08857 \n",
      "[396/500] train_loss: 0.05766 valid_loss: 0.08677 test_loss: 0.08779 \n",
      "[397/500] train_loss: 0.05767 valid_loss: 0.07735 test_loss: 0.08769 \n",
      "[398/500] train_loss: 0.05721 valid_loss: 0.08274 test_loss: 0.08921 \n",
      "[399/500] train_loss: 0.05658 valid_loss: 0.08160 test_loss: 0.08938 \n",
      "[400/500] train_loss: 0.05559 valid_loss: 0.08865 test_loss: 0.08866 \n",
      "[401/500] train_loss: 0.05717 valid_loss: 0.07888 test_loss: 0.08727 \n",
      "[402/500] train_loss: 0.05583 valid_loss: 0.07920 test_loss: 0.08921 \n",
      "[403/500] train_loss: 0.05646 valid_loss: 0.07568 test_loss: 0.08765 \n",
      "[404/500] train_loss: 0.05684 valid_loss: 0.07628 test_loss: 0.08882 \n",
      "[405/500] train_loss: 0.05605 valid_loss: 0.07571 test_loss: 0.08760 \n",
      "[406/500] train_loss: 0.05465 valid_loss: 0.07934 test_loss: 0.08914 \n",
      "[407/500] train_loss: 0.05758 valid_loss: 0.07948 test_loss: 0.08866 \n",
      "[408/500] train_loss: 0.05680 valid_loss: 0.07763 test_loss: 0.08821 \n",
      "[409/500] train_loss: 0.05506 valid_loss: 0.08007 test_loss: 0.08858 \n",
      "[410/500] train_loss: 0.05518 valid_loss: 0.08426 test_loss: 0.08812 \n",
      "[411/500] train_loss: 0.05616 valid_loss: 0.07978 test_loss: 0.08879 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[412/500] train_loss: 0.05525 valid_loss: 0.08133 test_loss: 0.08971 \n",
      "[413/500] train_loss: 0.05443 valid_loss: 0.08531 test_loss: 0.08742 \n",
      "[414/500] train_loss: 0.05603 valid_loss: 0.07567 test_loss: 0.08878 \n",
      "[415/500] train_loss: 0.05468 valid_loss: 0.07612 test_loss: 0.09005 \n",
      "[416/500] train_loss: 0.05481 valid_loss: 0.07931 test_loss: 0.08995 \n",
      "[417/500] train_loss: 0.05731 valid_loss: 0.08357 test_loss: 0.08892 \n",
      "[418/500] train_loss: 0.05533 valid_loss: 0.07497 test_loss: 0.08869 \n",
      "[419/500] train_loss: 0.05589 valid_loss: 0.07566 test_loss: 0.08908 \n",
      "[420/500] train_loss: 0.05486 valid_loss: 0.09013 test_loss: 0.08865 \n",
      "[421/500] train_loss: 0.05565 valid_loss: 0.08098 test_loss: 0.08849 \n",
      "[422/500] train_loss: 0.05577 valid_loss: 0.07648 test_loss: 0.08939 \n",
      "[423/500] train_loss: 0.05495 valid_loss: 0.07749 test_loss: 0.08763 \n",
      "[424/500] train_loss: 0.05415 valid_loss: 0.07896 test_loss: 0.08925 \n",
      "[425/500] train_loss: 0.05466 valid_loss: 0.07678 test_loss: 0.08858 \n",
      "[426/500] train_loss: 0.05368 valid_loss: 0.07614 test_loss: 0.08977 \n",
      "[427/500] train_loss: 0.05589 valid_loss: 0.07380 test_loss: 0.08869 \n",
      "[428/500] train_loss: 0.05576 valid_loss: 0.07891 test_loss: 0.09121 \n",
      "[429/500] train_loss: 0.05529 valid_loss: 0.08188 test_loss: 0.08916 \n",
      "[430/500] train_loss: 0.05427 valid_loss: 0.07684 test_loss: 0.08969 \n",
      "[431/500] train_loss: 0.05582 valid_loss: 0.07542 test_loss: 0.08806 \n",
      "[432/500] train_loss: 0.05362 valid_loss: 0.07542 test_loss: 0.08841 \n",
      "[433/500] train_loss: 0.05568 valid_loss: 0.07541 test_loss: 0.08812 \n",
      "[434/500] train_loss: 0.05431 valid_loss: 0.07746 test_loss: 0.08880 \n",
      "[435/500] train_loss: 0.05540 valid_loss: 0.08305 test_loss: 0.08933 \n",
      "[436/500] train_loss: 0.05522 valid_loss: 0.07950 test_loss: 0.08921 \n",
      "[437/500] train_loss: 0.05498 valid_loss: 0.07505 test_loss: 0.08749 \n",
      "[438/500] train_loss: 0.05391 valid_loss: 0.07562 test_loss: 0.09020 \n",
      "[439/500] train_loss: 0.05549 valid_loss: 0.07617 test_loss: 0.08841 \n",
      "[440/500] train_loss: 0.05475 valid_loss: 0.07964 test_loss: 0.08802 \n",
      "[441/500] train_loss: 0.05533 valid_loss: 0.08099 test_loss: 0.08925 \n",
      "[442/500] train_loss: 0.05352 valid_loss: 0.07814 test_loss: 0.08980 \n",
      "[443/500] train_loss: 0.05566 valid_loss: 0.07645 test_loss: 0.08893 \n",
      "[444/500] train_loss: 0.05496 valid_loss: 0.07603 test_loss: 0.08992 \n",
      "[445/500] train_loss: 0.05280 valid_loss: 0.08165 test_loss: 0.08834 \n",
      "[446/500] train_loss: 0.05463 valid_loss: 0.07369 test_loss: 0.08884 \n",
      "[447/500] train_loss: 0.05618 valid_loss: 0.08168 test_loss: 0.09005 \n",
      "[448/500] train_loss: 0.05614 valid_loss: 0.08089 test_loss: 0.08990 \n",
      "[449/500] train_loss: 0.05221 valid_loss: 0.07939 test_loss: 0.09141 \n",
      "[450/500] train_loss: 0.05461 valid_loss: 0.07726 test_loss: 0.08814 \n",
      "[451/500] train_loss: 0.05323 valid_loss: 0.07563 test_loss: 0.08790 \n",
      "[452/500] train_loss: 0.05424 valid_loss: 0.07837 test_loss: 0.09005 \n",
      "[453/500] train_loss: 0.05487 valid_loss: 0.07764 test_loss: 0.08889 \n",
      "[454/500] train_loss: 0.05316 valid_loss: 0.08865 test_loss: 0.08813 \n",
      "[455/500] train_loss: 0.05474 valid_loss: 0.09399 test_loss: 0.08832 \n",
      "[456/500] train_loss: 0.05527 valid_loss: 0.08278 test_loss: 0.08903 \n",
      "[457/500] train_loss: 0.05302 valid_loss: 0.08106 test_loss: 0.08959 \n",
      "[458/500] train_loss: 0.05439 valid_loss: 0.08332 test_loss: 0.08873 \n",
      "[459/500] train_loss: 0.05375 valid_loss: 0.07651 test_loss: 0.09050 \n",
      "[460/500] train_loss: 0.05436 valid_loss: 0.08196 test_loss: 0.09137 \n",
      "[461/500] train_loss: 0.05406 valid_loss: 0.07676 test_loss: 0.08896 \n",
      "[462/500] train_loss: 0.05417 valid_loss: 0.08640 test_loss: 0.09007 \n",
      "[463/500] train_loss: 0.05514 valid_loss: 0.08828 test_loss: 0.08792 \n",
      "[464/500] train_loss: 0.05459 valid_loss: 0.09987 test_loss: 0.08908 \n",
      "[465/500] train_loss: 0.05217 valid_loss: 0.08113 test_loss: 0.08944 \n",
      "[466/500] train_loss: 0.05366 valid_loss: 0.07837 test_loss: 0.08902 \n",
      "[467/500] train_loss: 0.05410 valid_loss: 0.08236 test_loss: 0.08833 \n",
      "[468/500] train_loss: 0.05377 valid_loss: 0.08932 test_loss: 0.08934 \n",
      "[469/500] train_loss: 0.05318 valid_loss: 0.08671 test_loss: 0.08905 \n",
      "[470/500] train_loss: 0.05258 valid_loss: 0.08399 test_loss: 0.08942 \n",
      "[471/500] train_loss: 0.05169 valid_loss: 0.08968 test_loss: 0.09118 \n",
      "[472/500] train_loss: 0.05340 valid_loss: 0.08120 test_loss: 0.08858 \n",
      "[473/500] train_loss: 0.05312 valid_loss: 0.08963 test_loss: 0.08923 \n",
      "[474/500] train_loss: 0.05272 valid_loss: 0.08924 test_loss: 0.08925 \n",
      "[475/500] train_loss: 0.05272 valid_loss: 0.08696 test_loss: 0.08888 \n",
      "[476/500] train_loss: 0.05316 valid_loss: 0.07448 test_loss: 0.08985 \n",
      "[477/500] train_loss: 0.05774 valid_loss: 0.07814 test_loss: 0.09235 \n",
      "[478/500] train_loss: 0.05470 valid_loss: 0.08333 test_loss: 0.09083 \n",
      "[479/500] train_loss: 0.05448 valid_loss: 0.08548 test_loss: 0.08964 \n",
      "[480/500] train_loss: 0.05559 valid_loss: 0.08123 test_loss: 0.08943 \n",
      "[481/500] train_loss: 0.05375 valid_loss: 0.08476 test_loss: 0.08963 \n",
      "[482/500] train_loss: 0.05314 valid_loss: 0.08062 test_loss: 0.08896 \n",
      "[483/500] train_loss: 0.05288 valid_loss: 0.09732 test_loss: 0.09051 \n",
      "[484/500] train_loss: 0.05430 valid_loss: 0.08409 test_loss: 0.09093 \n",
      "[485/500] train_loss: 0.05172 valid_loss: 0.09686 test_loss: 0.09003 \n",
      "[486/500] train_loss: 0.05480 valid_loss: 0.08408 test_loss: 0.08907 \n",
      "[487/500] train_loss: 0.05375 valid_loss: 0.08267 test_loss: 0.08935 \n",
      "[488/500] train_loss: 0.05309 valid_loss: 0.08236 test_loss: 0.09021 \n",
      "[489/500] train_loss: 0.05307 valid_loss: 0.08601 test_loss: 0.08963 \n",
      "[490/500] train_loss: 0.05118 valid_loss: 0.08246 test_loss: 0.08924 \n",
      "[491/500] train_loss: 0.05294 valid_loss: 0.10093 test_loss: 0.08982 \n",
      "[492/500] train_loss: 0.05273 valid_loss: 0.08865 test_loss: 0.08983 \n",
      "[493/500] train_loss: 0.05278 valid_loss: 0.09409 test_loss: 0.08947 \n",
      "[494/500] train_loss: 0.05376 valid_loss: 0.08559 test_loss: 0.08996 \n",
      "[495/500] train_loss: 0.05236 valid_loss: 0.08460 test_loss: 0.09098 \n",
      "[496/500] train_loss: 0.05314 valid_loss: 0.08190 test_loss: 0.09174 \n",
      "[497/500] train_loss: 0.05060 valid_loss: 0.08287 test_loss: 0.09006 \n",
      "[498/500] train_loss: 0.05227 valid_loss: 0.08960 test_loss: 0.08939 \n",
      "[499/500] train_loss: 0.05339 valid_loss: 0.08553 test_loss: 0.08993 \n",
      "[500/500] train_loss: 0.05256 valid_loss: 0.08945 test_loss: 0.08907 \n",
      "TRAINING MODEL 16\n",
      "[  1/500] train_loss: 0.44934 valid_loss: 0.30905 test_loss: 0.31557 \n",
      "验证损失减少 (inf --> 0.309048). 正在保存模型...\n",
      "[  2/500] train_loss: 0.23970 valid_loss: 0.21498 test_loss: 0.22577 \n",
      "验证损失减少 (0.309048 --> 0.214979). 正在保存模型...\n",
      "[  3/500] train_loss: 0.18577 valid_loss: 0.17654 test_loss: 0.18776 \n",
      "验证损失减少 (0.214979 --> 0.176544). 正在保存模型...\n",
      "[  4/500] train_loss: 0.15958 valid_loss: 0.15920 test_loss: 0.17235 \n",
      "验证损失减少 (0.176544 --> 0.159199). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15114 valid_loss: 0.15063 test_loss: 0.16258 \n",
      "验证损失减少 (0.159199 --> 0.150633). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14312 valid_loss: 0.14718 test_loss: 0.15985 \n",
      "验证损失减少 (0.150633 --> 0.147180). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13656 valid_loss: 0.13991 test_loss: 0.15171 \n",
      "验证损失减少 (0.147180 --> 0.139911). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13429 valid_loss: 0.13578 test_loss: 0.14942 \n",
      "验证损失减少 (0.139911 --> 0.135781). 正在保存模型...\n",
      "[  9/500] train_loss: 0.12887 valid_loss: 0.13112 test_loss: 0.14705 \n",
      "验证损失减少 (0.135781 --> 0.131124). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12954 valid_loss: 0.13024 test_loss: 0.14597 \n",
      "验证损失减少 (0.131124 --> 0.130237). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12365 valid_loss: 0.12997 test_loss: 0.14424 \n",
      "验证损失减少 (0.130237 --> 0.129967). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12384 valid_loss: 0.12292 test_loss: 0.13801 \n",
      "验证损失减少 (0.129967 --> 0.122921). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12090 valid_loss: 0.12210 test_loss: 0.13928 \n",
      "验证损失减少 (0.122921 --> 0.122096). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.11681 valid_loss: 0.11883 test_loss: 0.13541 \n",
      "验证损失减少 (0.122096 --> 0.118831). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11555 valid_loss: 0.11997 test_loss: 0.13491 \n",
      "[ 16/500] train_loss: 0.11479 valid_loss: 0.11802 test_loss: 0.13487 \n",
      "验证损失减少 (0.118831 --> 0.118024). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11339 valid_loss: 0.11569 test_loss: 0.13096 \n",
      "验证损失减少 (0.118024 --> 0.115691). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11084 valid_loss: 0.11603 test_loss: 0.12983 \n",
      "[ 19/500] train_loss: 0.10979 valid_loss: 0.11619 test_loss: 0.13165 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20/500] train_loss: 0.11120 valid_loss: 0.11267 test_loss: 0.12731 \n",
      "验证损失减少 (0.115691 --> 0.112666). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.11068 valid_loss: 0.11309 test_loss: 0.12635 \n",
      "[ 22/500] train_loss: 0.10712 valid_loss: 0.11099 test_loss: 0.12680 \n",
      "验证损失减少 (0.112666 --> 0.110988). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10747 valid_loss: 0.11195 test_loss: 0.12404 \n",
      "[ 24/500] train_loss: 0.10529 valid_loss: 0.11208 test_loss: 0.12722 \n",
      "[ 25/500] train_loss: 0.10534 valid_loss: 0.10928 test_loss: 0.12388 \n",
      "验证损失减少 (0.110988 --> 0.109284). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10382 valid_loss: 0.10883 test_loss: 0.12318 \n",
      "验证损失减少 (0.109284 --> 0.108831). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.10053 valid_loss: 0.10897 test_loss: 0.12345 \n",
      "[ 28/500] train_loss: 0.10153 valid_loss: 0.10580 test_loss: 0.12169 \n",
      "验证损失减少 (0.108831 --> 0.105795). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10020 valid_loss: 0.10537 test_loss: 0.12061 \n",
      "验证损失减少 (0.105795 --> 0.105375). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10110 valid_loss: 0.10321 test_loss: 0.11866 \n",
      "验证损失减少 (0.105375 --> 0.103210). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.09911 valid_loss: 0.10579 test_loss: 0.12150 \n",
      "[ 32/500] train_loss: 0.09758 valid_loss: 0.10442 test_loss: 0.11822 \n",
      "[ 33/500] train_loss: 0.09917 valid_loss: 0.10428 test_loss: 0.11851 \n",
      "[ 34/500] train_loss: 0.10017 valid_loss: 0.10428 test_loss: 0.11880 \n",
      "[ 35/500] train_loss: 0.09902 valid_loss: 0.10165 test_loss: 0.11775 \n",
      "验证损失减少 (0.103210 --> 0.101647). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09670 valid_loss: 0.10542 test_loss: 0.12110 \n",
      "[ 37/500] train_loss: 0.09727 valid_loss: 0.10261 test_loss: 0.11764 \n",
      "[ 38/500] train_loss: 0.09596 valid_loss: 0.10210 test_loss: 0.11734 \n",
      "[ 39/500] train_loss: 0.09848 valid_loss: 0.10104 test_loss: 0.11567 \n",
      "验证损失减少 (0.101647 --> 0.101042). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09588 valid_loss: 0.10329 test_loss: 0.11463 \n",
      "[ 41/500] train_loss: 0.09275 valid_loss: 0.09805 test_loss: 0.11289 \n",
      "验证损失减少 (0.101042 --> 0.098051). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.09518 valid_loss: 0.09988 test_loss: 0.11386 \n",
      "[ 43/500] train_loss: 0.09616 valid_loss: 0.10269 test_loss: 0.11276 \n",
      "[ 44/500] train_loss: 0.09401 valid_loss: 0.09997 test_loss: 0.11382 \n",
      "[ 45/500] train_loss: 0.09446 valid_loss: 0.09896 test_loss: 0.11173 \n",
      "[ 46/500] train_loss: 0.09386 valid_loss: 0.09674 test_loss: 0.10988 \n",
      "验证损失减少 (0.098051 --> 0.096736). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.09152 valid_loss: 0.09668 test_loss: 0.11138 \n",
      "验证损失减少 (0.096736 --> 0.096682). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.09270 valid_loss: 0.09899 test_loss: 0.11156 \n",
      "[ 49/500] train_loss: 0.09075 valid_loss: 0.09732 test_loss: 0.11069 \n",
      "[ 50/500] train_loss: 0.09053 valid_loss: 0.09670 test_loss: 0.11015 \n",
      "[ 51/500] train_loss: 0.08955 valid_loss: 0.09729 test_loss: 0.10996 \n",
      "[ 52/500] train_loss: 0.09412 valid_loss: 0.09567 test_loss: 0.10810 \n",
      "验证损失减少 (0.096682 --> 0.095672). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.08947 valid_loss: 0.09765 test_loss: 0.11079 \n",
      "[ 54/500] train_loss: 0.08803 valid_loss: 0.09592 test_loss: 0.11005 \n",
      "[ 55/500] train_loss: 0.08746 valid_loss: 0.09986 test_loss: 0.11135 \n",
      "[ 56/500] train_loss: 0.08988 valid_loss: 0.09395 test_loss: 0.10671 \n",
      "验证损失减少 (0.095672 --> 0.093951). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.08992 valid_loss: 0.09758 test_loss: 0.10963 \n",
      "[ 58/500] train_loss: 0.08855 valid_loss: 0.09641 test_loss: 0.10870 \n",
      "[ 59/500] train_loss: 0.08644 valid_loss: 0.09349 test_loss: 0.10746 \n",
      "验证损失减少 (0.093951 --> 0.093489). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.08683 valid_loss: 0.09261 test_loss: 0.10703 \n",
      "验证损失减少 (0.093489 --> 0.092614). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.08696 valid_loss: 0.09396 test_loss: 0.10654 \n",
      "[ 62/500] train_loss: 0.08559 valid_loss: 0.09204 test_loss: 0.10559 \n",
      "验证损失减少 (0.092614 --> 0.092044). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.08721 valid_loss: 0.09405 test_loss: 0.10559 \n",
      "[ 64/500] train_loss: 0.08718 valid_loss: 0.09421 test_loss: 0.10673 \n",
      "[ 65/500] train_loss: 0.08599 valid_loss: 0.09326 test_loss: 0.10598 \n",
      "[ 66/500] train_loss: 0.08732 valid_loss: 0.09335 test_loss: 0.10506 \n",
      "[ 67/500] train_loss: 0.08534 valid_loss: 0.09244 test_loss: 0.10530 \n",
      "[ 68/500] train_loss: 0.08370 valid_loss: 0.09265 test_loss: 0.10510 \n",
      "[ 69/500] train_loss: 0.08551 valid_loss: 0.09217 test_loss: 0.10342 \n",
      "[ 70/500] train_loss: 0.08374 valid_loss: 0.09111 test_loss: 0.10397 \n",
      "验证损失减少 (0.092044 --> 0.091106). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.08094 valid_loss: 0.09387 test_loss: 0.10590 \n",
      "[ 72/500] train_loss: 0.08461 valid_loss: 0.09060 test_loss: 0.10408 \n",
      "验证损失减少 (0.091106 --> 0.090597). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.08434 valid_loss: 0.09089 test_loss: 0.10398 \n",
      "[ 74/500] train_loss: 0.08660 valid_loss: 0.08948 test_loss: 0.10367 \n",
      "验证损失减少 (0.090597 --> 0.089480). 正在保存模型...\n",
      "[ 75/500] train_loss: 0.08485 valid_loss: 0.09144 test_loss: 0.10536 \n",
      "[ 76/500] train_loss: 0.08407 valid_loss: 0.09016 test_loss: 0.10249 \n",
      "[ 77/500] train_loss: 0.08171 valid_loss: 0.08990 test_loss: 0.10220 \n",
      "[ 78/500] train_loss: 0.08300 valid_loss: 0.08990 test_loss: 0.10292 \n",
      "[ 79/500] train_loss: 0.08502 valid_loss: 0.09056 test_loss: 0.10407 \n",
      "[ 80/500] train_loss: 0.08305 valid_loss: 0.08707 test_loss: 0.10206 \n",
      "验证损失减少 (0.089480 --> 0.087069). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.08427 valid_loss: 0.08988 test_loss: 0.10299 \n",
      "[ 82/500] train_loss: 0.08498 valid_loss: 0.09288 test_loss: 0.10188 \n",
      "[ 83/500] train_loss: 0.08383 valid_loss: 0.09062 test_loss: 0.10078 \n",
      "[ 84/500] train_loss: 0.08070 valid_loss: 0.08831 test_loss: 0.10085 \n",
      "[ 85/500] train_loss: 0.08027 valid_loss: 0.08855 test_loss: 0.10054 \n",
      "[ 86/500] train_loss: 0.08061 valid_loss: 0.08945 test_loss: 0.10117 \n",
      "[ 87/500] train_loss: 0.08280 valid_loss: 0.08855 test_loss: 0.10027 \n",
      "[ 88/500] train_loss: 0.08100 valid_loss: 0.08979 test_loss: 0.10024 \n",
      "[ 89/500] train_loss: 0.08328 valid_loss: 0.08944 test_loss: 0.10168 \n",
      "[ 90/500] train_loss: 0.07929 valid_loss: 0.08802 test_loss: 0.10168 \n",
      "[ 91/500] train_loss: 0.07975 valid_loss: 0.08765 test_loss: 0.10039 \n",
      "[ 92/500] train_loss: 0.07918 valid_loss: 0.08711 test_loss: 0.09949 \n",
      "[ 93/500] train_loss: 0.08065 valid_loss: 0.08613 test_loss: 0.09973 \n",
      "验证损失减少 (0.087069 --> 0.086129). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.07985 valid_loss: 0.08875 test_loss: 0.10069 \n",
      "[ 95/500] train_loss: 0.08027 valid_loss: 0.08601 test_loss: 0.09802 \n",
      "验证损失减少 (0.086129 --> 0.086010). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.08003 valid_loss: 0.08838 test_loss: 0.09885 \n",
      "[ 97/500] train_loss: 0.08189 valid_loss: 0.08599 test_loss: 0.09925 \n",
      "验证损失减少 (0.086010 --> 0.085988). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.07822 valid_loss: 0.08704 test_loss: 0.09854 \n",
      "[ 99/500] train_loss: 0.08001 valid_loss: 0.08546 test_loss: 0.09744 \n",
      "验证损失减少 (0.085988 --> 0.085457). 正在保存模型...\n",
      "[100/500] train_loss: 0.07785 valid_loss: 0.08439 test_loss: 0.09720 \n",
      "验证损失减少 (0.085457 --> 0.084389). 正在保存模型...\n",
      "[101/500] train_loss: 0.07960 valid_loss: 0.08510 test_loss: 0.09685 \n",
      "[102/500] train_loss: 0.07677 valid_loss: 0.08388 test_loss: 0.09849 \n",
      "验证损失减少 (0.084389 --> 0.083876). 正在保存模型...\n",
      "[103/500] train_loss: 0.08010 valid_loss: 0.08657 test_loss: 0.09900 \n",
      "[104/500] train_loss: 0.07391 valid_loss: 0.08629 test_loss: 0.09787 \n",
      "[105/500] train_loss: 0.07672 valid_loss: 0.08598 test_loss: 0.09796 \n",
      "[106/500] train_loss: 0.07865 valid_loss: 0.08514 test_loss: 0.09635 \n",
      "[107/500] train_loss: 0.07756 valid_loss: 0.08430 test_loss: 0.09777 \n",
      "[108/500] train_loss: 0.07647 valid_loss: 0.08642 test_loss: 0.09688 \n",
      "[109/500] train_loss: 0.07831 valid_loss: 0.08529 test_loss: 0.09728 \n",
      "[110/500] train_loss: 0.07607 valid_loss: 0.08677 test_loss: 0.09768 \n",
      "[111/500] train_loss: 0.07605 valid_loss: 0.08633 test_loss: 0.09739 \n",
      "[112/500] train_loss: 0.07721 valid_loss: 0.08577 test_loss: 0.09844 \n",
      "[113/500] train_loss: 0.07704 valid_loss: 0.08662 test_loss: 0.09952 \n",
      "[114/500] train_loss: 0.07342 valid_loss: 0.08536 test_loss: 0.09686 \n",
      "[115/500] train_loss: 0.07713 valid_loss: 0.08381 test_loss: 0.09685 \n",
      "验证损失减少 (0.083876 --> 0.083809). 正在保存模型...\n",
      "[116/500] train_loss: 0.07441 valid_loss: 0.08562 test_loss: 0.09936 \n",
      "[117/500] train_loss: 0.07470 valid_loss: 0.08542 test_loss: 0.09620 \n",
      "[118/500] train_loss: 0.07428 valid_loss: 0.08280 test_loss: 0.09708 \n",
      "验证损失减少 (0.083809 --> 0.082798). 正在保存模型...\n",
      "[119/500] train_loss: 0.07798 valid_loss: 0.08340 test_loss: 0.09627 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120/500] train_loss: 0.07558 valid_loss: 0.08750 test_loss: 0.09542 \n",
      "[121/500] train_loss: 0.07695 valid_loss: 0.08539 test_loss: 0.09667 \n",
      "[122/500] train_loss: 0.07371 valid_loss: 0.08817 test_loss: 0.09608 \n",
      "[123/500] train_loss: 0.07223 valid_loss: 0.08380 test_loss: 0.09583 \n",
      "[124/500] train_loss: 0.07468 valid_loss: 0.08253 test_loss: 0.09534 \n",
      "验证损失减少 (0.082798 --> 0.082533). 正在保存模型...\n",
      "[125/500] train_loss: 0.07278 valid_loss: 0.08242 test_loss: 0.09451 \n",
      "验证损失减少 (0.082533 --> 0.082422). 正在保存模型...\n",
      "[126/500] train_loss: 0.07306 valid_loss: 0.08402 test_loss: 0.09611 \n",
      "[127/500] train_loss: 0.07410 valid_loss: 0.08268 test_loss: 0.09488 \n",
      "[128/500] train_loss: 0.07418 valid_loss: 0.08690 test_loss: 0.09538 \n",
      "[129/500] train_loss: 0.07418 valid_loss: 0.08351 test_loss: 0.09477 \n",
      "[130/500] train_loss: 0.07368 valid_loss: 0.08292 test_loss: 0.09511 \n",
      "[131/500] train_loss: 0.07337 valid_loss: 0.08273 test_loss: 0.09468 \n",
      "[132/500] train_loss: 0.07304 valid_loss: 0.08427 test_loss: 0.09567 \n",
      "[133/500] train_loss: 0.07276 valid_loss: 0.08758 test_loss: 0.09691 \n",
      "[134/500] train_loss: 0.07271 valid_loss: 0.08451 test_loss: 0.09428 \n",
      "[135/500] train_loss: 0.07296 valid_loss: 0.08261 test_loss: 0.09364 \n",
      "[136/500] train_loss: 0.07347 valid_loss: 0.08645 test_loss: 0.09638 \n",
      "[137/500] train_loss: 0.07324 valid_loss: 0.08385 test_loss: 0.09405 \n",
      "[138/500] train_loss: 0.07472 valid_loss: 0.08240 test_loss: 0.09531 \n",
      "验证损失减少 (0.082422 --> 0.082402). 正在保存模型...\n",
      "[139/500] train_loss: 0.07170 valid_loss: 0.08214 test_loss: 0.09394 \n",
      "验证损失减少 (0.082402 --> 0.082137). 正在保存模型...\n",
      "[140/500] train_loss: 0.07140 valid_loss: 0.08227 test_loss: 0.09385 \n",
      "[141/500] train_loss: 0.07245 valid_loss: 0.08178 test_loss: 0.09371 \n",
      "验证损失减少 (0.082137 --> 0.081784). 正在保存模型...\n",
      "[142/500] train_loss: 0.07364 valid_loss: 0.08221 test_loss: 0.09364 \n",
      "[143/500] train_loss: 0.07206 valid_loss: 0.08320 test_loss: 0.09598 \n",
      "[144/500] train_loss: 0.07400 valid_loss: 0.08339 test_loss: 0.09501 \n",
      "[145/500] train_loss: 0.07142 valid_loss: 0.08149 test_loss: 0.09324 \n",
      "验证损失减少 (0.081784 --> 0.081489). 正在保存模型...\n",
      "[146/500] train_loss: 0.07204 valid_loss: 0.08912 test_loss: 0.09405 \n",
      "[147/500] train_loss: 0.07121 valid_loss: 0.08338 test_loss: 0.09421 \n",
      "[148/500] train_loss: 0.07137 valid_loss: 0.08096 test_loss: 0.09472 \n",
      "验证损失减少 (0.081489 --> 0.080964). 正在保存模型...\n",
      "[149/500] train_loss: 0.06817 valid_loss: 0.08208 test_loss: 0.09256 \n",
      "[150/500] train_loss: 0.07245 valid_loss: 0.08513 test_loss: 0.09270 \n",
      "[151/500] train_loss: 0.07062 valid_loss: 0.08239 test_loss: 0.09338 \n",
      "[152/500] train_loss: 0.07048 valid_loss: 0.08172 test_loss: 0.09473 \n",
      "[153/500] train_loss: 0.07136 valid_loss: 0.07995 test_loss: 0.09195 \n",
      "验证损失减少 (0.080964 --> 0.079948). 正在保存模型...\n",
      "[154/500] train_loss: 0.06845 valid_loss: 0.07907 test_loss: 0.09270 \n",
      "验证损失减少 (0.079948 --> 0.079072). 正在保存模型...\n",
      "[155/500] train_loss: 0.07125 valid_loss: 0.08039 test_loss: 0.09264 \n",
      "[156/500] train_loss: 0.07083 valid_loss: 0.08112 test_loss: 0.09235 \n",
      "[157/500] train_loss: 0.07163 valid_loss: 0.08382 test_loss: 0.09328 \n",
      "[158/500] train_loss: 0.07061 valid_loss: 0.07948 test_loss: 0.09140 \n",
      "[159/500] train_loss: 0.07013 valid_loss: 0.08021 test_loss: 0.09163 \n",
      "[160/500] train_loss: 0.06883 valid_loss: 0.08146 test_loss: 0.09258 \n",
      "[161/500] train_loss: 0.07240 valid_loss: 0.08585 test_loss: 0.09606 \n",
      "[162/500] train_loss: 0.06889 valid_loss: 0.07992 test_loss: 0.09266 \n",
      "[163/500] train_loss: 0.07075 valid_loss: 0.08081 test_loss: 0.09476 \n",
      "[164/500] train_loss: 0.06994 valid_loss: 0.08431 test_loss: 0.09234 \n",
      "[165/500] train_loss: 0.06875 valid_loss: 0.08018 test_loss: 0.09230 \n",
      "[166/500] train_loss: 0.07153 valid_loss: 0.08092 test_loss: 0.09234 \n",
      "[167/500] train_loss: 0.06907 valid_loss: 0.08037 test_loss: 0.09215 \n",
      "[168/500] train_loss: 0.06920 valid_loss: 0.08051 test_loss: 0.09177 \n",
      "[169/500] train_loss: 0.06679 valid_loss: 0.08135 test_loss: 0.09137 \n",
      "[170/500] train_loss: 0.06972 valid_loss: 0.08036 test_loss: 0.09205 \n",
      "[171/500] train_loss: 0.06933 valid_loss: 0.07958 test_loss: 0.09327 \n",
      "[172/500] train_loss: 0.06721 valid_loss: 0.07910 test_loss: 0.09230 \n",
      "[173/500] train_loss: 0.06866 valid_loss: 0.07934 test_loss: 0.09237 \n",
      "[174/500] train_loss: 0.06753 valid_loss: 0.08064 test_loss: 0.09254 \n",
      "[175/500] train_loss: 0.06976 valid_loss: 0.07897 test_loss: 0.09208 \n",
      "验证损失减少 (0.079072 --> 0.078973). 正在保存模型...\n",
      "[176/500] train_loss: 0.06868 valid_loss: 0.08040 test_loss: 0.09121 \n",
      "[177/500] train_loss: 0.06878 valid_loss: 0.08038 test_loss: 0.09134 \n",
      "[178/500] train_loss: 0.06906 valid_loss: 0.08074 test_loss: 0.09162 \n",
      "[179/500] train_loss: 0.06799 valid_loss: 0.08412 test_loss: 0.09188 \n",
      "[180/500] train_loss: 0.06836 valid_loss: 0.08219 test_loss: 0.09131 \n",
      "[181/500] train_loss: 0.06739 valid_loss: 0.08225 test_loss: 0.09242 \n",
      "[182/500] train_loss: 0.06487 valid_loss: 0.08562 test_loss: 0.09120 \n",
      "[183/500] train_loss: 0.06664 valid_loss: 0.08888 test_loss: 0.09211 \n",
      "[184/500] train_loss: 0.06657 valid_loss: 0.08799 test_loss: 0.09152 \n",
      "[185/500] train_loss: 0.06709 valid_loss: 0.10010 test_loss: 0.09254 \n",
      "[186/500] train_loss: 0.06901 valid_loss: 0.09359 test_loss: 0.09102 \n",
      "[187/500] train_loss: 0.06897 valid_loss: 0.07941 test_loss: 0.09344 \n",
      "[188/500] train_loss: 0.06725 valid_loss: 0.08500 test_loss: 0.09110 \n",
      "[189/500] train_loss: 0.06527 valid_loss: 0.08476 test_loss: 0.09114 \n",
      "[190/500] train_loss: 0.06573 valid_loss: 0.08195 test_loss: 0.09189 \n",
      "[191/500] train_loss: 0.06735 valid_loss: 0.07892 test_loss: 0.09194 \n",
      "验证损失减少 (0.078973 --> 0.078918). 正在保存模型...\n",
      "[192/500] train_loss: 0.06690 valid_loss: 0.07719 test_loss: 0.09016 \n",
      "验证损失减少 (0.078918 --> 0.077187). 正在保存模型...\n",
      "[193/500] train_loss: 0.06754 valid_loss: 0.07919 test_loss: 0.09071 \n",
      "[194/500] train_loss: 0.06616 valid_loss: 0.07972 test_loss: 0.09069 \n",
      "[195/500] train_loss: 0.06715 valid_loss: 0.07930 test_loss: 0.09191 \n",
      "[196/500] train_loss: 0.06625 valid_loss: 0.07749 test_loss: 0.09083 \n",
      "[197/500] train_loss: 0.06696 valid_loss: 0.07938 test_loss: 0.09130 \n",
      "[198/500] train_loss: 0.06548 valid_loss: 0.08156 test_loss: 0.09284 \n",
      "[199/500] train_loss: 0.06797 valid_loss: 0.07930 test_loss: 0.09163 \n",
      "[200/500] train_loss: 0.06597 valid_loss: 0.08261 test_loss: 0.09132 \n",
      "[201/500] train_loss: 0.06698 valid_loss: 0.07799 test_loss: 0.09103 \n",
      "[202/500] train_loss: 0.06571 valid_loss: 0.08284 test_loss: 0.09098 \n",
      "[203/500] train_loss: 0.06611 valid_loss: 0.08224 test_loss: 0.09183 \n",
      "[204/500] train_loss: 0.06603 valid_loss: 0.07753 test_loss: 0.09097 \n",
      "[205/500] train_loss: 0.06657 valid_loss: 0.08062 test_loss: 0.09086 \n",
      "[206/500] train_loss: 0.06666 valid_loss: 0.09794 test_loss: 0.08947 \n",
      "[207/500] train_loss: 0.06427 valid_loss: 0.08847 test_loss: 0.09052 \n",
      "[208/500] train_loss: 0.06473 valid_loss: 0.10141 test_loss: 0.09032 \n",
      "[209/500] train_loss: 0.06474 valid_loss: 0.09291 test_loss: 0.09137 \n",
      "[210/500] train_loss: 0.06478 valid_loss: 0.08853 test_loss: 0.09139 \n",
      "[211/500] train_loss: 0.06595 valid_loss: 0.08259 test_loss: 0.09111 \n",
      "[212/500] train_loss: 0.06401 valid_loss: 0.07803 test_loss: 0.09062 \n",
      "[213/500] train_loss: 0.06305 valid_loss: 0.07897 test_loss: 0.09128 \n",
      "[214/500] train_loss: 0.06453 valid_loss: 0.08351 test_loss: 0.08959 \n",
      "[215/500] train_loss: 0.06409 valid_loss: 0.07794 test_loss: 0.09016 \n",
      "[216/500] train_loss: 0.06296 valid_loss: 0.07994 test_loss: 0.08902 \n",
      "[217/500] train_loss: 0.06496 valid_loss: 0.08265 test_loss: 0.08944 \n",
      "[218/500] train_loss: 0.06426 valid_loss: 0.07820 test_loss: 0.09156 \n",
      "[219/500] train_loss: 0.06383 valid_loss: 0.07823 test_loss: 0.09028 \n",
      "[220/500] train_loss: 0.06333 valid_loss: 0.08297 test_loss: 0.08867 \n",
      "[221/500] train_loss: 0.06355 valid_loss: 0.07885 test_loss: 0.09035 \n",
      "[222/500] train_loss: 0.06411 valid_loss: 0.09086 test_loss: 0.08972 \n",
      "[223/500] train_loss: 0.06476 valid_loss: 0.08276 test_loss: 0.09191 \n",
      "[224/500] train_loss: 0.06522 valid_loss: 0.08529 test_loss: 0.08901 \n",
      "[225/500] train_loss: 0.06555 valid_loss: 0.08660 test_loss: 0.08965 \n",
      "[226/500] train_loss: 0.06278 valid_loss: 0.08016 test_loss: 0.08946 \n",
      "[227/500] train_loss: 0.06371 valid_loss: 0.09155 test_loss: 0.08935 \n",
      "[228/500] train_loss: 0.06224 valid_loss: 0.07900 test_loss: 0.09291 \n",
      "[229/500] train_loss: 0.06524 valid_loss: 0.08027 test_loss: 0.09021 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230/500] train_loss: 0.06338 valid_loss: 0.07771 test_loss: 0.08944 \n",
      "[231/500] train_loss: 0.06341 valid_loss: 0.07791 test_loss: 0.08972 \n",
      "[232/500] train_loss: 0.06279 valid_loss: 0.07761 test_loss: 0.08967 \n",
      "[233/500] train_loss: 0.06245 valid_loss: 0.07701 test_loss: 0.09124 \n",
      "验证损失减少 (0.077187 --> 0.077006). 正在保存模型...\n",
      "[234/500] train_loss: 0.06503 valid_loss: 0.07931 test_loss: 0.09127 \n",
      "[235/500] train_loss: 0.06273 valid_loss: 0.07708 test_loss: 0.09127 \n",
      "[236/500] train_loss: 0.06367 valid_loss: 0.07694 test_loss: 0.09065 \n",
      "验证损失减少 (0.077006 --> 0.076941). 正在保存模型...\n",
      "[237/500] train_loss: 0.06488 valid_loss: 0.07693 test_loss: 0.08929 \n",
      "验证损失减少 (0.076941 --> 0.076928). 正在保存模型...\n",
      "[238/500] train_loss: 0.06335 valid_loss: 0.07956 test_loss: 0.08967 \n",
      "[239/500] train_loss: 0.06372 valid_loss: 0.07845 test_loss: 0.08874 \n",
      "[240/500] train_loss: 0.06385 valid_loss: 0.07759 test_loss: 0.08997 \n",
      "[241/500] train_loss: 0.06515 valid_loss: 0.07711 test_loss: 0.08911 \n",
      "[242/500] train_loss: 0.06481 valid_loss: 0.08034 test_loss: 0.08911 \n",
      "[243/500] train_loss: 0.06081 valid_loss: 0.07951 test_loss: 0.09007 \n",
      "[244/500] train_loss: 0.06438 valid_loss: 0.07776 test_loss: 0.08889 \n",
      "[245/500] train_loss: 0.06216 valid_loss: 0.07819 test_loss: 0.09060 \n",
      "[246/500] train_loss: 0.06284 valid_loss: 0.07705 test_loss: 0.08892 \n",
      "[247/500] train_loss: 0.06317 valid_loss: 0.07782 test_loss: 0.08896 \n",
      "[248/500] train_loss: 0.06233 valid_loss: 0.07924 test_loss: 0.08898 \n",
      "[249/500] train_loss: 0.06299 valid_loss: 0.07748 test_loss: 0.08790 \n",
      "[250/500] train_loss: 0.06309 valid_loss: 0.08604 test_loss: 0.08855 \n",
      "[251/500] train_loss: 0.06290 valid_loss: 0.09690 test_loss: 0.08977 \n",
      "[252/500] train_loss: 0.06550 valid_loss: 0.08035 test_loss: 0.09055 \n",
      "[253/500] train_loss: 0.06162 valid_loss: 0.08546 test_loss: 0.08870 \n",
      "[254/500] train_loss: 0.06010 valid_loss: 0.08343 test_loss: 0.08907 \n",
      "[255/500] train_loss: 0.06207 valid_loss: 0.07891 test_loss: 0.09046 \n",
      "[256/500] train_loss: 0.06024 valid_loss: 0.07781 test_loss: 0.08953 \n",
      "[257/500] train_loss: 0.05980 valid_loss: 0.07710 test_loss: 0.08982 \n",
      "[258/500] train_loss: 0.06206 valid_loss: 0.07589 test_loss: 0.09012 \n",
      "验证损失减少 (0.076928 --> 0.075886). 正在保存模型...\n",
      "[259/500] train_loss: 0.06082 valid_loss: 0.08186 test_loss: 0.09088 \n",
      "[260/500] train_loss: 0.06273 valid_loss: 0.07661 test_loss: 0.08944 \n",
      "[261/500] train_loss: 0.06065 valid_loss: 0.07702 test_loss: 0.08947 \n",
      "[262/500] train_loss: 0.06423 valid_loss: 0.07747 test_loss: 0.08966 \n",
      "[263/500] train_loss: 0.06202 valid_loss: 0.07614 test_loss: 0.08990 \n",
      "[264/500] train_loss: 0.06221 valid_loss: 0.07853 test_loss: 0.08861 \n",
      "[265/500] train_loss: 0.06178 valid_loss: 0.07555 test_loss: 0.08808 \n",
      "验证损失减少 (0.075886 --> 0.075551). 正在保存模型...\n",
      "[266/500] train_loss: 0.06094 valid_loss: 0.08406 test_loss: 0.08833 \n",
      "[267/500] train_loss: 0.06031 valid_loss: 0.07833 test_loss: 0.08895 \n",
      "[268/500] train_loss: 0.06142 valid_loss: 0.08244 test_loss: 0.08934 \n",
      "[269/500] train_loss: 0.06092 valid_loss: 0.08019 test_loss: 0.08823 \n",
      "[270/500] train_loss: 0.06011 valid_loss: 0.07720 test_loss: 0.08858 \n",
      "[271/500] train_loss: 0.06159 valid_loss: 0.07583 test_loss: 0.08822 \n",
      "[272/500] train_loss: 0.06181 valid_loss: 0.07649 test_loss: 0.08869 \n",
      "[273/500] train_loss: 0.06076 valid_loss: 0.08597 test_loss: 0.08909 \n",
      "[274/500] train_loss: 0.05940 valid_loss: 0.08349 test_loss: 0.08765 \n",
      "[275/500] train_loss: 0.05990 valid_loss: 0.07702 test_loss: 0.08883 \n",
      "[276/500] train_loss: 0.05996 valid_loss: 0.07749 test_loss: 0.08870 \n",
      "[277/500] train_loss: 0.05970 valid_loss: 0.08465 test_loss: 0.08914 \n",
      "[278/500] train_loss: 0.05892 valid_loss: 0.08360 test_loss: 0.08990 \n",
      "[279/500] train_loss: 0.05911 valid_loss: 0.07648 test_loss: 0.08961 \n",
      "[280/500] train_loss: 0.06091 valid_loss: 0.07906 test_loss: 0.08808 \n",
      "[281/500] train_loss: 0.05953 valid_loss: 0.07974 test_loss: 0.08863 \n",
      "[282/500] train_loss: 0.06044 valid_loss: 0.09924 test_loss: 0.08899 \n",
      "[283/500] train_loss: 0.06178 valid_loss: 0.08764 test_loss: 0.08845 \n",
      "[284/500] train_loss: 0.05855 valid_loss: 0.08095 test_loss: 0.08810 \n",
      "[285/500] train_loss: 0.06030 valid_loss: 0.07810 test_loss: 0.08896 \n",
      "[286/500] train_loss: 0.05884 valid_loss: 0.07510 test_loss: 0.08909 \n",
      "验证损失减少 (0.075551 --> 0.075099). 正在保存模型...\n",
      "[287/500] train_loss: 0.05901 valid_loss: 0.07573 test_loss: 0.08907 \n",
      "[288/500] train_loss: 0.06001 valid_loss: 0.07548 test_loss: 0.08864 \n",
      "[289/500] train_loss: 0.06032 valid_loss: 0.07386 test_loss: 0.08817 \n",
      "验证损失减少 (0.075099 --> 0.073857). 正在保存模型...\n",
      "[290/500] train_loss: 0.05986 valid_loss: 0.07463 test_loss: 0.08857 \n",
      "[291/500] train_loss: 0.05973 valid_loss: 0.07443 test_loss: 0.08792 \n",
      "[292/500] train_loss: 0.05970 valid_loss: 0.07482 test_loss: 0.08887 \n",
      "[293/500] train_loss: 0.05897 valid_loss: 0.07511 test_loss: 0.08810 \n",
      "[294/500] train_loss: 0.05974 valid_loss: 0.07773 test_loss: 0.08900 \n",
      "[295/500] train_loss: 0.06064 valid_loss: 0.07743 test_loss: 0.08854 \n",
      "[296/500] train_loss: 0.05997 valid_loss: 0.07486 test_loss: 0.08772 \n",
      "[297/500] train_loss: 0.05899 valid_loss: 0.07520 test_loss: 0.08884 \n",
      "[298/500] train_loss: 0.05930 valid_loss: 0.07449 test_loss: 0.08744 \n",
      "[299/500] train_loss: 0.06156 valid_loss: 0.07477 test_loss: 0.08806 \n",
      "[300/500] train_loss: 0.05830 valid_loss: 0.07478 test_loss: 0.08808 \n",
      "[301/500] train_loss: 0.05849 valid_loss: 0.07388 test_loss: 0.08672 \n",
      "[302/500] train_loss: 0.06030 valid_loss: 0.07440 test_loss: 0.08841 \n",
      "[303/500] train_loss: 0.06029 valid_loss: 0.07525 test_loss: 0.08735 \n",
      "[304/500] train_loss: 0.05880 valid_loss: 0.07575 test_loss: 0.08819 \n",
      "[305/500] train_loss: 0.06257 valid_loss: 0.07581 test_loss: 0.08923 \n",
      "[306/500] train_loss: 0.05832 valid_loss: 0.07565 test_loss: 0.08811 \n",
      "[307/500] train_loss: 0.05891 valid_loss: 0.07467 test_loss: 0.08777 \n",
      "[308/500] train_loss: 0.05984 valid_loss: 0.07505 test_loss: 0.08844 \n",
      "[309/500] train_loss: 0.05905 valid_loss: 0.07538 test_loss: 0.08755 \n",
      "[310/500] train_loss: 0.05753 valid_loss: 0.07447 test_loss: 0.08854 \n",
      "[311/500] train_loss: 0.06002 valid_loss: 0.07517 test_loss: 0.08967 \n",
      "[312/500] train_loss: 0.05960 valid_loss: 0.07406 test_loss: 0.08958 \n",
      "[313/500] train_loss: 0.05645 valid_loss: 0.07602 test_loss: 0.08996 \n",
      "[314/500] train_loss: 0.05812 valid_loss: 0.07518 test_loss: 0.08811 \n",
      "[315/500] train_loss: 0.05785 valid_loss: 0.07450 test_loss: 0.08828 \n",
      "[316/500] train_loss: 0.05677 valid_loss: 0.07536 test_loss: 0.08761 \n",
      "[317/500] train_loss: 0.05898 valid_loss: 0.07516 test_loss: 0.08938 \n",
      "[318/500] train_loss: 0.05870 valid_loss: 0.07696 test_loss: 0.08872 \n",
      "[319/500] train_loss: 0.05853 valid_loss: 0.07335 test_loss: 0.08856 \n",
      "验证损失减少 (0.073857 --> 0.073350). 正在保存模型...\n",
      "[320/500] train_loss: 0.05809 valid_loss: 0.07440 test_loss: 0.08807 \n",
      "[321/500] train_loss: 0.05674 valid_loss: 0.07448 test_loss: 0.08928 \n",
      "[322/500] train_loss: 0.05858 valid_loss: 0.07510 test_loss: 0.08884 \n",
      "[323/500] train_loss: 0.05903 valid_loss: 0.07831 test_loss: 0.08885 \n",
      "[324/500] train_loss: 0.05850 valid_loss: 0.08237 test_loss: 0.08931 \n",
      "[325/500] train_loss: 0.05864 valid_loss: 0.07481 test_loss: 0.08848 \n",
      "[326/500] train_loss: 0.05784 valid_loss: 0.07572 test_loss: 0.08760 \n",
      "[327/500] train_loss: 0.05916 valid_loss: 0.07501 test_loss: 0.08710 \n",
      "[328/500] train_loss: 0.05662 valid_loss: 0.07507 test_loss: 0.08757 \n",
      "[329/500] train_loss: 0.05826 valid_loss: 0.07547 test_loss: 0.08810 \n",
      "[330/500] train_loss: 0.05738 valid_loss: 0.07829 test_loss: 0.08812 \n",
      "[331/500] train_loss: 0.05873 valid_loss: 0.07633 test_loss: 0.08871 \n",
      "[332/500] train_loss: 0.05635 valid_loss: 0.08444 test_loss: 0.08794 \n",
      "[333/500] train_loss: 0.05646 valid_loss: 0.07420 test_loss: 0.08750 \n",
      "[334/500] train_loss: 0.05773 valid_loss: 0.07456 test_loss: 0.08733 \n",
      "[335/500] train_loss: 0.05755 valid_loss: 0.07620 test_loss: 0.08842 \n",
      "[336/500] train_loss: 0.05707 valid_loss: 0.07651 test_loss: 0.08850 \n",
      "[337/500] train_loss: 0.05838 valid_loss: 0.07457 test_loss: 0.08725 \n",
      "[338/500] train_loss: 0.05847 valid_loss: 0.07676 test_loss: 0.08865 \n",
      "[339/500] train_loss: 0.05961 valid_loss: 0.07601 test_loss: 0.08890 \n",
      "[340/500] train_loss: 0.05708 valid_loss: 0.07656 test_loss: 0.08781 \n",
      "[341/500] train_loss: 0.05722 valid_loss: 0.07497 test_loss: 0.08867 \n",
      "[342/500] train_loss: 0.05569 valid_loss: 0.08715 test_loss: 0.08706 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[343/500] train_loss: 0.05719 valid_loss: 0.07436 test_loss: 0.08763 \n",
      "[344/500] train_loss: 0.05660 valid_loss: 0.07684 test_loss: 0.08888 \n",
      "[345/500] train_loss: 0.05669 valid_loss: 0.07615 test_loss: 0.08785 \n",
      "[346/500] train_loss: 0.05600 valid_loss: 0.07401 test_loss: 0.08762 \n",
      "[347/500] train_loss: 0.05621 valid_loss: 0.08114 test_loss: 0.08779 \n",
      "[348/500] train_loss: 0.05697 valid_loss: 0.07625 test_loss: 0.08764 \n",
      "[349/500] train_loss: 0.05530 valid_loss: 0.07972 test_loss: 0.08769 \n",
      "[350/500] train_loss: 0.05652 valid_loss: 0.07396 test_loss: 0.08666 \n",
      "[351/500] train_loss: 0.05666 valid_loss: 0.07548 test_loss: 0.08950 \n",
      "[352/500] train_loss: 0.05659 valid_loss: 0.07374 test_loss: 0.08728 \n",
      "[353/500] train_loss: 0.05634 valid_loss: 0.08335 test_loss: 0.08776 \n",
      "[354/500] train_loss: 0.05707 valid_loss: 0.07647 test_loss: 0.08790 \n",
      "[355/500] train_loss: 0.05611 valid_loss: 0.08639 test_loss: 0.08958 \n",
      "[356/500] train_loss: 0.05645 valid_loss: 0.09414 test_loss: 0.08720 \n",
      "[357/500] train_loss: 0.05739 valid_loss: 0.08023 test_loss: 0.08696 \n",
      "[358/500] train_loss: 0.05547 valid_loss: 0.09018 test_loss: 0.08690 \n",
      "[359/500] train_loss: 0.05597 valid_loss: 0.07473 test_loss: 0.08744 \n",
      "[360/500] train_loss: 0.05623 valid_loss: 0.08129 test_loss: 0.08700 \n",
      "[361/500] train_loss: 0.05627 valid_loss: 0.08104 test_loss: 0.08821 \n",
      "[362/500] train_loss: 0.05613 valid_loss: 0.07589 test_loss: 0.08700 \n",
      "[363/500] train_loss: 0.05510 valid_loss: 0.09901 test_loss: 0.08694 \n",
      "[364/500] train_loss: 0.05739 valid_loss: 0.07710 test_loss: 0.08921 \n",
      "[365/500] train_loss: 0.05633 valid_loss: 0.07509 test_loss: 0.08597 \n",
      "[366/500] train_loss: 0.05659 valid_loss: 0.07649 test_loss: 0.08700 \n",
      "[367/500] train_loss: 0.05604 valid_loss: 0.07476 test_loss: 0.08708 \n",
      "[368/500] train_loss: 0.05594 valid_loss: 0.07589 test_loss: 0.08668 \n",
      "[369/500] train_loss: 0.05492 valid_loss: 0.07600 test_loss: 0.08855 \n",
      "[370/500] train_loss: 0.05781 valid_loss: 0.07659 test_loss: 0.08971 \n",
      "[371/500] train_loss: 0.05696 valid_loss: 0.07476 test_loss: 0.08783 \n",
      "[372/500] train_loss: 0.05472 valid_loss: 0.07339 test_loss: 0.08741 \n",
      "[373/500] train_loss: 0.05391 valid_loss: 0.07448 test_loss: 0.08864 \n",
      "[374/500] train_loss: 0.05763 valid_loss: 0.07489 test_loss: 0.08784 \n",
      "[375/500] train_loss: 0.05637 valid_loss: 0.07620 test_loss: 0.08741 \n",
      "[376/500] train_loss: 0.05565 valid_loss: 0.07412 test_loss: 0.08770 \n",
      "[377/500] train_loss: 0.05483 valid_loss: 0.09582 test_loss: 0.08808 \n",
      "[378/500] train_loss: 0.05538 valid_loss: 0.08343 test_loss: 0.08901 \n",
      "[379/500] train_loss: 0.05338 valid_loss: 0.08643 test_loss: 0.08951 \n",
      "[380/500] train_loss: 0.05634 valid_loss: 0.08705 test_loss: 0.09006 \n",
      "[381/500] train_loss: 0.05549 valid_loss: 0.07990 test_loss: 0.08816 \n",
      "[382/500] train_loss: 0.05407 valid_loss: 0.07453 test_loss: 0.08835 \n",
      "[383/500] train_loss: 0.05526 valid_loss: 0.08254 test_loss: 0.08859 \n",
      "[384/500] train_loss: 0.05461 valid_loss: 0.07515 test_loss: 0.08750 \n",
      "[385/500] train_loss: 0.05627 valid_loss: 0.08292 test_loss: 0.08692 \n",
      "[386/500] train_loss: 0.05486 valid_loss: 0.07727 test_loss: 0.08738 \n",
      "[387/500] train_loss: 0.05484 valid_loss: 0.09539 test_loss: 0.08792 \n",
      "[388/500] train_loss: 0.05610 valid_loss: 0.07476 test_loss: 0.08806 \n",
      "[389/500] train_loss: 0.05446 valid_loss: 0.08505 test_loss: 0.08817 \n",
      "[390/500] train_loss: 0.05565 valid_loss: 0.07609 test_loss: 0.08843 \n",
      "[391/500] train_loss: 0.05418 valid_loss: 0.07277 test_loss: 0.08661 \n",
      "验证损失减少 (0.073350 --> 0.072770). 正在保存模型...\n",
      "[392/500] train_loss: 0.05473 valid_loss: 0.07290 test_loss: 0.08672 \n",
      "[393/500] train_loss: 0.05425 valid_loss: 0.07318 test_loss: 0.08626 \n",
      "[394/500] train_loss: 0.05357 valid_loss: 0.07343 test_loss: 0.08670 \n",
      "[395/500] train_loss: 0.05369 valid_loss: 0.07562 test_loss: 0.08669 \n",
      "[396/500] train_loss: 0.05508 valid_loss: 0.07279 test_loss: 0.08675 \n",
      "[397/500] train_loss: 0.05458 valid_loss: 0.07286 test_loss: 0.08888 \n",
      "[398/500] train_loss: 0.05553 valid_loss: 0.07415 test_loss: 0.08769 \n",
      "[399/500] train_loss: 0.05343 valid_loss: 0.09036 test_loss: 0.08808 \n",
      "[400/500] train_loss: 0.05328 valid_loss: 0.07735 test_loss: 0.08973 \n",
      "[401/500] train_loss: 0.05431 valid_loss: 0.07698 test_loss: 0.08834 \n",
      "[402/500] train_loss: 0.05478 valid_loss: 0.09973 test_loss: 0.08792 \n",
      "[403/500] train_loss: 0.05375 valid_loss: 0.08182 test_loss: 0.08818 \n",
      "[404/500] train_loss: 0.05326 valid_loss: 0.08193 test_loss: 0.08690 \n",
      "[405/500] train_loss: 0.05456 valid_loss: 0.08598 test_loss: 0.08860 \n",
      "[406/500] train_loss: 0.05368 valid_loss: 0.08090 test_loss: 0.08747 \n",
      "[407/500] train_loss: 0.05569 valid_loss: 0.07405 test_loss: 0.08767 \n",
      "[408/500] train_loss: 0.05531 valid_loss: 0.07392 test_loss: 0.08977 \n",
      "[409/500] train_loss: 0.05402 valid_loss: 0.07320 test_loss: 0.08806 \n",
      "[410/500] train_loss: 0.05475 valid_loss: 0.07426 test_loss: 0.08805 \n",
      "[411/500] train_loss: 0.05387 valid_loss: 0.07294 test_loss: 0.08737 \n",
      "[412/500] train_loss: 0.05301 valid_loss: 0.07320 test_loss: 0.08776 \n",
      "[413/500] train_loss: 0.05513 valid_loss: 0.07853 test_loss: 0.08821 \n",
      "[414/500] train_loss: 0.05411 valid_loss: 0.07474 test_loss: 0.08828 \n",
      "[415/500] train_loss: 0.05502 valid_loss: 0.09393 test_loss: 0.09109 \n",
      "[416/500] train_loss: 0.05481 valid_loss: 0.09148 test_loss: 0.08796 \n",
      "[417/500] train_loss: 0.05367 valid_loss: 0.07645 test_loss: 0.08814 \n",
      "[418/500] train_loss: 0.05437 valid_loss: 0.07379 test_loss: 0.08679 \n",
      "[419/500] train_loss: 0.05359 valid_loss: 0.07915 test_loss: 0.08698 \n",
      "[420/500] train_loss: 0.05449 valid_loss: 0.08118 test_loss: 0.08757 \n",
      "[421/500] train_loss: 0.05414 valid_loss: 0.07701 test_loss: 0.08750 \n",
      "[422/500] train_loss: 0.05436 valid_loss: 0.07660 test_loss: 0.08723 \n",
      "[423/500] train_loss: 0.05293 valid_loss: 0.08913 test_loss: 0.08904 \n",
      "[424/500] train_loss: 0.05536 valid_loss: 0.07298 test_loss: 0.08772 \n",
      "[425/500] train_loss: 0.05309 valid_loss: 0.07316 test_loss: 0.08709 \n",
      "[426/500] train_loss: 0.05346 valid_loss: 0.07325 test_loss: 0.08688 \n",
      "[427/500] train_loss: 0.05413 valid_loss: 0.07422 test_loss: 0.08650 \n",
      "[428/500] train_loss: 0.05351 valid_loss: 0.07448 test_loss: 0.08823 \n",
      "[429/500] train_loss: 0.05363 valid_loss: 0.09385 test_loss: 0.08754 \n",
      "[430/500] train_loss: 0.05269 valid_loss: 0.07400 test_loss: 0.08811 \n",
      "[431/500] train_loss: 0.05316 valid_loss: 0.07307 test_loss: 0.08731 \n",
      "[432/500] train_loss: 0.05225 valid_loss: 0.07336 test_loss: 0.08732 \n",
      "[433/500] train_loss: 0.05144 valid_loss: 0.07274 test_loss: 0.08738 \n",
      "验证损失减少 (0.072770 --> 0.072745). 正在保存模型...\n",
      "[434/500] train_loss: 0.05289 valid_loss: 0.07400 test_loss: 0.08810 \n",
      "[435/500] train_loss: 0.05298 valid_loss: 0.07249 test_loss: 0.08743 \n",
      "验证损失减少 (0.072745 --> 0.072486). 正在保存模型...\n",
      "[436/500] train_loss: 0.05239 valid_loss: 0.07388 test_loss: 0.08750 \n",
      "[437/500] train_loss: 0.05437 valid_loss: 0.07332 test_loss: 0.08717 \n",
      "[438/500] train_loss: 0.05397 valid_loss: 0.07390 test_loss: 0.08681 \n",
      "[439/500] train_loss: 0.05284 valid_loss: 0.07372 test_loss: 0.08700 \n",
      "[440/500] train_loss: 0.05262 valid_loss: 0.07365 test_loss: 0.08732 \n",
      "[441/500] train_loss: 0.05399 valid_loss: 0.07418 test_loss: 0.08670 \n",
      "[442/500] train_loss: 0.05314 valid_loss: 0.07405 test_loss: 0.08768 \n",
      "[443/500] train_loss: 0.05262 valid_loss: 0.07372 test_loss: 0.08790 \n",
      "[444/500] train_loss: 0.05302 valid_loss: 0.07314 test_loss: 0.08620 \n",
      "[445/500] train_loss: 0.05154 valid_loss: 0.07390 test_loss: 0.08694 \n",
      "[446/500] train_loss: 0.05224 valid_loss: 0.07753 test_loss: 0.08795 \n",
      "[447/500] train_loss: 0.05293 valid_loss: 0.07401 test_loss: 0.08786 \n",
      "[448/500] train_loss: 0.05143 valid_loss: 0.07287 test_loss: 0.08876 \n",
      "[449/500] train_loss: 0.05181 valid_loss: 0.07230 test_loss: 0.08852 \n",
      "验证损失减少 (0.072486 --> 0.072302). 正在保存模型...\n",
      "[450/500] train_loss: 0.05179 valid_loss: 0.08106 test_loss: 0.08845 \n",
      "[451/500] train_loss: 0.05203 valid_loss: 0.07358 test_loss: 0.08985 \n",
      "[452/500] train_loss: 0.05117 valid_loss: 0.07397 test_loss: 0.08848 \n",
      "[453/500] train_loss: 0.05125 valid_loss: 0.07419 test_loss: 0.08908 \n",
      "[454/500] train_loss: 0.05234 valid_loss: 0.07473 test_loss: 0.08776 \n",
      "[455/500] train_loss: 0.05162 valid_loss: 0.07788 test_loss: 0.08815 \n",
      "[456/500] train_loss: 0.05287 valid_loss: 0.08672 test_loss: 0.08710 \n",
      "[457/500] train_loss: 0.05216 valid_loss: 0.07455 test_loss: 0.08825 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[458/500] train_loss: 0.05116 valid_loss: 0.07459 test_loss: 0.08896 \n",
      "[459/500] train_loss: 0.05226 valid_loss: 0.09665 test_loss: 0.08774 \n",
      "[460/500] train_loss: 0.05322 valid_loss: 0.07576 test_loss: 0.08894 \n",
      "[461/500] train_loss: 0.05204 valid_loss: 0.07303 test_loss: 0.08711 \n",
      "[462/500] train_loss: 0.05099 valid_loss: 0.07412 test_loss: 0.08884 \n",
      "[463/500] train_loss: 0.05159 valid_loss: 0.09992 test_loss: 0.08880 \n",
      "[464/500] train_loss: 0.05271 valid_loss: 0.07352 test_loss: 0.08770 \n",
      "[465/500] train_loss: 0.05233 valid_loss: 0.07186 test_loss: 0.08656 \n",
      "验证损失减少 (0.072302 --> 0.071862). 正在保存模型...\n",
      "[466/500] train_loss: 0.05127 valid_loss: 0.10623 test_loss: 0.08707 \n",
      "[467/500] train_loss: 0.04993 valid_loss: 0.07342 test_loss: 0.08816 \n",
      "[468/500] train_loss: 0.05116 valid_loss: 0.07318 test_loss: 0.08847 \n",
      "[469/500] train_loss: 0.05176 valid_loss: 0.08213 test_loss: 0.08765 \n",
      "[470/500] train_loss: 0.05189 valid_loss: 0.07463 test_loss: 0.08764 \n",
      "[471/500] train_loss: 0.05259 valid_loss: 0.07548 test_loss: 0.08768 \n",
      "[472/500] train_loss: 0.05168 valid_loss: 0.07704 test_loss: 0.08701 \n",
      "[473/500] train_loss: 0.05267 valid_loss: 0.07134 test_loss: 0.08580 \n",
      "验证损失减少 (0.071862 --> 0.071338). 正在保存模型...\n",
      "[474/500] train_loss: 0.05229 valid_loss: 0.07640 test_loss: 0.08666 \n",
      "[475/500] train_loss: 0.05105 valid_loss: 0.07581 test_loss: 0.08861 \n",
      "[476/500] train_loss: 0.05116 valid_loss: 0.07483 test_loss: 0.08844 \n",
      "[477/500] train_loss: 0.05235 valid_loss: 0.07286 test_loss: 0.08723 \n",
      "[478/500] train_loss: 0.05189 valid_loss: 0.09395 test_loss: 0.08596 \n",
      "[479/500] train_loss: 0.05136 valid_loss: 0.07855 test_loss: 0.08905 \n",
      "[480/500] train_loss: 0.05036 valid_loss: 0.07621 test_loss: 0.08853 \n",
      "[481/500] train_loss: 0.05277 valid_loss: 0.07355 test_loss: 0.08777 \n",
      "[482/500] train_loss: 0.05238 valid_loss: 0.07408 test_loss: 0.08840 \n",
      "[483/500] train_loss: 0.05051 valid_loss: 0.07948 test_loss: 0.08669 \n",
      "[484/500] train_loss: 0.05119 valid_loss: 0.07754 test_loss: 0.08727 \n",
      "[485/500] train_loss: 0.05024 valid_loss: 0.07701 test_loss: 0.08875 \n",
      "[486/500] train_loss: 0.05054 valid_loss: 0.07576 test_loss: 0.08808 \n",
      "[487/500] train_loss: 0.05158 valid_loss: 0.07319 test_loss: 0.08815 \n",
      "[488/500] train_loss: 0.05143 valid_loss: 0.07264 test_loss: 0.08833 \n",
      "[489/500] train_loss: 0.05047 valid_loss: 0.07283 test_loss: 0.08882 \n",
      "[490/500] train_loss: 0.05185 valid_loss: 0.07386 test_loss: 0.08977 \n",
      "[491/500] train_loss: 0.05193 valid_loss: 0.07236 test_loss: 0.08822 \n",
      "[492/500] train_loss: 0.05122 valid_loss: 0.07342 test_loss: 0.08767 \n",
      "[493/500] train_loss: 0.05039 valid_loss: 0.07549 test_loss: 0.08949 \n",
      "[494/500] train_loss: 0.05079 valid_loss: 0.07363 test_loss: 0.08888 \n",
      "[495/500] train_loss: 0.05218 valid_loss: 0.07281 test_loss: 0.08785 \n",
      "[496/500] train_loss: 0.05228 valid_loss: 0.07369 test_loss: 0.08940 \n",
      "[497/500] train_loss: 0.05002 valid_loss: 0.07257 test_loss: 0.08673 \n",
      "[498/500] train_loss: 0.04896 valid_loss: 0.07403 test_loss: 0.08795 \n",
      "[499/500] train_loss: 0.05158 valid_loss: 0.08202 test_loss: 0.08752 \n",
      "[500/500] train_loss: 0.05113 valid_loss: 0.07432 test_loss: 0.08888 \n",
      "TRAINING MODEL 17\n",
      "[  1/500] train_loss: 0.39975 valid_loss: 0.28453 test_loss: 0.28877 \n",
      "验证损失减少 (inf --> 0.284532). 正在保存模型...\n",
      "[  2/500] train_loss: 0.22814 valid_loss: 0.21288 test_loss: 0.22005 \n",
      "验证损失减少 (0.284532 --> 0.212884). 正在保存模型...\n",
      "[  3/500] train_loss: 0.18571 valid_loss: 0.17863 test_loss: 0.18903 \n",
      "验证损失减少 (0.212884 --> 0.178633). 正在保存模型...\n",
      "[  4/500] train_loss: 0.16221 valid_loss: 0.16427 test_loss: 0.17657 \n",
      "验证损失减少 (0.178633 --> 0.164270). 正在保存模型...\n",
      "[  5/500] train_loss: 0.14889 valid_loss: 0.15261 test_loss: 0.16612 \n",
      "验证损失减少 (0.164270 --> 0.152606). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14161 valid_loss: 0.14639 test_loss: 0.15842 \n",
      "验证损失减少 (0.152606 --> 0.146390). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13961 valid_loss: 0.14374 test_loss: 0.15637 \n",
      "验证损失减少 (0.146390 --> 0.143736). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13224 valid_loss: 0.14192 test_loss: 0.15263 \n",
      "验证损失减少 (0.143736 --> 0.141918). 正在保存模型...\n",
      "[  9/500] train_loss: 0.12794 valid_loss: 0.13646 test_loss: 0.14977 \n",
      "验证损失减少 (0.141918 --> 0.136461). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12798 valid_loss: 0.13516 test_loss: 0.14898 \n",
      "验证损失减少 (0.136461 --> 0.135159). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12566 valid_loss: 0.12913 test_loss: 0.14495 \n",
      "验证损失减少 (0.135159 --> 0.129132). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12080 valid_loss: 0.12688 test_loss: 0.14148 \n",
      "验证损失减少 (0.129132 --> 0.126877). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12231 valid_loss: 0.12642 test_loss: 0.14155 \n",
      "验证损失减少 (0.126877 --> 0.126422). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.12017 valid_loss: 0.12380 test_loss: 0.13943 \n",
      "验证损失减少 (0.126422 --> 0.123802). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11879 valid_loss: 0.12812 test_loss: 0.14118 \n",
      "[ 16/500] train_loss: 0.11778 valid_loss: 0.12192 test_loss: 0.13648 \n",
      "验证损失减少 (0.123802 --> 0.121918). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11559 valid_loss: 0.11841 test_loss: 0.13390 \n",
      "验证损失减少 (0.121918 --> 0.118411). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11748 valid_loss: 0.11955 test_loss: 0.13659 \n",
      "[ 19/500] train_loss: 0.11019 valid_loss: 0.11552 test_loss: 0.13070 \n",
      "验证损失减少 (0.118411 --> 0.115523). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.10957 valid_loss: 0.11686 test_loss: 0.13371 \n",
      "[ 21/500] train_loss: 0.11345 valid_loss: 0.11765 test_loss: 0.13296 \n",
      "[ 22/500] train_loss: 0.10794 valid_loss: 0.11467 test_loss: 0.12865 \n",
      "验证损失减少 (0.115523 --> 0.114670). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.10987 valid_loss: 0.11388 test_loss: 0.13104 \n",
      "验证损失减少 (0.114670 --> 0.113879). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10762 valid_loss: 0.11330 test_loss: 0.13035 \n",
      "验证损失减少 (0.113879 --> 0.113303). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10617 valid_loss: 0.11065 test_loss: 0.12563 \n",
      "验证损失减少 (0.113303 --> 0.110648). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10945 valid_loss: 0.11520 test_loss: 0.12627 \n",
      "[ 27/500] train_loss: 0.10877 valid_loss: 0.11141 test_loss: 0.12338 \n",
      "[ 28/500] train_loss: 0.10464 valid_loss: 0.10831 test_loss: 0.12140 \n",
      "验证损失减少 (0.110648 --> 0.108311). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.10045 valid_loss: 0.10920 test_loss: 0.12337 \n",
      "[ 30/500] train_loss: 0.10135 valid_loss: 0.11075 test_loss: 0.12417 \n",
      "[ 31/500] train_loss: 0.10348 valid_loss: 0.10891 test_loss: 0.12434 \n",
      "[ 32/500] train_loss: 0.10152 valid_loss: 0.10547 test_loss: 0.12168 \n",
      "验证损失减少 (0.108311 --> 0.105470). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.10008 valid_loss: 0.10447 test_loss: 0.11852 \n",
      "验证损失减少 (0.105470 --> 0.104466). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.10074 valid_loss: 0.10586 test_loss: 0.12108 \n",
      "[ 35/500] train_loss: 0.10206 valid_loss: 0.10321 test_loss: 0.11769 \n",
      "验证损失减少 (0.104466 --> 0.103209). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09977 valid_loss: 0.10310 test_loss: 0.11787 \n",
      "验证损失减少 (0.103209 --> 0.103104). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.09818 valid_loss: 0.10497 test_loss: 0.11854 \n",
      "[ 38/500] train_loss: 0.09880 valid_loss: 0.10342 test_loss: 0.11839 \n",
      "[ 39/500] train_loss: 0.09583 valid_loss: 0.10596 test_loss: 0.12024 \n",
      "[ 40/500] train_loss: 0.09671 valid_loss: 0.10146 test_loss: 0.11664 \n",
      "验证损失减少 (0.103104 --> 0.101457). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.09863 valid_loss: 0.10175 test_loss: 0.11670 \n",
      "[ 42/500] train_loss: 0.09430 valid_loss: 0.10342 test_loss: 0.11700 \n",
      "[ 43/500] train_loss: 0.09644 valid_loss: 0.10005 test_loss: 0.11537 \n",
      "验证损失减少 (0.101457 --> 0.100047). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.09402 valid_loss: 0.10149 test_loss: 0.11285 \n",
      "[ 45/500] train_loss: 0.09415 valid_loss: 0.10067 test_loss: 0.11364 \n",
      "[ 46/500] train_loss: 0.09364 valid_loss: 0.09987 test_loss: 0.11340 \n",
      "验证损失减少 (0.100047 --> 0.099870). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.09513 valid_loss: 0.09883 test_loss: 0.11256 \n",
      "验证损失减少 (0.099870 --> 0.098828). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.09092 valid_loss: 0.09890 test_loss: 0.11210 \n",
      "[ 49/500] train_loss: 0.09408 valid_loss: 0.10025 test_loss: 0.11144 \n",
      "[ 50/500] train_loss: 0.09458 valid_loss: 0.09660 test_loss: 0.11173 \n",
      "验证损失减少 (0.098828 --> 0.096604). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.09372 valid_loss: 0.09645 test_loss: 0.10925 \n",
      "验证损失减少 (0.096604 --> 0.096455). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.09264 valid_loss: 0.10024 test_loss: 0.11269 \n",
      "[ 53/500] train_loss: 0.09564 valid_loss: 0.09846 test_loss: 0.11178 \n",
      "[ 54/500] train_loss: 0.09129 valid_loss: 0.09661 test_loss: 0.11072 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 55/500] train_loss: 0.09170 valid_loss: 0.09761 test_loss: 0.10971 \n",
      "[ 56/500] train_loss: 0.09318 valid_loss: 0.09649 test_loss: 0.11017 \n",
      "[ 57/500] train_loss: 0.08936 valid_loss: 0.09971 test_loss: 0.11064 \n",
      "[ 58/500] train_loss: 0.09028 valid_loss: 0.09724 test_loss: 0.10973 \n",
      "[ 59/500] train_loss: 0.08946 valid_loss: 0.09601 test_loss: 0.10902 \n",
      "验证损失减少 (0.096455 --> 0.096008). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.09115 valid_loss: 0.09581 test_loss: 0.10801 \n",
      "验证损失减少 (0.096008 --> 0.095810). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.08969 valid_loss: 0.09811 test_loss: 0.10811 \n",
      "[ 62/500] train_loss: 0.08738 valid_loss: 0.09495 test_loss: 0.10907 \n",
      "验证损失减少 (0.095810 --> 0.094951). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.08924 valid_loss: 0.09685 test_loss: 0.10900 \n",
      "[ 64/500] train_loss: 0.08697 valid_loss: 0.09444 test_loss: 0.10847 \n",
      "验证损失减少 (0.094951 --> 0.094435). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.08763 valid_loss: 0.09363 test_loss: 0.10779 \n",
      "验证损失减少 (0.094435 --> 0.093634). 正在保存模型...\n",
      "[ 66/500] train_loss: 0.08887 valid_loss: 0.09453 test_loss: 0.10861 \n",
      "[ 67/500] train_loss: 0.08792 valid_loss: 0.09641 test_loss: 0.10597 \n",
      "[ 68/500] train_loss: 0.08721 valid_loss: 0.09438 test_loss: 0.10633 \n",
      "[ 69/500] train_loss: 0.08800 valid_loss: 0.09526 test_loss: 0.10578 \n",
      "[ 70/500] train_loss: 0.08892 valid_loss: 0.09545 test_loss: 0.10678 \n",
      "[ 71/500] train_loss: 0.08719 valid_loss: 0.09287 test_loss: 0.10526 \n",
      "验证损失减少 (0.093634 --> 0.092871). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.08448 valid_loss: 0.09477 test_loss: 0.10740 \n",
      "[ 73/500] train_loss: 0.08687 valid_loss: 0.09323 test_loss: 0.10693 \n",
      "[ 74/500] train_loss: 0.08522 valid_loss: 0.09323 test_loss: 0.10521 \n",
      "[ 75/500] train_loss: 0.08485 valid_loss: 0.09344 test_loss: 0.10499 \n",
      "[ 76/500] train_loss: 0.08548 valid_loss: 0.09450 test_loss: 0.10536 \n",
      "[ 77/500] train_loss: 0.08258 valid_loss: 0.09336 test_loss: 0.10316 \n",
      "[ 78/500] train_loss: 0.08715 valid_loss: 0.09156 test_loss: 0.10500 \n",
      "验证损失减少 (0.092871 --> 0.091557). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08494 valid_loss: 0.08959 test_loss: 0.10155 \n",
      "验证损失减少 (0.091557 --> 0.089585). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.08365 valid_loss: 0.09331 test_loss: 0.10311 \n",
      "[ 81/500] train_loss: 0.08389 valid_loss: 0.09041 test_loss: 0.10284 \n",
      "[ 82/500] train_loss: 0.08267 valid_loss: 0.09415 test_loss: 0.10260 \n",
      "[ 83/500] train_loss: 0.08253 valid_loss: 0.08995 test_loss: 0.10317 \n",
      "[ 84/500] train_loss: 0.08405 valid_loss: 0.09300 test_loss: 0.10227 \n",
      "[ 85/500] train_loss: 0.08232 valid_loss: 0.08930 test_loss: 0.10144 \n",
      "验证损失减少 (0.089585 --> 0.089301). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.08436 valid_loss: 0.08908 test_loss: 0.10345 \n",
      "验证损失减少 (0.089301 --> 0.089076). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.08569 valid_loss: 0.09069 test_loss: 0.10157 \n",
      "[ 88/500] train_loss: 0.08407 valid_loss: 0.09186 test_loss: 0.10288 \n",
      "[ 89/500] train_loss: 0.08250 valid_loss: 0.08898 test_loss: 0.10136 \n",
      "验证损失减少 (0.089076 --> 0.088976). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.08338 valid_loss: 0.09070 test_loss: 0.10319 \n",
      "[ 91/500] train_loss: 0.08319 valid_loss: 0.09014 test_loss: 0.10203 \n",
      "[ 92/500] train_loss: 0.08066 valid_loss: 0.08883 test_loss: 0.10054 \n",
      "验证损失减少 (0.088976 --> 0.088830). 正在保存模型...\n",
      "[ 93/500] train_loss: 0.07744 valid_loss: 0.09388 test_loss: 0.10206 \n",
      "[ 94/500] train_loss: 0.08030 valid_loss: 0.08884 test_loss: 0.10037 \n",
      "[ 95/500] train_loss: 0.08142 valid_loss: 0.08810 test_loss: 0.10153 \n",
      "验证损失减少 (0.088830 --> 0.088097). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.08038 valid_loss: 0.09154 test_loss: 0.10176 \n",
      "[ 97/500] train_loss: 0.07880 valid_loss: 0.08726 test_loss: 0.09890 \n",
      "验证损失减少 (0.088097 --> 0.087259). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.07964 valid_loss: 0.08800 test_loss: 0.10117 \n",
      "[ 99/500] train_loss: 0.08015 valid_loss: 0.08858 test_loss: 0.10087 \n",
      "[100/500] train_loss: 0.08133 valid_loss: 0.08648 test_loss: 0.09959 \n",
      "验证损失减少 (0.087259 --> 0.086479). 正在保存模型...\n",
      "[101/500] train_loss: 0.07890 valid_loss: 0.08603 test_loss: 0.09893 \n",
      "验证损失减少 (0.086479 --> 0.086028). 正在保存模型...\n",
      "[102/500] train_loss: 0.07926 valid_loss: 0.08941 test_loss: 0.10168 \n",
      "[103/500] train_loss: 0.07976 valid_loss: 0.08673 test_loss: 0.09894 \n",
      "[104/500] train_loss: 0.08076 valid_loss: 0.08579 test_loss: 0.09958 \n",
      "验证损失减少 (0.086028 --> 0.085789). 正在保存模型...\n",
      "[105/500] train_loss: 0.07621 valid_loss: 0.08677 test_loss: 0.09933 \n",
      "[106/500] train_loss: 0.07646 valid_loss: 0.08642 test_loss: 0.10127 \n",
      "[107/500] train_loss: 0.08064 valid_loss: 0.08699 test_loss: 0.09865 \n",
      "[108/500] train_loss: 0.08064 valid_loss: 0.08736 test_loss: 0.09868 \n",
      "[109/500] train_loss: 0.07879 valid_loss: 0.08939 test_loss: 0.10060 \n",
      "[110/500] train_loss: 0.07841 valid_loss: 0.08849 test_loss: 0.09909 \n",
      "[111/500] train_loss: 0.07918 valid_loss: 0.08616 test_loss: 0.09959 \n",
      "[112/500] train_loss: 0.07839 valid_loss: 0.08658 test_loss: 0.09773 \n",
      "[113/500] train_loss: 0.07660 valid_loss: 0.08714 test_loss: 0.09723 \n",
      "[114/500] train_loss: 0.07662 valid_loss: 0.08711 test_loss: 0.09865 \n",
      "[115/500] train_loss: 0.07870 valid_loss: 0.08683 test_loss: 0.09812 \n",
      "[116/500] train_loss: 0.07498 valid_loss: 0.08698 test_loss: 0.09861 \n",
      "[117/500] train_loss: 0.08023 valid_loss: 0.08664 test_loss: 0.09912 \n",
      "[118/500] train_loss: 0.07760 valid_loss: 0.08648 test_loss: 0.09830 \n",
      "[119/500] train_loss: 0.07599 valid_loss: 0.08500 test_loss: 0.09895 \n",
      "验证损失减少 (0.085789 --> 0.085003). 正在保存模型...\n",
      "[120/500] train_loss: 0.07612 valid_loss: 0.08288 test_loss: 0.09617 \n",
      "验证损失减少 (0.085003 --> 0.082882). 正在保存模型...\n",
      "[121/500] train_loss: 0.07578 valid_loss: 0.08680 test_loss: 0.09663 \n",
      "[122/500] train_loss: 0.07837 valid_loss: 0.08593 test_loss: 0.09721 \n",
      "[123/500] train_loss: 0.07823 valid_loss: 0.08470 test_loss: 0.09698 \n",
      "[124/500] train_loss: 0.07594 valid_loss: 0.08415 test_loss: 0.09790 \n",
      "[125/500] train_loss: 0.07713 valid_loss: 0.08423 test_loss: 0.09714 \n",
      "[126/500] train_loss: 0.07607 valid_loss: 0.08446 test_loss: 0.09749 \n",
      "[127/500] train_loss: 0.07377 valid_loss: 0.08639 test_loss: 0.09598 \n",
      "[128/500] train_loss: 0.07728 valid_loss: 0.08509 test_loss: 0.09725 \n",
      "[129/500] train_loss: 0.07379 valid_loss: 0.08513 test_loss: 0.09852 \n",
      "[130/500] train_loss: 0.07455 valid_loss: 0.08538 test_loss: 0.09714 \n",
      "[131/500] train_loss: 0.07439 valid_loss: 0.08439 test_loss: 0.09577 \n",
      "[132/500] train_loss: 0.07522 valid_loss: 0.08515 test_loss: 0.09701 \n",
      "[133/500] train_loss: 0.07577 valid_loss: 0.08385 test_loss: 0.09706 \n",
      "[134/500] train_loss: 0.07538 valid_loss: 0.08399 test_loss: 0.09560 \n",
      "[135/500] train_loss: 0.07603 valid_loss: 0.08316 test_loss: 0.09630 \n",
      "[136/500] train_loss: 0.07662 valid_loss: 0.08559 test_loss: 0.09845 \n",
      "[137/500] train_loss: 0.07447 valid_loss: 0.08599 test_loss: 0.09700 \n",
      "[138/500] train_loss: 0.07683 valid_loss: 0.08248 test_loss: 0.09623 \n",
      "验证损失减少 (0.082882 --> 0.082482). 正在保存模型...\n",
      "[139/500] train_loss: 0.07373 valid_loss: 0.08412 test_loss: 0.09690 \n",
      "[140/500] train_loss: 0.07398 valid_loss: 0.08178 test_loss: 0.09443 \n",
      "验证损失减少 (0.082482 --> 0.081785). 正在保存模型...\n",
      "[141/500] train_loss: 0.07491 valid_loss: 0.08257 test_loss: 0.09573 \n",
      "[142/500] train_loss: 0.07368 valid_loss: 0.08265 test_loss: 0.09591 \n",
      "[143/500] train_loss: 0.07245 valid_loss: 0.08273 test_loss: 0.09451 \n",
      "[144/500] train_loss: 0.07440 valid_loss: 0.08364 test_loss: 0.09496 \n",
      "[145/500] train_loss: 0.07328 valid_loss: 0.08370 test_loss: 0.09484 \n",
      "[146/500] train_loss: 0.07633 valid_loss: 0.08400 test_loss: 0.09436 \n",
      "[147/500] train_loss: 0.07263 valid_loss: 0.08228 test_loss: 0.09557 \n",
      "[148/500] train_loss: 0.07367 valid_loss: 0.08253 test_loss: 0.09549 \n",
      "[149/500] train_loss: 0.07339 valid_loss: 0.08297 test_loss: 0.09402 \n",
      "[150/500] train_loss: 0.07471 valid_loss: 0.08167 test_loss: 0.09388 \n",
      "验证损失减少 (0.081785 --> 0.081672). 正在保存模型...\n",
      "[151/500] train_loss: 0.07216 valid_loss: 0.08409 test_loss: 0.09346 \n",
      "[152/500] train_loss: 0.07184 valid_loss: 0.08301 test_loss: 0.09535 \n",
      "[153/500] train_loss: 0.07043 valid_loss: 0.08163 test_loss: 0.09362 \n",
      "验证损失减少 (0.081672 --> 0.081628). 正在保存模型...\n",
      "[154/500] train_loss: 0.07239 valid_loss: 0.08303 test_loss: 0.09604 \n",
      "[155/500] train_loss: 0.07302 valid_loss: 0.08395 test_loss: 0.09575 \n",
      "[156/500] train_loss: 0.07215 valid_loss: 0.07962 test_loss: 0.09397 \n",
      "验证损失减少 (0.081628 --> 0.079623). 正在保存模型...\n",
      "[157/500] train_loss: 0.07130 valid_loss: 0.08180 test_loss: 0.09431 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158/500] train_loss: 0.07103 valid_loss: 0.08252 test_loss: 0.09522 \n",
      "[159/500] train_loss: 0.07079 valid_loss: 0.08071 test_loss: 0.09532 \n",
      "[160/500] train_loss: 0.07093 valid_loss: 0.08047 test_loss: 0.09454 \n",
      "[161/500] train_loss: 0.07258 valid_loss: 0.08233 test_loss: 0.09530 \n",
      "[162/500] train_loss: 0.07063 valid_loss: 0.08258 test_loss: 0.09403 \n",
      "[163/500] train_loss: 0.07186 valid_loss: 0.08056 test_loss: 0.09471 \n",
      "[164/500] train_loss: 0.07094 valid_loss: 0.08138 test_loss: 0.09484 \n",
      "[165/500] train_loss: 0.07246 valid_loss: 0.08679 test_loss: 0.09421 \n",
      "[166/500] train_loss: 0.07081 valid_loss: 0.08381 test_loss: 0.09401 \n",
      "[167/500] train_loss: 0.07065 valid_loss: 0.08602 test_loss: 0.09270 \n",
      "[168/500] train_loss: 0.07092 valid_loss: 0.08279 test_loss: 0.09387 \n",
      "[169/500] train_loss: 0.06958 valid_loss: 0.08232 test_loss: 0.09428 \n",
      "[170/500] train_loss: 0.07242 valid_loss: 0.08186 test_loss: 0.09261 \n",
      "[171/500] train_loss: 0.07159 valid_loss: 0.08338 test_loss: 0.09315 \n",
      "[172/500] train_loss: 0.06916 valid_loss: 0.08740 test_loss: 0.09602 \n",
      "[173/500] train_loss: 0.07127 valid_loss: 0.08302 test_loss: 0.09347 \n",
      "[174/500] train_loss: 0.07182 valid_loss: 0.08143 test_loss: 0.09219 \n",
      "[175/500] train_loss: 0.06831 valid_loss: 0.08043 test_loss: 0.09344 \n",
      "[176/500] train_loss: 0.06915 valid_loss: 0.07965 test_loss: 0.09269 \n",
      "[177/500] train_loss: 0.07081 valid_loss: 0.08086 test_loss: 0.09331 \n",
      "[178/500] train_loss: 0.06948 valid_loss: 0.08088 test_loss: 0.09353 \n",
      "[179/500] train_loss: 0.06873 valid_loss: 0.08385 test_loss: 0.09519 \n",
      "[180/500] train_loss: 0.06916 valid_loss: 0.08072 test_loss: 0.09383 \n",
      "[181/500] train_loss: 0.06949 valid_loss: 0.08164 test_loss: 0.09253 \n",
      "[182/500] train_loss: 0.06888 valid_loss: 0.07915 test_loss: 0.09338 \n",
      "验证损失减少 (0.079623 --> 0.079145). 正在保存模型...\n",
      "[183/500] train_loss: 0.07156 valid_loss: 0.08154 test_loss: 0.09199 \n",
      "[184/500] train_loss: 0.07059 valid_loss: 0.08316 test_loss: 0.09269 \n",
      "[185/500] train_loss: 0.06891 valid_loss: 0.08067 test_loss: 0.09392 \n",
      "[186/500] train_loss: 0.06993 valid_loss: 0.08076 test_loss: 0.09288 \n",
      "[187/500] train_loss: 0.07089 valid_loss: 0.08340 test_loss: 0.09347 \n",
      "[188/500] train_loss: 0.06812 valid_loss: 0.07971 test_loss: 0.09121 \n",
      "[189/500] train_loss: 0.06739 valid_loss: 0.08104 test_loss: 0.09281 \n",
      "[190/500] train_loss: 0.06953 valid_loss: 0.08207 test_loss: 0.09216 \n",
      "[191/500] train_loss: 0.06722 valid_loss: 0.08117 test_loss: 0.09275 \n",
      "[192/500] train_loss: 0.06804 valid_loss: 0.08068 test_loss: 0.09330 \n",
      "[193/500] train_loss: 0.06886 valid_loss: 0.07985 test_loss: 0.09267 \n",
      "[194/500] train_loss: 0.06913 valid_loss: 0.07966 test_loss: 0.09225 \n",
      "[195/500] train_loss: 0.06893 valid_loss: 0.07995 test_loss: 0.09236 \n",
      "[196/500] train_loss: 0.06774 valid_loss: 0.08172 test_loss: 0.09179 \n",
      "[197/500] train_loss: 0.06784 valid_loss: 0.07897 test_loss: 0.09127 \n",
      "验证损失减少 (0.079145 --> 0.078975). 正在保存模型...\n",
      "[198/500] train_loss: 0.06776 valid_loss: 0.08117 test_loss: 0.09254 \n",
      "[199/500] train_loss: 0.06912 valid_loss: 0.07997 test_loss: 0.09160 \n",
      "[200/500] train_loss: 0.06843 valid_loss: 0.08022 test_loss: 0.09214 \n",
      "[201/500] train_loss: 0.06657 valid_loss: 0.07949 test_loss: 0.09291 \n",
      "[202/500] train_loss: 0.06750 valid_loss: 0.08072 test_loss: 0.09501 \n",
      "[203/500] train_loss: 0.06824 valid_loss: 0.08551 test_loss: 0.09243 \n",
      "[204/500] train_loss: 0.06759 valid_loss: 0.07875 test_loss: 0.09222 \n",
      "验证损失减少 (0.078975 --> 0.078751). 正在保存模型...\n",
      "[205/500] train_loss: 0.06614 valid_loss: 0.08081 test_loss: 0.09137 \n",
      "[206/500] train_loss: 0.06814 valid_loss: 0.07898 test_loss: 0.09152 \n",
      "[207/500] train_loss: 0.06830 valid_loss: 0.08327 test_loss: 0.09114 \n",
      "[208/500] train_loss: 0.06586 valid_loss: 0.08256 test_loss: 0.09377 \n",
      "[209/500] train_loss: 0.06583 valid_loss: 0.08178 test_loss: 0.09337 \n",
      "[210/500] train_loss: 0.06718 valid_loss: 0.08036 test_loss: 0.09146 \n",
      "[211/500] train_loss: 0.06642 valid_loss: 0.08469 test_loss: 0.09129 \n",
      "[212/500] train_loss: 0.06520 valid_loss: 0.08040 test_loss: 0.09088 \n",
      "[213/500] train_loss: 0.06616 valid_loss: 0.07883 test_loss: 0.09099 \n",
      "[214/500] train_loss: 0.06654 valid_loss: 0.07849 test_loss: 0.09122 \n",
      "验证损失减少 (0.078751 --> 0.078492). 正在保存模型...\n",
      "[215/500] train_loss: 0.06507 valid_loss: 0.07971 test_loss: 0.09078 \n",
      "[216/500] train_loss: 0.06826 valid_loss: 0.08011 test_loss: 0.09163 \n",
      "[217/500] train_loss: 0.06778 valid_loss: 0.08006 test_loss: 0.09099 \n",
      "[218/500] train_loss: 0.06515 valid_loss: 0.07898 test_loss: 0.09260 \n",
      "[219/500] train_loss: 0.06705 valid_loss: 0.07869 test_loss: 0.09152 \n",
      "[220/500] train_loss: 0.06537 valid_loss: 0.07810 test_loss: 0.09139 \n",
      "验证损失减少 (0.078492 --> 0.078103). 正在保存模型...\n",
      "[221/500] train_loss: 0.06612 valid_loss: 0.07968 test_loss: 0.09104 \n",
      "[222/500] train_loss: 0.06626 valid_loss: 0.07921 test_loss: 0.09184 \n",
      "[223/500] train_loss: 0.06700 valid_loss: 0.07782 test_loss: 0.09014 \n",
      "验证损失减少 (0.078103 --> 0.077816). 正在保存模型...\n",
      "[224/500] train_loss: 0.06409 valid_loss: 0.07786 test_loss: 0.09144 \n",
      "[225/500] train_loss: 0.06667 valid_loss: 0.07826 test_loss: 0.09139 \n",
      "[226/500] train_loss: 0.06630 valid_loss: 0.07906 test_loss: 0.09030 \n",
      "[227/500] train_loss: 0.06620 valid_loss: 0.07811 test_loss: 0.08982 \n",
      "[228/500] train_loss: 0.06505 valid_loss: 0.07890 test_loss: 0.09128 \n",
      "[229/500] train_loss: 0.06769 valid_loss: 0.07836 test_loss: 0.09060 \n",
      "[230/500] train_loss: 0.06478 valid_loss: 0.07788 test_loss: 0.09051 \n",
      "[231/500] train_loss: 0.06444 valid_loss: 0.07724 test_loss: 0.08925 \n",
      "验证损失减少 (0.077816 --> 0.077240). 正在保存模型...\n",
      "[232/500] train_loss: 0.06462 valid_loss: 0.07723 test_loss: 0.08971 \n",
      "验证损失减少 (0.077240 --> 0.077234). 正在保存模型...\n",
      "[233/500] train_loss: 0.06566 valid_loss: 0.07741 test_loss: 0.08983 \n",
      "[234/500] train_loss: 0.06781 valid_loss: 0.07856 test_loss: 0.08952 \n",
      "[235/500] train_loss: 0.06267 valid_loss: 0.08264 test_loss: 0.09136 \n",
      "[236/500] train_loss: 0.06508 valid_loss: 0.08130 test_loss: 0.09016 \n",
      "[237/500] train_loss: 0.06353 valid_loss: 0.08777 test_loss: 0.09167 \n",
      "[238/500] train_loss: 0.06570 valid_loss: 0.07913 test_loss: 0.09006 \n",
      "[239/500] train_loss: 0.06596 valid_loss: 0.08120 test_loss: 0.08979 \n",
      "[240/500] train_loss: 0.06487 valid_loss: 0.07902 test_loss: 0.09175 \n",
      "[241/500] train_loss: 0.06604 valid_loss: 0.07921 test_loss: 0.09174 \n",
      "[242/500] train_loss: 0.06485 valid_loss: 0.07668 test_loss: 0.09073 \n",
      "验证损失减少 (0.077234 --> 0.076683). 正在保存模型...\n",
      "[243/500] train_loss: 0.06588 valid_loss: 0.07687 test_loss: 0.09078 \n",
      "[244/500] train_loss: 0.06403 valid_loss: 0.07775 test_loss: 0.09016 \n",
      "[245/500] train_loss: 0.06495 valid_loss: 0.07924 test_loss: 0.09105 \n",
      "[246/500] train_loss: 0.06367 valid_loss: 0.07638 test_loss: 0.08877 \n",
      "验证损失减少 (0.076683 --> 0.076384). 正在保存模型...\n",
      "[247/500] train_loss: 0.06358 valid_loss: 0.07743 test_loss: 0.08963 \n",
      "[248/500] train_loss: 0.06283 valid_loss: 0.07827 test_loss: 0.09104 \n",
      "[249/500] train_loss: 0.06455 valid_loss: 0.07691 test_loss: 0.08963 \n",
      "[250/500] train_loss: 0.06547 valid_loss: 0.07951 test_loss: 0.09143 \n",
      "[251/500] train_loss: 0.06391 valid_loss: 0.07680 test_loss: 0.08912 \n",
      "[252/500] train_loss: 0.06271 valid_loss: 0.07684 test_loss: 0.08829 \n",
      "[253/500] train_loss: 0.06486 valid_loss: 0.07687 test_loss: 0.08831 \n",
      "[254/500] train_loss: 0.06544 valid_loss: 0.07848 test_loss: 0.08969 \n",
      "[255/500] train_loss: 0.06414 valid_loss: 0.07744 test_loss: 0.08981 \n",
      "[256/500] train_loss: 0.06508 valid_loss: 0.07904 test_loss: 0.08831 \n",
      "[257/500] train_loss: 0.06293 valid_loss: 0.07610 test_loss: 0.08880 \n",
      "验证损失减少 (0.076384 --> 0.076099). 正在保存模型...\n",
      "[258/500] train_loss: 0.06238 valid_loss: 0.07572 test_loss: 0.08930 \n",
      "验证损失减少 (0.076099 --> 0.075716). 正在保存模型...\n",
      "[259/500] train_loss: 0.06331 valid_loss: 0.07608 test_loss: 0.08853 \n",
      "[260/500] train_loss: 0.06456 valid_loss: 0.07678 test_loss: 0.08875 \n",
      "[261/500] train_loss: 0.06206 valid_loss: 0.07685 test_loss: 0.08936 \n",
      "[262/500] train_loss: 0.06349 valid_loss: 0.08025 test_loss: 0.08850 \n",
      "[263/500] train_loss: 0.06525 valid_loss: 0.07783 test_loss: 0.08876 \n",
      "[264/500] train_loss: 0.06476 valid_loss: 0.07627 test_loss: 0.08909 \n",
      "[265/500] train_loss: 0.06362 valid_loss: 0.07709 test_loss: 0.08923 \n",
      "[266/500] train_loss: 0.06276 valid_loss: 0.07627 test_loss: 0.08923 \n",
      "[267/500] train_loss: 0.06343 valid_loss: 0.07617 test_loss: 0.08728 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[268/500] train_loss: 0.06463 valid_loss: 0.07718 test_loss: 0.08917 \n",
      "[269/500] train_loss: 0.06239 valid_loss: 0.07622 test_loss: 0.08977 \n",
      "[270/500] train_loss: 0.06664 valid_loss: 0.07975 test_loss: 0.09119 \n",
      "[271/500] train_loss: 0.06493 valid_loss: 0.07883 test_loss: 0.09039 \n",
      "[272/500] train_loss: 0.06224 valid_loss: 0.07921 test_loss: 0.09079 \n",
      "[273/500] train_loss: 0.06222 valid_loss: 0.07774 test_loss: 0.08992 \n",
      "[274/500] train_loss: 0.06234 valid_loss: 0.07911 test_loss: 0.08916 \n",
      "[275/500] train_loss: 0.06113 valid_loss: 0.08082 test_loss: 0.08931 \n",
      "[276/500] train_loss: 0.06368 valid_loss: 0.08274 test_loss: 0.08871 \n",
      "[277/500] train_loss: 0.06383 valid_loss: 0.08163 test_loss: 0.09020 \n",
      "[278/500] train_loss: 0.06325 valid_loss: 0.07648 test_loss: 0.08851 \n",
      "[279/500] train_loss: 0.06245 valid_loss: 0.07843 test_loss: 0.08933 \n",
      "[280/500] train_loss: 0.06184 valid_loss: 0.07986 test_loss: 0.08977 \n",
      "[281/500] train_loss: 0.06104 valid_loss: 0.07859 test_loss: 0.09046 \n",
      "[282/500] train_loss: 0.06178 valid_loss: 0.07912 test_loss: 0.09013 \n",
      "[283/500] train_loss: 0.06129 valid_loss: 0.08351 test_loss: 0.08955 \n",
      "[284/500] train_loss: 0.06133 valid_loss: 0.07965 test_loss: 0.08951 \n",
      "[285/500] train_loss: 0.06281 valid_loss: 0.08124 test_loss: 0.09044 \n",
      "[286/500] train_loss: 0.06215 valid_loss: 0.07783 test_loss: 0.08903 \n",
      "[287/500] train_loss: 0.06130 valid_loss: 0.07598 test_loss: 0.09019 \n",
      "[288/500] train_loss: 0.06277 valid_loss: 0.07943 test_loss: 0.08969 \n",
      "[289/500] train_loss: 0.06296 valid_loss: 0.07560 test_loss: 0.08869 \n",
      "验证损失减少 (0.075716 --> 0.075596). 正在保存模型...\n",
      "[290/500] train_loss: 0.06362 valid_loss: 0.07659 test_loss: 0.08994 \n",
      "[291/500] train_loss: 0.06090 valid_loss: 0.07449 test_loss: 0.08968 \n",
      "验证损失减少 (0.075596 --> 0.074488). 正在保存模型...\n",
      "[292/500] train_loss: 0.06201 valid_loss: 0.08033 test_loss: 0.09050 \n",
      "[293/500] train_loss: 0.06125 valid_loss: 0.07804 test_loss: 0.08984 \n",
      "[294/500] train_loss: 0.06326 valid_loss: 0.07576 test_loss: 0.08922 \n",
      "[295/500] train_loss: 0.06282 valid_loss: 0.07759 test_loss: 0.09008 \n",
      "[296/500] train_loss: 0.06084 valid_loss: 0.07721 test_loss: 0.09097 \n",
      "[297/500] train_loss: 0.05978 valid_loss: 0.07611 test_loss: 0.08876 \n",
      "[298/500] train_loss: 0.06069 valid_loss: 0.07514 test_loss: 0.08948 \n",
      "[299/500] train_loss: 0.05977 valid_loss: 0.07615 test_loss: 0.08816 \n",
      "[300/500] train_loss: 0.06021 valid_loss: 0.07589 test_loss: 0.08914 \n",
      "[301/500] train_loss: 0.05983 valid_loss: 0.07506 test_loss: 0.08871 \n",
      "[302/500] train_loss: 0.06133 valid_loss: 0.07644 test_loss: 0.08970 \n",
      "[303/500] train_loss: 0.05966 valid_loss: 0.07565 test_loss: 0.08873 \n",
      "[304/500] train_loss: 0.06149 valid_loss: 0.08056 test_loss: 0.08932 \n",
      "[305/500] train_loss: 0.05880 valid_loss: 0.07856 test_loss: 0.09100 \n",
      "[306/500] train_loss: 0.06151 valid_loss: 0.07810 test_loss: 0.08948 \n",
      "[307/500] train_loss: 0.05877 valid_loss: 0.08200 test_loss: 0.09113 \n",
      "[308/500] train_loss: 0.06064 valid_loss: 0.08294 test_loss: 0.08938 \n",
      "[309/500] train_loss: 0.05945 valid_loss: 0.08080 test_loss: 0.09035 \n",
      "[310/500] train_loss: 0.05979 valid_loss: 0.07736 test_loss: 0.08767 \n",
      "[311/500] train_loss: 0.05975 valid_loss: 0.08186 test_loss: 0.09083 \n",
      "[312/500] train_loss: 0.05924 valid_loss: 0.08651 test_loss: 0.08823 \n",
      "[313/500] train_loss: 0.06096 valid_loss: 0.08354 test_loss: 0.08931 \n",
      "[314/500] train_loss: 0.05930 valid_loss: 0.07715 test_loss: 0.08868 \n",
      "[315/500] train_loss: 0.05912 valid_loss: 0.07630 test_loss: 0.08892 \n",
      "[316/500] train_loss: 0.05942 valid_loss: 0.07692 test_loss: 0.08832 \n",
      "[317/500] train_loss: 0.05975 valid_loss: 0.07484 test_loss: 0.08740 \n",
      "[318/500] train_loss: 0.06125 valid_loss: 0.07635 test_loss: 0.08801 \n",
      "[319/500] train_loss: 0.05829 valid_loss: 0.08234 test_loss: 0.08723 \n",
      "[320/500] train_loss: 0.06022 valid_loss: 0.07668 test_loss: 0.08765 \n",
      "[321/500] train_loss: 0.05915 valid_loss: 0.07736 test_loss: 0.08988 \n",
      "[322/500] train_loss: 0.05987 valid_loss: 0.07831 test_loss: 0.08802 \n",
      "[323/500] train_loss: 0.05984 valid_loss: 0.09541 test_loss: 0.08724 \n",
      "[324/500] train_loss: 0.06288 valid_loss: 0.07525 test_loss: 0.08799 \n",
      "[325/500] train_loss: 0.05891 valid_loss: 0.07623 test_loss: 0.08752 \n",
      "[326/500] train_loss: 0.05849 valid_loss: 0.07674 test_loss: 0.08705 \n",
      "[327/500] train_loss: 0.05962 valid_loss: 0.07879 test_loss: 0.08667 \n",
      "[328/500] train_loss: 0.05882 valid_loss: 0.07808 test_loss: 0.08794 \n",
      "[329/500] train_loss: 0.06015 valid_loss: 0.07664 test_loss: 0.08692 \n",
      "[330/500] train_loss: 0.06041 valid_loss: 0.07717 test_loss: 0.08932 \n",
      "[331/500] train_loss: 0.05993 valid_loss: 0.07616 test_loss: 0.08815 \n",
      "[332/500] train_loss: 0.06017 valid_loss: 0.07593 test_loss: 0.08788 \n",
      "[333/500] train_loss: 0.06084 valid_loss: 0.07487 test_loss: 0.08775 \n",
      "[334/500] train_loss: 0.05939 valid_loss: 0.07655 test_loss: 0.08807 \n",
      "[335/500] train_loss: 0.05995 valid_loss: 0.07971 test_loss: 0.08872 \n",
      "[336/500] train_loss: 0.06019 valid_loss: 0.07553 test_loss: 0.08843 \n",
      "[337/500] train_loss: 0.05754 valid_loss: 0.07630 test_loss: 0.08841 \n",
      "[338/500] train_loss: 0.05738 valid_loss: 0.07630 test_loss: 0.08782 \n",
      "[339/500] train_loss: 0.05823 valid_loss: 0.07876 test_loss: 0.08792 \n",
      "[340/500] train_loss: 0.05835 valid_loss: 0.07487 test_loss: 0.08679 \n",
      "[341/500] train_loss: 0.06080 valid_loss: 0.07542 test_loss: 0.08719 \n",
      "[342/500] train_loss: 0.05985 valid_loss: 0.07712 test_loss: 0.08918 \n",
      "[343/500] train_loss: 0.05719 valid_loss: 0.07549 test_loss: 0.08755 \n",
      "[344/500] train_loss: 0.06003 valid_loss: 0.07462 test_loss: 0.08720 \n",
      "[345/500] train_loss: 0.05999 valid_loss: 0.07604 test_loss: 0.08743 \n",
      "[346/500] train_loss: 0.05844 valid_loss: 0.07673 test_loss: 0.08843 \n",
      "[347/500] train_loss: 0.05911 valid_loss: 0.07777 test_loss: 0.08708 \n",
      "[348/500] train_loss: 0.05928 valid_loss: 0.07752 test_loss: 0.08734 \n",
      "[349/500] train_loss: 0.05835 valid_loss: 0.08023 test_loss: 0.08713 \n",
      "[350/500] train_loss: 0.05704 valid_loss: 0.07844 test_loss: 0.08781 \n",
      "[351/500] train_loss: 0.05807 valid_loss: 0.07590 test_loss: 0.08870 \n",
      "[352/500] train_loss: 0.05874 valid_loss: 0.07686 test_loss: 0.08846 \n",
      "[353/500] train_loss: 0.05903 valid_loss: 0.07561 test_loss: 0.08810 \n",
      "[354/500] train_loss: 0.05802 valid_loss: 0.08486 test_loss: 0.08834 \n",
      "[355/500] train_loss: 0.05992 valid_loss: 0.08203 test_loss: 0.08776 \n",
      "[356/500] train_loss: 0.05889 valid_loss: 0.08017 test_loss: 0.08905 \n",
      "[357/500] train_loss: 0.05745 valid_loss: 0.08156 test_loss: 0.08726 \n",
      "[358/500] train_loss: 0.05922 valid_loss: 0.08105 test_loss: 0.08775 \n",
      "[359/500] train_loss: 0.05832 valid_loss: 0.07920 test_loss: 0.08840 \n",
      "[360/500] train_loss: 0.05642 valid_loss: 0.07677 test_loss: 0.08768 \n",
      "[361/500] train_loss: 0.05541 valid_loss: 0.08210 test_loss: 0.08852 \n",
      "[362/500] train_loss: 0.05720 valid_loss: 0.08009 test_loss: 0.08832 \n",
      "[363/500] train_loss: 0.05777 valid_loss: 0.08354 test_loss: 0.08783 \n",
      "[364/500] train_loss: 0.05770 valid_loss: 0.07982 test_loss: 0.08865 \n",
      "[365/500] train_loss: 0.05878 valid_loss: 0.07931 test_loss: 0.08893 \n",
      "[366/500] train_loss: 0.05723 valid_loss: 0.07731 test_loss: 0.08934 \n",
      "[367/500] train_loss: 0.05697 valid_loss: 0.08072 test_loss: 0.08868 \n",
      "[368/500] train_loss: 0.05709 valid_loss: 0.08162 test_loss: 0.08683 \n",
      "[369/500] train_loss: 0.05746 valid_loss: 0.07659 test_loss: 0.08916 \n",
      "[370/500] train_loss: 0.05676 valid_loss: 0.07587 test_loss: 0.08849 \n",
      "[371/500] train_loss: 0.05674 valid_loss: 0.07933 test_loss: 0.08868 \n",
      "[372/500] train_loss: 0.05584 valid_loss: 0.07614 test_loss: 0.08755 \n",
      "[373/500] train_loss: 0.05685 valid_loss: 0.07528 test_loss: 0.08756 \n",
      "[374/500] train_loss: 0.05611 valid_loss: 0.07999 test_loss: 0.08790 \n",
      "[375/500] train_loss: 0.05820 valid_loss: 0.07798 test_loss: 0.08803 \n",
      "[376/500] train_loss: 0.05794 valid_loss: 0.07933 test_loss: 0.08830 \n",
      "[377/500] train_loss: 0.05755 valid_loss: 0.07744 test_loss: 0.08879 \n",
      "[378/500] train_loss: 0.05728 valid_loss: 0.07747 test_loss: 0.08917 \n",
      "[379/500] train_loss: 0.05696 valid_loss: 0.08044 test_loss: 0.08747 \n",
      "[380/500] train_loss: 0.05659 valid_loss: 0.07682 test_loss: 0.09037 \n",
      "[381/500] train_loss: 0.05767 valid_loss: 0.07499 test_loss: 0.08768 \n",
      "[382/500] train_loss: 0.05669 valid_loss: 0.07513 test_loss: 0.08842 \n",
      "[383/500] train_loss: 0.05805 valid_loss: 0.07577 test_loss: 0.08763 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384/500] train_loss: 0.05710 valid_loss: 0.07491 test_loss: 0.08791 \n",
      "[385/500] train_loss: 0.05565 valid_loss: 0.07526 test_loss: 0.08863 \n",
      "[386/500] train_loss: 0.05678 valid_loss: 0.07769 test_loss: 0.08713 \n",
      "[387/500] train_loss: 0.05815 valid_loss: 0.07996 test_loss: 0.08775 \n",
      "[388/500] train_loss: 0.05504 valid_loss: 0.07608 test_loss: 0.08801 \n",
      "[389/500] train_loss: 0.05794 valid_loss: 0.08651 test_loss: 0.08908 \n",
      "[390/500] train_loss: 0.05675 valid_loss: 0.07463 test_loss: 0.08730 \n",
      "[391/500] train_loss: 0.05610 valid_loss: 0.07794 test_loss: 0.08766 \n",
      "[392/500] train_loss: 0.05593 valid_loss: 0.07509 test_loss: 0.08753 \n",
      "[393/500] train_loss: 0.05527 valid_loss: 0.07519 test_loss: 0.08934 \n",
      "[394/500] train_loss: 0.05606 valid_loss: 0.07745 test_loss: 0.08875 \n",
      "[395/500] train_loss: 0.05684 valid_loss: 0.07971 test_loss: 0.08764 \n",
      "[396/500] train_loss: 0.05682 valid_loss: 0.08601 test_loss: 0.08756 \n",
      "[397/500] train_loss: 0.05686 valid_loss: 0.07544 test_loss: 0.08739 \n",
      "[398/500] train_loss: 0.05503 valid_loss: 0.07780 test_loss: 0.08836 \n",
      "[399/500] train_loss: 0.05651 valid_loss: 0.07680 test_loss: 0.08984 \n",
      "[400/500] train_loss: 0.05658 valid_loss: 0.08101 test_loss: 0.08785 \n",
      "[401/500] train_loss: 0.05659 valid_loss: 0.08011 test_loss: 0.08882 \n",
      "[402/500] train_loss: 0.05538 valid_loss: 0.07803 test_loss: 0.08839 \n",
      "[403/500] train_loss: 0.05679 valid_loss: 0.08606 test_loss: 0.08872 \n",
      "[404/500] train_loss: 0.05555 valid_loss: 0.07966 test_loss: 0.08846 \n",
      "[405/500] train_loss: 0.05618 valid_loss: 0.08406 test_loss: 0.08764 \n",
      "[406/500] train_loss: 0.05693 valid_loss: 0.08092 test_loss: 0.08955 \n",
      "[407/500] train_loss: 0.05544 valid_loss: 0.08301 test_loss: 0.08783 \n",
      "[408/500] train_loss: 0.05530 valid_loss: 0.07904 test_loss: 0.08900 \n",
      "[409/500] train_loss: 0.05670 valid_loss: 0.07809 test_loss: 0.08794 \n",
      "[410/500] train_loss: 0.05732 valid_loss: 0.08017 test_loss: 0.08812 \n",
      "[411/500] train_loss: 0.05609 valid_loss: 0.07501 test_loss: 0.08793 \n",
      "[412/500] train_loss: 0.05486 valid_loss: 0.07952 test_loss: 0.08796 \n",
      "[413/500] train_loss: 0.05676 valid_loss: 0.07913 test_loss: 0.08834 \n",
      "[414/500] train_loss: 0.05587 valid_loss: 0.07704 test_loss: 0.08733 \n",
      "[415/500] train_loss: 0.05632 valid_loss: 0.08310 test_loss: 0.08782 \n",
      "[416/500] train_loss: 0.05614 valid_loss: 0.08425 test_loss: 0.08654 \n",
      "[417/500] train_loss: 0.05688 valid_loss: 0.07602 test_loss: 0.08817 \n",
      "[418/500] train_loss: 0.05368 valid_loss: 0.07798 test_loss: 0.08650 \n",
      "[419/500] train_loss: 0.05611 valid_loss: 0.08061 test_loss: 0.08750 \n",
      "[420/500] train_loss: 0.05652 valid_loss: 0.08400 test_loss: 0.08848 \n",
      "[421/500] train_loss: 0.05705 valid_loss: 0.08083 test_loss: 0.08657 \n",
      "[422/500] train_loss: 0.05512 valid_loss: 0.07576 test_loss: 0.08930 \n",
      "[423/500] train_loss: 0.05526 valid_loss: 0.07582 test_loss: 0.08706 \n",
      "[424/500] train_loss: 0.05577 valid_loss: 0.07549 test_loss: 0.08632 \n",
      "[425/500] train_loss: 0.05489 valid_loss: 0.07953 test_loss: 0.08969 \n",
      "[426/500] train_loss: 0.05486 valid_loss: 0.07558 test_loss: 0.08925 \n",
      "[427/500] train_loss: 0.05669 valid_loss: 0.07667 test_loss: 0.08868 \n",
      "[428/500] train_loss: 0.05562 valid_loss: 0.07955 test_loss: 0.08907 \n",
      "[429/500] train_loss: 0.05469 valid_loss: 0.08353 test_loss: 0.08731 \n",
      "[430/500] train_loss: 0.05389 valid_loss: 0.07868 test_loss: 0.08957 \n",
      "[431/500] train_loss: 0.05452 valid_loss: 0.08193 test_loss: 0.08874 \n",
      "[432/500] train_loss: 0.05800 valid_loss: 0.07732 test_loss: 0.08931 \n",
      "[433/500] train_loss: 0.05605 valid_loss: 0.07854 test_loss: 0.08685 \n",
      "[434/500] train_loss: 0.05527 valid_loss: 0.07829 test_loss: 0.08653 \n",
      "[435/500] train_loss: 0.05414 valid_loss: 0.07640 test_loss: 0.08864 \n",
      "[436/500] train_loss: 0.05316 valid_loss: 0.07530 test_loss: 0.08784 \n",
      "[437/500] train_loss: 0.05530 valid_loss: 0.07399 test_loss: 0.08834 \n",
      "验证损失减少 (0.074488 --> 0.073995). 正在保存模型...\n",
      "[438/500] train_loss: 0.05522 valid_loss: 0.07440 test_loss: 0.08778 \n",
      "[439/500] train_loss: 0.05411 valid_loss: 0.07603 test_loss: 0.08791 \n",
      "[440/500] train_loss: 0.05510 valid_loss: 0.07564 test_loss: 0.08871 \n",
      "[441/500] train_loss: 0.05593 valid_loss: 0.07480 test_loss: 0.08828 \n",
      "[442/500] train_loss: 0.05458 valid_loss: 0.07513 test_loss: 0.08803 \n",
      "[443/500] train_loss: 0.05345 valid_loss: 0.07470 test_loss: 0.08701 \n",
      "[444/500] train_loss: 0.05482 valid_loss: 0.08422 test_loss: 0.08915 \n",
      "[445/500] train_loss: 0.05369 valid_loss: 0.08441 test_loss: 0.08790 \n",
      "[446/500] train_loss: 0.05298 valid_loss: 0.08104 test_loss: 0.08776 \n",
      "[447/500] train_loss: 0.05439 valid_loss: 0.07757 test_loss: 0.08780 \n",
      "[448/500] train_loss: 0.05311 valid_loss: 0.08295 test_loss: 0.08717 \n",
      "[449/500] train_loss: 0.05447 valid_loss: 0.07731 test_loss: 0.08635 \n",
      "[450/500] train_loss: 0.05163 valid_loss: 0.08252 test_loss: 0.08688 \n",
      "[451/500] train_loss: 0.05375 valid_loss: 0.07436 test_loss: 0.08713 \n",
      "[452/500] train_loss: 0.05304 valid_loss: 0.07581 test_loss: 0.08683 \n",
      "[453/500] train_loss: 0.05487 valid_loss: 0.08651 test_loss: 0.08845 \n",
      "[454/500] train_loss: 0.05379 valid_loss: 0.07488 test_loss: 0.08763 \n",
      "[455/500] train_loss: 0.05341 valid_loss: 0.07555 test_loss: 0.08710 \n",
      "[456/500] train_loss: 0.05397 valid_loss: 0.07571 test_loss: 0.08728 \n",
      "[457/500] train_loss: 0.05503 valid_loss: 0.07507 test_loss: 0.08757 \n",
      "[458/500] train_loss: 0.05252 valid_loss: 0.08835 test_loss: 0.08848 \n",
      "[459/500] train_loss: 0.05360 valid_loss: 0.08293 test_loss: 0.08811 \n",
      "[460/500] train_loss: 0.05355 valid_loss: 0.07363 test_loss: 0.08656 \n",
      "验证损失减少 (0.073995 --> 0.073626). 正在保存模型...\n",
      "[461/500] train_loss: 0.05401 valid_loss: 0.07525 test_loss: 0.08753 \n",
      "[462/500] train_loss: 0.05175 valid_loss: 0.07450 test_loss: 0.08694 \n",
      "[463/500] train_loss: 0.05337 valid_loss: 0.07860 test_loss: 0.08891 \n",
      "[464/500] train_loss: 0.05459 valid_loss: 0.07634 test_loss: 0.08912 \n",
      "[465/500] train_loss: 0.05134 valid_loss: 0.07392 test_loss: 0.08826 \n",
      "[466/500] train_loss: 0.05371 valid_loss: 0.07474 test_loss: 0.08793 \n",
      "[467/500] train_loss: 0.05234 valid_loss: 0.07474 test_loss: 0.08692 \n",
      "[468/500] train_loss: 0.05383 valid_loss: 0.07432 test_loss: 0.08562 \n",
      "[469/500] train_loss: 0.05232 valid_loss: 0.07574 test_loss: 0.08814 \n",
      "[470/500] train_loss: 0.05489 valid_loss: 0.07478 test_loss: 0.08813 \n",
      "[471/500] train_loss: 0.05255 valid_loss: 0.07623 test_loss: 0.08963 \n",
      "[472/500] train_loss: 0.05233 valid_loss: 0.07466 test_loss: 0.08854 \n",
      "[473/500] train_loss: 0.05388 valid_loss: 0.07457 test_loss: 0.08707 \n",
      "[474/500] train_loss: 0.05255 valid_loss: 0.07411 test_loss: 0.08713 \n",
      "[475/500] train_loss: 0.05288 valid_loss: 0.07730 test_loss: 0.08788 \n",
      "[476/500] train_loss: 0.05487 valid_loss: 0.07348 test_loss: 0.08759 \n",
      "验证损失减少 (0.073626 --> 0.073485). 正在保存模型...\n",
      "[477/500] train_loss: 0.05328 valid_loss: 0.07834 test_loss: 0.08698 \n",
      "[478/500] train_loss: 0.05185 valid_loss: 0.08040 test_loss: 0.08743 \n",
      "[479/500] train_loss: 0.05243 valid_loss: 0.07740 test_loss: 0.08685 \n",
      "[480/500] train_loss: 0.05192 valid_loss: 0.07628 test_loss: 0.08831 \n",
      "[481/500] train_loss: 0.05123 valid_loss: 0.07710 test_loss: 0.08717 \n",
      "[482/500] train_loss: 0.05379 valid_loss: 0.08157 test_loss: 0.08740 \n",
      "[483/500] train_loss: 0.05388 valid_loss: 0.07805 test_loss: 0.08945 \n",
      "[484/500] train_loss: 0.05404 valid_loss: 0.08736 test_loss: 0.08657 \n",
      "[485/500] train_loss: 0.05418 valid_loss: 0.08332 test_loss: 0.08749 \n",
      "[486/500] train_loss: 0.05232 valid_loss: 0.08013 test_loss: 0.08891 \n",
      "[487/500] train_loss: 0.05395 valid_loss: 0.08158 test_loss: 0.08838 \n",
      "[488/500] train_loss: 0.05366 valid_loss: 0.07636 test_loss: 0.08835 \n",
      "[489/500] train_loss: 0.05233 valid_loss: 0.08489 test_loss: 0.08807 \n",
      "[490/500] train_loss: 0.05255 valid_loss: 0.07888 test_loss: 0.08777 \n",
      "[491/500] train_loss: 0.05110 valid_loss: 0.07625 test_loss: 0.08909 \n",
      "[492/500] train_loss: 0.05429 valid_loss: 0.07889 test_loss: 0.08756 \n",
      "[493/500] train_loss: 0.05306 valid_loss: 0.07607 test_loss: 0.08677 \n",
      "[494/500] train_loss: 0.05321 valid_loss: 0.08066 test_loss: 0.08669 \n",
      "[495/500] train_loss: 0.05192 valid_loss: 0.07601 test_loss: 0.08805 \n",
      "[496/500] train_loss: 0.05238 valid_loss: 0.07573 test_loss: 0.08809 \n",
      "[497/500] train_loss: 0.05214 valid_loss: 0.07812 test_loss: 0.08812 \n",
      "[498/500] train_loss: 0.05259 valid_loss: 0.08923 test_loss: 0.08842 \n",
      "[499/500] train_loss: 0.05237 valid_loss: 0.08016 test_loss: 0.08783 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500/500] train_loss: 0.05238 valid_loss: 0.07415 test_loss: 0.08712 \n",
      "TRAINING MODEL 18\n",
      "[  1/500] train_loss: 0.47238 valid_loss: 0.32367 test_loss: 0.32892 \n",
      "验证损失减少 (inf --> 0.323669). 正在保存模型...\n",
      "[  2/500] train_loss: 0.26077 valid_loss: 0.23512 test_loss: 0.24290 \n",
      "验证损失减少 (0.323669 --> 0.235123). 正在保存模型...\n",
      "[  3/500] train_loss: 0.19561 valid_loss: 0.19038 test_loss: 0.19884 \n",
      "验证损失减少 (0.235123 --> 0.190376). 正在保存模型...\n",
      "[  4/500] train_loss: 0.17271 valid_loss: 0.16869 test_loss: 0.18043 \n",
      "验证损失减少 (0.190376 --> 0.168686). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15799 valid_loss: 0.15847 test_loss: 0.17154 \n",
      "验证损失减少 (0.168686 --> 0.158471). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14892 valid_loss: 0.15001 test_loss: 0.16434 \n",
      "验证损失减少 (0.158471 --> 0.150005). 正在保存模型...\n",
      "[  7/500] train_loss: 0.14026 valid_loss: 0.14752 test_loss: 0.16046 \n",
      "验证损失减少 (0.150005 --> 0.147522). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13665 valid_loss: 0.14057 test_loss: 0.15243 \n",
      "验证损失减少 (0.147522 --> 0.140567). 正在保存模型...\n",
      "[  9/500] train_loss: 0.13369 valid_loss: 0.14276 test_loss: 0.15348 \n",
      "[ 10/500] train_loss: 0.13188 valid_loss: 0.14228 test_loss: 0.15385 \n",
      "[ 11/500] train_loss: 0.13030 valid_loss: 0.13368 test_loss: 0.14553 \n",
      "验证损失减少 (0.140567 --> 0.133679). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12709 valid_loss: 0.13461 test_loss: 0.14658 \n",
      "[ 13/500] train_loss: 0.12607 valid_loss: 0.13086 test_loss: 0.14415 \n",
      "验证损失减少 (0.133679 --> 0.130855). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.12301 valid_loss: 0.12649 test_loss: 0.14102 \n",
      "验证损失减少 (0.130855 --> 0.126487). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11999 valid_loss: 0.12556 test_loss: 0.14056 \n",
      "验证损失减少 (0.126487 --> 0.125556). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.11765 valid_loss: 0.12353 test_loss: 0.14101 \n",
      "验证损失减少 (0.125556 --> 0.123533). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11614 valid_loss: 0.12146 test_loss: 0.13625 \n",
      "验证损失减少 (0.123533 --> 0.121457). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11549 valid_loss: 0.12168 test_loss: 0.13466 \n",
      "[ 19/500] train_loss: 0.11404 valid_loss: 0.11813 test_loss: 0.13589 \n",
      "验证损失减少 (0.121457 --> 0.118130). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.11441 valid_loss: 0.11523 test_loss: 0.13151 \n",
      "验证损失减少 (0.118130 --> 0.115227). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.11255 valid_loss: 0.11939 test_loss: 0.13416 \n",
      "[ 22/500] train_loss: 0.11036 valid_loss: 0.11592 test_loss: 0.12939 \n",
      "[ 23/500] train_loss: 0.10958 valid_loss: 0.11410 test_loss: 0.13061 \n",
      "验证损失减少 (0.115227 --> 0.114101). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10964 valid_loss: 0.11348 test_loss: 0.12852 \n",
      "验证损失减少 (0.114101 --> 0.113480). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10608 valid_loss: 0.11756 test_loss: 0.13289 \n",
      "[ 26/500] train_loss: 0.10669 valid_loss: 0.11639 test_loss: 0.13285 \n",
      "[ 27/500] train_loss: 0.10728 valid_loss: 0.11013 test_loss: 0.12620 \n",
      "验证损失减少 (0.113480 --> 0.110135). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10414 valid_loss: 0.11241 test_loss: 0.12616 \n",
      "[ 29/500] train_loss: 0.10830 valid_loss: 0.11001 test_loss: 0.12499 \n",
      "验证损失减少 (0.110135 --> 0.110014). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.10588 valid_loss: 0.11110 test_loss: 0.12488 \n",
      "[ 31/500] train_loss: 0.10534 valid_loss: 0.10709 test_loss: 0.12232 \n",
      "验证损失减少 (0.110014 --> 0.107092). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.10365 valid_loss: 0.10799 test_loss: 0.12370 \n",
      "[ 33/500] train_loss: 0.10130 valid_loss: 0.10774 test_loss: 0.12398 \n",
      "[ 34/500] train_loss: 0.10208 valid_loss: 0.10614 test_loss: 0.12155 \n",
      "验证损失减少 (0.107092 --> 0.106143). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.10296 valid_loss: 0.10436 test_loss: 0.12052 \n",
      "验证损失减少 (0.106143 --> 0.104358). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09908 valid_loss: 0.10476 test_loss: 0.11969 \n",
      "[ 37/500] train_loss: 0.10021 valid_loss: 0.10328 test_loss: 0.11869 \n",
      "验证损失减少 (0.104358 --> 0.103280). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.10156 valid_loss: 0.10446 test_loss: 0.11845 \n",
      "[ 39/500] train_loss: 0.09922 valid_loss: 0.10595 test_loss: 0.12068 \n",
      "[ 40/500] train_loss: 0.09755 valid_loss: 0.10534 test_loss: 0.11846 \n",
      "[ 41/500] train_loss: 0.09821 valid_loss: 0.10358 test_loss: 0.11706 \n",
      "[ 42/500] train_loss: 0.09516 valid_loss: 0.10334 test_loss: 0.11747 \n",
      "[ 43/500] train_loss: 0.09829 valid_loss: 0.09965 test_loss: 0.11606 \n",
      "验证损失减少 (0.103280 --> 0.099645). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.09556 valid_loss: 0.10187 test_loss: 0.11660 \n",
      "[ 45/500] train_loss: 0.09428 valid_loss: 0.10318 test_loss: 0.11793 \n",
      "[ 46/500] train_loss: 0.09392 valid_loss: 0.10029 test_loss: 0.11389 \n",
      "[ 47/500] train_loss: 0.09628 valid_loss: 0.10074 test_loss: 0.11486 \n",
      "[ 48/500] train_loss: 0.09476 valid_loss: 0.10012 test_loss: 0.11395 \n",
      "[ 49/500] train_loss: 0.09371 valid_loss: 0.09927 test_loss: 0.11329 \n",
      "验证损失减少 (0.099645 --> 0.099273). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.09412 valid_loss: 0.10047 test_loss: 0.11524 \n",
      "[ 51/500] train_loss: 0.09339 valid_loss: 0.09799 test_loss: 0.11355 \n",
      "验证损失减少 (0.099273 --> 0.097991). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.09318 valid_loss: 0.09838 test_loss: 0.11137 \n",
      "[ 53/500] train_loss: 0.09360 valid_loss: 0.10139 test_loss: 0.11272 \n",
      "[ 54/500] train_loss: 0.09023 valid_loss: 0.09669 test_loss: 0.11095 \n",
      "验证损失减少 (0.097991 --> 0.096692). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.09143 valid_loss: 0.09717 test_loss: 0.11008 \n",
      "[ 56/500] train_loss: 0.09234 valid_loss: 0.09598 test_loss: 0.10980 \n",
      "验证损失减少 (0.096692 --> 0.095975). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.09124 valid_loss: 0.09811 test_loss: 0.11077 \n",
      "[ 58/500] train_loss: 0.09133 valid_loss: 0.09588 test_loss: 0.10989 \n",
      "验证损失减少 (0.095975 --> 0.095875). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.09082 valid_loss: 0.09412 test_loss: 0.10787 \n",
      "验证损失减少 (0.095875 --> 0.094124). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.08819 valid_loss: 0.09710 test_loss: 0.10851 \n",
      "[ 61/500] train_loss: 0.09159 valid_loss: 0.09479 test_loss: 0.10938 \n",
      "[ 62/500] train_loss: 0.08741 valid_loss: 0.09596 test_loss: 0.10905 \n",
      "[ 63/500] train_loss: 0.08895 valid_loss: 0.09566 test_loss: 0.10966 \n",
      "[ 64/500] train_loss: 0.08945 valid_loss: 0.09556 test_loss: 0.10816 \n",
      "[ 65/500] train_loss: 0.08759 valid_loss: 0.09535 test_loss: 0.10791 \n",
      "[ 66/500] train_loss: 0.08725 valid_loss: 0.09587 test_loss: 0.10664 \n",
      "[ 67/500] train_loss: 0.08715 valid_loss: 0.09373 test_loss: 0.10739 \n",
      "验证损失减少 (0.094124 --> 0.093734). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.08767 valid_loss: 0.09923 test_loss: 0.10802 \n",
      "[ 69/500] train_loss: 0.08676 valid_loss: 0.09645 test_loss: 0.10542 \n",
      "[ 70/500] train_loss: 0.08865 valid_loss: 0.09357 test_loss: 0.10573 \n",
      "验证损失减少 (0.093734 --> 0.093574). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.08788 valid_loss: 0.09341 test_loss: 0.10542 \n",
      "验证损失减少 (0.093574 --> 0.093409). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.08822 valid_loss: 0.09140 test_loss: 0.10500 \n",
      "验证损失减少 (0.093409 --> 0.091398). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.08683 valid_loss: 0.09232 test_loss: 0.10622 \n",
      "[ 74/500] train_loss: 0.08664 valid_loss: 0.09173 test_loss: 0.10426 \n",
      "[ 75/500] train_loss: 0.08581 valid_loss: 0.09245 test_loss: 0.10638 \n",
      "[ 76/500] train_loss: 0.08496 valid_loss: 0.09402 test_loss: 0.10595 \n",
      "[ 77/500] train_loss: 0.08304 valid_loss: 0.09336 test_loss: 0.10436 \n",
      "[ 78/500] train_loss: 0.08447 valid_loss: 0.08990 test_loss: 0.10282 \n",
      "验证损失减少 (0.091398 --> 0.089900). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08331 valid_loss: 0.09291 test_loss: 0.10388 \n",
      "[ 80/500] train_loss: 0.08562 valid_loss: 0.09121 test_loss: 0.10521 \n",
      "[ 81/500] train_loss: 0.08448 valid_loss: 0.09453 test_loss: 0.10365 \n",
      "[ 82/500] train_loss: 0.08475 valid_loss: 0.09099 test_loss: 0.10215 \n",
      "[ 83/500] train_loss: 0.08293 valid_loss: 0.09443 test_loss: 0.10659 \n",
      "[ 84/500] train_loss: 0.08327 valid_loss: 0.09094 test_loss: 0.10116 \n",
      "[ 85/500] train_loss: 0.08258 valid_loss: 0.09051 test_loss: 0.10279 \n",
      "[ 86/500] train_loss: 0.08469 valid_loss: 0.08866 test_loss: 0.10175 \n",
      "验证损失减少 (0.089900 --> 0.088655). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.08275 valid_loss: 0.08960 test_loss: 0.10164 \n",
      "[ 88/500] train_loss: 0.08350 valid_loss: 0.08968 test_loss: 0.10194 \n",
      "[ 89/500] train_loss: 0.08249 valid_loss: 0.09534 test_loss: 0.10177 \n",
      "[ 90/500] train_loss: 0.08205 valid_loss: 0.09073 test_loss: 0.10122 \n",
      "[ 91/500] train_loss: 0.07996 valid_loss: 0.08937 test_loss: 0.10086 \n",
      "[ 92/500] train_loss: 0.08095 valid_loss: 0.09000 test_loss: 0.10060 \n",
      "[ 93/500] train_loss: 0.07969 valid_loss: 0.09132 test_loss: 0.10108 \n",
      "[ 94/500] train_loss: 0.08101 valid_loss: 0.09150 test_loss: 0.10075 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 95/500] train_loss: 0.08145 valid_loss: 0.08822 test_loss: 0.09951 \n",
      "验证损失减少 (0.088655 --> 0.088224). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.07974 valid_loss: 0.08945 test_loss: 0.10134 \n",
      "[ 97/500] train_loss: 0.08182 valid_loss: 0.08929 test_loss: 0.09955 \n",
      "[ 98/500] train_loss: 0.08217 valid_loss: 0.08812 test_loss: 0.10055 \n",
      "验证损失减少 (0.088224 --> 0.088115). 正在保存模型...\n",
      "[ 99/500] train_loss: 0.07915 valid_loss: 0.08995 test_loss: 0.09883 \n",
      "[100/500] train_loss: 0.07946 valid_loss: 0.09050 test_loss: 0.10011 \n",
      "[101/500] train_loss: 0.07828 valid_loss: 0.09230 test_loss: 0.10191 \n",
      "[102/500] train_loss: 0.08123 valid_loss: 0.09022 test_loss: 0.10018 \n",
      "[103/500] train_loss: 0.07956 valid_loss: 0.08603 test_loss: 0.09902 \n",
      "验证损失减少 (0.088115 --> 0.086029). 正在保存模型...\n",
      "[104/500] train_loss: 0.07969 valid_loss: 0.08999 test_loss: 0.10090 \n",
      "[105/500] train_loss: 0.08070 valid_loss: 0.09468 test_loss: 0.10258 \n",
      "[106/500] train_loss: 0.07999 valid_loss: 0.08656 test_loss: 0.09964 \n",
      "[107/500] train_loss: 0.07672 valid_loss: 0.08828 test_loss: 0.09765 \n",
      "[108/500] train_loss: 0.07768 valid_loss: 0.08729 test_loss: 0.09947 \n",
      "[109/500] train_loss: 0.07765 valid_loss: 0.08616 test_loss: 0.09885 \n",
      "[110/500] train_loss: 0.07768 valid_loss: 0.08800 test_loss: 0.09858 \n",
      "[111/500] train_loss: 0.07706 valid_loss: 0.08658 test_loss: 0.09689 \n",
      "[112/500] train_loss: 0.07667 valid_loss: 0.08616 test_loss: 0.09868 \n",
      "[113/500] train_loss: 0.07746 valid_loss: 0.08850 test_loss: 0.09841 \n",
      "[114/500] train_loss: 0.07663 valid_loss: 0.08809 test_loss: 0.09663 \n",
      "[115/500] train_loss: 0.07636 valid_loss: 0.08619 test_loss: 0.09663 \n",
      "[116/500] train_loss: 0.07793 valid_loss: 0.08636 test_loss: 0.09775 \n",
      "[117/500] train_loss: 0.07777 valid_loss: 0.08516 test_loss: 0.09733 \n",
      "验证损失减少 (0.086029 --> 0.085156). 正在保存模型...\n",
      "[118/500] train_loss: 0.07580 valid_loss: 0.08943 test_loss: 0.09676 \n",
      "[119/500] train_loss: 0.07579 valid_loss: 0.08786 test_loss: 0.09782 \n",
      "[120/500] train_loss: 0.07569 valid_loss: 0.08633 test_loss: 0.09705 \n",
      "[121/500] train_loss: 0.07509 valid_loss: 0.08762 test_loss: 0.09822 \n",
      "[122/500] train_loss: 0.07576 valid_loss: 0.08729 test_loss: 0.09776 \n",
      "[123/500] train_loss: 0.07523 valid_loss: 0.08850 test_loss: 0.09614 \n",
      "[124/500] train_loss: 0.07595 valid_loss: 0.08624 test_loss: 0.09747 \n",
      "[125/500] train_loss: 0.07671 valid_loss: 0.08940 test_loss: 0.09777 \n",
      "[126/500] train_loss: 0.07528 valid_loss: 0.08638 test_loss: 0.09719 \n",
      "[127/500] train_loss: 0.07382 valid_loss: 0.08530 test_loss: 0.09562 \n",
      "[128/500] train_loss: 0.07653 valid_loss: 0.08376 test_loss: 0.09669 \n",
      "验证损失减少 (0.085156 --> 0.083758). 正在保存模型...\n",
      "[129/500] train_loss: 0.07203 valid_loss: 0.08386 test_loss: 0.09718 \n",
      "[130/500] train_loss: 0.07582 valid_loss: 0.08337 test_loss: 0.09478 \n",
      "验证损失减少 (0.083758 --> 0.083369). 正在保存模型...\n",
      "[131/500] train_loss: 0.07356 valid_loss: 0.08313 test_loss: 0.09679 \n",
      "验证损失减少 (0.083369 --> 0.083130). 正在保存模型...\n",
      "[132/500] train_loss: 0.07537 valid_loss: 0.08527 test_loss: 0.09492 \n",
      "[133/500] train_loss: 0.07644 valid_loss: 0.08456 test_loss: 0.09538 \n",
      "[134/500] train_loss: 0.07690 valid_loss: 0.08400 test_loss: 0.09394 \n",
      "[135/500] train_loss: 0.07461 valid_loss: 0.08659 test_loss: 0.09656 \n",
      "[136/500] train_loss: 0.07445 valid_loss: 0.08905 test_loss: 0.09494 \n",
      "[137/500] train_loss: 0.07460 valid_loss: 0.08596 test_loss: 0.09570 \n",
      "[138/500] train_loss: 0.07572 valid_loss: 0.08635 test_loss: 0.09602 \n",
      "[139/500] train_loss: 0.07288 valid_loss: 0.08832 test_loss: 0.09615 \n",
      "[140/500] train_loss: 0.07174 valid_loss: 0.08766 test_loss: 0.09595 \n",
      "[141/500] train_loss: 0.07436 valid_loss: 0.08596 test_loss: 0.09495 \n",
      "[142/500] train_loss: 0.07128 valid_loss: 0.08581 test_loss: 0.09435 \n",
      "[143/500] train_loss: 0.07246 valid_loss: 0.08566 test_loss: 0.09623 \n",
      "[144/500] train_loss: 0.07127 valid_loss: 0.08657 test_loss: 0.09441 \n",
      "[145/500] train_loss: 0.07462 valid_loss: 0.08334 test_loss: 0.09522 \n",
      "[146/500] train_loss: 0.07152 valid_loss: 0.08246 test_loss: 0.09568 \n",
      "验证损失减少 (0.083130 --> 0.082464). 正在保存模型...\n",
      "[147/500] train_loss: 0.07305 valid_loss: 0.08693 test_loss: 0.09534 \n",
      "[148/500] train_loss: 0.07231 valid_loss: 0.08262 test_loss: 0.09352 \n",
      "[149/500] train_loss: 0.07383 valid_loss: 0.08250 test_loss: 0.09185 \n",
      "[150/500] train_loss: 0.07171 valid_loss: 0.08243 test_loss: 0.09403 \n",
      "验证损失减少 (0.082464 --> 0.082427). 正在保存模型...\n",
      "[151/500] train_loss: 0.06955 valid_loss: 0.08302 test_loss: 0.09318 \n",
      "[152/500] train_loss: 0.07136 valid_loss: 0.08211 test_loss: 0.09313 \n",
      "验证损失减少 (0.082427 --> 0.082111). 正在保存模型...\n",
      "[153/500] train_loss: 0.07205 valid_loss: 0.08961 test_loss: 0.09346 \n",
      "[154/500] train_loss: 0.07331 valid_loss: 0.08173 test_loss: 0.09223 \n",
      "验证损失减少 (0.082111 --> 0.081728). 正在保存模型...\n",
      "[155/500] train_loss: 0.07028 valid_loss: 0.08184 test_loss: 0.09509 \n",
      "[156/500] train_loss: 0.07113 valid_loss: 0.09171 test_loss: 0.09502 \n",
      "[157/500] train_loss: 0.06983 valid_loss: 0.08418 test_loss: 0.09348 \n",
      "[158/500] train_loss: 0.06998 valid_loss: 0.09001 test_loss: 0.09361 \n",
      "[159/500] train_loss: 0.07085 valid_loss: 0.08356 test_loss: 0.09468 \n",
      "[160/500] train_loss: 0.07327 valid_loss: 0.08528 test_loss: 0.09228 \n",
      "[161/500] train_loss: 0.07056 valid_loss: 0.08194 test_loss: 0.09307 \n",
      "[162/500] train_loss: 0.07061 valid_loss: 0.08043 test_loss: 0.09335 \n",
      "验证损失减少 (0.081728 --> 0.080435). 正在保存模型...\n",
      "[163/500] train_loss: 0.07029 valid_loss: 0.08021 test_loss: 0.09306 \n",
      "验证损失减少 (0.080435 --> 0.080212). 正在保存模型...\n",
      "[164/500] train_loss: 0.07061 valid_loss: 0.08418 test_loss: 0.09197 \n",
      "[165/500] train_loss: 0.06898 valid_loss: 0.08417 test_loss: 0.09339 \n",
      "[166/500] train_loss: 0.06965 valid_loss: 0.09025 test_loss: 0.09310 \n",
      "[167/500] train_loss: 0.06960 valid_loss: 0.08418 test_loss: 0.09268 \n",
      "[168/500] train_loss: 0.06903 valid_loss: 0.08831 test_loss: 0.09318 \n",
      "[169/500] train_loss: 0.06859 valid_loss: 0.08711 test_loss: 0.09142 \n",
      "[170/500] train_loss: 0.07014 valid_loss: 0.08133 test_loss: 0.09337 \n",
      "[171/500] train_loss: 0.06809 valid_loss: 0.08194 test_loss: 0.09427 \n",
      "[172/500] train_loss: 0.07032 valid_loss: 0.08256 test_loss: 0.09190 \n",
      "[173/500] train_loss: 0.06904 valid_loss: 0.08272 test_loss: 0.09572 \n",
      "[174/500] train_loss: 0.06746 valid_loss: 0.08013 test_loss: 0.09218 \n",
      "验证损失减少 (0.080212 --> 0.080130). 正在保存模型...\n",
      "[175/500] train_loss: 0.06714 valid_loss: 0.08065 test_loss: 0.09305 \n",
      "[176/500] train_loss: 0.06978 valid_loss: 0.08517 test_loss: 0.09157 \n",
      "[177/500] train_loss: 0.06892 valid_loss: 0.08324 test_loss: 0.09237 \n",
      "[178/500] train_loss: 0.06759 valid_loss: 0.08599 test_loss: 0.09307 \n",
      "[179/500] train_loss: 0.06759 valid_loss: 0.08804 test_loss: 0.09207 \n",
      "[180/500] train_loss: 0.06851 valid_loss: 0.08590 test_loss: 0.09233 \n",
      "[181/500] train_loss: 0.06927 valid_loss: 0.08305 test_loss: 0.09172 \n",
      "[182/500] train_loss: 0.06914 valid_loss: 0.08888 test_loss: 0.09090 \n",
      "[183/500] train_loss: 0.06886 valid_loss: 0.08691 test_loss: 0.09223 \n",
      "[184/500] train_loss: 0.06985 valid_loss: 0.08630 test_loss: 0.09233 \n",
      "[185/500] train_loss: 0.06868 valid_loss: 0.08000 test_loss: 0.09065 \n",
      "验证损失减少 (0.080130 --> 0.080000). 正在保存模型...\n",
      "[186/500] train_loss: 0.06848 valid_loss: 0.07993 test_loss: 0.09167 \n",
      "验证损失减少 (0.080000 --> 0.079928). 正在保存模型...\n",
      "[187/500] train_loss: 0.06798 valid_loss: 0.08208 test_loss: 0.09039 \n",
      "[188/500] train_loss: 0.06704 valid_loss: 0.08692 test_loss: 0.09146 \n",
      "[189/500] train_loss: 0.06813 valid_loss: 0.08481 test_loss: 0.09177 \n",
      "[190/500] train_loss: 0.06767 valid_loss: 0.08212 test_loss: 0.09373 \n",
      "[191/500] train_loss: 0.06827 valid_loss: 0.08086 test_loss: 0.09143 \n",
      "[192/500] train_loss: 0.06733 valid_loss: 0.07905 test_loss: 0.09111 \n",
      "验证损失减少 (0.079928 --> 0.079052). 正在保存模型...\n",
      "[193/500] train_loss: 0.06788 valid_loss: 0.08036 test_loss: 0.09153 \n",
      "[194/500] train_loss: 0.06761 valid_loss: 0.08015 test_loss: 0.09158 \n",
      "[195/500] train_loss: 0.06724 valid_loss: 0.07998 test_loss: 0.09177 \n",
      "[196/500] train_loss: 0.06777 valid_loss: 0.08002 test_loss: 0.09136 \n",
      "[197/500] train_loss: 0.06692 valid_loss: 0.07889 test_loss: 0.09116 \n",
      "验证损失减少 (0.079052 --> 0.078888). 正在保存模型...\n",
      "[198/500] train_loss: 0.06606 valid_loss: 0.08602 test_loss: 0.09260 \n",
      "[199/500] train_loss: 0.06729 valid_loss: 0.08215 test_loss: 0.09078 \n",
      "[200/500] train_loss: 0.06680 valid_loss: 0.08910 test_loss: 0.09156 \n",
      "[201/500] train_loss: 0.06618 valid_loss: 0.08192 test_loss: 0.09089 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202/500] train_loss: 0.06388 valid_loss: 0.08522 test_loss: 0.09216 \n",
      "[203/500] train_loss: 0.06616 valid_loss: 0.08334 test_loss: 0.09223 \n",
      "[204/500] train_loss: 0.06677 valid_loss: 0.08907 test_loss: 0.09189 \n",
      "[205/500] train_loss: 0.06875 valid_loss: 0.08030 test_loss: 0.08955 \n",
      "[206/500] train_loss: 0.06605 valid_loss: 0.08685 test_loss: 0.08997 \n",
      "[207/500] train_loss: 0.06565 valid_loss: 0.08379 test_loss: 0.09099 \n",
      "[208/500] train_loss: 0.06574 valid_loss: 0.08567 test_loss: 0.09054 \n",
      "[209/500] train_loss: 0.06607 valid_loss: 0.07996 test_loss: 0.09158 \n",
      "[210/500] train_loss: 0.06554 valid_loss: 0.08511 test_loss: 0.08996 \n",
      "[211/500] train_loss: 0.06792 valid_loss: 0.08527 test_loss: 0.09050 \n",
      "[212/500] train_loss: 0.06717 valid_loss: 0.08463 test_loss: 0.09151 \n",
      "[213/500] train_loss: 0.06745 valid_loss: 0.08445 test_loss: 0.09003 \n",
      "[214/500] train_loss: 0.06585 valid_loss: 0.08507 test_loss: 0.08959 \n",
      "[215/500] train_loss: 0.06686 valid_loss: 0.08322 test_loss: 0.09069 \n",
      "[216/500] train_loss: 0.06582 valid_loss: 0.08124 test_loss: 0.09154 \n",
      "[217/500] train_loss: 0.06586 valid_loss: 0.08045 test_loss: 0.09087 \n",
      "[218/500] train_loss: 0.06465 valid_loss: 0.07883 test_loss: 0.08987 \n",
      "验证损失减少 (0.078888 --> 0.078827). 正在保存模型...\n",
      "[219/500] train_loss: 0.06661 valid_loss: 0.08322 test_loss: 0.09091 \n",
      "[220/500] train_loss: 0.06574 valid_loss: 0.08048 test_loss: 0.09113 \n",
      "[221/500] train_loss: 0.06578 valid_loss: 0.08495 test_loss: 0.09124 \n",
      "[222/500] train_loss: 0.06543 valid_loss: 0.08291 test_loss: 0.09103 \n",
      "[223/500] train_loss: 0.06455 valid_loss: 0.07928 test_loss: 0.09085 \n",
      "[224/500] train_loss: 0.06596 valid_loss: 0.08055 test_loss: 0.08800 \n",
      "[225/500] train_loss: 0.06372 valid_loss: 0.08294 test_loss: 0.08975 \n",
      "[226/500] train_loss: 0.06489 valid_loss: 0.07740 test_loss: 0.09030 \n",
      "验证损失减少 (0.078827 --> 0.077401). 正在保存模型...\n",
      "[227/500] train_loss: 0.06689 valid_loss: 0.07792 test_loss: 0.09105 \n",
      "[228/500] train_loss: 0.06573 valid_loss: 0.08459 test_loss: 0.09063 \n",
      "[229/500] train_loss: 0.06470 valid_loss: 0.08357 test_loss: 0.08988 \n",
      "[230/500] train_loss: 0.06464 valid_loss: 0.08582 test_loss: 0.09271 \n",
      "[231/500] train_loss: 0.06487 valid_loss: 0.08385 test_loss: 0.09226 \n",
      "[232/500] train_loss: 0.06441 valid_loss: 0.08290 test_loss: 0.09171 \n",
      "[233/500] train_loss: 0.06552 valid_loss: 0.08066 test_loss: 0.08962 \n",
      "[234/500] train_loss: 0.06369 valid_loss: 0.08047 test_loss: 0.09067 \n",
      "[235/500] train_loss: 0.06359 valid_loss: 0.08023 test_loss: 0.09249 \n",
      "[236/500] train_loss: 0.06525 valid_loss: 0.08030 test_loss: 0.09168 \n",
      "[237/500] train_loss: 0.06662 valid_loss: 0.08487 test_loss: 0.08981 \n",
      "[238/500] train_loss: 0.06486 valid_loss: 0.07986 test_loss: 0.08958 \n",
      "[239/500] train_loss: 0.06391 valid_loss: 0.08551 test_loss: 0.08931 \n",
      "[240/500] train_loss: 0.06450 valid_loss: 0.08247 test_loss: 0.08893 \n",
      "[241/500] train_loss: 0.06425 valid_loss: 0.07961 test_loss: 0.09303 \n",
      "[242/500] train_loss: 0.06552 valid_loss: 0.08865 test_loss: 0.08936 \n",
      "[243/500] train_loss: 0.06389 valid_loss: 0.07849 test_loss: 0.08908 \n",
      "[244/500] train_loss: 0.06432 valid_loss: 0.07963 test_loss: 0.08935 \n",
      "[245/500] train_loss: 0.06140 valid_loss: 0.08231 test_loss: 0.09084 \n",
      "[246/500] train_loss: 0.06550 valid_loss: 0.08025 test_loss: 0.09049 \n",
      "[247/500] train_loss: 0.06221 valid_loss: 0.08095 test_loss: 0.09041 \n",
      "[248/500] train_loss: 0.06438 valid_loss: 0.07893 test_loss: 0.08941 \n",
      "[249/500] train_loss: 0.06462 valid_loss: 0.07770 test_loss: 0.09075 \n",
      "[250/500] train_loss: 0.06208 valid_loss: 0.08120 test_loss: 0.09014 \n",
      "[251/500] train_loss: 0.06268 valid_loss: 0.07932 test_loss: 0.09010 \n",
      "[252/500] train_loss: 0.06426 valid_loss: 0.08841 test_loss: 0.09138 \n",
      "[253/500] train_loss: 0.06330 valid_loss: 0.08314 test_loss: 0.09061 \n",
      "[254/500] train_loss: 0.06143 valid_loss: 0.07674 test_loss: 0.08928 \n",
      "验证损失减少 (0.077401 --> 0.076744). 正在保存模型...\n",
      "[255/500] train_loss: 0.06058 valid_loss: 0.08079 test_loss: 0.08981 \n",
      "[256/500] train_loss: 0.06540 valid_loss: 0.07868 test_loss: 0.08891 \n",
      "[257/500] train_loss: 0.06205 valid_loss: 0.07737 test_loss: 0.09009 \n",
      "[258/500] train_loss: 0.06238 valid_loss: 0.07846 test_loss: 0.09003 \n",
      "[259/500] train_loss: 0.06471 valid_loss: 0.07709 test_loss: 0.08941 \n",
      "[260/500] train_loss: 0.06392 valid_loss: 0.07678 test_loss: 0.09040 \n",
      "[261/500] train_loss: 0.06215 valid_loss: 0.07520 test_loss: 0.08855 \n",
      "验证损失减少 (0.076744 --> 0.075205). 正在保存模型...\n",
      "[262/500] train_loss: 0.06149 valid_loss: 0.07647 test_loss: 0.09050 \n",
      "[263/500] train_loss: 0.06093 valid_loss: 0.07745 test_loss: 0.09035 \n",
      "[264/500] train_loss: 0.06300 valid_loss: 0.07962 test_loss: 0.08905 \n",
      "[265/500] train_loss: 0.06028 valid_loss: 0.07772 test_loss: 0.08983 \n",
      "[266/500] train_loss: 0.06055 valid_loss: 0.07870 test_loss: 0.09008 \n",
      "[267/500] train_loss: 0.06284 valid_loss: 0.07765 test_loss: 0.08986 \n",
      "[268/500] train_loss: 0.06064 valid_loss: 0.07750 test_loss: 0.09000 \n",
      "[269/500] train_loss: 0.06054 valid_loss: 0.07691 test_loss: 0.09047 \n",
      "[270/500] train_loss: 0.06257 valid_loss: 0.07726 test_loss: 0.09020 \n",
      "[271/500] train_loss: 0.06046 valid_loss: 0.07906 test_loss: 0.09003 \n",
      "[272/500] train_loss: 0.06073 valid_loss: 0.08233 test_loss: 0.09027 \n",
      "[273/500] train_loss: 0.06141 valid_loss: 0.08245 test_loss: 0.08982 \n",
      "[274/500] train_loss: 0.06226 valid_loss: 0.08605 test_loss: 0.08832 \n",
      "[275/500] train_loss: 0.06162 valid_loss: 0.07951 test_loss: 0.08879 \n",
      "[276/500] train_loss: 0.06117 valid_loss: 0.07853 test_loss: 0.08914 \n",
      "[277/500] train_loss: 0.06265 valid_loss: 0.08059 test_loss: 0.09133 \n",
      "[278/500] train_loss: 0.06055 valid_loss: 0.07999 test_loss: 0.08957 \n",
      "[279/500] train_loss: 0.06086 valid_loss: 0.07692 test_loss: 0.08935 \n",
      "[280/500] train_loss: 0.06120 valid_loss: 0.07763 test_loss: 0.08926 \n",
      "[281/500] train_loss: 0.06244 valid_loss: 0.07959 test_loss: 0.08868 \n",
      "[282/500] train_loss: 0.06039 valid_loss: 0.08047 test_loss: 0.09029 \n",
      "[283/500] train_loss: 0.06111 valid_loss: 0.07729 test_loss: 0.08988 \n",
      "[284/500] train_loss: 0.06077 valid_loss: 0.07690 test_loss: 0.08912 \n",
      "[285/500] train_loss: 0.06376 valid_loss: 0.07637 test_loss: 0.08984 \n",
      "[286/500] train_loss: 0.06003 valid_loss: 0.07846 test_loss: 0.09136 \n",
      "[287/500] train_loss: 0.05957 valid_loss: 0.07634 test_loss: 0.08846 \n",
      "[288/500] train_loss: 0.06356 valid_loss: 0.07763 test_loss: 0.08821 \n",
      "[289/500] train_loss: 0.06179 valid_loss: 0.08269 test_loss: 0.09041 \n",
      "[290/500] train_loss: 0.06088 valid_loss: 0.07814 test_loss: 0.08903 \n",
      "[291/500] train_loss: 0.05944 valid_loss: 0.08037 test_loss: 0.09087 \n",
      "[292/500] train_loss: 0.06005 valid_loss: 0.07888 test_loss: 0.08969 \n",
      "[293/500] train_loss: 0.06031 valid_loss: 0.08210 test_loss: 0.08989 \n",
      "[294/500] train_loss: 0.06141 valid_loss: 0.08385 test_loss: 0.09079 \n",
      "[295/500] train_loss: 0.06149 valid_loss: 0.08031 test_loss: 0.09065 \n",
      "[296/500] train_loss: 0.06042 valid_loss: 0.07616 test_loss: 0.09079 \n",
      "[297/500] train_loss: 0.05968 valid_loss: 0.07842 test_loss: 0.08981 \n",
      "[298/500] train_loss: 0.06057 valid_loss: 0.08010 test_loss: 0.08860 \n",
      "[299/500] train_loss: 0.06065 valid_loss: 0.07886 test_loss: 0.09076 \n",
      "[300/500] train_loss: 0.05929 valid_loss: 0.07930 test_loss: 0.08996 \n",
      "[301/500] train_loss: 0.05977 valid_loss: 0.07778 test_loss: 0.08872 \n",
      "[302/500] train_loss: 0.05964 valid_loss: 0.07683 test_loss: 0.08942 \n",
      "[303/500] train_loss: 0.05958 valid_loss: 0.07741 test_loss: 0.09066 \n",
      "[304/500] train_loss: 0.05999 valid_loss: 0.07620 test_loss: 0.09084 \n",
      "[305/500] train_loss: 0.06094 valid_loss: 0.07875 test_loss: 0.08939 \n",
      "[306/500] train_loss: 0.05999 valid_loss: 0.08271 test_loss: 0.09009 \n",
      "[307/500] train_loss: 0.06049 valid_loss: 0.08126 test_loss: 0.09083 \n",
      "[308/500] train_loss: 0.05992 valid_loss: 0.08039 test_loss: 0.08988 \n",
      "[309/500] train_loss: 0.05861 valid_loss: 0.08102 test_loss: 0.08958 \n",
      "[310/500] train_loss: 0.05763 valid_loss: 0.08376 test_loss: 0.08863 \n",
      "[311/500] train_loss: 0.05977 valid_loss: 0.08659 test_loss: 0.08950 \n",
      "[312/500] train_loss: 0.05918 valid_loss: 0.07938 test_loss: 0.09043 \n",
      "[313/500] train_loss: 0.05973 valid_loss: 0.08242 test_loss: 0.08858 \n",
      "[314/500] train_loss: 0.05962 valid_loss: 0.08158 test_loss: 0.08977 \n",
      "[315/500] train_loss: 0.05871 valid_loss: 0.08194 test_loss: 0.08902 \n",
      "[316/500] train_loss: 0.06012 valid_loss: 0.07928 test_loss: 0.08955 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[317/500] train_loss: 0.05882 valid_loss: 0.07541 test_loss: 0.08889 \n",
      "[318/500] train_loss: 0.05827 valid_loss: 0.08423 test_loss: 0.08944 \n",
      "[319/500] train_loss: 0.05913 valid_loss: 0.08357 test_loss: 0.08935 \n",
      "[320/500] train_loss: 0.05896 valid_loss: 0.07896 test_loss: 0.08946 \n",
      "[321/500] train_loss: 0.06038 valid_loss: 0.07873 test_loss: 0.09039 \n",
      "[322/500] train_loss: 0.05958 valid_loss: 0.07820 test_loss: 0.09082 \n",
      "[323/500] train_loss: 0.05840 valid_loss: 0.08317 test_loss: 0.08808 \n",
      "[324/500] train_loss: 0.06053 valid_loss: 0.08351 test_loss: 0.08913 \n",
      "[325/500] train_loss: 0.05788 valid_loss: 0.07947 test_loss: 0.08903 \n",
      "[326/500] train_loss: 0.06007 valid_loss: 0.08171 test_loss: 0.08911 \n",
      "[327/500] train_loss: 0.05706 valid_loss: 0.08054 test_loss: 0.08845 \n",
      "[328/500] train_loss: 0.05904 valid_loss: 0.07840 test_loss: 0.08929 \n",
      "[329/500] train_loss: 0.05972 valid_loss: 0.08299 test_loss: 0.08946 \n",
      "[330/500] train_loss: 0.05867 valid_loss: 0.08057 test_loss: 0.08887 \n",
      "[331/500] train_loss: 0.05794 valid_loss: 0.08025 test_loss: 0.08980 \n",
      "[332/500] train_loss: 0.05855 valid_loss: 0.07687 test_loss: 0.08737 \n",
      "[333/500] train_loss: 0.05927 valid_loss: 0.07618 test_loss: 0.08832 \n",
      "[334/500] train_loss: 0.05743 valid_loss: 0.07942 test_loss: 0.08746 \n",
      "[335/500] train_loss: 0.05952 valid_loss: 0.07559 test_loss: 0.08972 \n",
      "[336/500] train_loss: 0.05949 valid_loss: 0.07990 test_loss: 0.08802 \n",
      "[337/500] train_loss: 0.06120 valid_loss: 0.07613 test_loss: 0.08806 \n",
      "[338/500] train_loss: 0.05980 valid_loss: 0.07597 test_loss: 0.08845 \n",
      "[339/500] train_loss: 0.05740 valid_loss: 0.07759 test_loss: 0.08846 \n",
      "[340/500] train_loss: 0.05761 valid_loss: 0.07612 test_loss: 0.08847 \n",
      "[341/500] train_loss: 0.05807 valid_loss: 0.07639 test_loss: 0.08838 \n",
      "[342/500] train_loss: 0.05966 valid_loss: 0.07601 test_loss: 0.08808 \n",
      "[343/500] train_loss: 0.05698 valid_loss: 0.07816 test_loss: 0.08932 \n",
      "[344/500] train_loss: 0.05841 valid_loss: 0.07579 test_loss: 0.08828 \n",
      "[345/500] train_loss: 0.05971 valid_loss: 0.07603 test_loss: 0.08973 \n",
      "[346/500] train_loss: 0.05768 valid_loss: 0.08654 test_loss: 0.08875 \n",
      "[347/500] train_loss: 0.05801 valid_loss: 0.07653 test_loss: 0.08820 \n",
      "[348/500] train_loss: 0.05651 valid_loss: 0.08228 test_loss: 0.08915 \n",
      "[349/500] train_loss: 0.05663 valid_loss: 0.07889 test_loss: 0.08843 \n",
      "[350/500] train_loss: 0.05792 valid_loss: 0.07784 test_loss: 0.08957 \n",
      "[351/500] train_loss: 0.05764 valid_loss: 0.07514 test_loss: 0.08979 \n",
      "验证损失减少 (0.075205 --> 0.075136). 正在保存模型...\n",
      "[352/500] train_loss: 0.05803 valid_loss: 0.07766 test_loss: 0.08862 \n",
      "[353/500] train_loss: 0.05642 valid_loss: 0.08265 test_loss: 0.08867 \n",
      "[354/500] train_loss: 0.05783 valid_loss: 0.07491 test_loss: 0.08792 \n",
      "验证损失减少 (0.075136 --> 0.074906). 正在保存模型...\n",
      "[355/500] train_loss: 0.05899 valid_loss: 0.07838 test_loss: 0.08848 \n",
      "[356/500] train_loss: 0.05747 valid_loss: 0.07641 test_loss: 0.08835 \n",
      "[357/500] train_loss: 0.05762 valid_loss: 0.07976 test_loss: 0.08866 \n",
      "[358/500] train_loss: 0.05518 valid_loss: 0.07854 test_loss: 0.08828 \n",
      "[359/500] train_loss: 0.05630 valid_loss: 0.08505 test_loss: 0.08863 \n",
      "[360/500] train_loss: 0.05557 valid_loss: 0.07936 test_loss: 0.08687 \n",
      "[361/500] train_loss: 0.05722 valid_loss: 0.09207 test_loss: 0.08903 \n",
      "[362/500] train_loss: 0.05628 valid_loss: 0.08407 test_loss: 0.08887 \n",
      "[363/500] train_loss: 0.05908 valid_loss: 0.07839 test_loss: 0.08747 \n",
      "[364/500] train_loss: 0.05580 valid_loss: 0.07637 test_loss: 0.08697 \n",
      "[365/500] train_loss: 0.05682 valid_loss: 0.08064 test_loss: 0.08830 \n",
      "[366/500] train_loss: 0.05556 valid_loss: 0.08186 test_loss: 0.08995 \n",
      "[367/500] train_loss: 0.05468 valid_loss: 0.07638 test_loss: 0.09023 \n",
      "[368/500] train_loss: 0.05603 valid_loss: 0.07696 test_loss: 0.08988 \n",
      "[369/500] train_loss: 0.05592 valid_loss: 0.07473 test_loss: 0.08907 \n",
      "验证损失减少 (0.074906 --> 0.074725). 正在保存模型...\n",
      "[370/500] train_loss: 0.05766 valid_loss: 0.07487 test_loss: 0.08958 \n",
      "[371/500] train_loss: 0.05810 valid_loss: 0.08355 test_loss: 0.08960 \n",
      "[372/500] train_loss: 0.05624 valid_loss: 0.08300 test_loss: 0.09182 \n",
      "[373/500] train_loss: 0.05706 valid_loss: 0.08528 test_loss: 0.09091 \n",
      "[374/500] train_loss: 0.05558 valid_loss: 0.08141 test_loss: 0.08934 \n",
      "[375/500] train_loss: 0.05800 valid_loss: 0.08254 test_loss: 0.08957 \n",
      "[376/500] train_loss: 0.05791 valid_loss: 0.07820 test_loss: 0.09014 \n",
      "[377/500] train_loss: 0.05603 valid_loss: 0.07972 test_loss: 0.09062 \n",
      "[378/500] train_loss: 0.05646 valid_loss: 0.08284 test_loss: 0.08849 \n",
      "[379/500] train_loss: 0.05630 valid_loss: 0.08040 test_loss: 0.08843 \n",
      "[380/500] train_loss: 0.05570 valid_loss: 0.08046 test_loss: 0.08883 \n",
      "[381/500] train_loss: 0.05618 valid_loss: 0.08343 test_loss: 0.08904 \n",
      "[382/500] train_loss: 0.05631 valid_loss: 0.08163 test_loss: 0.09015 \n",
      "[383/500] train_loss: 0.05541 valid_loss: 0.08229 test_loss: 0.08829 \n",
      "[384/500] train_loss: 0.05545 valid_loss: 0.07845 test_loss: 0.08970 \n",
      "[385/500] train_loss: 0.05630 valid_loss: 0.07673 test_loss: 0.08734 \n",
      "[386/500] train_loss: 0.05594 valid_loss: 0.09213 test_loss: 0.08787 \n",
      "[387/500] train_loss: 0.05491 valid_loss: 0.08020 test_loss: 0.08930 \n",
      "[388/500] train_loss: 0.05621 valid_loss: 0.08203 test_loss: 0.08919 \n",
      "[389/500] train_loss: 0.05496 valid_loss: 0.09384 test_loss: 0.08956 \n",
      "[390/500] train_loss: 0.05585 valid_loss: 0.07830 test_loss: 0.08826 \n",
      "[391/500] train_loss: 0.05657 valid_loss: 0.08997 test_loss: 0.08887 \n",
      "[392/500] train_loss: 0.05638 valid_loss: 0.09082 test_loss: 0.08734 \n",
      "[393/500] train_loss: 0.05505 valid_loss: 0.09323 test_loss: 0.08761 \n",
      "[394/500] train_loss: 0.05539 valid_loss: 0.09353 test_loss: 0.08678 \n",
      "[395/500] train_loss: 0.05331 valid_loss: 0.08162 test_loss: 0.08918 \n",
      "[396/500] train_loss: 0.05623 valid_loss: 0.08716 test_loss: 0.08799 \n",
      "[397/500] train_loss: 0.05448 valid_loss: 0.08654 test_loss: 0.08897 \n",
      "[398/500] train_loss: 0.05577 valid_loss: 0.08928 test_loss: 0.08839 \n",
      "[399/500] train_loss: 0.05533 valid_loss: 0.08528 test_loss: 0.08882 \n",
      "[400/500] train_loss: 0.05417 valid_loss: 0.08599 test_loss: 0.08991 \n",
      "[401/500] train_loss: 0.05791 valid_loss: 0.07972 test_loss: 0.08865 \n",
      "[402/500] train_loss: 0.05461 valid_loss: 0.08695 test_loss: 0.08737 \n",
      "[403/500] train_loss: 0.05443 valid_loss: 0.08617 test_loss: 0.08885 \n",
      "[404/500] train_loss: 0.05583 valid_loss: 0.08597 test_loss: 0.08787 \n",
      "[405/500] train_loss: 0.05706 valid_loss: 0.08674 test_loss: 0.08860 \n",
      "[406/500] train_loss: 0.05524 valid_loss: 0.08627 test_loss: 0.08867 \n",
      "[407/500] train_loss: 0.05388 valid_loss: 0.08257 test_loss: 0.08830 \n",
      "[408/500] train_loss: 0.05611 valid_loss: 0.09065 test_loss: 0.08909 \n",
      "[409/500] train_loss: 0.05558 valid_loss: 0.09945 test_loss: 0.09081 \n",
      "[410/500] train_loss: 0.05433 valid_loss: 0.08084 test_loss: 0.08900 \n",
      "[411/500] train_loss: 0.05490 valid_loss: 0.08463 test_loss: 0.08993 \n",
      "[412/500] train_loss: 0.05416 valid_loss: 0.09793 test_loss: 0.08820 \n",
      "[413/500] train_loss: 0.05356 valid_loss: 0.09666 test_loss: 0.08919 \n",
      "[414/500] train_loss: 0.05523 valid_loss: 0.09170 test_loss: 0.08908 \n",
      "[415/500] train_loss: 0.05368 valid_loss: 0.10053 test_loss: 0.08953 \n",
      "[416/500] train_loss: 0.05484 valid_loss: 0.09665 test_loss: 0.08918 \n",
      "[417/500] train_loss: 0.05576 valid_loss: 0.09243 test_loss: 0.09017 \n",
      "[418/500] train_loss: 0.05384 valid_loss: 0.09361 test_loss: 0.08928 \n",
      "[419/500] train_loss: 0.05720 valid_loss: 0.08151 test_loss: 0.08911 \n",
      "[420/500] train_loss: 0.05376 valid_loss: 0.09245 test_loss: 0.08958 \n",
      "[421/500] train_loss: 0.05406 valid_loss: 0.10035 test_loss: 0.08941 \n",
      "[422/500] train_loss: 0.05418 valid_loss: 0.09090 test_loss: 0.08815 \n",
      "[423/500] train_loss: 0.05480 valid_loss: 0.11099 test_loss: 0.08837 \n",
      "[424/500] train_loss: 0.05349 valid_loss: 0.08451 test_loss: 0.08851 \n",
      "[425/500] train_loss: 0.05498 valid_loss: 0.08283 test_loss: 0.08758 \n",
      "[426/500] train_loss: 0.05287 valid_loss: 0.09948 test_loss: 0.08925 \n",
      "[427/500] train_loss: 0.05388 valid_loss: 0.08669 test_loss: 0.08790 \n",
      "[428/500] train_loss: 0.05427 valid_loss: 0.10477 test_loss: 0.08748 \n",
      "[429/500] train_loss: 0.05426 valid_loss: 0.09184 test_loss: 0.08791 \n",
      "[430/500] train_loss: 0.05286 valid_loss: 0.10338 test_loss: 0.08822 \n",
      "[431/500] train_loss: 0.05371 valid_loss: 0.09514 test_loss: 0.08809 \n",
      "[432/500] train_loss: 0.05432 valid_loss: 0.10402 test_loss: 0.08789 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[433/500] train_loss: 0.05516 valid_loss: 0.09299 test_loss: 0.08792 \n",
      "[434/500] train_loss: 0.05499 valid_loss: 0.11279 test_loss: 0.08802 \n",
      "[435/500] train_loss: 0.05395 valid_loss: 0.09059 test_loss: 0.09020 \n",
      "[436/500] train_loss: 0.05395 valid_loss: 0.09998 test_loss: 0.08829 \n",
      "[437/500] train_loss: 0.05260 valid_loss: 0.09556 test_loss: 0.08887 \n",
      "[438/500] train_loss: 0.05529 valid_loss: 0.08403 test_loss: 0.08931 \n",
      "[439/500] train_loss: 0.05359 valid_loss: 0.08415 test_loss: 0.08971 \n",
      "[440/500] train_loss: 0.05288 valid_loss: 0.08228 test_loss: 0.09005 \n",
      "[441/500] train_loss: 0.05600 valid_loss: 0.07642 test_loss: 0.09053 \n",
      "[442/500] train_loss: 0.05383 valid_loss: 0.07740 test_loss: 0.08940 \n",
      "[443/500] train_loss: 0.05299 valid_loss: 0.07964 test_loss: 0.09005 \n",
      "[444/500] train_loss: 0.05413 valid_loss: 0.07580 test_loss: 0.08931 \n",
      "[445/500] train_loss: 0.05345 valid_loss: 0.07746 test_loss: 0.09025 \n",
      "[446/500] train_loss: 0.05463 valid_loss: 0.07548 test_loss: 0.08812 \n",
      "[447/500] train_loss: 0.05359 valid_loss: 0.07489 test_loss: 0.09040 \n",
      "[448/500] train_loss: 0.05303 valid_loss: 0.07665 test_loss: 0.08953 \n",
      "[449/500] train_loss: 0.05188 valid_loss: 0.07990 test_loss: 0.09006 \n",
      "[450/500] train_loss: 0.05136 valid_loss: 0.07790 test_loss: 0.08875 \n",
      "[451/500] train_loss: 0.05372 valid_loss: 0.09148 test_loss: 0.09123 \n",
      "[452/500] train_loss: 0.05262 valid_loss: 0.07506 test_loss: 0.08769 \n",
      "[453/500] train_loss: 0.05311 valid_loss: 0.07886 test_loss: 0.08745 \n",
      "[454/500] train_loss: 0.05387 valid_loss: 0.07798 test_loss: 0.08692 \n",
      "[455/500] train_loss: 0.05146 valid_loss: 0.07661 test_loss: 0.08837 \n",
      "[456/500] train_loss: 0.05335 valid_loss: 0.08869 test_loss: 0.08707 \n",
      "[457/500] train_loss: 0.05263 valid_loss: 0.10077 test_loss: 0.08702 \n",
      "[458/500] train_loss: 0.05374 valid_loss: 0.08523 test_loss: 0.08851 \n",
      "[459/500] train_loss: 0.05324 valid_loss: 0.07862 test_loss: 0.08734 \n",
      "[460/500] train_loss: 0.05450 valid_loss: 0.07446 test_loss: 0.08769 \n",
      "验证损失减少 (0.074725 --> 0.074460). 正在保存模型...\n",
      "[461/500] train_loss: 0.05177 valid_loss: 0.07764 test_loss: 0.08817 \n",
      "[462/500] train_loss: 0.05254 valid_loss: 0.07546 test_loss: 0.08836 \n",
      "[463/500] train_loss: 0.05241 valid_loss: 0.07905 test_loss: 0.08797 \n",
      "[464/500] train_loss: 0.05300 valid_loss: 0.08590 test_loss: 0.08868 \n",
      "[465/500] train_loss: 0.05457 valid_loss: 0.07478 test_loss: 0.08816 \n",
      "[466/500] train_loss: 0.05286 valid_loss: 0.07768 test_loss: 0.08796 \n",
      "[467/500] train_loss: 0.05236 valid_loss: 0.07576 test_loss: 0.08930 \n",
      "[468/500] train_loss: 0.05287 valid_loss: 0.07653 test_loss: 0.08787 \n",
      "[469/500] train_loss: 0.05185 valid_loss: 0.07591 test_loss: 0.08778 \n",
      "[470/500] train_loss: 0.05233 valid_loss: 0.07830 test_loss: 0.08660 \n",
      "[471/500] train_loss: 0.05257 valid_loss: 0.07834 test_loss: 0.08694 \n",
      "[472/500] train_loss: 0.05439 valid_loss: 0.07676 test_loss: 0.08957 \n",
      "[473/500] train_loss: 0.05439 valid_loss: 0.08265 test_loss: 0.08691 \n",
      "[474/500] train_loss: 0.05309 valid_loss: 0.08030 test_loss: 0.08808 \n",
      "[475/500] train_loss: 0.05299 valid_loss: 0.07747 test_loss: 0.08758 \n",
      "[476/500] train_loss: 0.05421 valid_loss: 0.07802 test_loss: 0.08936 \n",
      "[477/500] train_loss: 0.05282 valid_loss: 0.08510 test_loss: 0.08750 \n",
      "[478/500] train_loss: 0.05228 valid_loss: 0.10066 test_loss: 0.08772 \n",
      "[479/500] train_loss: 0.05398 valid_loss: 0.08316 test_loss: 0.08837 \n",
      "[480/500] train_loss: 0.05236 valid_loss: 0.08131 test_loss: 0.08910 \n",
      "[481/500] train_loss: 0.05508 valid_loss: 0.14610 test_loss: 0.08800 \n",
      "[482/500] train_loss: 0.05304 valid_loss: 0.07775 test_loss: 0.08932 \n",
      "[483/500] train_loss: 0.05144 valid_loss: 0.08585 test_loss: 0.09037 \n",
      "[484/500] train_loss: 0.05310 valid_loss: 0.09487 test_loss: 0.08858 \n",
      "[485/500] train_loss: 0.05237 valid_loss: 0.07956 test_loss: 0.08923 \n",
      "[486/500] train_loss: 0.05189 valid_loss: 0.09182 test_loss: 0.08918 \n",
      "[487/500] train_loss: 0.05225 valid_loss: 0.09151 test_loss: 0.08943 \n",
      "[488/500] train_loss: 0.05154 valid_loss: 0.09459 test_loss: 0.08884 \n",
      "[489/500] train_loss: 0.05255 valid_loss: 0.10394 test_loss: 0.08936 \n",
      "[490/500] train_loss: 0.05197 valid_loss: 0.09994 test_loss: 0.08861 \n",
      "[491/500] train_loss: 0.05142 valid_loss: 0.08056 test_loss: 0.08975 \n",
      "[492/500] train_loss: 0.05185 valid_loss: 0.07990 test_loss: 0.08867 \n",
      "[493/500] train_loss: 0.05112 valid_loss: 0.09111 test_loss: 0.08842 \n",
      "[494/500] train_loss: 0.05127 valid_loss: 0.09184 test_loss: 0.08734 \n",
      "[495/500] train_loss: 0.05063 valid_loss: 0.10356 test_loss: 0.09006 \n",
      "[496/500] train_loss: 0.05220 valid_loss: 0.08021 test_loss: 0.08786 \n",
      "[497/500] train_loss: 0.05375 valid_loss: 0.09773 test_loss: 0.08691 \n",
      "[498/500] train_loss: 0.05193 valid_loss: 0.09745 test_loss: 0.08936 \n",
      "[499/500] train_loss: 0.05147 valid_loss: 0.08270 test_loss: 0.08927 \n",
      "[500/500] train_loss: 0.05111 valid_loss: 0.08336 test_loss: 0.08868 \n",
      "TRAINING MODEL 19\n",
      "[  1/500] train_loss: 0.41849 valid_loss: 0.29835 test_loss: 0.30465 \n",
      "验证损失减少 (inf --> 0.298350). 正在保存模型...\n",
      "[  2/500] train_loss: 0.23356 valid_loss: 0.21107 test_loss: 0.21858 \n",
      "验证损失减少 (0.298350 --> 0.211069). 正在保存模型...\n",
      "[  3/500] train_loss: 0.18504 valid_loss: 0.17833 test_loss: 0.18602 \n",
      "验证损失减少 (0.211069 --> 0.178329). 正在保存模型...\n",
      "[  4/500] train_loss: 0.16317 valid_loss: 0.15968 test_loss: 0.17087 \n",
      "验证损失减少 (0.178329 --> 0.159684). 正在保存模型...\n",
      "[  5/500] train_loss: 0.15084 valid_loss: 0.15234 test_loss: 0.16405 \n",
      "验证损失减少 (0.159684 --> 0.152342). 正在保存模型...\n",
      "[  6/500] train_loss: 0.14087 valid_loss: 0.14578 test_loss: 0.15857 \n",
      "验证损失减少 (0.152342 --> 0.145776). 正在保存模型...\n",
      "[  7/500] train_loss: 0.13846 valid_loss: 0.14262 test_loss: 0.15531 \n",
      "验证损失减少 (0.145776 --> 0.142619). 正在保存模型...\n",
      "[  8/500] train_loss: 0.13285 valid_loss: 0.13578 test_loss: 0.14947 \n",
      "验证损失减少 (0.142619 --> 0.135777). 正在保存模型...\n",
      "[  9/500] train_loss: 0.12877 valid_loss: 0.13393 test_loss: 0.14728 \n",
      "验证损失减少 (0.135777 --> 0.133929). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.12886 valid_loss: 0.12992 test_loss: 0.14428 \n",
      "验证损失减少 (0.133929 --> 0.129917). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.12460 valid_loss: 0.12863 test_loss: 0.14260 \n",
      "验证损失减少 (0.129917 --> 0.128625). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.12447 valid_loss: 0.12704 test_loss: 0.14196 \n",
      "验证损失减少 (0.128625 --> 0.127043). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.12137 valid_loss: 0.12272 test_loss: 0.13829 \n",
      "验证损失减少 (0.127043 --> 0.122716). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.11665 valid_loss: 0.12162 test_loss: 0.13577 \n",
      "验证损失减少 (0.122716 --> 0.121619). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.11869 valid_loss: 0.12570 test_loss: 0.13839 \n",
      "[ 16/500] train_loss: 0.11475 valid_loss: 0.11776 test_loss: 0.13308 \n",
      "验证损失减少 (0.121619 --> 0.117755). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.11460 valid_loss: 0.11699 test_loss: 0.13178 \n",
      "验证损失减少 (0.117755 --> 0.116991). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.11040 valid_loss: 0.11622 test_loss: 0.13109 \n",
      "验证损失减少 (0.116991 --> 0.116218). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.11102 valid_loss: 0.11529 test_loss: 0.12951 \n",
      "验证损失减少 (0.116218 --> 0.115287). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.10982 valid_loss: 0.11599 test_loss: 0.12956 \n",
      "[ 21/500] train_loss: 0.10556 valid_loss: 0.11354 test_loss: 0.12775 \n",
      "验证损失减少 (0.115287 --> 0.113541). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.10761 valid_loss: 0.11415 test_loss: 0.12769 \n",
      "[ 23/500] train_loss: 0.10500 valid_loss: 0.11238 test_loss: 0.12602 \n",
      "验证损失减少 (0.113541 --> 0.112380). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.10396 valid_loss: 0.11053 test_loss: 0.12313 \n",
      "验证损失减少 (0.112380 --> 0.110528). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.10485 valid_loss: 0.10879 test_loss: 0.12223 \n",
      "验证损失减少 (0.110528 --> 0.108794). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.10532 valid_loss: 0.11229 test_loss: 0.12302 \n",
      "[ 27/500] train_loss: 0.10422 valid_loss: 0.10600 test_loss: 0.12024 \n",
      "验证损失减少 (0.108794 --> 0.106000). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.10285 valid_loss: 0.10963 test_loss: 0.12196 \n",
      "[ 29/500] train_loss: 0.10125 valid_loss: 0.10642 test_loss: 0.11976 \n",
      "[ 30/500] train_loss: 0.10080 valid_loss: 0.10661 test_loss: 0.11915 \n",
      "[ 31/500] train_loss: 0.09889 valid_loss: 0.10407 test_loss: 0.11814 \n",
      "验证损失减少 (0.106000 --> 0.104069). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.10025 valid_loss: 0.10509 test_loss: 0.11660 \n",
      "[ 33/500] train_loss: 0.09765 valid_loss: 0.10696 test_loss: 0.11732 \n",
      "[ 34/500] train_loss: 0.09466 valid_loss: 0.10636 test_loss: 0.11850 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 35/500] train_loss: 0.09677 valid_loss: 0.10181 test_loss: 0.11382 \n",
      "验证损失减少 (0.104069 --> 0.101813). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.09629 valid_loss: 0.10307 test_loss: 0.11665 \n",
      "[ 37/500] train_loss: 0.09748 valid_loss: 0.10390 test_loss: 0.11482 \n",
      "[ 38/500] train_loss: 0.09668 valid_loss: 0.10163 test_loss: 0.11169 \n",
      "验证损失减少 (0.101813 --> 0.101626). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.09510 valid_loss: 0.10063 test_loss: 0.11209 \n",
      "验证损失减少 (0.101626 --> 0.100631). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.09470 valid_loss: 0.10171 test_loss: 0.11312 \n",
      "[ 41/500] train_loss: 0.09516 valid_loss: 0.10080 test_loss: 0.11228 \n",
      "[ 42/500] train_loss: 0.09620 valid_loss: 0.10118 test_loss: 0.11251 \n",
      "[ 43/500] train_loss: 0.09438 valid_loss: 0.10102 test_loss: 0.11171 \n",
      "[ 44/500] train_loss: 0.09222 valid_loss: 0.09871 test_loss: 0.11114 \n",
      "验证损失减少 (0.100631 --> 0.098713). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.09302 valid_loss: 0.09737 test_loss: 0.11024 \n",
      "验证损失减少 (0.098713 --> 0.097373). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.09438 valid_loss: 0.09973 test_loss: 0.11018 \n",
      "[ 47/500] train_loss: 0.09115 valid_loss: 0.09747 test_loss: 0.11030 \n",
      "[ 48/500] train_loss: 0.09377 valid_loss: 0.09651 test_loss: 0.10812 \n",
      "验证损失减少 (0.097373 --> 0.096509). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.09086 valid_loss: 0.09995 test_loss: 0.11147 \n",
      "[ 50/500] train_loss: 0.09215 valid_loss: 0.10083 test_loss: 0.11132 \n",
      "[ 51/500] train_loss: 0.09194 valid_loss: 0.09721 test_loss: 0.11103 \n",
      "[ 52/500] train_loss: 0.08951 valid_loss: 0.09667 test_loss: 0.10890 \n",
      "[ 53/500] train_loss: 0.08791 valid_loss: 0.09907 test_loss: 0.10896 \n",
      "[ 54/500] train_loss: 0.08827 valid_loss: 0.09666 test_loss: 0.10687 \n",
      "[ 55/500] train_loss: 0.08907 valid_loss: 0.09730 test_loss: 0.10757 \n",
      "[ 56/500] train_loss: 0.09037 valid_loss: 0.09524 test_loss: 0.10689 \n",
      "验证损失减少 (0.096509 --> 0.095240). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.08835 valid_loss: 0.09714 test_loss: 0.10688 \n",
      "[ 58/500] train_loss: 0.08787 valid_loss: 0.09506 test_loss: 0.10691 \n",
      "验证损失减少 (0.095240 --> 0.095055). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.08615 valid_loss: 0.09835 test_loss: 0.10649 \n",
      "[ 60/500] train_loss: 0.08821 valid_loss: 0.09613 test_loss: 0.10643 \n",
      "[ 61/500] train_loss: 0.08476 valid_loss: 0.09871 test_loss: 0.10716 \n",
      "[ 62/500] train_loss: 0.08841 valid_loss: 0.09565 test_loss: 0.10646 \n",
      "[ 63/500] train_loss: 0.08611 valid_loss: 0.09466 test_loss: 0.10505 \n",
      "验证损失减少 (0.095055 --> 0.094656). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.08720 valid_loss: 0.09160 test_loss: 0.10294 \n",
      "验证损失减少 (0.094656 --> 0.091604). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.08455 valid_loss: 0.09327 test_loss: 0.10290 \n",
      "[ 66/500] train_loss: 0.08374 valid_loss: 0.09275 test_loss: 0.10281 \n",
      "[ 67/500] train_loss: 0.08536 valid_loss: 0.09509 test_loss: 0.10590 \n",
      "[ 68/500] train_loss: 0.08601 valid_loss: 0.09318 test_loss: 0.10253 \n",
      "[ 69/500] train_loss: 0.08332 valid_loss: 0.09522 test_loss: 0.10558 \n",
      "[ 70/500] train_loss: 0.08576 valid_loss: 0.09438 test_loss: 0.10361 \n",
      "[ 71/500] train_loss: 0.08447 valid_loss: 0.09444 test_loss: 0.10716 \n",
      "[ 72/500] train_loss: 0.08309 valid_loss: 0.09141 test_loss: 0.10228 \n",
      "验证损失减少 (0.091604 --> 0.091410). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.08488 valid_loss: 0.09145 test_loss: 0.10214 \n",
      "[ 74/500] train_loss: 0.08317 valid_loss: 0.09166 test_loss: 0.10343 \n",
      "[ 75/500] train_loss: 0.08202 valid_loss: 0.09300 test_loss: 0.10427 \n",
      "[ 76/500] train_loss: 0.08514 valid_loss: 0.09127 test_loss: 0.10151 \n",
      "验证损失减少 (0.091410 --> 0.091268). 正在保存模型...\n",
      "[ 77/500] train_loss: 0.08346 valid_loss: 0.09081 test_loss: 0.10175 \n",
      "验证损失减少 (0.091268 --> 0.090814). 正在保存模型...\n",
      "[ 78/500] train_loss: 0.08439 valid_loss: 0.09074 test_loss: 0.10073 \n",
      "验证损失减少 (0.090814 --> 0.090741). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.08436 valid_loss: 0.09001 test_loss: 0.10089 \n",
      "验证损失减少 (0.090741 --> 0.090010). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.08340 valid_loss: 0.09176 test_loss: 0.10126 \n",
      "[ 81/500] train_loss: 0.08376 valid_loss: 0.09036 test_loss: 0.10231 \n",
      "[ 82/500] train_loss: 0.08212 valid_loss: 0.09037 test_loss: 0.10045 \n",
      "[ 83/500] train_loss: 0.08110 valid_loss: 0.08946 test_loss: 0.10090 \n",
      "验证损失减少 (0.090010 --> 0.089463). 正在保存模型...\n",
      "[ 84/500] train_loss: 0.08170 valid_loss: 0.08975 test_loss: 0.10062 \n",
      "[ 85/500] train_loss: 0.08123 valid_loss: 0.08974 test_loss: 0.10088 \n",
      "[ 86/500] train_loss: 0.08438 valid_loss: 0.08871 test_loss: 0.10175 \n",
      "验证损失减少 (0.089463 --> 0.088711). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.08147 valid_loss: 0.08891 test_loss: 0.09962 \n",
      "[ 88/500] train_loss: 0.08062 valid_loss: 0.08810 test_loss: 0.10104 \n",
      "验证损失减少 (0.088711 --> 0.088100). 正在保存模型...\n",
      "[ 89/500] train_loss: 0.07946 valid_loss: 0.09112 test_loss: 0.09993 \n",
      "[ 90/500] train_loss: 0.07920 valid_loss: 0.08852 test_loss: 0.09961 \n",
      "[ 91/500] train_loss: 0.07964 valid_loss: 0.08854 test_loss: 0.10004 \n",
      "[ 92/500] train_loss: 0.08027 valid_loss: 0.08765 test_loss: 0.09821 \n",
      "验证损失减少 (0.088100 --> 0.087647). 正在保存模型...\n",
      "[ 93/500] train_loss: 0.08088 valid_loss: 0.08552 test_loss: 0.09927 \n",
      "验证损失减少 (0.087647 --> 0.085524). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.07853 valid_loss: 0.08794 test_loss: 0.09909 \n",
      "[ 95/500] train_loss: 0.07912 valid_loss: 0.08747 test_loss: 0.09901 \n",
      "[ 96/500] train_loss: 0.08127 valid_loss: 0.08595 test_loss: 0.09867 \n",
      "[ 97/500] train_loss: 0.08173 valid_loss: 0.08655 test_loss: 0.09945 \n",
      "[ 98/500] train_loss: 0.07991 valid_loss: 0.08720 test_loss: 0.10078 \n",
      "[ 99/500] train_loss: 0.07877 valid_loss: 0.08684 test_loss: 0.09810 \n",
      "[100/500] train_loss: 0.07927 valid_loss: 0.08683 test_loss: 0.09996 \n",
      "[101/500] train_loss: 0.07692 valid_loss: 0.08614 test_loss: 0.09875 \n",
      "[102/500] train_loss: 0.07830 valid_loss: 0.08676 test_loss: 0.09748 \n",
      "[103/500] train_loss: 0.07915 valid_loss: 0.08652 test_loss: 0.09862 \n",
      "[104/500] train_loss: 0.07595 valid_loss: 0.08787 test_loss: 0.09837 \n",
      "[105/500] train_loss: 0.07725 valid_loss: 0.08562 test_loss: 0.09735 \n",
      "[106/500] train_loss: 0.07727 valid_loss: 0.08598 test_loss: 0.09637 \n",
      "[107/500] train_loss: 0.07684 valid_loss: 0.08830 test_loss: 0.09963 \n",
      "[108/500] train_loss: 0.07643 valid_loss: 0.08752 test_loss: 0.09760 \n",
      "[109/500] train_loss: 0.07576 valid_loss: 0.08487 test_loss: 0.09727 \n",
      "验证损失减少 (0.085524 --> 0.084871). 正在保存模型...\n",
      "[110/500] train_loss: 0.07574 valid_loss: 0.09021 test_loss: 0.09851 \n",
      "[111/500] train_loss: 0.07614 valid_loss: 0.08514 test_loss: 0.09660 \n",
      "[112/500] train_loss: 0.07603 valid_loss: 0.08587 test_loss: 0.09614 \n",
      "[113/500] train_loss: 0.07494 valid_loss: 0.08524 test_loss: 0.09693 \n",
      "[114/500] train_loss: 0.07701 valid_loss: 0.08554 test_loss: 0.09834 \n",
      "[115/500] train_loss: 0.07684 valid_loss: 0.08428 test_loss: 0.09676 \n",
      "验证损失减少 (0.084871 --> 0.084284). 正在保存模型...\n",
      "[116/500] train_loss: 0.07656 valid_loss: 0.08536 test_loss: 0.09620 \n",
      "[117/500] train_loss: 0.07546 valid_loss: 0.08414 test_loss: 0.09504 \n",
      "验证损失减少 (0.084284 --> 0.084143). 正在保存模型...\n",
      "[118/500] train_loss: 0.07373 valid_loss: 0.08572 test_loss: 0.09594 \n",
      "[119/500] train_loss: 0.07608 valid_loss: 0.08519 test_loss: 0.09699 \n",
      "[120/500] train_loss: 0.07568 valid_loss: 0.08387 test_loss: 0.09568 \n",
      "验证损失减少 (0.084143 --> 0.083871). 正在保存模型...\n",
      "[121/500] train_loss: 0.07606 valid_loss: 0.08387 test_loss: 0.09635 \n",
      "[122/500] train_loss: 0.07455 valid_loss: 0.08321 test_loss: 0.09553 \n",
      "验证损失减少 (0.083871 --> 0.083214). 正在保存模型...\n",
      "[123/500] train_loss: 0.07588 valid_loss: 0.08481 test_loss: 0.09407 \n",
      "[124/500] train_loss: 0.07391 valid_loss: 0.08353 test_loss: 0.09666 \n",
      "[125/500] train_loss: 0.07307 valid_loss: 0.08431 test_loss: 0.09532 \n",
      "[126/500] train_loss: 0.07234 valid_loss: 0.08385 test_loss: 0.09659 \n",
      "[127/500] train_loss: 0.07390 valid_loss: 0.08371 test_loss: 0.09537 \n",
      "[128/500] train_loss: 0.07325 valid_loss: 0.08622 test_loss: 0.09637 \n",
      "[129/500] train_loss: 0.07301 valid_loss: 0.08327 test_loss: 0.09602 \n",
      "[130/500] train_loss: 0.07441 valid_loss: 0.08233 test_loss: 0.09581 \n",
      "验证损失减少 (0.083214 --> 0.082327). 正在保存模型...\n",
      "[131/500] train_loss: 0.07308 valid_loss: 0.08115 test_loss: 0.09482 \n",
      "验证损失减少 (0.082327 --> 0.081145). 正在保存模型...\n",
      "[132/500] train_loss: 0.07402 valid_loss: 0.08387 test_loss: 0.09693 \n",
      "[133/500] train_loss: 0.07268 valid_loss: 0.08324 test_loss: 0.09533 \n",
      "[134/500] train_loss: 0.07438 valid_loss: 0.08245 test_loss: 0.09625 \n",
      "[135/500] train_loss: 0.07579 valid_loss: 0.08387 test_loss: 0.09745 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136/500] train_loss: 0.07189 valid_loss: 0.08467 test_loss: 0.09590 \n",
      "[137/500] train_loss: 0.07244 valid_loss: 0.08330 test_loss: 0.09653 \n",
      "[138/500] train_loss: 0.07008 valid_loss: 0.08300 test_loss: 0.09491 \n",
      "[139/500] train_loss: 0.07278 valid_loss: 0.08412 test_loss: 0.09589 \n",
      "[140/500] train_loss: 0.07177 valid_loss: 0.08315 test_loss: 0.09444 \n",
      "[141/500] train_loss: 0.07020 valid_loss: 0.08078 test_loss: 0.09403 \n",
      "验证损失减少 (0.081145 --> 0.080785). 正在保存模型...\n",
      "[142/500] train_loss: 0.07287 valid_loss: 0.08203 test_loss: 0.09615 \n",
      "[143/500] train_loss: 0.07507 valid_loss: 0.08484 test_loss: 0.09581 \n",
      "[144/500] train_loss: 0.07247 valid_loss: 0.08144 test_loss: 0.09433 \n",
      "[145/500] train_loss: 0.07222 valid_loss: 0.08519 test_loss: 0.09503 \n",
      "[146/500] train_loss: 0.07063 valid_loss: 0.08219 test_loss: 0.09496 \n",
      "[147/500] train_loss: 0.07214 valid_loss: 0.08229 test_loss: 0.09468 \n",
      "[148/500] train_loss: 0.07005 valid_loss: 0.08221 test_loss: 0.09533 \n",
      "[149/500] train_loss: 0.07360 valid_loss: 0.08551 test_loss: 0.09334 \n",
      "[150/500] train_loss: 0.07084 valid_loss: 0.08767 test_loss: 0.09488 \n",
      "[151/500] train_loss: 0.07208 valid_loss: 0.08075 test_loss: 0.09278 \n",
      "验证损失减少 (0.080785 --> 0.080752). 正在保存模型...\n",
      "[152/500] train_loss: 0.07008 valid_loss: 0.08062 test_loss: 0.09256 \n",
      "验证损失减少 (0.080752 --> 0.080620). 正在保存模型...\n",
      "[153/500] train_loss: 0.07208 valid_loss: 0.08082 test_loss: 0.09414 \n",
      "[154/500] train_loss: 0.07060 valid_loss: 0.07980 test_loss: 0.09512 \n",
      "验证损失减少 (0.080620 --> 0.079803). 正在保存模型...\n",
      "[155/500] train_loss: 0.06926 valid_loss: 0.08301 test_loss: 0.09309 \n",
      "[156/500] train_loss: 0.07047 valid_loss: 0.08043 test_loss: 0.09530 \n",
      "[157/500] train_loss: 0.07059 valid_loss: 0.07928 test_loss: 0.09319 \n",
      "验证损失减少 (0.079803 --> 0.079276). 正在保存模型...\n",
      "[158/500] train_loss: 0.07011 valid_loss: 0.08050 test_loss: 0.09308 \n",
      "[159/500] train_loss: 0.07091 valid_loss: 0.07972 test_loss: 0.09210 \n",
      "[160/500] train_loss: 0.06941 valid_loss: 0.08445 test_loss: 0.09397 \n",
      "[161/500] train_loss: 0.07080 valid_loss: 0.08126 test_loss: 0.09355 \n",
      "[162/500] train_loss: 0.06841 valid_loss: 0.07909 test_loss: 0.09341 \n",
      "验证损失减少 (0.079276 --> 0.079088). 正在保存模型...\n",
      "[163/500] train_loss: 0.07029 valid_loss: 0.07887 test_loss: 0.09190 \n",
      "验证损失减少 (0.079088 --> 0.078875). 正在保存模型...\n",
      "[164/500] train_loss: 0.07095 valid_loss: 0.08037 test_loss: 0.09315 \n",
      "[165/500] train_loss: 0.06764 valid_loss: 0.08009 test_loss: 0.09259 \n",
      "[166/500] train_loss: 0.06873 valid_loss: 0.08358 test_loss: 0.09332 \n",
      "[167/500] train_loss: 0.06745 valid_loss: 0.07993 test_loss: 0.09332 \n",
      "[168/500] train_loss: 0.07000 valid_loss: 0.07914 test_loss: 0.09368 \n",
      "[169/500] train_loss: 0.06918 valid_loss: 0.08392 test_loss: 0.09280 \n",
      "[170/500] train_loss: 0.06874 valid_loss: 0.08019 test_loss: 0.09350 \n",
      "[171/500] train_loss: 0.07039 valid_loss: 0.07918 test_loss: 0.09208 \n",
      "[172/500] train_loss: 0.06973 valid_loss: 0.08070 test_loss: 0.09373 \n",
      "[173/500] train_loss: 0.07009 valid_loss: 0.07964 test_loss: 0.09408 \n",
      "[174/500] train_loss: 0.06933 valid_loss: 0.07916 test_loss: 0.09245 \n",
      "[175/500] train_loss: 0.06947 valid_loss: 0.07864 test_loss: 0.09133 \n",
      "验证损失减少 (0.078875 --> 0.078636). 正在保存模型...\n",
      "[176/500] train_loss: 0.06648 valid_loss: 0.07902 test_loss: 0.09330 \n",
      "[177/500] train_loss: 0.06881 valid_loss: 0.07942 test_loss: 0.09203 \n",
      "[178/500] train_loss: 0.06681 valid_loss: 0.07924 test_loss: 0.09419 \n",
      "[179/500] train_loss: 0.06695 valid_loss: 0.07973 test_loss: 0.09375 \n",
      "[180/500] train_loss: 0.06843 valid_loss: 0.07891 test_loss: 0.09164 \n",
      "[181/500] train_loss: 0.06600 valid_loss: 0.07843 test_loss: 0.09137 \n",
      "验证损失减少 (0.078636 --> 0.078426). 正在保存模型...\n",
      "[182/500] train_loss: 0.06636 valid_loss: 0.08101 test_loss: 0.09214 \n",
      "[183/500] train_loss: 0.06730 valid_loss: 0.08052 test_loss: 0.09527 \n",
      "[184/500] train_loss: 0.06951 valid_loss: 0.08141 test_loss: 0.09198 \n",
      "[185/500] train_loss: 0.06829 valid_loss: 0.08151 test_loss: 0.09347 \n",
      "[186/500] train_loss: 0.06890 valid_loss: 0.07866 test_loss: 0.09234 \n",
      "[187/500] train_loss: 0.06676 valid_loss: 0.07826 test_loss: 0.09206 \n",
      "验证损失减少 (0.078426 --> 0.078256). 正在保存模型...\n",
      "[188/500] train_loss: 0.06716 valid_loss: 0.07884 test_loss: 0.09155 \n",
      "[189/500] train_loss: 0.06778 valid_loss: 0.08656 test_loss: 0.09348 \n",
      "[190/500] train_loss: 0.06873 valid_loss: 0.07800 test_loss: 0.09030 \n",
      "验证损失减少 (0.078256 --> 0.077999). 正在保存模型...\n",
      "[191/500] train_loss: 0.06936 valid_loss: 0.07738 test_loss: 0.09093 \n",
      "验证损失减少 (0.077999 --> 0.077377). 正在保存模型...\n",
      "[192/500] train_loss: 0.06668 valid_loss: 0.08233 test_loss: 0.09228 \n",
      "[193/500] train_loss: 0.06666 valid_loss: 0.07973 test_loss: 0.09205 \n",
      "[194/500] train_loss: 0.06897 valid_loss: 0.08037 test_loss: 0.09385 \n",
      "[195/500] train_loss: 0.06658 valid_loss: 0.07935 test_loss: 0.09191 \n",
      "[196/500] train_loss: 0.06673 valid_loss: 0.07916 test_loss: 0.09239 \n",
      "[197/500] train_loss: 0.06653 valid_loss: 0.07926 test_loss: 0.09147 \n",
      "[198/500] train_loss: 0.06797 valid_loss: 0.07927 test_loss: 0.09213 \n",
      "[199/500] train_loss: 0.06684 valid_loss: 0.07763 test_loss: 0.09336 \n",
      "[200/500] train_loss: 0.06615 valid_loss: 0.07814 test_loss: 0.09140 \n",
      "[201/500] train_loss: 0.06733 valid_loss: 0.07829 test_loss: 0.09228 \n",
      "[202/500] train_loss: 0.06701 valid_loss: 0.07963 test_loss: 0.09384 \n",
      "[203/500] train_loss: 0.06829 valid_loss: 0.07782 test_loss: 0.09193 \n",
      "[204/500] train_loss: 0.06659 valid_loss: 0.07774 test_loss: 0.09158 \n",
      "[205/500] train_loss: 0.06585 valid_loss: 0.07824 test_loss: 0.09098 \n",
      "[206/500] train_loss: 0.06560 valid_loss: 0.07762 test_loss: 0.09111 \n",
      "[207/500] train_loss: 0.06468 valid_loss: 0.07932 test_loss: 0.09136 \n",
      "[208/500] train_loss: 0.06350 valid_loss: 0.07721 test_loss: 0.09182 \n",
      "验证损失减少 (0.077377 --> 0.077211). 正在保存模型...\n",
      "[209/500] train_loss: 0.06586 valid_loss: 0.07772 test_loss: 0.09297 \n",
      "[210/500] train_loss: 0.06455 valid_loss: 0.07774 test_loss: 0.09187 \n",
      "[211/500] train_loss: 0.06584 valid_loss: 0.08131 test_loss: 0.09320 \n",
      "[212/500] train_loss: 0.06431 valid_loss: 0.08025 test_loss: 0.09142 \n",
      "[213/500] train_loss: 0.06537 valid_loss: 0.07773 test_loss: 0.09296 \n",
      "[214/500] train_loss: 0.06660 valid_loss: 0.07784 test_loss: 0.09170 \n",
      "[215/500] train_loss: 0.06730 valid_loss: 0.07668 test_loss: 0.09029 \n",
      "验证损失减少 (0.077211 --> 0.076678). 正在保存模型...\n",
      "[216/500] train_loss: 0.06446 valid_loss: 0.07719 test_loss: 0.09139 \n",
      "[217/500] train_loss: 0.06506 valid_loss: 0.07751 test_loss: 0.09136 \n",
      "[218/500] train_loss: 0.06436 valid_loss: 0.07776 test_loss: 0.09188 \n",
      "[219/500] train_loss: 0.06515 valid_loss: 0.08022 test_loss: 0.09433 \n",
      "[220/500] train_loss: 0.06321 valid_loss: 0.07819 test_loss: 0.09203 \n",
      "[221/500] train_loss: 0.06446 valid_loss: 0.07707 test_loss: 0.09207 \n",
      "[222/500] train_loss: 0.06507 valid_loss: 0.07686 test_loss: 0.09058 \n",
      "[223/500] train_loss: 0.06342 valid_loss: 0.07668 test_loss: 0.09135 \n",
      "[224/500] train_loss: 0.06450 valid_loss: 0.07706 test_loss: 0.09154 \n",
      "[225/500] train_loss: 0.06445 valid_loss: 0.08045 test_loss: 0.09168 \n",
      "[226/500] train_loss: 0.06433 valid_loss: 0.07843 test_loss: 0.09117 \n",
      "[227/500] train_loss: 0.06382 valid_loss: 0.08088 test_loss: 0.09089 \n",
      "[228/500] train_loss: 0.06402 valid_loss: 0.08100 test_loss: 0.09136 \n",
      "[229/500] train_loss: 0.06293 valid_loss: 0.07821 test_loss: 0.09121 \n",
      "[230/500] train_loss: 0.06322 valid_loss: 0.07859 test_loss: 0.09218 \n",
      "[231/500] train_loss: 0.06483 valid_loss: 0.07693 test_loss: 0.09219 \n",
      "[232/500] train_loss: 0.06397 valid_loss: 0.07732 test_loss: 0.09052 \n",
      "[233/500] train_loss: 0.06504 valid_loss: 0.07982 test_loss: 0.09190 \n",
      "[234/500] train_loss: 0.06463 valid_loss: 0.08129 test_loss: 0.09119 \n",
      "[235/500] train_loss: 0.06228 valid_loss: 0.07559 test_loss: 0.09035 \n",
      "验证损失减少 (0.076678 --> 0.075590). 正在保存模型...\n",
      "[236/500] train_loss: 0.06455 valid_loss: 0.07770 test_loss: 0.09155 \n",
      "[237/500] train_loss: 0.06383 valid_loss: 0.07664 test_loss: 0.09054 \n",
      "[238/500] train_loss: 0.06134 valid_loss: 0.07672 test_loss: 0.09023 \n",
      "[239/500] train_loss: 0.06299 valid_loss: 0.07664 test_loss: 0.09091 \n",
      "[240/500] train_loss: 0.06316 valid_loss: 0.07591 test_loss: 0.09166 \n",
      "[241/500] train_loss: 0.06177 valid_loss: 0.07563 test_loss: 0.08992 \n",
      "[242/500] train_loss: 0.06351 valid_loss: 0.07624 test_loss: 0.09075 \n",
      "[243/500] train_loss: 0.06374 valid_loss: 0.07657 test_loss: 0.09149 \n",
      "[244/500] train_loss: 0.06331 valid_loss: 0.07713 test_loss: 0.09037 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[245/500] train_loss: 0.06115 valid_loss: 0.07748 test_loss: 0.09245 \n",
      "[246/500] train_loss: 0.06332 valid_loss: 0.07780 test_loss: 0.09193 \n",
      "[247/500] train_loss: 0.06314 valid_loss: 0.07676 test_loss: 0.09019 \n",
      "[248/500] train_loss: 0.06115 valid_loss: 0.07805 test_loss: 0.09087 \n",
      "[249/500] train_loss: 0.06298 valid_loss: 0.07638 test_loss: 0.09271 \n",
      "[250/500] train_loss: 0.06221 valid_loss: 0.07717 test_loss: 0.09176 \n",
      "[251/500] train_loss: 0.06026 valid_loss: 0.07822 test_loss: 0.09328 \n",
      "[252/500] train_loss: 0.06147 valid_loss: 0.07597 test_loss: 0.09023 \n",
      "[253/500] train_loss: 0.06344 valid_loss: 0.07952 test_loss: 0.09051 \n",
      "[254/500] train_loss: 0.06130 valid_loss: 0.07576 test_loss: 0.09054 \n",
      "[255/500] train_loss: 0.06071 valid_loss: 0.07593 test_loss: 0.09086 \n",
      "[256/500] train_loss: 0.06153 valid_loss: 0.07577 test_loss: 0.08953 \n",
      "[257/500] train_loss: 0.06358 valid_loss: 0.07610 test_loss: 0.08993 \n",
      "[258/500] train_loss: 0.06284 valid_loss: 0.07882 test_loss: 0.09187 \n",
      "[259/500] train_loss: 0.06263 valid_loss: 0.07971 test_loss: 0.09015 \n",
      "[260/500] train_loss: 0.06343 valid_loss: 0.07815 test_loss: 0.08938 \n",
      "[261/500] train_loss: 0.06216 valid_loss: 0.08082 test_loss: 0.08983 \n",
      "[262/500] train_loss: 0.06205 valid_loss: 0.07729 test_loss: 0.09054 \n",
      "[263/500] train_loss: 0.06174 valid_loss: 0.07660 test_loss: 0.08791 \n",
      "[264/500] train_loss: 0.05980 valid_loss: 0.07840 test_loss: 0.08958 \n",
      "[265/500] train_loss: 0.06138 valid_loss: 0.07747 test_loss: 0.09076 \n",
      "[266/500] train_loss: 0.05950 valid_loss: 0.07700 test_loss: 0.09124 \n",
      "[267/500] train_loss: 0.06056 valid_loss: 0.07632 test_loss: 0.08951 \n",
      "[268/500] train_loss: 0.05852 valid_loss: 0.07656 test_loss: 0.09027 \n",
      "[269/500] train_loss: 0.06066 valid_loss: 0.07977 test_loss: 0.08995 \n",
      "[270/500] train_loss: 0.06096 valid_loss: 0.07669 test_loss: 0.09018 \n",
      "[271/500] train_loss: 0.06300 valid_loss: 0.07979 test_loss: 0.09026 \n",
      "[272/500] train_loss: 0.06331 valid_loss: 0.07948 test_loss: 0.08994 \n",
      "[273/500] train_loss: 0.06070 valid_loss: 0.07542 test_loss: 0.09054 \n",
      "验证损失减少 (0.075590 --> 0.075420). 正在保存模型...\n",
      "[274/500] train_loss: 0.06119 valid_loss: 0.07732 test_loss: 0.08979 \n",
      "[275/500] train_loss: 0.06139 valid_loss: 0.07751 test_loss: 0.09084 \n",
      "[276/500] train_loss: 0.05904 valid_loss: 0.07642 test_loss: 0.09105 \n",
      "[277/500] train_loss: 0.05966 valid_loss: 0.07626 test_loss: 0.09065 \n",
      "[278/500] train_loss: 0.05853 valid_loss: 0.07665 test_loss: 0.08985 \n",
      "[279/500] train_loss: 0.06007 valid_loss: 0.07642 test_loss: 0.09060 \n",
      "[280/500] train_loss: 0.05918 valid_loss: 0.07560 test_loss: 0.09146 \n",
      "[281/500] train_loss: 0.05985 valid_loss: 0.07521 test_loss: 0.09031 \n",
      "验证损失减少 (0.075420 --> 0.075213). 正在保存模型...\n",
      "[282/500] train_loss: 0.06012 valid_loss: 0.07567 test_loss: 0.09168 \n",
      "[283/500] train_loss: 0.06045 valid_loss: 0.07635 test_loss: 0.09154 \n",
      "[284/500] train_loss: 0.06020 valid_loss: 0.07597 test_loss: 0.09082 \n",
      "[285/500] train_loss: 0.06058 valid_loss: 0.07648 test_loss: 0.09065 \n",
      "[286/500] train_loss: 0.05949 valid_loss: 0.07615 test_loss: 0.09001 \n",
      "[287/500] train_loss: 0.05958 valid_loss: 0.07527 test_loss: 0.09006 \n",
      "[288/500] train_loss: 0.05954 valid_loss: 0.07643 test_loss: 0.09009 \n",
      "[289/500] train_loss: 0.05950 valid_loss: 0.07599 test_loss: 0.09203 \n",
      "[290/500] train_loss: 0.05896 valid_loss: 0.07574 test_loss: 0.08992 \n",
      "[291/500] train_loss: 0.05955 valid_loss: 0.07550 test_loss: 0.09086 \n",
      "[292/500] train_loss: 0.05983 valid_loss: 0.07625 test_loss: 0.09095 \n",
      "[293/500] train_loss: 0.05933 valid_loss: 0.07631 test_loss: 0.09050 \n",
      "[294/500] train_loss: 0.05797 valid_loss: 0.07609 test_loss: 0.09075 \n",
      "[295/500] train_loss: 0.05903 valid_loss: 0.07622 test_loss: 0.08943 \n",
      "[296/500] train_loss: 0.05799 valid_loss: 0.07572 test_loss: 0.08968 \n",
      "[297/500] train_loss: 0.05924 valid_loss: 0.07560 test_loss: 0.09021 \n",
      "[298/500] train_loss: 0.06069 valid_loss: 0.07512 test_loss: 0.08976 \n",
      "验证损失减少 (0.075213 --> 0.075116). 正在保存模型...\n",
      "[299/500] train_loss: 0.05906 valid_loss: 0.07634 test_loss: 0.09023 \n",
      "[300/500] train_loss: 0.05949 valid_loss: 0.07601 test_loss: 0.08978 \n",
      "[301/500] train_loss: 0.05940 valid_loss: 0.07597 test_loss: 0.09081 \n",
      "[302/500] train_loss: 0.05921 valid_loss: 0.07618 test_loss: 0.08948 \n",
      "[303/500] train_loss: 0.05779 valid_loss: 0.07587 test_loss: 0.08998 \n",
      "[304/500] train_loss: 0.05921 valid_loss: 0.07605 test_loss: 0.09147 \n",
      "[305/500] train_loss: 0.05943 valid_loss: 0.07485 test_loss: 0.09049 \n",
      "验证损失减少 (0.075116 --> 0.074848). 正在保存模型...\n",
      "[306/500] train_loss: 0.05713 valid_loss: 0.07913 test_loss: 0.09039 \n",
      "[307/500] train_loss: 0.05867 valid_loss: 0.07555 test_loss: 0.09074 \n",
      "[308/500] train_loss: 0.05925 valid_loss: 0.07697 test_loss: 0.09020 \n",
      "[309/500] train_loss: 0.05859 valid_loss: 0.07590 test_loss: 0.09108 \n",
      "[310/500] train_loss: 0.05866 valid_loss: 0.08222 test_loss: 0.09082 \n",
      "[311/500] train_loss: 0.05886 valid_loss: 0.08619 test_loss: 0.09023 \n",
      "[312/500] train_loss: 0.05762 valid_loss: 0.07685 test_loss: 0.09168 \n",
      "[313/500] train_loss: 0.05835 valid_loss: 0.07594 test_loss: 0.08976 \n",
      "[314/500] train_loss: 0.06006 valid_loss: 0.07511 test_loss: 0.09004 \n",
      "[315/500] train_loss: 0.05793 valid_loss: 0.07717 test_loss: 0.09093 \n",
      "[316/500] train_loss: 0.05817 valid_loss: 0.07432 test_loss: 0.09024 \n",
      "验证损失减少 (0.074848 --> 0.074316). 正在保存模型...\n",
      "[317/500] train_loss: 0.05847 valid_loss: 0.07533 test_loss: 0.09049 \n",
      "[318/500] train_loss: 0.05823 valid_loss: 0.07729 test_loss: 0.09355 \n",
      "[319/500] train_loss: 0.05756 valid_loss: 0.07552 test_loss: 0.09080 \n",
      "[320/500] train_loss: 0.05649 valid_loss: 0.07458 test_loss: 0.09126 \n",
      "[321/500] train_loss: 0.05861 valid_loss: 0.07522 test_loss: 0.09015 \n",
      "[322/500] train_loss: 0.05725 valid_loss: 0.07892 test_loss: 0.09139 \n",
      "[323/500] train_loss: 0.05851 valid_loss: 0.07456 test_loss: 0.09015 \n",
      "[324/500] train_loss: 0.05612 valid_loss: 0.07503 test_loss: 0.09098 \n",
      "[325/500] train_loss: 0.05880 valid_loss: 0.07413 test_loss: 0.08911 \n",
      "验证损失减少 (0.074316 --> 0.074131). 正在保存模型...\n",
      "[326/500] train_loss: 0.05655 valid_loss: 0.07464 test_loss: 0.08976 \n",
      "[327/500] train_loss: 0.05713 valid_loss: 0.07637 test_loss: 0.09208 \n",
      "[328/500] train_loss: 0.05826 valid_loss: 0.07565 test_loss: 0.08977 \n",
      "[329/500] train_loss: 0.05810 valid_loss: 0.07451 test_loss: 0.08824 \n",
      "[330/500] train_loss: 0.05704 valid_loss: 0.07347 test_loss: 0.08948 \n",
      "验证损失减少 (0.074131 --> 0.073469). 正在保存模型...\n",
      "[331/500] train_loss: 0.05779 valid_loss: 0.07465 test_loss: 0.09004 \n",
      "[332/500] train_loss: 0.05749 valid_loss: 0.07496 test_loss: 0.09067 \n",
      "[333/500] train_loss: 0.05953 valid_loss: 0.07567 test_loss: 0.09091 \n",
      "[334/500] train_loss: 0.05764 valid_loss: 0.07489 test_loss: 0.08993 \n",
      "[335/500] train_loss: 0.05671 valid_loss: 0.07512 test_loss: 0.09027 \n",
      "[336/500] train_loss: 0.05728 valid_loss: 0.07401 test_loss: 0.09007 \n",
      "[337/500] train_loss: 0.05780 valid_loss: 0.07473 test_loss: 0.09062 \n",
      "[338/500] train_loss: 0.05732 valid_loss: 0.07750 test_loss: 0.09252 \n",
      "[339/500] train_loss: 0.05833 valid_loss: 0.07390 test_loss: 0.08975 \n",
      "[340/500] train_loss: 0.05624 valid_loss: 0.07409 test_loss: 0.09116 \n",
      "[341/500] train_loss: 0.05652 valid_loss: 0.07579 test_loss: 0.09047 \n",
      "[342/500] train_loss: 0.05703 valid_loss: 0.07518 test_loss: 0.08859 \n",
      "[343/500] train_loss: 0.05641 valid_loss: 0.07400 test_loss: 0.09046 \n",
      "[344/500] train_loss: 0.05691 valid_loss: 0.07510 test_loss: 0.09084 \n",
      "[345/500] train_loss: 0.05750 valid_loss: 0.07568 test_loss: 0.09092 \n",
      "[346/500] train_loss: 0.05547 valid_loss: 0.07613 test_loss: 0.09133 \n",
      "[347/500] train_loss: 0.05575 valid_loss: 0.07504 test_loss: 0.08986 \n",
      "[348/500] train_loss: 0.05635 valid_loss: 0.07514 test_loss: 0.09073 \n",
      "[349/500] train_loss: 0.05892 valid_loss: 0.07663 test_loss: 0.09183 \n",
      "[350/500] train_loss: 0.05578 valid_loss: 0.07496 test_loss: 0.09050 \n",
      "[351/500] train_loss: 0.05618 valid_loss: 0.07617 test_loss: 0.09157 \n",
      "[352/500] train_loss: 0.05754 valid_loss: 0.07601 test_loss: 0.09117 \n",
      "[353/500] train_loss: 0.05540 valid_loss: 0.07684 test_loss: 0.09134 \n",
      "[354/500] train_loss: 0.05609 valid_loss: 0.07667 test_loss: 0.09055 \n",
      "[355/500] train_loss: 0.05531 valid_loss: 0.07565 test_loss: 0.08904 \n",
      "[356/500] train_loss: 0.05574 valid_loss: 0.07432 test_loss: 0.09016 \n",
      "[357/500] train_loss: 0.05552 valid_loss: 0.07637 test_loss: 0.09040 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[358/500] train_loss: 0.05686 valid_loss: 0.07462 test_loss: 0.08940 \n",
      "[359/500] train_loss: 0.05546 valid_loss: 0.07526 test_loss: 0.09020 \n",
      "[360/500] train_loss: 0.05649 valid_loss: 0.07605 test_loss: 0.08899 \n",
      "[361/500] train_loss: 0.05703 valid_loss: 0.07574 test_loss: 0.09078 \n",
      "[362/500] train_loss: 0.05626 valid_loss: 0.07474 test_loss: 0.08961 \n",
      "[363/500] train_loss: 0.05587 valid_loss: 0.07631 test_loss: 0.09049 \n",
      "[364/500] train_loss: 0.05603 valid_loss: 0.07673 test_loss: 0.08905 \n",
      "[365/500] train_loss: 0.05600 valid_loss: 0.07705 test_loss: 0.08911 \n",
      "[366/500] train_loss: 0.05676 valid_loss: 0.07559 test_loss: 0.08805 \n",
      "[367/500] train_loss: 0.05642 valid_loss: 0.07772 test_loss: 0.09077 \n",
      "[368/500] train_loss: 0.05642 valid_loss: 0.07818 test_loss: 0.09101 \n",
      "[369/500] train_loss: 0.05593 valid_loss: 0.07674 test_loss: 0.09025 \n",
      "[370/500] train_loss: 0.05540 valid_loss: 0.07564 test_loss: 0.09002 \n",
      "[371/500] train_loss: 0.05628 valid_loss: 0.07776 test_loss: 0.08865 \n",
      "[372/500] train_loss: 0.05869 valid_loss: 0.07439 test_loss: 0.08893 \n",
      "[373/500] train_loss: 0.05633 valid_loss: 0.07526 test_loss: 0.08911 \n",
      "[374/500] train_loss: 0.05466 valid_loss: 0.07592 test_loss: 0.08899 \n",
      "[375/500] train_loss: 0.05676 valid_loss: 0.07580 test_loss: 0.08955 \n",
      "[376/500] train_loss: 0.05520 valid_loss: 0.07618 test_loss: 0.08942 \n",
      "[377/500] train_loss: 0.05590 valid_loss: 0.07588 test_loss: 0.08905 \n",
      "[378/500] train_loss: 0.05460 valid_loss: 0.07494 test_loss: 0.08872 \n",
      "[379/500] train_loss: 0.05629 valid_loss: 0.07526 test_loss: 0.08848 \n",
      "[380/500] train_loss: 0.05319 valid_loss: 0.07496 test_loss: 0.08891 \n",
      "[381/500] train_loss: 0.05527 valid_loss: 0.07657 test_loss: 0.09083 \n",
      "[382/500] train_loss: 0.05446 valid_loss: 0.07635 test_loss: 0.08950 \n",
      "[383/500] train_loss: 0.05443 valid_loss: 0.07349 test_loss: 0.08895 \n",
      "[384/500] train_loss: 0.05363 valid_loss: 0.07635 test_loss: 0.08992 \n",
      "[385/500] train_loss: 0.05368 valid_loss: 0.07448 test_loss: 0.09046 \n",
      "[386/500] train_loss: 0.05555 valid_loss: 0.07579 test_loss: 0.08870 \n",
      "[387/500] train_loss: 0.05469 valid_loss: 0.07454 test_loss: 0.08999 \n",
      "[388/500] train_loss: 0.05397 valid_loss: 0.07498 test_loss: 0.09014 \n",
      "[389/500] train_loss: 0.05701 valid_loss: 0.07473 test_loss: 0.08892 \n",
      "[390/500] train_loss: 0.05410 valid_loss: 0.07520 test_loss: 0.08941 \n",
      "[391/500] train_loss: 0.05231 valid_loss: 0.07468 test_loss: 0.08974 \n",
      "[392/500] train_loss: 0.05543 valid_loss: 0.07515 test_loss: 0.08926 \n",
      "[393/500] train_loss: 0.05458 valid_loss: 0.07603 test_loss: 0.08867 \n",
      "[394/500] train_loss: 0.05429 valid_loss: 0.07752 test_loss: 0.08862 \n",
      "[395/500] train_loss: 0.05474 valid_loss: 0.07581 test_loss: 0.08834 \n",
      "[396/500] train_loss: 0.05542 valid_loss: 0.07658 test_loss: 0.08952 \n",
      "[397/500] train_loss: 0.05342 valid_loss: 0.07774 test_loss: 0.09076 \n",
      "[398/500] train_loss: 0.05456 valid_loss: 0.07762 test_loss: 0.08902 \n",
      "[399/500] train_loss: 0.05500 valid_loss: 0.07787 test_loss: 0.08867 \n",
      "[400/500] train_loss: 0.05440 valid_loss: 0.07501 test_loss: 0.08782 \n",
      "[401/500] train_loss: 0.05439 valid_loss: 0.07483 test_loss: 0.08877 \n",
      "[402/500] train_loss: 0.05517 valid_loss: 0.07620 test_loss: 0.09074 \n",
      "[403/500] train_loss: 0.05500 valid_loss: 0.07433 test_loss: 0.08897 \n",
      "[404/500] train_loss: 0.05347 valid_loss: 0.07425 test_loss: 0.09033 \n",
      "[405/500] train_loss: 0.05350 valid_loss: 0.07844 test_loss: 0.08935 \n",
      "[406/500] train_loss: 0.05371 valid_loss: 0.07445 test_loss: 0.08967 \n",
      "[407/500] train_loss: 0.05479 valid_loss: 0.07441 test_loss: 0.08884 \n",
      "[408/500] train_loss: 0.05337 valid_loss: 0.07511 test_loss: 0.08853 \n",
      "[409/500] train_loss: 0.05476 valid_loss: 0.07478 test_loss: 0.08912 \n",
      "[410/500] train_loss: 0.05324 valid_loss: 0.08235 test_loss: 0.08945 \n",
      "[411/500] train_loss: 0.05440 valid_loss: 0.07542 test_loss: 0.08932 \n",
      "[412/500] train_loss: 0.05292 valid_loss: 0.07961 test_loss: 0.09104 \n",
      "[413/500] train_loss: 0.05505 valid_loss: 0.08085 test_loss: 0.08816 \n",
      "[414/500] train_loss: 0.05494 valid_loss: 0.07755 test_loss: 0.08887 \n",
      "[415/500] train_loss: 0.05373 valid_loss: 0.07754 test_loss: 0.09432 \n",
      "[416/500] train_loss: 0.05576 valid_loss: 0.07592 test_loss: 0.08966 \n",
      "[417/500] train_loss: 0.05522 valid_loss: 0.07465 test_loss: 0.08986 \n",
      "[418/500] train_loss: 0.05266 valid_loss: 0.07532 test_loss: 0.09046 \n",
      "[419/500] train_loss: 0.05377 valid_loss: 0.07408 test_loss: 0.08912 \n",
      "[420/500] train_loss: 0.05304 valid_loss: 0.07614 test_loss: 0.08899 \n",
      "[421/500] train_loss: 0.05393 valid_loss: 0.07512 test_loss: 0.08848 \n",
      "[422/500] train_loss: 0.05439 valid_loss: 0.07482 test_loss: 0.09003 \n",
      "[423/500] train_loss: 0.05478 valid_loss: 0.07461 test_loss: 0.08766 \n",
      "[424/500] train_loss: 0.05245 valid_loss: 0.07341 test_loss: 0.08855 \n",
      "验证损失减少 (0.073469 --> 0.073405). 正在保存模型...\n",
      "[425/500] train_loss: 0.05320 valid_loss: 0.08197 test_loss: 0.08963 \n",
      "[426/500] train_loss: 0.05257 valid_loss: 0.07515 test_loss: 0.09030 \n",
      "[427/500] train_loss: 0.05327 valid_loss: 0.07438 test_loss: 0.09020 \n",
      "[428/500] train_loss: 0.05301 valid_loss: 0.07528 test_loss: 0.08964 \n",
      "[429/500] train_loss: 0.05443 valid_loss: 0.07428 test_loss: 0.08926 \n",
      "[430/500] train_loss: 0.05420 valid_loss: 0.08381 test_loss: 0.08987 \n",
      "[431/500] train_loss: 0.05342 valid_loss: 0.07664 test_loss: 0.08930 \n",
      "[432/500] train_loss: 0.05133 valid_loss: 0.07527 test_loss: 0.08865 \n",
      "[433/500] train_loss: 0.05319 valid_loss: 0.07559 test_loss: 0.09012 \n",
      "[434/500] train_loss: 0.05160 valid_loss: 0.07444 test_loss: 0.08939 \n",
      "[435/500] train_loss: 0.05401 valid_loss: 0.07449 test_loss: 0.08846 \n",
      "[436/500] train_loss: 0.05252 valid_loss: 0.07505 test_loss: 0.09010 \n",
      "[437/500] train_loss: 0.05254 valid_loss: 0.07909 test_loss: 0.09001 \n",
      "[438/500] train_loss: 0.05290 valid_loss: 0.07507 test_loss: 0.08991 \n",
      "[439/500] train_loss: 0.05223 valid_loss: 0.07590 test_loss: 0.08906 \n",
      "[440/500] train_loss: 0.05311 valid_loss: 0.07519 test_loss: 0.09022 \n",
      "[441/500] train_loss: 0.05210 valid_loss: 0.07600 test_loss: 0.09010 \n",
      "[442/500] train_loss: 0.05280 valid_loss: 0.07597 test_loss: 0.08947 \n",
      "[443/500] train_loss: 0.05353 valid_loss: 0.07597 test_loss: 0.08971 \n",
      "[444/500] train_loss: 0.05432 valid_loss: 0.07505 test_loss: 0.08906 \n",
      "[445/500] train_loss: 0.05408 valid_loss: 0.07389 test_loss: 0.08954 \n",
      "[446/500] train_loss: 0.05299 valid_loss: 0.07582 test_loss: 0.08950 \n",
      "[447/500] train_loss: 0.05218 valid_loss: 0.07519 test_loss: 0.09031 \n",
      "[448/500] train_loss: 0.05255 valid_loss: 0.07604 test_loss: 0.08879 \n",
      "[449/500] train_loss: 0.05179 valid_loss: 0.07439 test_loss: 0.08968 \n",
      "[450/500] train_loss: 0.05306 valid_loss: 0.07505 test_loss: 0.09043 \n",
      "[451/500] train_loss: 0.05293 valid_loss: 0.07530 test_loss: 0.08933 \n",
      "[452/500] train_loss: 0.05380 valid_loss: 0.07445 test_loss: 0.08888 \n",
      "[453/500] train_loss: 0.05123 valid_loss: 0.07493 test_loss: 0.08791 \n",
      "[454/500] train_loss: 0.05229 valid_loss: 0.07508 test_loss: 0.08980 \n",
      "[455/500] train_loss: 0.05094 valid_loss: 0.07630 test_loss: 0.08886 \n",
      "[456/500] train_loss: 0.05288 valid_loss: 0.07635 test_loss: 0.08929 \n",
      "[457/500] train_loss: 0.05160 valid_loss: 0.07470 test_loss: 0.08999 \n",
      "[458/500] train_loss: 0.05123 valid_loss: 0.07533 test_loss: 0.08944 \n",
      "[459/500] train_loss: 0.05157 valid_loss: 0.07515 test_loss: 0.08826 \n",
      "[460/500] train_loss: 0.05214 valid_loss: 0.07447 test_loss: 0.08968 \n",
      "[461/500] train_loss: 0.05223 valid_loss: 0.07431 test_loss: 0.08812 \n",
      "[462/500] train_loss: 0.05089 valid_loss: 0.07386 test_loss: 0.09020 \n",
      "[463/500] train_loss: 0.05140 valid_loss: 0.07467 test_loss: 0.09028 \n",
      "[464/500] train_loss: 0.05321 valid_loss: 0.07429 test_loss: 0.08844 \n",
      "[465/500] train_loss: 0.05239 valid_loss: 0.07599 test_loss: 0.08913 \n",
      "[466/500] train_loss: 0.05310 valid_loss: 0.07470 test_loss: 0.08894 \n",
      "[467/500] train_loss: 0.05212 valid_loss: 0.07372 test_loss: 0.08915 \n",
      "[468/500] train_loss: 0.05170 valid_loss: 0.07467 test_loss: 0.08907 \n",
      "[469/500] train_loss: 0.05124 valid_loss: 0.07376 test_loss: 0.08874 \n",
      "[470/500] train_loss: 0.05083 valid_loss: 0.07463 test_loss: 0.09002 \n",
      "[471/500] train_loss: 0.05227 valid_loss: 0.07428 test_loss: 0.08781 \n",
      "[472/500] train_loss: 0.04901 valid_loss: 0.07743 test_loss: 0.08943 \n",
      "[473/500] train_loss: 0.05215 valid_loss: 0.07534 test_loss: 0.09015 \n",
      "[474/500] train_loss: 0.05131 valid_loss: 0.07624 test_loss: 0.08948 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475/500] train_loss: 0.04983 valid_loss: 0.07512 test_loss: 0.08884 \n",
      "[476/500] train_loss: 0.05094 valid_loss: 0.07529 test_loss: 0.08916 \n",
      "[477/500] train_loss: 0.05311 valid_loss: 0.07513 test_loss: 0.08829 \n",
      "[478/500] train_loss: 0.05166 valid_loss: 0.07530 test_loss: 0.08815 \n",
      "[479/500] train_loss: 0.05242 valid_loss: 0.07410 test_loss: 0.08953 \n",
      "[480/500] train_loss: 0.05209 valid_loss: 0.07565 test_loss: 0.09119 \n",
      "[481/500] train_loss: 0.05064 valid_loss: 0.07482 test_loss: 0.08934 \n",
      "[482/500] train_loss: 0.05124 valid_loss: 0.07464 test_loss: 0.08874 \n",
      "[483/500] train_loss: 0.05065 valid_loss: 0.07526 test_loss: 0.08999 \n",
      "[484/500] train_loss: 0.05137 valid_loss: 0.07509 test_loss: 0.08988 \n",
      "[485/500] train_loss: 0.05069 valid_loss: 0.07516 test_loss: 0.09038 \n",
      "[486/500] train_loss: 0.04960 valid_loss: 0.07412 test_loss: 0.08811 \n",
      "[487/500] train_loss: 0.05067 valid_loss: 0.07495 test_loss: 0.08964 \n",
      "[488/500] train_loss: 0.05112 valid_loss: 0.07405 test_loss: 0.08785 \n",
      "[489/500] train_loss: 0.05176 valid_loss: 0.07405 test_loss: 0.08943 \n",
      "[490/500] train_loss: 0.05053 valid_loss: 0.07435 test_loss: 0.08874 \n",
      "[491/500] train_loss: 0.05131 valid_loss: 0.07442 test_loss: 0.08935 \n",
      "[492/500] train_loss: 0.05154 valid_loss: 0.07397 test_loss: 0.09094 \n",
      "[493/500] train_loss: 0.05108 valid_loss: 0.07383 test_loss: 0.08930 \n",
      "[494/500] train_loss: 0.04996 valid_loss: 0.07400 test_loss: 0.08957 \n",
      "[495/500] train_loss: 0.05176 valid_loss: 0.07424 test_loss: 0.08943 \n",
      "[496/500] train_loss: 0.05081 valid_loss: 0.07460 test_loss: 0.08882 \n",
      "[497/500] train_loss: 0.05059 valid_loss: 0.07496 test_loss: 0.08910 \n",
      "[498/500] train_loss: 0.05201 valid_loss: 0.07476 test_loss: 0.08829 \n",
      "[499/500] train_loss: 0.05018 valid_loss: 0.07423 test_loss: 0.08861 \n",
      "[500/500] train_loss: 0.05019 valid_loss: 0.07391 test_loss: 0.08805 \n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 500\n",
    "\n",
    "train_loader = dl_train_seen\n",
    "valid_loader = dl_valid_seen\n",
    "test_loader = dl_test_seen\n",
    "\n",
    "#i = 0\n",
    "for i in range(20):\n",
    "#for i in range(1):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    #model = LSTMModel(input_size, hidden_size, output_size).cuda()\n",
    "    model = PTPNet(1,3,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    fn = 'UKDALE_seen_13_1110%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347f75b",
   "metadata": {},
   "source": [
    "这段代码的作用是训练模型。首先，设置了批量大小（batch_size）和训练轮数（n_epochs）。然后，分别设置了训练数据集（train_loader）、验证数据集（valid_loader）和测试数据集（test_loader）的加载器。接下来，通过循环来训练多个模型（在这里只训练一个模型）。\n",
    "\n",
    "在循环内部，首先打印出当前正在训练的模型的信息。然后，实例化了一个PTPNet模型，并将其移动到GPU上进行加速。接着，定义了优化器（optimizer）和损失函数（criterion）。最后，指定了保存模型的文件名（fn），并调用train_model函数来训练模型，并将训练过程中的损失值保存在train_loss、valid_loss和test_loss变量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15f2961d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d55c8c52e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, color='blue', label='train loss')\n",
    "plt.plot(valid_loss, color='green', label='valid loss')\n",
    "plt.plot(test_loss, color='red', label='test loss')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13df4fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x215adb6ad60>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, color='blue', label='train loss')\n",
    "plt.plot(valid_loss, color='green', label='valid loss')\n",
    "plt.plot(test_loss, color='red', label='test loss')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09708ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/200] train_loss: 0.55720 valid_loss: 0.41295 test_loss: 0.37032 \n",
      "验证损失减少 (inf --> 0.412950). 正在保存模型...\n",
      "[  2/200] train_loss: 0.32048 valid_loss: 0.31326 test_loss: 0.26562 \n",
      "验证损失减少 (0.412950 --> 0.313255). 正在保存模型...\n",
      "[  3/200] train_loss: 0.29012 valid_loss: 0.29401 test_loss: 0.25199 \n",
      "验证损失减少 (0.313255 --> 0.294015). 正在保存模型...\n",
      "[  4/200] train_loss: 0.27359 valid_loss: 0.27707 test_loss: 0.24691 \n",
      "验证损失减少 (0.294015 --> 0.277070). 正在保存模型...\n",
      "[  5/200] train_loss: 0.25713 valid_loss: 0.26521 test_loss: 0.23920 \n",
      "验证损失减少 (0.277070 --> 0.265210). 正在保存模型...\n",
      "[  6/200] train_loss: 0.25281 valid_loss: 0.26050 test_loss: 0.23716 \n",
      "验证损失减少 (0.265210 --> 0.260498). 正在保存模型...\n",
      "[  7/200] train_loss: 0.24944 valid_loss: 0.25866 test_loss: 0.23595 \n",
      "验证损失减少 (0.260498 --> 0.258662). 正在保存模型...\n",
      "[  8/200] train_loss: 0.24987 valid_loss: 0.25570 test_loss: 0.23446 \n",
      "验证损失减少 (0.258662 --> 0.255697). 正在保存模型...\n",
      "[  9/200] train_loss: 0.24751 valid_loss: 0.25428 test_loss: 0.23455 \n",
      "验证损失减少 (0.255697 --> 0.254279). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.24358 valid_loss: 0.24860 test_loss: 0.22876 \n",
      "验证损失减少 (0.254279 --> 0.248601). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.24050 valid_loss: 0.25579 test_loss: 0.23241 \n",
      "[ 12/200] train_loss: 0.23899 valid_loss: 0.25419 test_loss: 0.23461 \n",
      "[ 13/200] train_loss: 0.23418 valid_loss: 0.25866 test_loss: 0.23788 \n",
      "[ 14/200] train_loss: 0.23138 valid_loss: 0.23928 test_loss: 0.22643 \n",
      "验证损失减少 (0.248601 --> 0.239275). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.22687 valid_loss: 0.22770 test_loss: 0.22249 \n",
      "验证损失减少 (0.239275 --> 0.227703). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.22369 valid_loss: 0.22834 test_loss: 0.22430 \n",
      "[ 17/200] train_loss: 0.22225 valid_loss: 0.22837 test_loss: 0.22587 \n",
      "[ 18/200] train_loss: 0.21766 valid_loss: 0.21928 test_loss: 0.22025 \n",
      "验证损失减少 (0.227703 --> 0.219282). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.21529 valid_loss: 0.21313 test_loss: 0.22173 \n",
      "验证损失减少 (0.219282 --> 0.213126). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.20808 valid_loss: 0.20598 test_loss: 0.21365 \n",
      "验证损失减少 (0.213126 --> 0.205983). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.19389 valid_loss: 0.19198 test_loss: 0.20021 \n",
      "验证损失减少 (0.205983 --> 0.191983). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.17827 valid_loss: 0.17754 test_loss: 0.18704 \n",
      "验证损失减少 (0.191983 --> 0.177543). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.16786 valid_loss: 0.17387 test_loss: 0.17663 \n",
      "验证损失减少 (0.177543 --> 0.173874). 正在保存模型...\n",
      "[ 24/200] train_loss: 0.15770 valid_loss: 0.16009 test_loss: 0.15568 \n",
      "验证损失减少 (0.173874 --> 0.160086). 正在保存模型...\n",
      "[ 25/200] train_loss: 0.15311 valid_loss: 0.15958 test_loss: 0.15148 \n",
      "验证损失减少 (0.160086 --> 0.159583). 正在保存模型...\n",
      "[ 26/200] train_loss: 0.14288 valid_loss: 0.15637 test_loss: 0.14076 \n",
      "验证损失减少 (0.159583 --> 0.156367). 正在保存模型...\n",
      "[ 27/200] train_loss: 0.14383 valid_loss: 0.16429 test_loss: 0.14871 \n",
      "[ 28/200] train_loss: 0.13765 valid_loss: 0.14146 test_loss: 0.12803 \n",
      "验证损失减少 (0.156367 --> 0.141455). 正在保存模型...\n",
      "[ 29/200] train_loss: 0.13422 valid_loss: 0.13789 test_loss: 0.12326 \n",
      "验证损失减少 (0.141455 --> 0.137892). 正在保存模型...\n",
      "[ 30/200] train_loss: 0.13037 valid_loss: 0.13834 test_loss: 0.12280 \n",
      "[ 31/200] train_loss: 0.12593 valid_loss: 0.17805 test_loss: 0.15867 \n",
      "[ 32/200] train_loss: 0.13084 valid_loss: 0.13551 test_loss: 0.12302 \n",
      "验证损失减少 (0.137892 --> 0.135511). 正在保存模型...\n",
      "[ 33/200] train_loss: 0.12781 valid_loss: 0.13404 test_loss: 0.12272 \n",
      "验证损失减少 (0.135511 --> 0.134040). 正在保存模型...\n",
      "[ 34/200] train_loss: 0.12262 valid_loss: 0.12711 test_loss: 0.11554 \n",
      "验证损失减少 (0.134040 --> 0.127106). 正在保存模型...\n",
      "[ 35/200] train_loss: 0.11727 valid_loss: 0.12976 test_loss: 0.11785 \n",
      "[ 36/200] train_loss: 0.11920 valid_loss: 0.13239 test_loss: 0.12206 \n",
      "[ 37/200] train_loss: 0.11894 valid_loss: 0.13039 test_loss: 0.12117 \n",
      "[ 38/200] train_loss: 0.11711 valid_loss: 0.12866 test_loss: 0.11933 \n",
      "[ 39/200] train_loss: 0.11497 valid_loss: 0.11997 test_loss: 0.11007 \n",
      "验证损失减少 (0.127106 --> 0.119970). 正在保存模型...\n",
      "[ 40/200] train_loss: 0.11462 valid_loss: 0.12944 test_loss: 0.12465 \n",
      "[ 41/200] train_loss: 0.11184 valid_loss: 0.11733 test_loss: 0.10954 \n",
      "验证损失减少 (0.119970 --> 0.117325). 正在保存模型...\n",
      "[ 42/200] train_loss: 0.10893 valid_loss: 0.11435 test_loss: 0.10789 \n",
      "验证损失减少 (0.117325 --> 0.114350). 正在保存模型...\n",
      "[ 43/200] train_loss: 0.10878 valid_loss: 0.11555 test_loss: 0.11178 \n",
      "[ 44/200] train_loss: 0.10958 valid_loss: 0.12002 test_loss: 0.11735 \n",
      "[ 45/200] train_loss: 0.10828 valid_loss: 0.11578 test_loss: 0.11304 \n",
      "[ 46/200] train_loss: 0.10763 valid_loss: 0.11587 test_loss: 0.11423 \n",
      "[ 47/200] train_loss: 0.10559 valid_loss: 0.11102 test_loss: 0.10677 \n",
      "验证损失减少 (0.114350 --> 0.111021). 正在保存模型...\n",
      "[ 48/200] train_loss: 0.10537 valid_loss: 0.11554 test_loss: 0.11482 \n",
      "[ 49/200] train_loss: 0.10556 valid_loss: 0.11736 test_loss: 0.11557 \n",
      "[ 50/200] train_loss: 0.10598 valid_loss: 0.11043 test_loss: 0.10752 \n",
      "验证损失减少 (0.111021 --> 0.110428). 正在保存模型...\n",
      "[ 51/200] train_loss: 0.10533 valid_loss: 0.10830 test_loss: 0.10949 \n",
      "验证损失减少 (0.110428 --> 0.108302). 正在保存模型...\n",
      "[ 52/200] train_loss: 0.10253 valid_loss: 0.10637 test_loss: 0.10799 \n",
      "验证损失减少 (0.108302 --> 0.106368). 正在保存模型...\n",
      "[ 53/200] train_loss: 0.10136 valid_loss: 0.10685 test_loss: 0.10456 \n",
      "[ 54/200] train_loss: 0.10119 valid_loss: 0.10903 test_loss: 0.10650 \n",
      "[ 55/200] train_loss: 0.09903 valid_loss: 0.10426 test_loss: 0.10380 \n",
      "验证损失减少 (0.106368 --> 0.104262). 正在保存模型...\n",
      "[ 56/200] train_loss: 0.09872 valid_loss: 0.10678 test_loss: 0.10775 \n",
      "[ 57/200] train_loss: 0.09972 valid_loss: 0.11053 test_loss: 0.11376 \n",
      "[ 58/200] train_loss: 0.09884 valid_loss: 0.10146 test_loss: 0.10130 \n",
      "验证损失减少 (0.104262 --> 0.101456). 正在保存模型...\n",
      "[ 59/200] train_loss: 0.09816 valid_loss: 0.10728 test_loss: 0.10837 \n",
      "[ 60/200] train_loss: 0.09698 valid_loss: 0.10449 test_loss: 0.10577 \n",
      "[ 61/200] train_loss: 0.09717 valid_loss: 0.10145 test_loss: 0.10293 \n",
      "验证损失减少 (0.101456 --> 0.101448). 正在保存模型...\n",
      "[ 62/200] train_loss: 0.09671 valid_loss: 0.10477 test_loss: 0.10843 \n",
      "[ 63/200] train_loss: 0.09660 valid_loss: 0.10691 test_loss: 0.10820 \n",
      "[ 64/200] train_loss: 0.09461 valid_loss: 0.10228 test_loss: 0.10741 \n",
      "[ 65/200] train_loss: 0.09497 valid_loss: 0.10399 test_loss: 0.10398 \n",
      "[ 66/200] train_loss: 0.09551 valid_loss: 0.11297 test_loss: 0.11636 \n",
      "[ 67/200] train_loss: 0.09465 valid_loss: 0.10151 test_loss: 0.10427 \n",
      "[ 68/200] train_loss: 0.09373 valid_loss: 0.09839 test_loss: 0.10194 \n",
      "验证损失减少 (0.101448 --> 0.098393). 正在保存模型...\n",
      "[ 69/200] train_loss: 0.09093 valid_loss: 0.09882 test_loss: 0.10435 \n",
      "[ 70/200] train_loss: 0.08977 valid_loss: 0.10347 test_loss: 0.10321 \n",
      "[ 71/200] train_loss: 0.09216 valid_loss: 0.09658 test_loss: 0.09974 \n",
      "验证损失减少 (0.098393 --> 0.096580). 正在保存模型...\n",
      "[ 72/200] train_loss: 0.08862 valid_loss: 0.09832 test_loss: 0.10145 \n",
      "[ 73/200] train_loss: 0.09082 valid_loss: 0.09512 test_loss: 0.09666 \n",
      "验证损失减少 (0.096580 --> 0.095119). 正在保存模型...\n",
      "[ 74/200] train_loss: 0.09269 valid_loss: 0.10552 test_loss: 0.10703 \n",
      "[ 75/200] train_loss: 0.09032 valid_loss: 0.09904 test_loss: 0.10467 \n",
      "[ 76/200] train_loss: 0.09019 valid_loss: 0.10527 test_loss: 0.10832 \n",
      "[ 77/200] train_loss: 0.09064 valid_loss: 0.09915 test_loss: 0.10282 \n",
      "[ 78/200] train_loss: 0.08868 valid_loss: 0.09530 test_loss: 0.09719 \n",
      "[ 79/200] train_loss: 0.08795 valid_loss: 0.09580 test_loss: 0.10080 \n",
      "[ 80/200] train_loss: 0.08946 valid_loss: 0.09602 test_loss: 0.10291 \n",
      "[ 81/200] train_loss: 0.08837 valid_loss: 0.09544 test_loss: 0.10539 \n",
      "[ 82/200] train_loss: 0.08710 valid_loss: 0.09429 test_loss: 0.09932 \n",
      "验证损失减少 (0.095119 --> 0.094290). 正在保存模型...\n",
      "[ 83/200] train_loss: 0.08951 valid_loss: 0.09417 test_loss: 0.10092 \n",
      "验证损失减少 (0.094290 --> 0.094166). 正在保存模型...\n",
      "[ 84/200] train_loss: 0.08947 valid_loss: 0.09356 test_loss: 0.10255 \n",
      "验证损失减少 (0.094166 --> 0.093565). 正在保存模型...\n",
      "[ 85/200] train_loss: 0.08394 valid_loss: 0.09700 test_loss: 0.10203 \n",
      "[ 86/200] train_loss: 0.08753 valid_loss: 0.09764 test_loss: 0.10553 \n",
      "[ 87/200] train_loss: 0.08727 valid_loss: 0.09262 test_loss: 0.10080 \n",
      "验证损失减少 (0.093565 --> 0.092618). 正在保存模型...\n",
      "[ 88/200] train_loss: 0.08760 valid_loss: 0.09194 test_loss: 0.11712 \n",
      "验证损失减少 (0.092618 --> 0.091939). 正在保存模型...\n",
      "[ 89/200] train_loss: 0.08680 valid_loss: 0.09429 test_loss: 0.09681 \n",
      "[ 90/200] train_loss: 0.08582 valid_loss: 0.09421 test_loss: 0.10377 \n",
      "[ 91/200] train_loss: 0.08548 valid_loss: 0.09860 test_loss: 0.10585 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 92/200] train_loss: 0.08556 valid_loss: 0.09269 test_loss: 0.09506 \n",
      "[ 93/200] train_loss: 0.08337 valid_loss: 0.09147 test_loss: 0.10274 \n",
      "验证损失减少 (0.091939 --> 0.091465). 正在保存模型...\n",
      "[ 94/200] train_loss: 0.08279 valid_loss: 0.09193 test_loss: 0.09939 \n",
      "[ 95/200] train_loss: 0.08319 valid_loss: 0.09137 test_loss: 0.09755 \n",
      "验证损失减少 (0.091465 --> 0.091365). 正在保存模型...\n",
      "[ 96/200] train_loss: 0.08437 valid_loss: 0.09042 test_loss: 0.09903 \n",
      "验证损失减少 (0.091365 --> 0.090424). 正在保存模型...\n",
      "[ 97/200] train_loss: 0.08420 valid_loss: 0.08964 test_loss: 0.09437 \n",
      "验证损失减少 (0.090424 --> 0.089637). 正在保存模型...\n",
      "[ 98/200] train_loss: 0.08453 valid_loss: 0.09320 test_loss: 0.10003 \n",
      "[ 99/200] train_loss: 0.08205 valid_loss: 0.08888 test_loss: 0.10465 \n",
      "验证损失减少 (0.089637 --> 0.088884). 正在保存模型...\n",
      "[100/200] train_loss: 0.08401 valid_loss: 0.09271 test_loss: 0.10525 \n",
      "[101/200] train_loss: 0.08406 valid_loss: 0.09182 test_loss: 0.09717 \n",
      "[102/200] train_loss: 0.08372 valid_loss: 0.09027 test_loss: 0.09536 \n",
      "[103/200] train_loss: 0.08090 valid_loss: 0.08933 test_loss: 0.09463 \n",
      "[104/200] train_loss: 0.08365 valid_loss: 0.09083 test_loss: 0.09681 \n",
      "[105/200] train_loss: 0.08374 valid_loss: 0.08973 test_loss: 0.09855 \n",
      "[106/200] train_loss: 0.08251 valid_loss: 0.08810 test_loss: 0.09521 \n",
      "验证损失减少 (0.088884 --> 0.088102). 正在保存模型...\n",
      "[107/200] train_loss: 0.08257 valid_loss: 0.08893 test_loss: 0.09574 \n",
      "[108/200] train_loss: 0.08076 valid_loss: 0.08815 test_loss: 0.09156 \n",
      "[109/200] train_loss: 0.08231 valid_loss: 0.08879 test_loss: 0.09399 \n",
      "[110/200] train_loss: 0.08184 valid_loss: 0.08747 test_loss: 0.09744 \n",
      "验证损失减少 (0.088102 --> 0.087468). 正在保存模型...\n",
      "[111/200] train_loss: 0.08198 valid_loss: 0.08848 test_loss: 0.09351 \n",
      "[112/200] train_loss: 0.08104 valid_loss: 0.08709 test_loss: 0.10571 \n",
      "验证损失减少 (0.087468 --> 0.087090). 正在保存模型...\n",
      "[113/200] train_loss: 0.07949 valid_loss: 0.08627 test_loss: 0.09877 \n",
      "验证损失减少 (0.087090 --> 0.086274). 正在保存模型...\n",
      "[114/200] train_loss: 0.07961 valid_loss: 0.08692 test_loss: 0.11593 \n",
      "[115/200] train_loss: 0.08367 valid_loss: 0.08777 test_loss: 0.09630 \n",
      "[116/200] train_loss: 0.08293 valid_loss: 0.08690 test_loss: 0.10287 \n",
      "[117/200] train_loss: 0.07896 valid_loss: 0.08841 test_loss: 0.09697 \n",
      "[118/200] train_loss: 0.07967 valid_loss: 0.08689 test_loss: 0.09391 \n",
      "[119/200] train_loss: 0.08051 valid_loss: 0.08595 test_loss: 0.09747 \n",
      "验证损失减少 (0.086274 --> 0.085953). 正在保存模型...\n",
      "[120/200] train_loss: 0.07970 valid_loss: 0.08587 test_loss: 0.09247 \n",
      "验证损失减少 (0.085953 --> 0.085874). 正在保存模型...\n",
      "[121/200] train_loss: 0.07832 valid_loss: 0.08682 test_loss: 0.10760 \n",
      "[122/200] train_loss: 0.07934 valid_loss: 0.08761 test_loss: 0.09720 \n",
      "[123/200] train_loss: 0.07799 valid_loss: 0.08839 test_loss: 0.09516 \n",
      "[124/200] train_loss: 0.08107 valid_loss: 0.08653 test_loss: 0.09895 \n",
      "[125/200] train_loss: 0.07756 valid_loss: 0.08934 test_loss: 0.09471 \n",
      "[126/200] train_loss: 0.07728 valid_loss: 0.08544 test_loss: 0.10308 \n",
      "验证损失减少 (0.085874 --> 0.085435). 正在保存模型...\n",
      "[127/200] train_loss: 0.07865 valid_loss: 0.08622 test_loss: 0.10473 \n",
      "[128/200] train_loss: 0.07896 valid_loss: 0.08574 test_loss: 0.10008 \n",
      "[129/200] train_loss: 0.07725 valid_loss: 0.08661 test_loss: 0.09688 \n",
      "[130/200] train_loss: 0.07739 valid_loss: 0.08569 test_loss: 0.09429 \n",
      "[131/200] train_loss: 0.07887 valid_loss: 0.08431 test_loss: 0.09871 \n",
      "验证损失减少 (0.085435 --> 0.084306). 正在保存模型...\n",
      "[132/200] train_loss: 0.07728 valid_loss: 0.08575 test_loss: 0.09064 \n",
      "[133/200] train_loss: 0.07679 valid_loss: 0.08503 test_loss: 0.09421 \n",
      "[134/200] train_loss: 0.07684 valid_loss: 0.08620 test_loss: 0.10282 \n",
      "[135/200] train_loss: 0.07842 valid_loss: 0.08716 test_loss: 0.09533 \n",
      "[136/200] train_loss: 0.07592 valid_loss: 0.08519 test_loss: 0.10443 \n",
      "[137/200] train_loss: 0.07624 valid_loss: 0.08562 test_loss: 0.10062 \n",
      "[138/200] train_loss: 0.07661 valid_loss: 0.08301 test_loss: 0.09563 \n",
      "验证损失减少 (0.084306 --> 0.083007). 正在保存模型...\n",
      "[139/200] train_loss: 0.07732 valid_loss: 0.08529 test_loss: 0.09811 \n",
      "[140/200] train_loss: 0.07690 valid_loss: 0.08763 test_loss: 0.09769 \n",
      "[141/200] train_loss: 0.07603 valid_loss: 0.08964 test_loss: 0.09552 \n",
      "[142/200] train_loss: 0.07525 valid_loss: 0.08506 test_loss: 0.09869 \n",
      "[143/200] train_loss: 0.07413 valid_loss: 0.08683 test_loss: 0.09562 \n",
      "[144/200] train_loss: 0.07537 valid_loss: 0.08354 test_loss: 0.11210 \n",
      "[145/200] train_loss: 0.07710 valid_loss: 0.08371 test_loss: 0.12570 \n",
      "[146/200] train_loss: 0.07722 valid_loss: 0.08716 test_loss: 0.11598 \n",
      "[147/200] train_loss: 0.07710 valid_loss: 0.08550 test_loss: 0.09277 \n",
      "[148/200] train_loss: 0.07448 valid_loss: 0.08303 test_loss: 0.08978 \n",
      "[149/200] train_loss: 0.07496 valid_loss: 0.08444 test_loss: 0.11546 \n",
      "[150/200] train_loss: 0.07586 valid_loss: 0.08545 test_loss: 0.09525 \n",
      "[151/200] train_loss: 0.07412 valid_loss: 0.08384 test_loss: 0.09407 \n",
      "[152/200] train_loss: 0.07327 valid_loss: 0.08599 test_loss: 0.09416 \n",
      "[153/200] train_loss: 0.07385 valid_loss: 0.08294 test_loss: 0.10586 \n",
      "验证损失减少 (0.083007 --> 0.082940). 正在保存模型...\n",
      "[154/200] train_loss: 0.07446 valid_loss: 0.08392 test_loss: 0.09561 \n",
      "[155/200] train_loss: 0.07345 valid_loss: 0.08521 test_loss: 0.10249 \n",
      "[156/200] train_loss: 0.07367 valid_loss: 0.08577 test_loss: 0.09089 \n",
      "[157/200] train_loss: 0.07084 valid_loss: 0.08378 test_loss: 0.09256 \n",
      "[158/200] train_loss: 0.07238 valid_loss: 0.08669 test_loss: 0.09363 \n",
      "[159/200] train_loss: 0.07374 valid_loss: 0.08224 test_loss: 0.10281 \n",
      "验证损失减少 (0.082940 --> 0.082238). 正在保存模型...\n",
      "[160/200] train_loss: 0.07276 valid_loss: 0.08256 test_loss: 0.09557 \n",
      "[161/200] train_loss: 0.07448 valid_loss: 0.08689 test_loss: 0.09247 \n",
      "[162/200] train_loss: 0.07333 valid_loss: 0.08394 test_loss: 0.09018 \n",
      "[163/200] train_loss: 0.07012 valid_loss: 0.08355 test_loss: 0.10021 \n",
      "[164/200] train_loss: 0.07175 valid_loss: 0.08440 test_loss: 0.09612 \n",
      "[165/200] train_loss: 0.07426 valid_loss: 0.08369 test_loss: 0.09141 \n",
      "[166/200] train_loss: 0.07142 valid_loss: 0.08242 test_loss: 0.10362 \n",
      "[167/200] train_loss: 0.07233 valid_loss: 0.08375 test_loss: 0.10548 \n",
      "[168/200] train_loss: 0.07053 valid_loss: 0.08196 test_loss: 0.13441 \n",
      "验证损失减少 (0.082238 --> 0.081960). 正在保存模型...\n",
      "[169/200] train_loss: 0.07287 valid_loss: 0.08211 test_loss: 0.10680 \n",
      "[170/200] train_loss: 0.07181 valid_loss: 0.08170 test_loss: 0.09255 \n",
      "验证损失减少 (0.081960 --> 0.081700). 正在保存模型...\n",
      "[171/200] train_loss: 0.07114 valid_loss: 0.08246 test_loss: 0.09378 \n",
      "[172/200] train_loss: 0.07188 valid_loss: 0.08414 test_loss: 0.11124 \n",
      "[173/200] train_loss: 0.07439 valid_loss: 0.08475 test_loss: 0.10076 \n",
      "[174/200] train_loss: 0.07305 valid_loss: 0.08361 test_loss: 0.09762 \n",
      "[175/200] train_loss: 0.07110 valid_loss: 0.08315 test_loss: 0.11137 \n",
      "[176/200] train_loss: 0.07086 valid_loss: 0.08589 test_loss: 0.10930 \n",
      "[177/200] train_loss: 0.06920 valid_loss: 0.08110 test_loss: 0.09653 \n",
      "验证损失减少 (0.081700 --> 0.081101). 正在保存模型...\n",
      "[178/200] train_loss: 0.07045 valid_loss: 0.08152 test_loss: 0.10859 \n",
      "[179/200] train_loss: 0.07151 valid_loss: 0.08209 test_loss: 0.13289 \n",
      "[180/200] train_loss: 0.07239 valid_loss: 0.08120 test_loss: 0.11353 \n",
      "[181/200] train_loss: 0.07214 valid_loss: 0.08145 test_loss: 0.10579 \n",
      "[182/200] train_loss: 0.07125 valid_loss: 0.08122 test_loss: 0.10418 \n",
      "[183/200] train_loss: 0.06964 valid_loss: 0.08387 test_loss: 0.10557 \n",
      "[184/200] train_loss: 0.06906 valid_loss: 0.08367 test_loss: 0.09717 \n",
      "[185/200] train_loss: 0.06950 valid_loss: 0.08353 test_loss: 0.09458 \n",
      "[186/200] train_loss: 0.07028 valid_loss: 0.08244 test_loss: 0.09482 \n",
      "[187/200] train_loss: 0.07031 valid_loss: 0.08024 test_loss: 0.10778 \n",
      "验证损失减少 (0.081101 --> 0.080245). 正在保存模型...\n",
      "[188/200] train_loss: 0.06813 valid_loss: 0.08330 test_loss: 0.09392 \n",
      "[189/200] train_loss: 0.06914 valid_loss: 0.08294 test_loss: 0.10436 \n",
      "[190/200] train_loss: 0.06908 valid_loss: 0.08128 test_loss: 0.10175 \n",
      "[191/200] train_loss: 0.06882 valid_loss: 0.08214 test_loss: 0.09564 \n",
      "[192/200] train_loss: 0.06842 valid_loss: 0.08121 test_loss: 0.09162 \n",
      "[193/200] train_loss: 0.06821 valid_loss: 0.08138 test_loss: 0.09801 \n",
      "[194/200] train_loss: 0.06746 valid_loss: 0.08011 test_loss: 0.10875 \n",
      "验证损失减少 (0.080245 --> 0.080107). 正在保存模型...\n",
      "[195/200] train_loss: 0.06724 valid_loss: 0.08177 test_loss: 0.09352 \n",
      "[196/200] train_loss: 0.06823 valid_loss: 0.08086 test_loss: 0.09001 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197/200] train_loss: 0.06925 valid_loss: 0.08193 test_loss: 0.09181 \n",
      "[198/200] train_loss: 0.06944 valid_loss: 0.08267 test_loss: 0.11145 \n",
      "[199/200] train_loss: 0.06906 valid_loss: 0.08096 test_loss: 0.10751 \n",
      "[200/200] train_loss: 0.06826 valid_loss: 0.08118 test_loss: 0.09283 \n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 200\n",
    "\n",
    "train_loader = dl_train_unseen\n",
    "valid_loader = dl_valid_unseen\n",
    "test_loader = dl_test_unseen\n",
    "\n",
    "#i = 0\n",
    "for i in range(1):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    model =PTPNet(1,3,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    fn = 'UKDALE_unseen_8_1110%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3fc0234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x266b088d220>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, color='blue', label='train loss')\n",
    "plt.plot(valid_loss, color='green', label='valid loss')\n",
    "plt.plot(test_loss, color='red', label='test loss')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e10a0afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTPNet(\n",
       "  (encoder1): Encoder(\n",
       "    (conv): Conv1d(1, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool1_1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (encoder2): Encoder(\n",
       "    (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2_1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (encoder3): Encoder(\n",
       "    (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool3_1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (encoder4): Encoder(\n",
       "    (conv): Conv1d(128, 320, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (MUSEA): MUSEAttention(\n",
       "    (fc_q): Linear(in_features=30, out_features=240, bias=True)\n",
       "    (fc_k): Linear(in_features=30, out_features=240, bias=True)\n",
       "    (fc_v): Linear(in_features=30, out_features=240, bias=True)\n",
       "    (fc_o): Linear(in_features=240, out_features=30, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (conv1): Depth_Pointwise_Conv1d(\n",
       "      (depth_conv): Identity()\n",
       "      (pointwise_conv): Conv1d(240, 30, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (conv3): Depth_Pointwise_Conv1d(\n",
       "      (depth_conv): Conv1d(240, 240, kernel_size=(3,), stride=(1,), padding=(1,), groups=240)\n",
       "      (pointwise_conv): Conv1d(240, 30, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (conv5): Depth_Pointwise_Conv1d(\n",
       "      (depth_conv): Conv1d(240, 240, kernel_size=(5,), stride=(1,), padding=(2,), groups=240)\n",
       "      (pointwise_conv): Conv1d(240, 30, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (tpool1): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
       "    (conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool2): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(10,), stride=(10,), padding=(0,))\n",
       "    (conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool3): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(15,), stride=(15,), padding=(0,))\n",
       "    (conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool4): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
       "    (conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool5): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(25,), stride=(25,), padding=(0,))\n",
       "    (conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool6): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(15,), stride=(15,), padding=(0,))\n",
       "    (conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool7): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
       "    (conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv): ConvTranspose1d(768, 32, kernel_size=(8,), stride=(8,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (activation): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PTPNet(1,3,32).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16197488",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为model的神经网络模型。该模型是一个PTPNet模型，输入维度为1，输出维度为3，隐藏层维度为32。通过调用.cuda()方法，将模型移动到GPU上进行计算。接下来，代码调用model.eval()将模型设置为评估模式，表示在进行推理时不进行梯度计算和参数更新。\n",
    "\n",
    "总结：这段代码定义了一个PTPNet模型，并将其移动到GPU上进行评估模式的设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "763c371a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_seen_12_11100.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.881 (0.881, 0.881)\n",
      "Precision : 0.891 (0.891, 0.891)\n",
      "Recall    : 0.872 (0.872, 0.872)\n",
      "Accuracy  : 0.893 (0.893, 0.893)\n",
      "MCC       : 0.784 (0.784, 0.784)\n",
      "MAE       : 14.10 (14.10, 14.10)\n",
      "SAE       : -0.024 (-0.024, -0.024)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.875 (0.875, 0.875)\n",
      "Precision : 0.834 (0.834, 0.834)\n",
      "Recall    : 0.920 (0.920, 0.920)\n",
      "Accuracy  : 0.994 (0.994, 0.994)\n",
      "MCC       : 0.873 (0.873, 0.873)\n",
      "MAE       : 22.35 (22.35, 22.35)\n",
      "SAE       : 0.082 (0.082, 0.082)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.981 (0.981, 0.981)\n",
      "Precision : 0.983 (0.983, 0.983)\n",
      "Recall    : 0.979 (0.979, 0.979)\n",
      "Accuracy  : 0.997 (0.997, 0.997)\n",
      "MCC       : 0.979 (0.979, 0.979)\n",
      "MAE       : 41.69 (41.69, 41.69)\n",
      "SAE       : -0.087 (-0.087, -0.087)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.5\n",
    "for i in range(1):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_seen_%d.pth' %i\n",
    "    filename = './UKDALE_seen_12_1110%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        #计算了一个家电设备的平均功率（pm），并将其除以最大功率（MAX_POWER）进行归一化处理。\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "        \n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[0], sorted(scores[i]['F1'])[0]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[0], sorted(scores[i]['Precision'])[0]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[0], sorted(scores[i]['Recall'])[0]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[0], sorted(scores[i]['Accuracy'])[0]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[0], sorted(scores[i]['MCC'])[0]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[0], sorted(scores[i]['MAE'])[0]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[0], sorted(scores[i]['SAE'])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "074d919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_seen_11_11100.pth\n",
      "./UKDALE_seen_11_11101.pth\n",
      "./UKDALE_seen_11_11102.pth\n",
      "./UKDALE_seen_11_11103.pth\n",
      "./UKDALE_seen_11_11104.pth\n",
      "./UKDALE_seen_11_11105.pth\n",
      "./UKDALE_seen_11_11106.pth\n",
      "./UKDALE_seen_11_11107.pth\n",
      "./UKDALE_seen_11_11108.pth\n",
      "./UKDALE_seen_11_11109.pth\n",
      "./UKDALE_seen_11_111010.pth\n",
      "./UKDALE_seen_11_111011.pth\n",
      "./UKDALE_seen_11_111012.pth\n",
      "./UKDALE_seen_11_111013.pth\n",
      "./UKDALE_seen_11_111014.pth\n",
      "./UKDALE_seen_11_111015.pth\n",
      "./UKDALE_seen_11_111016.pth\n",
      "./UKDALE_seen_11_111017.pth\n",
      "./UKDALE_seen_11_111018.pth\n",
      "./UKDALE_seen_11_111019.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.899 (0.895, 0.902)\n",
      "Precision : 0.902 (0.896, 0.907)\n",
      "Recall    : 0.895 (0.890, 0.902)\n",
      "Accuracy  : 0.908 (0.905, 0.911)\n",
      "MCC       : 0.815 (0.809, 0.821)\n",
      "MAE       : 12.77 (12.52, 13.03)\n",
      "SAE       : -0.009 (-0.021, 0.006)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.943 (0.921, 0.966)\n",
      "Precision : 0.938 (0.896, 0.965)\n",
      "Recall    : 0.948 (0.923, 0.967)\n",
      "Accuracy  : 0.997 (0.996, 0.998)\n",
      "MCC       : 0.941 (0.919, 0.965)\n",
      "MAE       : 20.58 (20.14, 21.33)\n",
      "SAE       : -0.008 (-0.051, 0.048)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.985 (0.981, 0.988)\n",
      "Precision : 0.983 (0.974, 0.989)\n",
      "Recall    : 0.987 (0.981, 0.990)\n",
      "Accuracy  : 0.998 (0.997, 0.998)\n",
      "MCC       : 0.984 (0.979, 0.987)\n",
      "MAE       : 41.64 (41.38, 41.95)\n",
      "SAE       : -0.079 (-0.088, -0.068)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.5\n",
    "for i in range(20):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_seen_%d.pth' %i\n",
    "    filename = './UKDALE_seen_11_1110%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        #计算了一个家电设备的平均功率（pm），并将其除以最大功率（MAX_POWER）进行归一化处理。\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "        \n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[1], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[1], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[1], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[1], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[1], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[1], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[1], sorted(scores[i]['SAE'])[18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a87b6",
   "metadata": {},
   "source": [
    " 这段代码主要是为了评估一个模型在多个房屋的表现。首先，它创建了一个名为scores的字典，其中包含了每个房屋的评价指标，如F1得分、精度、召回率、准确率、MCC、MAE和SAE。然后，它对20个不同的模型进行了评估，每个模型分别对应一个.pth文件。对于每个模型和每个房屋，它计算了相应的预测结果，并将这些结果与真实结果进行比较，从而计算出每个房屋的评价指标。最后，它输出了每个房屋的平均评价指标，并显示了每个指标的最小值和最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化数据\n",
    "a_values = [0, 1, 2]\n",
    "f1_scores = [[] for _ in range(3)]  # 用于存储三种a值对应的F1分值\n",
    "\n",
    "# 从scores字典中提取F1分值\n",
    "for a in range(3):\n",
    "    f1_scores[a] = scores[a]['F1']\n",
    "\n",
    "# 家电设备名称\n",
    "appliance_labels = ['Fridge', 'Dish Washer', 'Washing Machine']\n",
    "\n",
    "# 绘制折线图\n",
    "plt.figure(figsize=(8, 6))\n",
    "for a in range(3):\n",
    "    plt.plot(range(20), f1_scores[a], label=appliance_labels[a])  # 每种a值对应的F1分值折线\n",
    "plt.xticks(range(0, 21, 2))  # 设置横坐标刻度为2一格\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score vs Iteration for Different Appliances\")\n",
    "plt.legend()\n",
    "# 设置分辨率为600ppi\n",
    "dpi = 600\n",
    "\n",
    "# 选择文件路径和文件名，保存为PNG格式\n",
    "save_path = r'D:\\NILM\\绘图\\F1折线图.png'\n",
    "\n",
    "# 导出图片\n",
    "#plt.savefig(save_path, dpi=dpi)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a1782c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_seen_9_11100_300.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "thr = 0.5\n",
    "for i in range(1):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_seen_%d.pth' %i\n",
    "    filename = './UKDALE_seen_11_1110%d.pth' %(i+15)\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):#1为fridge，2为dish_washer，3为washing_machine\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        #计算了一个家电设备的平均功率（pm），并将其除以最大功率（MAX_POWER）进行归一化处理。\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = 0.8*s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "        # 绘制折线图\n",
    "        #plt.plot(s_true[:1000], label='s_true')\n",
    "        #plt.plot(p_true[:5000], label='p_true')\n",
    "        #plt.plot(p_hat[:1000], label='p_hat')\n",
    "\n",
    "        if a == 0:\n",
    "            device_name = \"Fridge\"\n",
    "            plt.plot(s_true[:10000], label='s_true')\n",
    "            plt.plot(p_hat[:10000], label='p_hat')\n",
    "            # 添加标题、x轴标签和y轴标签\n",
    "            plt.title(device_name)\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            # 添加图例\n",
    "            plt.legend()\n",
    "            # 设置分辨率为600ppi\n",
    "            dpi = 600\n",
    "\n",
    "            # 选择文件路径和文件名，保存为PNG格式\n",
    "            save_path = r'D:\\NILM\\绘图\\冰箱图(10000).png'\n",
    "\n",
    "            # 导出图片\n",
    "            #plt.savefig(save_path, dpi=dpi)\n",
    "            #plt.savefig('D:/NILM/小论文/基于并行多尺度注意力的非侵入式负荷分解/论文图/冰箱图.pdf')\n",
    "        elif a == 1:\n",
    "            device_name = \"Dish Washer\"\n",
    "            plt.plot(s_true[:10000]*mean[a], label='s_true')\n",
    "            plt.plot(p_hat[:10000]*mean[a], label='p_hat')\n",
    "            # 添加标题、x轴标签和y轴标签\n",
    "            plt.title(device_name)\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            # 添加图例\n",
    "            plt.legend()\n",
    "            #plt.savefig('D:/NILM/小论文/基于并行多尺度注意力的非侵入式负荷分解/论文图/洗碗机图.pdf')\n",
    "            # 设置分辨率为600ppi\n",
    "            dpi = 600\n",
    "\n",
    "            # 选择文件路径和文件名，保存为PNG格式\n",
    "            save_path = r'D:\\NILM\\绘图\\洗碗机图(10000).png'\n",
    "\n",
    "            # 导出图片\n",
    "            #plt.savefig(save_path, dpi=dpi)\n",
    "        elif a == 2:\n",
    "            device_name = \"Washing Machine\"\n",
    "            s_true[0:100] = 0\n",
    "            plt.plot(s_true[:10000], label='s_true')\n",
    "            plt.plot(p_hat[:10000], label='p_hat')\n",
    "            plt.title(device_name)\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            # 添加图例\n",
    "            plt.legend()\n",
    "            #plt.savefig('D:/NILM/小论文/基于并行多尺度注意力的非侵入式负荷分解/论文图/洗衣机图.pdf')\n",
    "            # 设置分辨率为600ppi\n",
    "            dpi = 600\n",
    "\n",
    "            # 选择文件路径和文件名，保存为PNG格式\n",
    "            save_path = r'D:\\NILM\\绘图\\洗衣机图(10000).png'\n",
    "\n",
    "            # 导出图片\n",
    "            #plt.savefig(save_path, dpi=dpi)\n",
    "        else:\n",
    "            device_name = \"Appliance \" + str(a)\n",
    "\n",
    "\n",
    "        # 添加标题、x轴标签和y轴标签\n",
    "        plt.title(device_name)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Value\")\n",
    "\n",
    "        # 添加图例\n",
    "        plt.legend()\n",
    "\n",
    "        # 显示图形\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee1a1427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_unseen_8_11100.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.847 (0.847, 0.847)\n",
      "Precision : 0.818 (0.818, 0.818)\n",
      "Recall    : 0.877 (0.877, 0.877)\n",
      "Accuracy  : 0.880 (0.880, 0.880)\n",
      "MCC       : 0.750 (0.750, 0.750)\n",
      "MAE       : 19.90 (19.90, 19.90)\n",
      "SAE       : 0.072 (0.072, 0.072)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.776 (0.776, 0.776)\n",
      "Precision : 0.704 (0.704, 0.704)\n",
      "Recall    : 0.864 (0.864, 0.864)\n",
      "Accuracy  : 0.986 (0.986, 0.986)\n",
      "MCC       : 0.773 (0.773, 0.773)\n",
      "MAE       : 38.38 (38.38, 38.38)\n",
      "SAE       : 0.226 (0.226, 0.226)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.838 (0.838, 0.838)\n",
      "Precision : 0.846 (0.846, 0.846)\n",
      "Recall    : 0.830 (0.830, 0.830)\n",
      "Accuracy  : 0.996 (0.996, 0.996)\n",
      "MCC       : 0.836 (0.836, 0.836)\n",
      "MAE       : 8.33 (8.33, 8.33)\n",
      "SAE       : -0.020 (-0.020, -0.020)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "    scores[a]['s_true'] = []\n",
    "    scores[a]['s_hat'] = []\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "for i in range(1):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_unseen_%d.pth' %i\n",
    "    filename = './UKDALE_unseen_8_1110%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        pm = ds_appliance[1][APPLIANCE[a]].sum() / ds_status[1][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['s_true'].append(s_true)\n",
    "        scores[a]['s_hat'].append(s_hat)\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[0], sorted(scores[i]['F1'])[0]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[0], sorted(scores[i]['Precision'])[0]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[0], sorted(scores[i]['Recall'])[0]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[0], sorted(scores[i]['Accuracy'])[0]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[0], sorted(scores[i]['MCC'])[0]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[0], sorted(scores[i]['MAE'])[0]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[0], sorted(scores[i]['SAE'])[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilmtk-env",
   "language": "python",
   "name": "nilmtk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
